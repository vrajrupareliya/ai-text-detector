[
  {
    "text": "Science is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe. Modern science is typically divided into two – or three – major branches: the natural sciences, which study the physical world, and the social sciences, which study individuals and societies. While referred to as the formal sciences, the study of logic, mathematics, and theoretical computer science are typically regarded as separate because they rely on deductive reasoning instead of the scientific method as their main methodology. Meanwhile, applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine. The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c. 3000–1200 BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity and later medieval scholarship, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes; while further advancements, including the introduction of the Hindu–Arabic numeral system, were made during the Golden Age of India and Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe during the Renaissance revived natural philosophy, which was later transformed by the Scientific Revolution that began in the 16th century as new ideas and discoveries departed from previous Greek conceptions and traditions. The scientific method soon played a greater role in the acquisition of knowledge, and in the 19th century, many of the institutional and professional features of science began to take shape, along with the changing of \"natural philosophy\" to \"natural science\". New knowledge in science is advanced by research from scientists who are motivated by curiosity about the",
    "label": 0
  },
  {
    "text": "many of the institutional and professional features of science began to take shape, along with the changing of \"natural philosophy\" to \"natural science\". New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems. Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions, government agencies, and companies. The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection. Etymology The word science has been used in English since the 14th century (Middle English) in the sense of \"the state of knowing\". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning 'knowledge, awareness, understanding', a noun derivative of sciens meaning 'knowing', itself the present active participle of sciō 'to know'. There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning 'to know', which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io meaning 'to incise'. The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning 'to not know, be unfamiliar with', which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2- from *sḱʰeh2(i)- meaning 'to cut'. In the past, science was a synonym for \"knowledge\" or \"study\", in keeping with its Latin origin. A person who conducted scientific research was called a \"natural philosopher\" or \"man of science\". In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the",
    "label": 0
  },
  {
    "text": "A person who conducted scientific research was called a \"natural philosopher\" or \"man of science\". In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences, crediting it to \"some ingenious gentleman\" (possibly himself). History Early history Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years, taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science, as did religious rituals. Some scholars use the term \"protoscience\" to label activities in the past that resemble modern science in some but not all features; however, this label has also been criticised as denigrating, or too suggestive of presentism, thinking about those activities only in relation to modern categories. Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c. 3000–1200 BCE), creating the earliest written records in the history of science. Although the words and concepts of \"science\" and \"nature\" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine. From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system, solved practical problems using geometry, and developed a calendar. Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals. The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing. They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes. The",
    "label": 0
  },
  {
    "text": "as prayers, incantations, and rituals. The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing. They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes. The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur. They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity. Classical antiquity In classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time. Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural \"way\" in which a plant grows, and the \"way\" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish \"nature\" and \"convention\". The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural. The Pythagoreans developed a complex number philosophy and contributed significantly to the development of mathematical science. The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus. Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a \"canon\" (ruler, standard) which established physical criteria or standards of scientific truth. The Greek doctor Hippocrates established the tradition of systematic medical science and is known as \"The",
    "label": 0
  },
  {
    "text": "develop a full natural cosmology based on atomism, and would adopt a \"canon\" (ruler, standard) which established physical criteria or standards of scientific truth. The Greek doctor Hippocrates established the tradition of systematic medical science and is known as \"The Father of Medicine\". A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly held truths that shape beliefs and scrutinises them for consistency. Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism. In the 4th century BCE, Aristotle created a systematic programme of teleological philosophy. In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it. Aristarchus's model was widely rejected because it was believed to violate the laws of physics, while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead. The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus. Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History. Positional notation for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide. Middle Ages Due to the collapse of the Western Roman Empire, the 5th century",
    "label": 0
  },
  {
    "text": "3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide. Middle Ages Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe. Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge. In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning. John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus. His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later. During late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause. Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were made by Christians, mainly Nestorians and Miaphysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists. By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world. Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq and the flourished until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study. Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used",
    "label": 0
  },
  {
    "text": "13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study. Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century. By the 11th century most of Europe had become Christian, and in 1088, the University of Bologna emerged as the first university in Europe. As such, demand for Latin translation of ancient and scientific texts grew, a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature. In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi. Renaissance New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle. A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final. In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs",
    "label": 0
  },
  {
    "text": "that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model. Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light. Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres. Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model. The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature. Francis Bacon and René Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life. Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature. Age of Enlightenment At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica greatly influencing future physicists. Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no",
    "label": 0
  },
  {
    "text": "his Philosophiæ Naturalis Principia Mathematica greatly influencing future physicists. Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes. During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, \"the real and legitimate goal of sciences is the endowment of human life with new inventions and riches\", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond \"the fume of subtle, sublime or pleasing [speculation]\". Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population. Enlightenment philosophers turned to a few of their scientific predecessors – Galileo, Kepler, Boyle, and Newton principally – as the guides to every physical and social field of the day. The 18th century saw significant advancements in the practice of medicine and physics; the development of biological taxonomy by Carl Linnaeus; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline. Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar",
    "label": 0
  },
  {
    "text": "a discipline. Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement. In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics. 19th century During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as \"biologist\", \"physicist\", and \"scientist\"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals. During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879. During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859. Separately, Gregor Mendel presented his paper, \"Experiments on Plant Hybridisation\" in 1865, which outlined the principles of biological inheritance, serving as the basis for modern genetics. Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms. The laws of conservation of energy, conservation",
    "label": 0
  },
  {
    "text": "principles of biological inheritance, serving as the basis for modern genetics. Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms. The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy. This realisation led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time. The electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted, André-Marie Ampère, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896, Marie Curie then became the first person to win two Nobel Prizes. In the next year came the discovery of the first subatomic particle, the electron. 20th century In the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally. Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies. During this period scientific experimentation became increasingly larger in scale and funding. The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as",
    "label": 0
  },
  {
    "text": "onset of environmental studies. During this period scientific experimentation became increasingly larger in scale and funding. The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race. Substantial international collaborations were also made, despite armed conflicts. In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields. The discovery of the cosmic microwave background in 1964 led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lemaître. The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics. Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity. Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling. 21st century The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome. The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body. With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard",
    "label": 0
  },
  {
    "text": "were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body. With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found. In 2015, gravitational waves, predicted by general relativity a century before, were first observed. In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc. Branches Modern science is commonly divided into three major branches: natural science, social science, and formal science. Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise. Both natural and social sciences are empirical sciences, as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions. Natural Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings. Today, \"natural history\" suggests observational descriptions aimed at popular audiences. Social Social science is the study of human behaviour and the functioning of societies. It has many disciplines",
    "label": 0
  },
  {
    "text": "century by describing and classifying plants, animals, minerals, and other biotic beings. Today, \"natural history\" suggests observational descriptions aimed at popular audiences. Social Social science is the study of human behaviour and the functioning of societies. It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology. In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists, conflict theorists, and interactionists in sociology. Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes. Formal Formal science is an area of study that generates knowledge using formal systems. A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules. It includes mathematics, systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts. The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science. Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics. Natural and social sciences that rely heavily on mathematical applications include mathematical physics, chemistry, biology, finance, and economics. Applied Applied science is the use of the",
    "label": 0
  },
  {
    "text": "the empirical sciences. Calculus, for example, was initially invented to understand motion in physics. Natural and social sciences that rely heavily on mathematical applications include mathematical physics, chemistry, biology, finance, and economics. Applied Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine. Engineering is the use of scientific principles to invent, design and build machines, structures and technologies. Science may contribute to the development of new technologies. Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease. Basic The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world. Blue skies Computational Computational science applies computer simulations to science, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science, for example in agent-based computational economics, random forests, topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans. Interdisciplinary Interdisciplinary science involves the combination of two or more disciplines into one, such as bioinformatics, a combination of biology and computer science or cognitive sciences. The concept has existed since the ancient Greek period and it became popular again in the 20th century. Research Scientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most",
    "label": 0
  },
  {
    "text": "popular again in the 20th century. Research Scientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable. Scientific method Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way. Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws; these laws were discovered by means of systematic observation and experimentation. Mathematics is essential in the formation of hypotheses, theories, and laws, because it is used extensively in quantitative modelling, observing, and collecting measurements. Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results. In the scientific method an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience – fitting with other accepted facts related to an observation or scientific question. This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress. Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy, though in some sciences such as astronomy or geology, a predicted observation might be more appropriate. When a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory, a validly reasoned, self-consistent model",
    "label": 0
  },
  {
    "text": "or geology, a predicted observation might be more appropriate. When a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory, a validly reasoned, self-consistent model or framework for describing the behaviour of certain natural events. A theory typically describes the behaviour of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model, an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation. While performing experiments to test hypotheses, scientists may have a preference for one outcome over another. Eliminating the bias can be achieved through transparency, careful experimental design, and a thorough peer review process of the experimental results and conclusions. After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be. Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and confirmation bias. Intersubjective verifiability, the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge. Literature Scientific research is published in a range of literature. Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal",
    "label": 0
  },
  {
    "text": "Literature Scientific research is published in a range of literature. Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal des sçavans followed by Philosophical Transactions, began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500. Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population. Challenges The replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences. In subsequent investigations, the results of many scientific studies have been proven to be unrepeatable. The crisis has long-standing roots; the phrase was coined in the early 2010s as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste. An area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science. Physicist Richard Feynman coined the term \"cargo cult science\" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated. Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has",
    "label": 0
  },
  {
    "text": "believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated. Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as \"the most important tool\" for separating valid claims from invalid ones. There can also be an element of political bias or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as \"bad science\", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term scientific misconduct refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person. Philosophy There are different schools of thought in the philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation; scientific theories generalise observations. Empiricism generally encompasses inductivism, a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method. Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation. Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation. Popper proposed",
    "label": 0
  },
  {
    "text": "that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation. Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories, replacing induction with falsification as the empirical method. Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error, covering all products of the human mind, including science, mathematics, philosophy, and art. Another approach, instrumentalism, emphasises the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored. Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true. Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent \"portrait\" of the world that is consistent with observations made from its framing. He characterised normal science as the process of observation and \"puzzle solving\", which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift. Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more \"portraits\" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even",
    "label": 0
  },
  {
    "text": "is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism. Another approach often cited in debates of scientific scepticism against controversial movements like \"creation science\" is methodological naturalism. Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations. Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification. Community The scientific community is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having peer review, through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results. Scientists Scientists are individuals who conduct scientific research to advance knowledge in an area of interest. Scientists may exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of public health, nations, the environment, or industries; other motivations include recognition by peers and prestige. In modern times, many scientists study within specific areas of science in academic institutions, often obtaining advanced degrees in the process. Many scientists pursue careers in various fields such as academia, industry, government, and nonprofit organisations. Science has historically been a male-dominated field, with notable exceptions. Women have faced considerable discrimination in science, much as they have in other areas of male-dominated societies. For example, women were frequently passed over for job",
    "label": 0
  },
  {
    "text": "government, and nonprofit organisations. Science has historically been a male-dominated field, with notable exceptions. Women have faced considerable discrimination in science, much as they have in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work. The achievements of women in science have been attributed to the defiance of their traditional role as labourers within the domestic sphere. Learned societies Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance. Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines. Membership may either be open to all, require possession of scientific credentials, or conferred by election. Most scientific societies are nonprofit organisations, and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies, regulating the activities of their members in the public interest, or the collective interest of the membership. The professionalisation of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603, the British Royal Society in 1660, the French Academy of Sciences in 1666, the American National Academy of Sciences in 1863, the German Kaiser Wilhelm Society in 1911, and the Chinese Academy of Sciences in 1949. International scientific organisations, such as the International Science Council, are devoted to international cooperation for science advancement. Awards Science awards are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist",
    "label": 0
  },
  {
    "text": "international cooperation for science advancement. Awards Science awards are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry. Society Funding and policies Funding of science is often through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP. In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the humanities. In less developed nations, the government provides the bulk of the funds for their basic scientific research. Many governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States, the National Scientific and Technical Research Council in Argentina, Commonwealth Scientific and Industrial Research Organisation in Australia, National Centre for Scientific Research in France, the Max Planck Society in Germany, and National Research Council in Spain. In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity. Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national",
    "label": 0
  },
  {
    "text": "but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity. Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public. Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organisations that fund research. Education and awareness Science education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organisations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history. Scientific literacy is chiefly concerned with an understanding of the scientific method, units and methods of measurement, empiricism, a basic understanding of statistics (correlations, qualitative versus quantitative observations, aggregate statistics), and a basic understanding of core scientific fields such as physics, chemistry, biology, ecology, geology, and computation. As a student advances into higher stages of formal education, the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well. The mass media face pressures that can prevent them from accurately depicting competing scientific claims in",
    "label": 0
  },
  {
    "text": "in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well. The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter. Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover. Science magazines such as New Scientist, Science & Vie, and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research. The science fiction genre, primarily speculative fiction, can transmit the ideas and methods of science to the general public. Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund. Anti-science attitudes While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021) or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020). Psychologists have pointed to four factors driving rejection of scientific results: Scientific authorities are sometimes seen as inexpert, untrustworthy, or biased. Some marginalised social groups hold anti-science attitudes, in part because these groups have",
    "label": 0
  },
  {
    "text": "in late 2019 and early 2020). Psychologists have pointed to four factors driving rejection of scientific results: Scientific authorities are sometimes seen as inexpert, untrustworthy, or biased. Some marginalised social groups hold anti-science attitudes, in part because these groups have often been exploited in unethical experiments. Messages from scientists may contradict deeply held existing beliefs or morals. The delivery of a scientific message may not be appropriately targeted to a recipient's learning style. Anti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left. That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status. Politics Attitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicisation of science such as anti-intellectualism, perceived threats to religious beliefs, and fear for business interests. Politicisation of science is usually accomplished when scientific information is presented in a way that emphasises the uncertainty associated with the scientific evidence. Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence. Examples of issues that have involved the politicisation of science include the global warming controversy, health effects of pesticides, and health effects of tobacco. See also List of scientific occupations List of",
    "label": 0
  },
  {
    "text": "The following outline is provided as a topical overview of science. Science is the systematic study of the world and the resulting knowledge obtained from this study. The word \"science\" derives from the Latin word scientia meaning knowledge. A practitioner of science is called a \"scientist\". Modern science respects objective logical reasoning, and follows a set of core procedures or rules to determine the nature and underlying natural laws of all things, with a scope encompassing the entire universe. These procedures, or rules, are known as the scientific method. Study and experimentation Experimentation is the use of controlled conditions to test an idea. A single independent variable is altered while all other conditions are kept the same to test the alteration's effect on a dependent variable. Design of experiments History of experiments Descriptive and normative science are contrasting methods to explain scientific ideas. Descriptive science explains ideas objectively while normative science explains what should be true using value judgments. Empirical research is conducted using observation and experimentation instead of theory. Empirical evidence is evidence gathered through direct observation instead of indirect theory. Falsifiability is the ability to test a hypothesis through experimentation to determine whether it is false. Karl Popper argued that a claim must be falsifiable to be recognized as scientific. Hard and soft science are descriptions of how measurable and precise a branch of science is. Hard sciences like biology and physics are more measurable while soft sciences like anthropology and psychology are less measurable. Laboratories are places where scientists engage in research and study. Measurement is the use of precise units to describe a quantity. Models are representations of scientific phenomena to assist in studying or explaining them. Observations are the use of one's senses to obtain information, and the resulting discoveries. Observational studies are a type",
    "label": 0
  },
  {
    "text": "use of precise units to describe a quantity. Models are representations of scientific phenomena to assist in studying or explaining them. Observations are the use of one's senses to obtain information, and the resulting discoveries. Observational studies are a type of research conducted solely by observing without controlling variables or testing specific hypotheses. Reproducibility or replicability is the ability for subsequent experiments to confirm the accuracy of previous ones by producing the same result. This may be through an identical experiment or a test of the same hypothesis under different conditions. Prediction is the use of observation to determine future results through inference. The scientific method is a series of steps taken to engage in experimentation and produce factual results. The exact steps to be taken, or whether an all-encompassing sequence exists, is the subject of debate. History of scientific method Outline of scientific method Timeline of the history of the scientific method Scientific knowledge Anomalies are abnormal or deviating phenomena that are inconsistent with previous data or cannot be precisely classified or explained. Classification is the use of categories to organize and describe individual subjects. This can be done descriptively to explain existing differences or prescriptively to create groups in a way that is useful. Consilience is the process in which distinct findings can produce novel conclusions when considered together. Data are sets of facts or information. Deductive reasoning is reasoning conducted purely through logic. Discoveries are the finding or explanation of new information. Inductive reasoning is the use of varied observations to make an inference. Explanation is the understanding of why a phenomenon occurs. Hypotheses are proposals of scientific fact that have yet to be definitively verified. Objectivity is the answering of scientific questions impartially without affecting the results with biases. Confirmation bias is a cognitive bias that",
    "label": 0
  },
  {
    "text": "understanding of why a phenomenon occurs. Hypotheses are proposals of scientific fact that have yet to be definitively verified. Objectivity is the answering of scientific questions impartially without affecting the results with biases. Confirmation bias is a cognitive bias that leads people to seek evidence that supports existing beliefs and interpret new evidence as supporting these beliefs. Reliability is the consistency in data as it is collected to demonstrate reproducibility. Scientific laws are descriptions of scientific fact that apply universally under all circumstances. Scientific theories are descriptions of scientific fact that are known to be true but cannot be proven to apply universally. Validity is the accurate correspondence and relevance of data to the real-world phenomena it is meant to measure. Valid data is derived from objective observation or experimentation. Verisimilitude is the degree to which a claim approaches the truth. The verisimilitude between two false ideas can be compared to determine which is less flawed. Branches of science Science is divided into disciplines that explore different subject matter. Each discipline has its own considerations when being studied, and different methods are used between them. Scientists typically specialize in one discipline. Interdisciplinary sciences pull from multiple fields of study. History Timeline Science in the ancient world Science in the middle ages Science in the Renaissance – The Renaissance allowed for expanded intellectual thought that influenced later scientific developments. The Scientific Revolution – A period of activity occurred c. 1550 – c. 1700 which developed the modern conception of what is now considered science. The scientific movement remained tied with Christianity, and most theories of the world blended empiricism and religion. It culminated in the studies of Isaac Newton and his 1687 treatise Principia. It also included the Copernican Revolution that was initiated by Nicolaus Copernicus and his argument for heliocentrism.",
    "label": 0
  },
  {
    "text": "and most theories of the world blended empiricism and religion. It culminated in the studies of Isaac Newton and his 1687 treatise Principia. It also included the Copernican Revolution that was initiated by Nicolaus Copernicus and his argument for heliocentrism. Science in the Age of Enlightenment 19th century in science – Science first developed in the 19th century as its own subject that encompassed varying fields of inquiry. Biology and chemistry continued a period of growth that had begun in the late-18th century. 20th century in science – Physics became the dominant branch of science in the 20th century through the development of atomic technology. Logical empiricism was a major influence in the mid-20th century, but it lost favor by the 1970s. The science wars were a period of disagreement in the late-20th century about whether mainstream science should be held as an authoritative feature of society. 21st century in science Historical disciplines Alchemy is the historical study of what is now associated with chemistry. It was accepted as a science until the end of the 17th century. Astrology is a method used in ancient and medieval times to study the social sciences through physical phenomena. Cosmogony is the study of Earth's origins through divine creation. Natural history is the historical name for study of subjects that are now associated with biology. Natural philosophy is the historical name for study of subjects that are now associated with physics and astronomy. Philosophy of science Philosophy of science encompasses the questions, assumptions, foundations, methods and implications of science. Anti-realism is the opposition to scientific realism. Anti-realists believe that scientific theories cannot be objectively true or that they do not correlate to objectively real phenomena. Antiscience is a criticism and rejection of modern science and the scientific community. Denialism is the rejection of",
    "label": 0
  },
  {
    "text": "to scientific realism. Anti-realists believe that scientific theories cannot be objectively true or that they do not correlate to objectively real phenomena. Antiscience is a criticism and rejection of modern science and the scientific community. Denialism is the rejection of scientific facts that conflict with one's previous beliefs. Empiricism is the belief that truth is obtained from sense experience. Empiricists believe that science is a systematic and detailed application of common everyday thought and inquiry. Constructive empiricism is the belief that scientific theories can be true but successful testing does not affirm their truth. Logical positivism is an empiricist school of thought that was developed in Europe by the Vienna Circle in the 20th century. Operationalism is an empiricist school of thought developed by Percy Williams Bridgman in 1927. It holds that all terms used in science must correspond to an observational test. Verificationism is the empiricist belief that testability and verifiability must be possible for a claim to have meaning. Evidentialism is the belief that a claim should only be accepted if there is evidence supporting it. Fallibilism is the belief that no claim can ever be known with absolute certainty. The term was defined by Charles Sanders Peirce. Holism is the belief that individual scientific claims cannot be understood without also considering related claims, as it is only a network of claims that allows scientific prediction. This argument, the Duhem–Quine thesis, was developed by Willard Van Orman Quine as a response to logical positivism by adapting the philosophy of Pierre Duhem. Instrumentalism is the belief that science should be used as a guide predict phenomena without presenting it as a means of finding truth. Normal science is a system defined by Thomas Kuhn which described science in a given field as beginning with a paradigm shift that emerges",
    "label": 0
  },
  {
    "text": "be used as a guide predict phenomena without presenting it as a means of finding truth. Normal science is a system defined by Thomas Kuhn which described science in a given field as beginning with a paradigm shift that emerges from a new theory. Pragmatism is the belief that claims should be accepted based on value rather than evidence. Realism is the belief that true scientific theories can describe existing phenomena instead of merely hypothetical phenomena. Reductionism is the understanding of phenomena through fundamental causes and explanations. Relativism is the belief that knowledge cannot be understood objectively, but in relation to other forms of knowledge. Reliabilism is the belief that a fact is considered knowledge when it is derived from reliable methods. Science studies is the blending of perspectives and theories on scientific study to create a holistic understanding of science. Scientism is the belief that science should go beyond mere explanation and become the guiding force in society. Skepticism is the belief that unproven or widely-accepted beliefs should be questioned. Scientific community The scientific community encompasses scientists, their interactions, and their influences on one another. Consensus is general agreement among scientists on a conclusion or finding. Demarcation is the division of scientific and non-scientific ideas, and the resulting dispute over how to divide them. Different fields of study may be evaluated on the level of experimental rigor, how much they engage in abstraction, how closely related they are with the humanities, or other qualities. Funding of science can come from governments and donors. Junk science is the presentation of uncertain scientific claims as facts, typically to a legal or political end. List of topics characterized as pseudoscience Meta-analysis is the comparing of several studies on the same topic to draw conclusions. A paradigm is the overall understanding and accepted",
    "label": 0
  },
  {
    "text": "uncertain scientific claims as facts, typically to a legal or political end. List of topics characterized as pseudoscience Meta-analysis is the comparing of several studies on the same topic to draw conclusions. A paradigm is the overall understanding and accepted system of how science functions. Paradigm shifts are historical periods of total change in how science is practiced. The concept was introduced by Thomas Kuhn. Peer review is a process in which an academic provides feedback on scientific writing, often anonymously, before publication. Pseudoscience is unscientific practice or belief that is presented as scientific or uses scientific language to suggest credibility. History of pseudoscience Regulation of science involves the use of policy to limit scientific activity that regulators determine to be dangerous, unethical, or ineffective. Scientific controversy occurs when multiple schools of thought within a discipline contradict each other. This can include disputes about methods or theory. Scientific dissent occurs when a scientist disagrees with the scientific community over accepted practices or findings. Scientific misconduct is the publication of false, misleading, or plagiarized findings. List of scientific misconduct incidents Data fabrication is the use of fake data to present a conclusion. HARKing (hypothesizing after results are known) is the practice of writing hypotheses to falsely claim that one had correctly predicted results before testing them. P-hacking is the selective use or presentation of data to guarantee certain findings. Scientific papers describe data and findings and compare them to previous hypotheses. Lists of publications in science Abstracts are summaries of a paper's goals and findings that precede the full paper. Citation analysis is the tracking of when scientific papers are cited by other papers. Scientific journals are the primary venue for publishing scientific papers. Scientific priority is the recognition of a scientist's claim over a discovery. Scientific societies are organizations that",
    "label": 0
  },
  {
    "text": "analysis is the tracking of when scientific papers are cited by other papers. Scientific journals are the primary venue for publishing scientific papers. Scientific priority is the recognition of a scientist's claim over a discovery. Scientific societies are organizations that emerged in Europe during the mid-17th century as an alternative to universities. Scientific writing is the recording and description of scientific knowledge or research, written in a way that it can be precisely explained to other members of the scientific community. Scientists are the practitioners of scientific study. The term scientist was coined by William Whewell in 1840. Sociology of science considers interactions, incentives, and norms within the scientific community. It was developed as an independent field in the mid-20th century by Robert K. Merton. Women in science and their role has changed over time. Women were historically excluded from scientific activity in most cases, but an increased role has developed with the rise of feminist movements. Timeline of women in science Science in society Funding of science can come from both public and private sources, including governments and corporations. Politicization of science encompasses challenges to scientific activities, or regulations on their practice, for political purposes. This can be instigated by governments, advocacy groups, or the public. List of books about the politics of science Religion and science are distinguished by science's dependence on known facts and its constraint to explain what can be demonstrated in nature, while religion depends on faith and can be interpreted more broadly. Baháʼí and science Buddhism and science Christianity and science Hinduism and science Islam and science Science communication is the description of science to the general public. It involves the translation of precise technical terms to ones that are more generally understandable to those without background knowledge in a scientific field. Popular science",
    "label": 0
  },
  {
    "text": "Islam and science Science communication is the description of science to the general public. It involves the translation of precise technical terms to ones that are more generally understandable to those without background knowledge in a scientific field. Popular science is a genre of writing on scientific subjects intended for consumption by the general public. It developed in the late-19th and early-20th centuries. Science fiction is a genre of speculative fiction in which scientific knowledge, ideas, and technology are central in its stories. Science journalism is the coverage of news about science and scientific developments. Science policy is the public policy governing how science can be conducted. It may be used to organize scientific activity to be more efficient, or to apply science for purposes like economic growth, social benefit, and military strength. History of science policy Scientific literacy is the ability to understand science, particularly in the context of the general public. See also Lists of scientists Outline of academia Outline of academic disciplines Outline of history Scientific terminology Notes References Agassi, Joseph (1981). Science and Society: Studies in the Sociology of Science. D. Reidel. ISBN 978-90-277-1244-8. Armstrong, Jon Scott; Green, Kesten C. (2022). The Scientific Method: A Guide to Finding Useful Knowledge. Cambridge University Press. ISBN 978-1-009-09642-3. Bird, Alexander (2005). Philosophy of Science. Routledge. ISBN 978-1-85728-504-8. Browne, Janet; Porter, Roy; Bynum, William F., eds. (1981). Dictionary of the History of Science. Macmillan. ISBN 978-0-333-29316-4. Collocott, Thomas C., ed. (1971). Dictionary of Science and Technology. W. & R. Chambers. ISBN 978-0-550-13202-4. Daintith, John; Martin, Elizabeth (2010) . Oxford Dictionary of Science (6th ed.). Oxford University Press. ISBN 978-0-19-956146-9. Erickson, Mark (2005). Science, Culture and Society: Understanding Science in the 21st Century. Polity. ISBN 978-0-7456-2974-2. Godfrey-Smith, Peter (2003). Theory and Reality: An Introduction to the Philosophy of Science. University of",
    "label": 0
  },
  {
    "text": "of Science (6th ed.). Oxford University Press. ISBN 978-0-19-956146-9. Erickson, Mark (2005). Science, Culture and Society: Understanding Science in the 21st Century. Polity. ISBN 978-0-7456-2974-2. Godfrey-Smith, Peter (2003). Theory and Reality: An Introduction to the Philosophy of Science. University of Chicago Press. ISBN 978-0-226-30062-7. Hagstrom, Warren O. (1965). The Scientific Community. Basic Books. LCCN 65-10539. Heilbron, J. L., ed. (2003). The Oxford Companion to the History of Modern Science. Oxford University Press. ISBN 978-0-19-511229-0. Morris, Christopher G., ed. (1992). Academic Press Dictionary of Science and Technology. Elsevier Science. ISBN 978-0-12-200400-1. Pigliucci, Massimo; Boudry, Maarten, eds. (2013). Philosophy of Pseudoscience: Reconsidering the Demarcation Problem. The University of Chicago Press. ISBN 978-0-226-05179-6. Nickles, Thomas. \"The Problem of Demarcation\". In Pigliucci & Boudry (2013), pp. 101–120. Prothero, Donald. \"The Holocaust Denier's Playbook and the Tobacco Smokescreen\". In Pigliucci & Boudry (2013), pp. 341–360. Shackel, Nicholas. \"Pseudoscience and Idiosyncratic Theories of Rational Belief\". In Pigliucci & Boudry (2013), pp. 417–438. Wilkins, John S. \"The Salem Region\". In Pigliucci & Boudry (2013), pp. 397–416. Stocklmayer, Susan M.; Gore, Michael M.; Bryant, Chris, eds. (2001). Science Communication in Theory and Practice. Springer. ISBN 978-1-4020-0130-7. Aikenhead, Glenn. \"Science Communication with the Public: A Cross Cultural Event\". In Stocklmayer, Gore & Bryant (2001), pp. 23–46. Gilbert, J.K. \"Towards a Unified Model of Education and Entertainment in Science Centres\". In Stocklmayer, Gore & Bryant (2001), pp. 123–142. Spinks, Peter. \"Science Journalism: The Inside Story\". In Stocklmayer, Gore & Bryant (2001), pp. 151–168. Webster, Andrew (1991). Science, Technology, and Society: New Directions. Rutgers University Press. ISBN 978-0-8135-1723-0. External links Media related to Science at Wikimedia Commons Quotations related to Science at Wikiquote Works related to Science at Wikisource",
    "label": 0
  },
  {
    "text": "History is the systematic study of the past, focusing primarily on the human past. As an academic discipline, it analyses and interprets evidence to construct narratives about what happened and explain why it happened. Some theorists categorize history as a social science, while others see it as part of the humanities or consider it a hybrid discipline. Similar debates surround the purpose of history—for example, whether its main aim is theoretical, to uncover the truth, or practical, to learn lessons from the past. In a more general sense, the term history refers not to an academic field but to the past itself, times in the past, or to individual texts about the past. Historical research relies on primary and secondary sources to reconstruct past events and validate interpretations. Source criticism is used to evaluate these sources, assessing their authenticity, content, and reliability. Historians strive to integrate the perspectives of several sources to develop a coherent narrative. Different schools of thought, such as positivism, the Annales school, Marxism, and postmodernism, have distinct methodological approaches. History is a broad discipline encompassing many branches. Some focus on specific time periods, such as ancient history, while others concentrate on particular geographic regions, such as the history of Africa. Thematic categorizations include political history, military history, social history, and economic history. Branches associated with specific research methods and sources include quantitative history, comparative history, and oral history. History emerged as a field of inquiry in antiquity to replace myth-infused narratives, with influential early traditions originating in Greece, China, and later in the Islamic world. Historical writing evolved throughout the ages and became increasingly professional, particularly during the 19th century, when a rigorous methodology and various academic institutions were established. History is related to many fields, including historiography, philosophy, education, and politics. Definition As an academic",
    "label": 0
  },
  {
    "text": "writing evolved throughout the ages and became increasingly professional, particularly during the 19th century, when a rigorous methodology and various academic institutions were established. History is related to many fields, including historiography, philosophy, education, and politics. Definition As an academic discipline, history is the study of the past with the main focus on the human past. It conceptualizes and describes what happened by collecting and analysing evidence to construct narratives. These narratives cover not only how events developed over time but also why they happened and in which contexts, providing an explanation of relevant background conditions and causal mechanisms. History further examines the meaning of historical events and the underlying human motives driving them. In a slightly different sense, history refers to the past events themselves. Under this interpretation, history is what happened rather than the academic field studying what happened. When used as a countable noun, a history is a representation of the past in the form of a history text. History texts are cultural products involving active interpretation and reconstruction. The narratives presented in them can change as historians discover new evidence or reinterpret already-known sources. The past itself, by contrast, is static and unchangeable. Some historians focus on the interpretative and explanatory aspects to distinguish histories from chronicles, arguing that chronicles only catalogue events in chronological order, whereas histories aim at a comprehensive understanding of their causes, contexts, and consequences. History has been primarily concerned with written documents. It focused on recorded history since the invention of writing, leaving prehistory to other fields, such as archaeology. Its scope broadened in the 20th century as historians became interested in the human past before the invention of writing. Historians debate whether history is a social science or forms part of the humanities. Like social scientists, historians formulate hypotheses, gather",
    "label": 0
  },
  {
    "text": "scope broadened in the 20th century as historians became interested in the human past before the invention of writing. Historians debate whether history is a social science or forms part of the humanities. Like social scientists, historians formulate hypotheses, gather objective evidence, and present arguments based on this evidence. At the same time, history aligns closely with the humanities because of its reliance on subjective aspects associated with interpretation, storytelling, human experience, and cultural heritage. Some historians strongly support one or the other classification while others characterize history as a hybrid discipline that does not belong to one category at the exclusion of the other. History contrasts with pseudohistory, a label used to describe practices that deviate from historiographical standards by relying on disputed historical evidence, selectively ignoring genuine evidence, or using other means to distort the historical record. Often motivated by specific ideological agendas, pseudohistorical practices mimic historical methodology to promote biased, misleading narratives that lack rigorous analysis and scholarly consensus. Purpose Various suggestions about the purpose or value of history have been made. Some historians propose that its primary function is the pure discovery of truth about the past. This view emphasizes that the disinterested pursuit of truth is an end in itself, while external purposes, associated with ideology or politics, threaten to undermine the accuracy of historical research by distorting the past. In this role, history also challenges traditional myths lacking factual support. A different perspective suggests that the main value of history lies in the lessons it teaches for the present. This view is based on the idea that an understanding of the past can guide decision-making, for example, to avoid repeating previous mistakes. A related perspective focuses on a general understanding of the human condition, making people aware of the diversity of human behaviour across",
    "label": 0
  },
  {
    "text": "the idea that an understanding of the past can guide decision-making, for example, to avoid repeating previous mistakes. A related perspective focuses on a general understanding of the human condition, making people aware of the diversity of human behaviour across different contexts—similar to what one can learn by visiting foreign countries. History can also foster social cohesion by providing people with a collective identity through a shared past, helping to preserve and cultivate cultural heritage and values across generations. For some scholars, including Whig historians and the Marxist scholar E. H. Carr, history is a key to understanding the present and, in Carr's case, shaping the future. History has sometimes been used for political or ideological purposes, for instance, to justify the status quo by emphasising the respectability of certain traditions or to promote change by highlighting past injustices. In extreme forms, evidence is intentionally ignored or misinterpreted to construct misleading narratives, which can result in pseudohistory or historical denialism. Influential examples are Holocaust denial, Armenian genocide denial, Nanjing Massacre denial, and Holodomor denial. Etymology The word history comes from the Ancient Greek term ἵστωρ (histōr), meaning 'learned, wise man'. It gave rise to the Ancient Greek word ἱστορία (historiā), which had a wide meaning associated with inquiry in general and giving testimony. The term was later adopted into Classical Latin as historia. In Hellenistic and Roman times, the meaning of the term shifted, placing more emphasis on narrative aspects and the art of presentation rather than focusing on investigation and testimony. The word entered Middle English in the 14th century via the Old French term histoire. At this time, it meant 'story, tale', encompassing both factual and fictional narratives. In the 15th century, its meaning shifted to cover the branch of knowledge studying the past in addition to narratives",
    "label": 0
  },
  {
    "text": "century via the Old French term histoire. At this time, it meant 'story, tale', encompassing both factual and fictional narratives. In the 15th century, its meaning shifted to cover the branch of knowledge studying the past in addition to narratives about the past. In the 18th and 19th centuries, the word history became more closely associated with factual accounts and evidence-based inquiry, coinciding with the professionalization of historical inquiry, a meaning still dominant in contemporary usage. The dual meaning, referring to both mere stories and factual accounts of the past, is present in the terms for history in many other European languages. They include the French histoire, the Italian storia, and the German Geschichte. Methods The historical method is a set of techniques historians use to research and interpret the past, covering the processes of collecting, evaluating, and synthesizing evidence. It seeks to ensure scholarly rigour, accuracy, and reliability in how historical evidence is chosen, analysed, and interpreted. Historical research often starts with a research question to define the scope of the inquiry. Some research questions focus on a simple description of what happened. Others aim to explain why a particular event occurred, refute an existing theory, or confirm a new hypothesis. Sources and source criticism To answer research questions, historians rely on various types of evidence to reconstruct the past and support their conclusions. Historical evidence is usually divided into primary and secondary sources. A primary source is a source that originated during the period that is studied. Primary sources can take various forms, such as official documents, letters, diaries, eyewitness accounts, photographs, and audio or video recordings. They also include historical remains examined in archaeology, geology, and the medical sciences, such as artefacts and fossils unearthed from excavations. Primary sources offer the most direct evidence of historical events.",
    "label": 0
  },
  {
    "text": "diaries, eyewitness accounts, photographs, and audio or video recordings. They also include historical remains examined in archaeology, geology, and the medical sciences, such as artefacts and fossils unearthed from excavations. Primary sources offer the most direct evidence of historical events. A secondary source is a source that analyses or interprets information found in other sources. Whether a document is a primary or a secondary source depends not only on the document itself but also on the purpose for which it is used. For example, if a historian writes a text about slavery based on an analysis of historical documents, then the text is a secondary source on slavery and a primary source on the historian's opinion. Consistency with available sources is one of the main standards of historical works. For instance, the discovery of new sources may lead historians to revise or dismiss previously accepted narratives. To find and access primary and secondary sources, historians consult archives, libraries, and museums. Archives play a central role by preserving countless original sources and making them available to researchers in a systematic and accessible manner. Thanks to technological advances, historians increasingly rely on online resources, which offer vast digital databases with methods to search and access specific documents. Source criticism is the process of analysing and evaluating the information a source provides. Typically, this process begins with external criticism, which evaluates the authenticity of a source. It addresses the questions of when and where the source was created and seeks to identify the author, understand their reasons for producing the source, and determine if it has undergone some type of modification since its creation. Additionally, the process involves distinguishing between original works, copies, and deceptive forgeries. Internal criticism evaluates the content of a source, typically beginning with the clarification of the meaning within",
    "label": 0
  },
  {
    "text": "if it has undergone some type of modification since its creation. Additionally, the process involves distinguishing between original works, copies, and deceptive forgeries. Internal criticism evaluates the content of a source, typically beginning with the clarification of the meaning within the source. This involves disambiguating individual terms that could be misunderstood but may also require a general translation if the source is written in an unfamiliar language. Once the information content of a source is understood, internal criticism is specifically interested in determining accuracy. Critics ask whether the information is reliable or misrepresents the topic and further question whether the source is comprehensive or omits important details. One way to make these assessments is to evaluate whether the author was able, in principle, to provide a faithful presentation of the studied event. Other approaches include the assessment of the influences of the author's intentions and prejudices, and cross-referencing information with other credible sources. Being aware of the inadequacies of a source helps historians decide whether and which aspects of it to trust, and how to use it to construct a narrative. Synthesis and schools of thought The selection, analysis, and criticism of sources result in the validation of a large collection of mostly isolated statements about the past. As a next step, sometimes termed historical synthesis, historians examine how the individual pieces of evidence fit together to form part of a larger story. Constructing this broader perspective is crucial for a comprehensive understanding of the topic as a whole. It is a creative aspect of historical writing that reconstructs, interprets, and explains what happened by showing how different events are connected. In this way, historians address not only which events occurred but also why they occurred and what consequences they had. While there are no universally accepted techniques for this",
    "label": 0
  },
  {
    "text": "and explains what happened by showing how different events are connected. In this way, historians address not only which events occurred but also why they occurred and what consequences they had. While there are no universally accepted techniques for this synthesis, historians rely on various interpretative tools and approaches in this process. One tool to provide an accessible overview of complex developments is the use of periodization, which divides a timeframe into different periods, each organized around central themes or developments that shaped the period. For example, the three-age system is traditionally used to divide early human history into Stone Age, Bronze Age, and Iron Age based on the predominant materials and technologies during these periods. Another methodological tool is the examination of silences, gaps or omissions in the historical record of events that occurred but did not leave significant evidential traces. Silences can happen when contemporaries find information too obvious to document but may also occur if there are specific reasons to withhold or destroy information. Conversely, when large datasets are available, quantitative approaches can be used. For instance, economic and social historians commonly employ statistical analysis to identify patterns and trends associated with large groups. Different schools of thought often come with their own methodological implications for how to write history. Positivists emphasize the scientific nature of historical inquiry, focusing on empirical evidence to discover objective truths. In contrast, postmodernists reject grand narratives that claim to offer a single, objective truth. Instead, they highlight the subjective nature of historical interpretation, which leads to a multiplicity of divergent perspectives. Marxists interpret historical developments as expressions of economic forces and class struggles. The Annales school highlights long-term social and economic trends while relying on quantitative and interdisciplinary methods. Feminist historians study the role of gender in history, with a particular",
    "label": 0
  },
  {
    "text": "Marxists interpret historical developments as expressions of economic forces and class struggles. The Annales school highlights long-term social and economic trends while relying on quantitative and interdisciplinary methods. Feminist historians study the role of gender in history, with a particular interest in analysing the experiences of women to challenge patriarchal perspectives. Areas of study History is a wide field of inquiry encompassing many branches. Some branches focus on a specific time period, while others concentrate on a particular geographic region or a distinct theme. Specializations of different types can usually be combined; for example, a work on economic history in ancient Egypt merges temporal, regional, and thematic perspectives. For topics with a broad scope, the amount of primary sources is often too extensive for an individual historian to review, forcing them to either narrow the scope of their topic or also rely on secondary sources to arrive at a wide overview. By period Chronological division is a common approach to organizing the vast expanse of history into more manageable segments. Different periods are often defined based on dominant themes that characterize a specific time frame and significant events that initiated these developments or brought them to an end. Depending on the selected context and level of detail, a period may be as short as a decade or longer than several centuries. A traditionally influential approach divides human history into prehistory, ancient history, post-classical history, early modern history, and modern history. Depending on the region and theme, the time frames covered by these periods can vary and historians may use entirely different periodizations. For example, traditional periodizations of Chinese history follow the main dynasties, and the division into pre-Columbian, colonial, and post-colonial periods plays a central role in the history of the Americas. The study of prehistory includes the examination of",
    "label": 0
  },
  {
    "text": "different periodizations. For example, traditional periodizations of Chinese history follow the main dynasties, and the division into pre-Columbian, colonial, and post-colonial periods plays a central role in the history of the Americas. The study of prehistory includes the examination of the evolution of human-like species several million years ago, leading to the emergence of anatomically modern humans about 200,000 years ago. Subsequently, humans migrated out of Africa to populate most of the earth. Towards the end of prehistory, technological advances in the form of new and improved tools led many groups to give up their established nomadic lifestyle, based on hunting and gathering, in favour of a sedentary lifestyle supported by early forms of agriculture. The absence of written documents from this period presents researchers with unique challenges. It results in an interdisciplinary approach relying on other forms of evidence from fields such as archaeology, anthropology, palaeontology, and geology. Historians studying the ancient period examine the emergence of the first major civilizations in regions such as Mesopotamia, Egypt, the Indus Valley, China, and Peru, beginning approximately 3500 BCE in some regions. The new social, economic, and political complexities necessitated the development of writing systems. Thanks to advancements in agriculture, surplus food allowed these civilizations to support larger populations, leading to urbanization, the establishment of trade networks, and the emergence of regional empires. In the later part of the ancient period, sometimes termed the classical period, societies in China, India, Persia, and the Mediterranean expanded further, reaching new cultural, scientific, and political heights. Meanwhile, influential religious systems and philosophical ideas were first formulated, such as Hinduism, Buddhism, Confucianism, Judaism, and Greek philosophy. In the study of post-classical or medieval history, which began around 500 CE, historians note the growing influence of major religions. Missionary religions, like Buddhism, Christianity, and Islam, spread",
    "label": 0
  },
  {
    "text": "first formulated, such as Hinduism, Buddhism, Confucianism, Judaism, and Greek philosophy. In the study of post-classical or medieval history, which began around 500 CE, historians note the growing influence of major religions. Missionary religions, like Buddhism, Christianity, and Islam, spread rapidly and established themselves as world religions, marking a cultural shift as they gradually replaced other belief systems. Meanwhile, inter-regional trade networks flourished, leading to increased technological and cultural exchange. Conquering many territories in Asia and Europe, the Mongol Empire became a dominant force during the 13th and 14th centuries. Historians focused on early modern history, which started roughly in 1500 CE, commonly highlight how European states rose to global power. As gunpowder empires, they explored and colonized large parts of the world. As a result, the Americas were integrated into the global network, triggering a vast biological exchange of plants, animals, people, and diseases. The Scientific Revolution prompted major discoveries and accelerated technological progress. It was accompanied by other intellectual developments, such as humanism and the Enlightenment, which ushered in secularization. In the study of modern history, which began at the end of the 18th century, historians are interested in how the Industrial Revolution transformed economies by introducing more efficient modes of production. Western powers established vast colonial empires, gaining superiority through industrialized military technology. The increased international exchange of goods, ideas, and people marked the beginning of globalization. Various social revolutions challenged autocratic and colonial regimes, paving the way for democracies. Many developments in fields like science, technology, economy, living standards, and human population accelerated at unprecedented rates. This happened despite the widespread destruction caused by two world wars, which rebalanced international power relations by undermining European dominance. By geographic location Areas of historical study can also be categorized by the geographic locations they examine. Geography plays a",
    "label": 0
  },
  {
    "text": "This happened despite the widespread destruction caused by two world wars, which rebalanced international power relations by undermining European dominance. By geographic location Areas of historical study can also be categorized by the geographic locations they examine. Geography plays a central role in history through its influence on food production, natural resources, economic activities, political boundaries, and cultural interactions. Some historical works limit their scope to small regions, such as a village or a settlement. Others focus on broad territories that encompass entire continents, like the histories of Africa, Asia, Europe, the Americas, and Oceania. The history of Africa begins with the examination of the evolution of anatomically modern humans. Ancient historians describe how the invention of writing and the establishment of civilization happened in ancient Egypt in the 4th millennium BCE. Over the next millennia, other notable civilizations and kingdoms formed in Nubia, Axum, Carthage, Ghana, Mali, and Songhay. Islam began spreading across North Africa in the 7th century CE and became the dominant faith in many empires. Meanwhile, trade along the trans-Saharan route intensified. Beginning in the 15th century, millions of Africans were enslaved and forcibly transported to the Americas as part of the Atlantic slave trade. Most of the continent was colonized by European powers in the late 19th and early 20th centuries. Amid rising nationalism, African states gradually gained independence in the aftermath of World War II, a period that saw economic progress, rapid population growth, and struggles for political stability. Historians studying the history of Asia note the arrival of anatomically modern humans around 100,000 years ago. They explore Asia's role as one of the cradles of civilization, with the emergence of some of the first ancient civilizations in Mesopotamia, the Indus Valley, and China beginning in the 4th and 3rd millennia BCE. In the",
    "label": 0
  },
  {
    "text": "years ago. They explore Asia's role as one of the cradles of civilization, with the emergence of some of the first ancient civilizations in Mesopotamia, the Indus Valley, and China beginning in the 4th and 3rd millennia BCE. In the following millennia, civilisations on the Asian continent gave birth to all major world religions and several influential philosophical traditions, such as Hinduism, Buddhism, Confucianism, Taoism, Christianity, and Islam. Other developments were the establishment of the Silk Road, which facilitated trade and cultural exchange across Eurasia, and the formation of powerful empires, such as the Mongol Empire. European influence grew over the following centuries, ushering in the modern era. It culminated in the 19th and early 20th centuries when many parts of Asia came under direct colonial control until the end of World War II. The post-independence period was characterized by modernization, economic growth, and a steep increase in population. In the study of the history of Europe, historians describe the arrival of the first anatomically modern humans about 45,000 years ago. They explore how in the first millennium BCE the Ancient Greeks contributed key elements to the culture, philosophy, and politics associated with the Western world, and how their cultural heritage influenced the Roman and Byzantine Empires. The medieval period began with the fall of the Western Roman Empire in the 5th century CE and was marked by the spread of Christianity. Starting in the 15th century, European exploration and colonization interconnected the globe, while cultural, intellectual, and scientific developments transformed Western societies. From the late 18th to the early 20th centuries, European global dominance was further solidified by the Industrial Revolution and the establishment of large overseas colonies. It came to an end because of the devastating effects of two world wars. In the following Cold War era, the",
    "label": 0
  },
  {
    "text": "20th centuries, European global dominance was further solidified by the Industrial Revolution and the establishment of large overseas colonies. It came to an end because of the devastating effects of two world wars. In the following Cold War era, the continent was divided into a Western and an Eastern bloc. They pursued political and economic integration in the aftermath of the Cold War. Historians examining the history of the Americas document the arrival of the first humans around 20,000 to 15,000 years ago. The Americas were home to some of the earliest civilizations, like the Norte Chico civilization in South America and the Maya and Olmec civilizations in Central America. Over the next millennia, major empires arose beside them, such as the Teotihuacan, Aztec, and Inca empires. Following the arrival of the Europeans from the late 15th century onwards, the spread of newly introduced diseases drastically reduced the local population. Together with colonization, it led to the collapse of major empires as demographic and cultural landscapes were reshaped. Independence movements in the 18th and 19th centuries led to the formation of new nations across the Americas. In the 20th century, the United States emerged as a dominant global power and a key player in the Cold War. In the study of the history of Oceania, historians note the arrival of humans about 60,000 to 50,000 years ago. They explore the establishment of diverse regional societies and cultures, first in Australia and Papua New Guinea and later also on other Pacific Islands. The arrival of the Europeans in the 16th century prompted significant transformations, and by the end of the 19th century, most of the region had come under Western control. Oceania became involved in various conflicts during the world wars and experienced decolonization in the post-war period. By theme Historians",
    "label": 0
  },
  {
    "text": "prompted significant transformations, and by the end of the 19th century, most of the region had come under Western control. Oceania became involved in various conflicts during the world wars and experienced decolonization in the post-war period. By theme Historians often limit their inquiry to a specific theme. Some propose a general subdivision into three major themes: political history, economic history, and social history. However, the boundaries between these branches are vague and their relation to other thematic branches, such as intellectual history, is not always clear. Political history studies the organization of power in society, examining how power structures arise, develop, and interact. Throughout most of recorded history, states or state-like structures have been central to this field of study. It explores how a state was organized internally, like factions, parties, leaders, and other political institutions. It also examines which policies were implemented and how the state interacted with other states. Political history has been studied since antiquity by historians such as Herodotus and Thucydides, making it one of the oldest branches of history, while other major subfields have only become established branches in the past century. Diplomatic and military history are associated with political history. Diplomatic history examines international relations between states. It covers foreign policy topics such as negotiations, strategic considerations, treaties, and conflicts between nations as well as the role of international organizations in these processes. Military history studies the impact and development of armed conflicts in human history. This includes the examination of specific events, like the analysis of a particular battle and the discussion of the different causes of a war. It also involves more general considerations about the evolution of warfare, including advancements in military technology, strategies, tactics, logistics, and institutions. Economic history examines how commodities are produced, exchanged, and consumed. It covers",
    "label": 0
  },
  {
    "text": "of the different causes of a war. It also involves more general considerations about the evolution of warfare, including advancements in military technology, strategies, tactics, logistics, and institutions. Economic history examines how commodities are produced, exchanged, and consumed. It covers economic aspects such as the use of land, labour, and capital, the supply and demand of goods, the costs and means of production, and the distribution of income and wealth. Economic historians typically focus on general trends in the form of impersonal forces, such as inflation, rather than the actions and decisions of individuals. If enough data is available, they rely on quantitative methods, like statistical analysis. For periods before the modern era, available data is often limited, forcing economic historians to rely on scarce sources and extrapolate information from them. Social history is a broad field investigating social phenomena, but its precise definition is disputed. Some theorists understand it as the study of everyday life outside the domains of politics and economics, including cultural practices, family structures, community interactions, and education. A closely related approach focuses on experience rather than activities, examining how members of particular social groups, like social classes, races, genders, or age groups, experienced their world. Other definitions see social history as the study of social problems, like poverty, disease, and crime, or take a broader perspective by examining how whole societies developed. Closely related fields include cultural history, gender history, and religious history. Intellectual history is the history of ideas and studies how concepts, philosophies, and ideologies have evolved. It is particularly interested in academic fields but not limited to them, including the study of the beliefs and prejudices of ordinary people. In addition to studying intellectual movements themselves, it also examines the cultural and social contexts that shaped them and their influence on other",
    "label": 0
  },
  {
    "text": "fields but not limited to them, including the study of the beliefs and prejudices of ordinary people. In addition to studying intellectual movements themselves, it also examines the cultural and social contexts that shaped them and their influence on other historical developments. As closely related fields, the history of philosophy investigates the development of philosophical thought while the history of science studies the evolution of scientific theories and practices, such as the scientific contributions of Charles Darwin and Albert Einstein. Art history, another connected discipline, examines historical works of art and the development of artistic activities, styles, and movements. It includes a discussion of the cultural, social, and political contexts of art production. Environmental history studies the relation between humans and their environment. It seeks to understand how humans and the rest of nature have affected each other in the course of history. Other thematic branches include constitutional history, legal history, urban history, business history, history of technology, medical history, history of education, and people's history. Others Some branches of history are characterized by the methods they employ, such as quantitative history and digital history, which rely on quantitative methods and digital media. Comparative history compares historical phenomena from distinct times, regions, or cultures to examine their similarities and differences. Unlike most other branches, oral history relies on oral reports rather than written documents, encompassing eyewitness accounts, hearsay, and communal legends. It reflects the personal experiences, interpretations, and memories of common people, showcasing how people subjectively remember the past. Counterfactual history uses counterfactual thinking to examine alternative courses of history, exploring what could have happened under different circumstances. Certain branches of history are distinguished by their theoretical outlook, such as Marxist and feminist history. Some distinctions focus on the scope of the studied topic. Big History is the branch with",
    "label": 0
  },
  {
    "text": "exploring what could have happened under different circumstances. Certain branches of history are distinguished by their theoretical outlook, such as Marxist and feminist history. Some distinctions focus on the scope of the studied topic. Big History is the branch with the broadest scope, covering everything from the Big Bang to the present, incorporating elements of cosmology, geology, biology, and anthropology. World history is another branch with a wide topic. It examines human history as a whole, starting with the evolution of human-like species. The terms macrohistory, mesohistory, and microhistory refer to different scales of analysis, ranging from large-scale patterns that affect the whole globe to detailed studies of local contexts, small communities, family histories, particular individuals, or specific events. Closely related to microhistory is the genre of historical biography, which recounts an individual's life in its historical context and the legacy it left. Public history involves activities that present history to the general public. It usually happens outside the traditional academic settings in contexts like museums, historical sites, heritage tourism, and popular media. Evolution of the discipline Before the invention of writing, the preservation and transmission of historical knowledge were limited to oral traditions. Early forms of historical writing mixed facts with mythological elements, such as the Epic of Gilgamesh from ancient Mesopotamia and the Odyssey, an ancient Greek text attributed to Homer. Published in the 5th century BCE, the Histories by Herodotus was one of the foundational texts of the Western historical tradition, putting more emphasis on rational and evidence-based inquiry than the stories of Homer and other poets. Thucydides followed and further refined Herodotus's approach but focused more on particular political and military developments in contrast to the wide scope and ethnographic elements of Herodotus's work. Roman historiography was heavily influenced by Greek traditions. It often included not",
    "label": 0
  },
  {
    "text": "Thucydides followed and further refined Herodotus's approach but focused more on particular political and military developments in contrast to the wide scope and ethnographic elements of Herodotus's work. Roman historiography was heavily influenced by Greek traditions. It often included not only historical facts but also moral judgments of historical figures. Early Roman historians used an annalistic style, arranging past events by year with little commentary, while later ones preferred a more narrative and analytical approach. Another complex tradition of historical writing emerged in ancient China, with early precursors starting in the late 2nd millennium BCE. It considered annals the highest form of historical writing and emphasized verification through sources. This tradition was associated with Confucian philosophy and closely tied to the government in the form of the ruling dynasty, each responsible for writing the official history of its predecessor. Chinese historians established a coherent and systematic method for recording historical events earlier than other traditions. Of particular influence was the work of Sima Qian, whose meticulous research method and inclusion of alternative viewpoints shaped subsequent historiographical standards. In ancient India, historical narratives were closely associated with religion. They often mixed factual accounts with supernatural elements, as seen in works like the Mahabharata. In Europe during the medieval period, history was primarily documented by the clergy in the form of chronicles. Christian historians drew from Greco-Roman and Jewish traditions and reinterpreted the past from a religious perspective as a narrative highlighting God's divine plan. Influential contributions shaping this tradition were made by the historians Eusebius of Caesarea and Bede and by the theologian Augustine of Hippo. In the Islamic world, historical writing was similarly influenced by religion, interpreting the past from a Muslim perspective. It placed great importance on the chain of transmission to preserve the authority of historical accounts. Al-Tabari",
    "label": 0
  },
  {
    "text": "the theologian Augustine of Hippo. In the Islamic world, historical writing was similarly influenced by religion, interpreting the past from a Muslim perspective. It placed great importance on the chain of transmission to preserve the authority of historical accounts. Al-Tabari wrote a comprehensive history, spanning from the creation of the world to his present day. Ibn Khaldun reflected on philosophical issues underlying the practice of historians, such as universal patterns shaping historical changes and the limits of historical truth. With the emergence of the Tang Dynasty (618–907 CE) in China, historical writing became increasingly institutionalized as a bureau for the writing of history was established in 629 CE. The bureau oversaw the establishment of Veritable Records, a comprehensive compilation serving as the basis of the standard national history. Tang dynasty historians emphasized the difference between actual events that occurred in the past and the way these events are documented in historical texts. Historical writing in the Song dynasty (960–1279 CE) happened in a variety of historical genres, including encyclopedias, biographies, and historical novels, while history became a standard subject in the Chinese educational system. Influenced by the Chinese model, a tradition of historical writing emerged in Japan in the 8th century CE. Like in China, historical writing was closely related to the imperial household, but Japanese historians placed less importance on critical source evaluation than their Chinese counterparts. During the Renaissance and the early modern period (approximately 1500 to 1800), the different historical traditions came increasingly into contact with each other. Starting in 14th-century Europe, the Renaissance led to a shift away from medieval religious outlooks towards a renewed interest in the earlier classical tradition of Greece and Rome. Renaissance humanists used sophisticated text criticism to scrutinize earlier religious historical works, which contributed to the secularization of historical writing. During",
    "label": 0
  },
  {
    "text": "shift away from medieval religious outlooks towards a renewed interest in the earlier classical tradition of Greece and Rome. Renaissance humanists used sophisticated text criticism to scrutinize earlier religious historical works, which contributed to the secularization of historical writing. During the 15th to 17th centuries, historians placed greater emphasis on the didactic role of history, using it to promote the established order or argue for a return to an idealised vision of the past. As the invention of the printing press made written documents more accessible and affordable, interest in history expanded outside the clergy and nobility. At the same time, empiricist thought associated with the Scientific Revolution questioned the possibility of arriving at universal historical truths. During the Age of Enlightenment in the 18th century, historical writing was influenced by rationalism and scepticism. Aiming to challenge traditional authority and dogma through reason and empirical methods, historians tried to uncover deeper patterns and meaning in the past, while the scope of historical inquiry expanded with an increased focus on societal and economic topics as well as comparisons between different cultures. In China during the Ming dynasty (1368–1644), public interest in historical writings and their availability also increased. In addition to the continuation of the Veritable Records by official governmental historians, non-official works by private scholars flourished. These scholars tended to use a more creative style and sometimes challenged orthodox accounts. In the Islamic world, new traditions of historical writings emerged in the Safavid, Mughal, and Ottoman Empires. Meanwhile, in the Americas, European explorers recorded and interpreted indigenous narratives, which had been passed down through oral and pictographic practices. These views sometimes contested traditional European perspectives. Historical writing was transformed in the 19th century as it became more professional and science-oriented. Following the work of Leopold von Ranke, a systematic method",
    "label": 0
  },
  {
    "text": "been passed down through oral and pictographic practices. These views sometimes contested traditional European perspectives. Historical writing was transformed in the 19th century as it became more professional and science-oriented. Following the work of Leopold von Ranke, a systematic method of source criticism was widely accepted while academic institutions dedicated to history were established in the form of university departments, professional associations, and journals. In tune with this scientific outlook, Auguste Comte formulated the school of positivism and aimed to discover general laws of history, similar to the laws of nature studied by physicists. Building on the philosophy of Georg Wilhelm Friedrich Hegel, Karl Marx proposed one such general law in his theory of historical materialism, arguing that economic forces and class struggle are the fundamental drivers of historical change. Another influential development was the spread of European historiographical methods, which became the dominant approach to the academic study of the past worldwide. In the 20th century, traditional historical assumptions and practices were challenged while the scope of historical research broadened. The Annales school used insights from sociology, psychology, and economics to study long-term developments. Authoritarian regimes, like Nazi Germany, the Soviet Union, and China, manipulated historical narratives for ideological purposes. Various historians covered unconventional perspectives, focusing on the experiences of marginalized groups through approaches such as history from below, microhistory, oral history, and feminist history. Postcolonialism aimed to undermine the hegemony of the Western approach and postmodernism rejected the claim to a single universal truth in history. Intellectual historians examined the historical development of ideas. In the second half of the century, renewed attempts to write histories of the world as a whole gained momentum, while technological advances fostered the growth of quantitative and digital history. Related fields Historiography Historiography is the study of the methods and development of",
    "label": 0
  },
  {
    "text": "of the century, renewed attempts to write histories of the world as a whole gained momentum, while technological advances fostered the growth of quantitative and digital history. Related fields Historiography Historiography is the study of the methods and development of historical research. Historiographers examine what historians do, resulting in a metatheory in the form of a history of history. Some theorists use the term historiography in a different sense to refer to written accounts of the past. A central topic in historiography as a metatheory focuses on the standards of evidence and reasoning in historical inquiry. Historiographers examine and codify how historians use sources to construct narratives about the past, including the analysis of the interpretative assumptions from which they proceed. Closely related issues include the style and rhetorical presentation of works of history. By comparing the works of different historians, historiographers identify schools of thought based on shared research methods, assumptions, and styles. For example, they examine the characteristics of the Annales school, like its use of quantitative data from various disciplines and its interest in economic and social developments taking place over extended periods. Comparisons also extend to whole eras from ancient to modern times. This way, historiography traces the development of history as an academic discipline, highlighting how the dominant methods, themes, and research goals have changed over time. Philosophy of history The philosophy of history investigates the theoretical foundations of history. It is interested both in the past itself as a series of interconnected events and in the academic field studying this process. Insights and approaches from various branches of philosophy are relevant to this endeavour, such as metaphysics, epistemology, hermeneutics, and ethics. In examining history as a process, philosophers explore the basic entities that make up historical phenomena. Some approaches rely primarily on the beliefs",
    "label": 0
  },
  {
    "text": "from various branches of philosophy are relevant to this endeavour, such as metaphysics, epistemology, hermeneutics, and ethics. In examining history as a process, philosophers explore the basic entities that make up historical phenomena. Some approaches rely primarily on the beliefs and actions of individual humans, while others include collective and other general entities, such as civilizations, institutions, ideologies, and social forces. A related topic concerns the nature of causal mechanisms connecting historic events with their causes and consequences. One view holds that there are general laws of history that determine the course of events, similar to the laws of nature studied in the natural sciences. According to another perspective, causal relations between historic events are unique and shaped by contingent factors. Historically, some philosophers have suggested that the general direction of the course of history follows large patterns. According to one proposal, history is cyclic, meaning that on a sufficiently large scale, individual events or general trends repeat. Another such theory asserts that history is a linear, teleological process moving towards a predetermined goal. The topics of philosophy of history and historiography overlap as both are interested in the standards of historical reasoning. Historiographers typically focus more on describing specific methods and developments encountered in the study of history. Philosophers of history, by contrast, tend to explore more general patterns, including evaluative questions about which methods and assumptions are correct. Historical reasoning is sometimes used in philosophy and other disciplines as a method to explain phenomena. This approach, known as historicism, argues that understanding something requires knowledge of its unique history or how it evolved. For instance, historicism about truth states that truth depends on historical circumstances, meaning that there are no transhistorical truths. Historicism contrasts with approaches that seek a timeless and universal understanding of their subject matter. Historical",
    "label": 0
  },
  {
    "text": "history or how it evolved. For instance, historicism about truth states that truth depends on historical circumstances, meaning that there are no transhistorical truths. Historicism contrasts with approaches that seek a timeless and universal understanding of their subject matter. Historical objectivity Diverse debates in the philosophy of history focus on the possibility of an objective account of history. Various theorists argue that this ideal is not achievable, pointing to the subjective nature of interpretation, the narrative aspect of history, and the influence of personal values and biases on the perspective and actions of both historic individuals and historians. According to one view, some particular facts are objective, for example, facts about when a drought occurred or which army was defeated. However, this view does not ensure general objectivity since historians have to interpret and synthesize facts to arrive at an overall narrative describing large trends and developments. As a result, some historians, such as G. M. Trevelyan and Keith Jenkins, assert that all history is biased, arguing that historical narratives are never free of subjective presuppositions and value judgments. Some outlooks associated with realism, empiricism, and reconstructionism, conceptualise history as the search for truth or knowledge, which they see as recoverable through rigorous evaluation and careful interpretation of evidence. Other scholars critique this view, emphasising the subjective and partial nature of historical knowledge. Perspectivists claim that historical perspectives are inherently subjective, as they require selecting particular sources and inquiries, and ascertaining what information can be regarded as historical fact. They argue that statements can only be objective within or relative to one of several competing historical perspectives. A stronger scepticist or relativist outlook states that no historical knowledge can be proven objective. This emphasis on subjectivities has been extended by postmodernist theories that suggest that it is impossible to know",
    "label": 0
  },
  {
    "text": "to one of several competing historical perspectives. A stronger scepticist or relativist outlook states that no historical knowledge can be proven objective. This emphasis on subjectivities has been extended by postmodernist theories that suggest that it is impossible to know the past objectively, adding that meaning is created through human-made texts, the language of which \"constitute our world as we perceive it\". Neo-realists have responded to this trend by reemphasising the centrality of empiricist methodologies to historical analysis. They acknowledge the influence of subjective evaluations but contend that historical truth is reachable nonetheless. Education History is part of the school curriculum in most countries. Early history education aims to make students interested in the past and familiarize them with fundamental concepts of historical thought. By fostering a basic historical awareness, it seeks to instil a sense of identity by helping them understand their cultural roots. It often takes a narrative form by presenting children with simple stories, which may focus on historic individuals or the origins of local holidays, festivals, and food. More advanced history education encountered in secondary school covers a broader spectrum of topics, ranging from ancient to modern history, at both local and global levels. It further aims to acquaint students with historical research methodologies, including the abilities to interpret and critically evaluate historical claims. History teachers employ a variety of teaching methods. They include narrative presentations of historical developments, questions to engage students and prompt critical thinking, and discussions on historical topics. Students work with historical sources directly to learn how to analyse and interpret evidence, both individually and in group activities. They engage in historical writing to develop the skills of articulating their thoughts clearly and persuasively. Assessment through oral or written tests aims to ensure that learning goals are reached. Traditional methodologies in history",
    "label": 0
  },
  {
    "text": "both individually and in group activities. They engage in historical writing to develop the skills of articulating their thoughts clearly and persuasively. Assessment through oral or written tests aims to ensure that learning goals are reached. Traditional methodologies in history education often present numerous facts, like dates of significant events and names of historical figures, which students are expected to memorize. Some modern approaches, by contrast, seek to foster a more active engagement and a deeper interdisciplinary understanding of general patterns, focusing not only on what happened but also on why it happened and its lasting historical significance. History education in state schools serves a variety of purposes. A key skill is historical literacy, the ability to comprehend, critically analyse, and respond to historical claims. By making students aware of significant developments in the past, they can become familiar with various contexts of human life, helping them understand the present and its diverse cultures. At the same time, history education can foster a sense of cultural identity by connecting students with their heritage, traditions, and practices, for example, by introducing them to iconic elements ranging from national landmarks and monuments to historical figures and traditional festivities. Knowledge of a shared past and cultural heritage can contribute to the formation of a national identity and prepares students for active citizenship. This political aspect of history education may spark disputes about which topics school textbooks should cover. In various regions, it has resulted in so-called history wars over the curriculum. It can lead to a biased treatment of controversial topics in an attempt to present their national heritage in a favourable light. In addition to the formal education provided in public schools, history is also taught in informal settings outside the classroom. Public history takes place in locations like museums and memorial",
    "label": 0
  },
  {
    "text": "attempt to present their national heritage in a favourable light. In addition to the formal education provided in public schools, history is also taught in informal settings outside the classroom. Public history takes place in locations like museums and memorial sites, where selected artefacts are often used to tell specific stories. It includes popular history, which aims to make the past accessible and appealing to a wide audience of non-specialists in media such as books, television programmes, and online content. Informal history education also happens in oral traditions as narratives about the past are transmitted across generations. Other fields History employs an interdisciplinary methodology, drawing on findings from fields such as archaeology, geology, genetics, anthropology, and linguistics. Archaeologists study human-made historical artefacts and other forms of material culture. Their findings provide crucial insights into past human activities and cultural developments. The interpretation of archaeological evidence presents challenges that differ from standard historical work with written documents. At the same time, it offers new possibilities by presenting information that was not recorded, allowing historians to access the past of non-literate societies and marginalized groups within literate societies by studying the remains of their material culture. Before the advent of modern archaeology in the 19th century, antiquarianism laid the groundwork for this discipline and played a vital role in preserving historical artefacts. Geology and other earth sciences help historians understand the environmental contexts and physical processes that affected past societies, including climate conditions, landscapes, and natural events. Genetics provides key information about the evolutionary origins of humans as a species, human migration, ancestry, and demographic changes. Anthropologists investigate human culture and behaviour, such as social structures, belief systems, and ritual practices. This knowledge offers contexts for the interpretation of historical events. Historical linguistics studies the development of languages over time, which can",
    "label": 0
  },
  {
    "text": "ancestry, and demographic changes. Anthropologists investigate human culture and behaviour, such as social structures, belief systems, and ritual practices. This knowledge offers contexts for the interpretation of historical events. Historical linguistics studies the development of languages over time, which can be crucial for the interpretation of ancient documents and can also provide information about migration patterns and cultural exchanges. Historians further rely on evidence from various other fields belonging to the physical, biological, and social sciences as well as the humanities. In virtue of its relation to ideology and national identity, history is closely connected to politics and historical theories can directly impact political decisions. For example, irredentist attempts by one state to annex territory of another state often rely on historical theories claiming that the disputed territory belonged to the first state in the past. History also plays a central role in so-called historical religions, which base some of their core doctrines on historical events. For instance, Christianity is often categorized as a historical religion because it is centred around historical events surrounding Jesus Christ. History is relevant to many fields through the study of their past, including the history of science, mathematics, philosophy, and art. See also Glossary of history Outline of history References Notes Citations Sources External links Internet History Sourcebooks Project See also Internet History Sourcebooks Project (Collections of public domain and copy-permitted historical texts for educational use)",
    "label": 0
  },
  {
    "text": "This glossary of history is a list of definitions of terms and concepts relevant to the study of history and its related fields and sub-disciplines, including both prehistory and the period of human history. A ab urbe condita (AUC) absolute monarchy A system of government headed by a monarch as the only source of power, controlling all functions of the state. abstract A summary of a textual source. access rights Information about who can access the resource or an indication of its security status. accrual method The method by which items are added to a collection. accrual periodicity The frequency with which items are added to a collection. accrual policy The policy governing the addition of items to a collection. administrative history A subdiscipline of historiography which studies the history of state administrations and bureaucracies, focusing especially on changes in administrative ideology and legal codes over time as well as the role of civil servants and the relationship between government and society. aeon Also eon. age Age of Discovery Also called the Age of Exploration. The time period between approximately the late 15th century and the 17th century during which seafarers from various European polities traveled to, explored, and charted regions across the globe which had previously been unknown or unfamiliar to Europeans and, more broadly, during which previously isolated human populations became socially, politically, and/or economically aware of and connected to each other. These explorations, often commissioned and funded by state governments, were spurred by advances in cartography and maritime technology at the beginning of the early modern period, especially the introduction of sailing vessels capable of enduring extremely long trans-oceanic voyages, and so the Age of Discovery largely overlaps with the Age of Sail. The period is also associated with the emergence of colonialism and imperialism as practiced",
    "label": 0
  },
  {
    "text": "especially the introduction of sailing vessels capable of enduring extremely long trans-oceanic voyages, and so the Age of Discovery largely overlaps with the Age of Sail. The period is also associated with the emergence of colonialism and imperialism as practiced by European monarchies, particularly the colonization of the Americas and the establishment of oceanic trade routes to India and Southeast Asia, which are sometimes identified with the origins of the global economy and of globalization, as well as the exchange, intentional and unintentional, of plants, animals, diseases, technologies, and ideas between previously isolated parts of the world. Age of Enlightenment agent provocateur A person who goes undercover in the ranks of the enemy during a social or political conflict with the intention of damaging or compromising the enemy from within by provoking actions that might not otherwise have taken place. Agents provocateurs have sometimes been employed by governments or businesses to provoke armed clashes between groups, to create disorder, or to incite controversies which might be used as an excuse for war or foreign intervention. Alltagsgeschichte alternate modernity In postcolonialism and development studies, the theory that different parts of the world experienced the onset of modernity at different times and in their own fashion, such that to people living in certain places \"modernity\" means something quite specific and distinct from that experienced elsewhere. anachronism A chronological inconsistency, in particular the introduction of an object, linguistic term, technology, idea, or anything else into a period in time to which it does not belong. ancient history Also antiquity. Annales school A style of historiography linked to the French scholarly journal Annales d'histoire économique et sociale and broadly associated with the social history of cultural practices. annals Historical accounts of facts and events arranged in chronological order, year by year. The term is",
    "label": 0
  },
  {
    "text": "of historiography linked to the French scholarly journal Annales d'histoire économique et sociale and broadly associated with the social history of cultural practices. annals Historical accounts of facts and events arranged in chronological order, year by year. The term is also used more loosely to describe any historical record. Anno Domini (AD) anthropology The study of humanity, culturally and physically, in all times and places. antiquarian Also antiquary. A historian who studies antiquities or things of the past, often with particular attention to artifacts, archives, manuscripts, or archaeological sites from ancient history, as opposed to more recent history. In a broader sense, an antiquarian may also be a person who is simply a collector or aficionado of such artifacts and not necessarily a professional historian. antiquarianism Historical study focusing on the empirical evidence of the past, including manuscripts and archives, and archaeological and historic sites and artifacts. The term is now often used in a pejorative sense to refer to an excessively narrow interest in historical trivia, to the exclusion of a sense of historical context or process. antiquities Objects or artifacts from ancient history, especially from the Classical civilizations and cultures of the Mediterranean region and the ancient Near East. antiquity See ancient history and Classical antiquity. archaeology The study of human history and prehistory through the excavation of sites and the analysis of physical remains. architectural history The study of buildings in their historical and stylistic contexts. archival bond The relationship that each archival record has with other records produced as part of the same transaction or activity and located within the same group. archival research archival science The study and theory of building and curating archives. archive An accumulation of historical documents and records, or the physical repository in which they are located. archontology The study of",
    "label": 0
  },
  {
    "text": "and located within the same group. archival research archival science The study and theory of building and curating archives. archive An accumulation of historical documents and records, or the physical repository in which they are located. archontology The study of historical offices and important positions in state, international, political, religious, and other organizations and societies, including chronologies, succession of officeholders, their biographies, and related records. armiger A person entitled to bear a coat of arms; more broadly, a gentleman or an esquire. armory The study of coats of armorial bearings; another name for heraldry. arrested decay A state or condition in which certain historic sites are preserved, where little or no significant restoration or renovation work is performed in modern times except that which is necessary to maintain basic structural integrity and prevent total deterioration of historical objects. The concept is often contrasted with conservation efforts which attempt to preserve or improve buildings or artifacts by deliberately applying techniques which may dramatically alter their appearance or material properties, particularly when these efforts are perceived as damaging their intrinsic historical value. The term is used especially when referring to immovable cultural property such as historic buildings and places such as ghost towns which have remained largely unchanged since they were last occupied, with neither demolition nor re-development having significantly altered them in the intervening time, as well as places which have been extensively damaged but which for various reasons it is undesirable or impossible to restore, such as the ruins of ancient civilizations or of battlefields intended to serve as war memorials. art history The study of objects of art in their historical and stylistic contexts. artifact Also artefact. Any material object associated with a culture, such as a tool, an article of clothing, or a prepared food item. audience A",
    "label": 0
  },
  {
    "text": "memorials. art history The study of objects of art in their historical and stylistic contexts. artifact Also artefact. Any material object associated with a culture, such as a tool, an article of clothing, or a prepared food item. audience A class of entity, often a specific demographic, for whom a given resource is intended or useful. authorized biography autobiography An individual's account of his or her own life. auxiliary sciences of history Also ancillary sciences of history. The set of specialist scholarly disciplines which help evaluate and use historical sources and are often used to support historical research. These disciplines may include but are not limited to archaeology, archival science, philology, genealogy, numismatics, philately, and heraldry. Avalonia A separate plate in the Early Paleozoic consisting of much of what is now Northern Europe, Newfoundland, Nova Scotia, and some coastal parts of New England. B Baltica A separate continental plate of the Early Paleozoic composed of what is now the United Kingdom, Scandinavia, European Russia and Central Europe. It is named for the Baltic Sea. barbarian A Greek word adopted by the Romans to refer to any people who did not adopt the Roman way of life. It is said to have come originally from the sound \"bar-bar\", which, according to the Greeks, was supposed to be the noise that people made when speaking foreign languages. Before Christ (BC) Before the Common Era (BCE) Bering Land Bridge Also Beringia. The vast tundra plain that was exposed as a land bridge between the continents of Asia and North America during the Last Glacial Maximum, about 21,000 years ago. It is theorized to have served as a migration route for people, animals, and plants for several thousand years before being once again submerged beneath rising sea levels. bibliography A list of written works,",
    "label": 0
  },
  {
    "text": "Glacial Maximum, about 21,000 years ago. It is theorized to have served as a migration route for people, animals, and plants for several thousand years before being once again submerged beneath rising sea levels. bibliography A list of written works, including books, journals, and essays, about or detailing a particular subject. Big History big lie biography An account of an individual's life, especially one written by someone other than the individual featured in the account. black legend Blitzkrieg German for \"lightning war\". A military strategy used by the German Army at the beginning of World War II to achieve victory through a series of quick offensives, especially in Belgium, the Netherlands and France. The strategy involved a heavy initial bombardment, followed by the rapid mobilisation of armour and motorised infantry to break the weakest parts of the enemy line. Bolsheviks A small, tightly organised, revolutionary Marxist group in early 20th-century Russia which split from the Russian Socialist movement in 1903 and was led by Vladimir Lenin. In November 1917, during the so-called October Revolution, the Bolsheviks (\"Majority\") took control of a chaotic Russia, becoming the de facto rulers after the subsequent civil war. They then renamed themselves the Communist Party of the Soviet Union (CPSU). book review A critical examination of a text, usually including a summary of the work and opposing views. bottom-up approach An approach to historical scholarship that attempts to explain the experiences or perspectives of ordinary people, as opposed to elites or leaders. Contrast top-down approach. bourgeoisie The capitalist class that came to be known as the middle class, between the aristocracy and the working class. A new middle class of merchants and businessmen prospered throughout Europe from the 16th century, and especially in Britain, which Napoleon described as a \"nation of shopkeepers\". In modern times,",
    "label": 0
  },
  {
    "text": "the middle class, between the aristocracy and the working class. A new middle class of merchants and businessmen prospered throughout Europe from the 16th century, and especially in Britain, which Napoleon described as a \"nation of shopkeepers\". In modern times, the term bourgeois is often used derogatorily to describe anything considered humdrum, unimaginative and/or selfishly materialistic. Bronze Age In Britain, a period from about 2300 to 700 BCE when metal first began to be widely used, possibly as a result of the increase in contact with mainland Europe. However, various types of stone, particularly flint, remained very important long after metal became available. The Bronze Age saw the introduction of cremation of the dead and burials in round barrows. The later (and best-known) phases of construction at Stonehenge also date from this period. Buranji Written chronicles of Ahoms, a medieval kingdom of Assam, India. C Caesar A Roman family name best known for being used by several rulers of Ancient Rome. Contrary to popular opinion, the name \"Caesar\" did not originally mean \"emperor\", although in modern times it has come to be defined as a synonym for autocrat. When the Roman leader Gaius Julius Caesar was assassinated in 44 BCE, his nephew and successor Augustus had himself formally adopted by the dead man and so also adopted the family name Caesar. Tiberius and Caligula inherited it by adoption as well. Later Roman emperors acquired the name upon their succession or when they were formally adopted as heirs. calendar A descriptive list of archival documents, sometimes compiled in sufficient detail that it can be used as a substitute for the originals. cartulary A register of lands and privileges granted by charter, occasionally recorded on a roll of paper but more often in book form. Cathaysian Terranes A set of small landmasses",
    "label": 0
  },
  {
    "text": "it can be used as a substitute for the originals. cartulary A register of lands and privileges granted by charter, occasionally recorded on a roll of paper but more often in book form. Cathaysian Terranes A set of small landmasses that developed in tropical to subtropical latitudes on the eastern side of Pangaea during the Permian and Triassic, comprising what is now North China (Sino-Korea), South China (Yangtze), Eastern Qiangtang, Tarim, and Indochina. century A period of 100 years. Centuries are numbered ordinally (e.g. 15th, 16th, 17th) in English and many other languages. chancery charge A heraldic device emblazoned on the face of a shield. charter A legal grant of authority or rights. chirograph 1. An historical record that has been torn or cut into two pieces, sometimes with writing across the division, such that each piece serves to authenticate the other by exactly matching with it; an indenture. 2. Any handwritten document. chorography The geographical description of regions, often with reference to their history and antiquities. chronicle A historical account of facts and events arranged in chronological order. chronology The study of the sequence of past events. classical antiquity Also sometimes classical era, classical period, or classical civilization. The period of cultural history between the 8th century BC and the 6th century AD in the geographical area centered on the Mediterranean Sea, particularly relating to the contemporaneous civilizations of ancient Greece and ancient Rome, known as the Greco-Roman world, which flourished and wielded enormous influence across much of Europe, North Africa, and Western Asia during this time. Though its boundaries are imprecise, the classical period is traditionally considered to have begun with the earliest writings of the Greek poet Homer and ended with the fall of the Western Roman Empire and the decline of classical culture during Late antiquity",
    "label": 0
  },
  {
    "text": "boundaries are imprecise, the classical period is traditionally considered to have begun with the earliest writings of the Greek poet Homer and ended with the fall of the Western Roman Empire and the decline of classical culture during Late antiquity and the Early Middle Ages. Cimmerian Terranes Also called Cimmeria. An archipelago of small landmasses that developed in tropical and subtropical latitudes on the eastern side of Pangaea during the Triassic. Blocks that comprised it include what is now Turkey, Iran, Afghanistan, Tibet, and Malaysia. circa Variously abbreviated c., ca., circ., or cca. Approximately, about, around; near or in the vicinity of. A Latin term signifying approximation or uncertainty, usually by immediately preceding a date or a numerical measure. Circa is widely used in historical writing and genealogy when the dates of events are not accurately known. When used with date ranges, it or its abbreviation is applied before each approximate date, while dates without circa preceding them are generally assumed to be known with certainty. citation A reference to a published or unpublished source for an assertion or argument. classical tradition classics Also called classical studies. The study of classical antiquity, in particular of Ancient Greek and Latin literature and their respective languages, and traditionally also Greco-Roman art, philosophy, history, mythology, and society. cliodynamics cliometrics The systematic application of economic theory, econometric techniques, and other formal or mathematical methods to the study of history; a quantitative economic history. codex (pl.) codices A book constructed of a number of sheets of paper, vellum, papyrus, parchment, or similar materials, especially a manuscript book with handwritten contents and formatted so that individual pages are stacked and fixed to a spine along one edge. codicology The study of codices or manuscript books as physical objects, specifically the materials and techniques used to make",
    "label": 0
  },
  {
    "text": "manuscript book with handwritten contents and formatted so that individual pages are stacked and fixed to a spine along one edge. codicology The study of codices or manuscript books as physical objects, specifically the materials and techniques used to make books, including writing surfaces (such as parchment or vellum), pigments, inks, bindings, handwriting, marginalia, glosses, and so on. coherence theory of truth A theory that regards statements as true if they are coherent within some specified set of sentences, propositions, or beliefs. Cold War colonialism The practice or policy by which one people or sovereignty exerts social, political, and/or economic control over other people or geographic areas, typically by establishing a colony whose administration is distinct from that of the colonizers' home territory and generally with the aim of economic dominance. The foreign administrators rule the colony in pursuit of their own interests, often imposing their language, religion, and culture upon the colonized region while seeking to benefit from the exploitation of its people and resources. Colonialism is often associated with though is distinct from imperialism. Common Era (CE) comparative history The comparison of different societies which existed during the same time period or shared similar cultural conditions. computational history Congo craton A separate continental plate that rifted from the supercontinent Rodinia in the Late Precambrian. It contained a large part of what is now north-central Africa. conjectural history conjectural portrait context In archaeology, a discrete physical location, distinguishable from other contexts, which forms one of the units making up an overall archaeological site. The context in which an artifact is found provides important evidence for its interpretation. correspondence theory of truth A theory that regards statements as true if they correspond to the world that we know by perception. counterfactual history A form of historiography that seeks to explore",
    "label": 0
  },
  {
    "text": "is found provides important evidence for its interpretation. correspondence theory of truth A theory that regards statements as true if they correspond to the world that we know by perception. counterfactual history A form of historiography that seeks to explore history by extrapolating a timeline in which key events happened in ways other than the ways in which they did in fact occur. Cretaceous Western Interior Seaway Also referred to simply as the Western Interior Seaway. The epicontinental sea that formed as marine waters from the north spread over North America from around 130 to 70 million years ago (Ma). At its peak in the Middle Cretaceous (~90 Ma), it extended from present-day Utah to the Appalachians and from the Arctic to the Gulf of Mexico. cryptohistory cultural history The academic study of the origins and history of the culture and cultural practices (e.g. music, theater, literature, fine art) of a particular group of people. culture D Dark Ages date A specific point or period of time. deep history The study of the distant past of the human species, i.e. the earliest parts of human prehistory, or any other aspect of the time period during which the earliest humans existed (with \"humans\" usually meaning anatomically modern humans, as opposed to earlier hominid species). Deep history incorporates a wide range of methods from disciplines such as archaeology, primatology, anthropology, genetics, evolutionary biology, and linguistics with the goal of assembling a common narrative about the origins and evolution of human populations prior to the beginning of recorded history, and also of correcting a perceived scholarly bias towards the study of more recent historical periods. demographic history digital history The use of digital media in the academic study of history, in order to aid historical analysis, research, or presentation, including digital archives, data",
    "label": 0
  },
  {
    "text": "a perceived scholarly bias towards the study of more recent historical periods. demographic history digital history The use of digital media in the academic study of history, in order to aid historical analysis, research, or presentation, including digital archives, data visualizations, interactive maps and timelines, audio files, virtual representations of historical periods and places, etc., often in an online format. See also computational history. diplomatics The study and textual analysis of historical documents. discipline The study, or practice, of a specific subject using a specific set of methods, terms and approaches. History is a discipline, as is archaeology, chemistry, and biology. dominant narrative dossier A group of documents deliberately assembled to provide information about a specific topic. The term often connotes information that has been purposefully collected from various sources, as opposed to documents that exist in an organic collection originating from a single source or resulting from routine activities. dynasty E early modern period eclogue economic determinism The socioeconomic theory that economic relationships have been the main or sole driving force in all of human history. economic history The study of economies or economic phenomena of the past. Edwardian 1. The period of British history that spanned the reign of King Edward VII (1901–1910), or more generally the period between the turn of the 20th century and the outbreak of the First World War in August 1914. 2. Of or related to this period; an adjective used to describe any person, object, event, idea, or concept characterizing or associated with the Edwardian era, either by having originated or flourished during the period or by retrospectively coming to represent it, especially in the United Kingdom but more broadly in any part of the British Empire. effect of reality Elizabethan 1. The period of English history that spanned the reign of",
    "label": 0
  },
  {
    "text": "during the period or by retrospectively coming to represent it, especially in the United Kingdom but more broadly in any part of the British Empire. effect of reality Elizabethan 1. The period of English history that spanned the reign of Elizabeth I, Queen of England and Ireland (1558–1603). Elizabeth was the last monarch of the Tudor period, and the Elizabethan era is often depicted as a golden age in English history, an age of economic growth, naval supremacy, and national pride. 2. Of or related to this period; an adjective used to describe any person, object, event, idea, or concept characterizing or associated with the Elizabethan era, either by having originated or flourished during the period or by retrospectively coming to represent it. empire A type of sovereign state made up of multiple territories and peoples subject to a single and supreme ruling authority, often an emperor or empress. Empires can be composed exclusively of contiguous territories, e.g. the Russian Empire, or may include territories which are remote from the empire's home territory or metropole, as with a colonial empire. The concept of an empire is often associated with the concept of imperialism, though the latter also refers to a political policy or ideology that is not necessarily practiced by empires and can apply to many other forms of government. end of history Enlightenment A cultural and intellectual movement of the late 17th to late 18th centuries that emphasized reason and individualism rather than faith and tradition, predominantly among Western European cultures but also in other parts of the world; or the time period itself during which this movement flourished. environmental history An approach to history that examines how nature and natural processes (i.e. plants, animals, geology, etc.) have shaped human agency and affairs, and conversely how humans have shaped",
    "label": 0
  },
  {
    "text": "or the time period itself during which this movement flourished. environmental history An approach to history that examines how nature and natural processes (i.e. plants, animals, geology, etc.) have shaped human agency and affairs, and conversely how humans have shaped nature. eon See aeon. epigraphy The study of ancient inscriptions. episteme The dominant mode of knowledge or understanding of a particular era, common to many or all forms of knowledge produced at the time. epoch An instant in time chosen (sometimes arbitrarily) as the origin or beginning of a particular calendar era, thereby serving as a reference point from which time is measured and by which historical events are temporally related. era Any span of time defined for the purposes of chronology or historiography. In chronology, an era is the highest level of organization for the measurement of time, as used in defining calendar eras for a given calendar and regnal eras in the history of a monarchy. The term is also used in geologic time, where an era is a subdivision of an aeon. essentialize To assume the existence of an inner \"essence\" or an essential character shared by all of the members of a group which in reality is diverse, variable, and fluid. ethnohistory A branch of history or an approach to historical scholarship which addresses the history of the native peoples of a particular place or region, in particular the indigenous peoples of the Americas. Ethnohistory is an interdisciplinary approach that often supplements written historical documents with methods from anthropology, folklore, oral history, and archaeology. euhemerism In mythology, the presumption that mythological accounts were based on or originated from real historical events or persons, having accumulated elaborations and exaggerations over many generations of retelling until reaching their present form. Euramerica A supercontinent that existed in the Late",
    "label": 0
  },
  {
    "text": "mythology, the presumption that mythological accounts were based on or originated from real historical events or persons, having accumulated elaborations and exaggerations over many generations of retelling until reaching their present form. Euramerica A supercontinent that existed in the Late Silurian through Devonian, formed by the collision of Baltica, Laurentia, and Avalonia. It included what is now North America, Greenland, Scandinavia, and Europe. It is also sometimes referred to as the “Old Red Continent” for the red color of its oxidized deposits. Eurocentrism A worldview that is centered on Western civilization or Western culture, particularly that originating in or associated with Western Europe, to the exclusion of or in a way that is biased against non-Western cultures. The term may also apply to the whole continent of Europe or beyond to countries and cultures whose histories are strongly tied to Western Europe by immigration, colonization, or influence. F fakelore Also called pseudo-folklore. Inauthentic, manufactured folklore that is presented as if it were genuinely traditional. Compare invented tradition. farm book feudalism The legal and social order prevailing through much of medieval Europe, in which society was structured around a set of reciprocal legal and military obligations. fin de siècle floruit (fl.) Denoting a date or period during which a particular person or group is known to have been alive or active, or to which their works or contributions are dated (i.e. when they \"flourished\"), used especially when a person's dates of birth and death are not precisely known. folklore The expressive body of culture shared by a particular group of people, encompassing the oral traditions (e.g. tales, proverbs, and jokes) and the material culture as well as the customs, lore, folk beliefs, rituals, celebrations and ceremonies, holidays, and initiation rites practiced by that group, and in particular those cultural elements which",
    "label": 0
  },
  {
    "text": "encompassing the oral traditions (e.g. tales, proverbs, and jokes) and the material culture as well as the customs, lore, folk beliefs, rituals, celebrations and ceremonies, holidays, and initiation rites practiced by that group, and in particular those cultural elements which are transmitted informally from one individual to another and from one generation to the next either through verbal instruction or demonstration. fonds In archival science, an aggregation of documents which all originate from the same source. foreign domination G genealogy The study of family relationships. geological time golden age Gondwana A supercontinent that existed from the Cambrian to Jurassic, mainly composed of what is now South America, Africa, Madagascar, India, Antarctica, and Australia. great man theory Gregorian calendar H hagiography A biography of a saint or saints, or more broadly any biography in which the author is uncritical or reverential towards the subject. hegemony The political, economic, military, and/or cultural predominance of one state over other states, or more generally of any group or regime which exerts undue influence within a society. heraldic badge heraldry The design, display, and study of armorial bearings and devices, often practiced together with the study of ceremony, rank, and pedigree. heritage tourism Tourism involving the exploration and appreciation of the cultural, historical, or environmental heritage of a particular place or a particular group of people. The term encompasses both tangible and intangible aspects of history, including historic sites, monuments, and artifacts as well as the traditions, customs, and practices associated with a particular culture or historical period. hermeneutics The theory and methodology of the interpretation of texts. histoinformatics histoire des mentalités An approach to social or cultural history which attempts to describe and analyze the ways in which historical people thought about, interacted with, and classified the world around them, i.e. the mentalities, perspectives,",
    "label": 0
  },
  {
    "text": "interpretation of texts. histoinformatics histoire des mentalités An approach to social or cultural history which attempts to describe and analyze the ways in which historical people thought about, interacted with, and classified the world around them, i.e. the mentalities, perspectives, or modes of thought through which they interpreted historical events as well as their own lives. This methodology thus aims to understand the psychology of people who lived in the past. It is often associated with the Annales school of historiography and with microhistory. histoire totale historian A scholar who studies or writes about history. historian's fallacy historic preservation historic recurrence historic site historical anthropology historical classification historical demography historical materialism A branch of Marxism which takes the position that the development of history is not determined by the subjective desires or actions of specific human beings but is instead shaped by the objective facts of material existence. It sees human history unfolding as a consequence of humans attempting to alter the natural environment to suit their particular biological needs. historical method The collection of techniques and guidelines that historians use to research and write histories of the past. The historical method involves the historian identifying and drawing upon primary sources, secondary sources, and material evidence such as that derived from archaeology, evaluating the relative authority of these sources, and then combining their testimony appropriately in order to construct an accurate and reliable picture of past events and environments. historical metrology historical negationism Falsification or distortion of the historical record, especially by the practice of denialism. The term is sometimes used interchangeably with historical revisionism but may also be considered technically distinct, in that the latter can be applied to newly evidenced, fairly reasoned reinterpretations of history. Historical negationism, by contrast, is always illegitimate in its attempts to revise the",
    "label": 0
  },
  {
    "text": "used interchangeably with historical revisionism but may also be considered technically distinct, in that the latter can be applied to newly evidenced, fairly reasoned reinterpretations of history. Historical negationism, by contrast, is always illegitimate in its attempts to revise the past because it is practiced without impartiality or because it uses techniques that are inadmissible in proper academic discourse, such as presenting known forgeries as if they were genuine, inventing implausible reasons for distrusting genuine historical documents, and manipulating statistical figures to support a particular point of view. historical realism The view that there is a continuity and correspondence between the real world and the narration of that world in historians' narratives. historical record historical reenactment historical revisionism historical significance historical society An organization dedicated to preserving and promoting interest in the history of a particular place, time period, or subject, or of the study of history in general. historical source historical thinking The practice of critical thinking and literacy skills in evaluating and analyzing primary source documents in order to construct a meaningful and reliable account of the past. See also historical method. historical value historicism 1. A mode of historical enquiry that insists that the past must be understood on its own terms, as opposed to trying to understand it from the perspectives permitted by modern knowledge, values, and beliefs, known as presentism. 2. A determinist philosophy of history which holds that the course of historical events is governed by discoverable laws or by some overarching theme or pattern to which historical trends must invariably adhere, permitting historians to predict the likely direction in which future events will unfold, generally by assuming that the trends of past events will recur in a predictable sequence or manner. historicity The historical actuality or authenticity of persons or events in the",
    "label": 0
  },
  {
    "text": "to predict the likely direction in which future events will unfold, generally by assuming that the trends of past events will recur in a predictable sequence or manner. historicity The historical actuality or authenticity of persons or events in the past; the quality of being part of history instead of being a myth, legend, or fiction. The historicity of a claim about the past is its factual status. historiography 1. The study of the methods, sources, and theoretical approaches used by historians in developing history as an academic discipline. 2. A body of historical work on a particular topic. 3. The history of historical writing about a particular topic. historism history The study of the past as it is described in written documents; events occurring before written record are generally considered prehistory. The term is also commonly used to refer to any set of events which happened earlier in time, written or otherwise. History in academic study is considered the product of our attempts to understand the past, rather than the past itself. History relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. history from below See people's history. history of science homily human history 1. The complete narrative of humanity's past, generally as reckoned from the emergence of anatomically modern humans c. 300,000 years ago to the present day (though sometimes inclusive of much earlier periods in human evolution), and thereby encompassing both prehistory and written history. 2. The scientific study of this narrative, as it is understood through archaeology, anthropology, genetics, linguistics, and since the advent of writing, from primary and secondary written sources. humanism An intellectual movement of the Renaissance associated with the re-discovery of classical ideas. I Iapetus Ocean A relatively small ocean that existed",
    "label": 0
  },
  {
    "text": "through archaeology, anthropology, genetics, linguistics, and since the advent of writing, from primary and secondary written sources. humanism An intellectual movement of the Renaissance associated with the re-discovery of classical ideas. I Iapetus Ocean A relatively small ocean that existed between the continents of Laurentia, Baltica, and Avalonia from the Late Precambrian to the Devonian. illuminated manuscript A manuscript in which the text is supplemented by the addition of decoration. imperialism impresa Also impress, heraldic badge, livery badge, personal device, and cognizance. An emblem, badge, or para-heraldic device worn by nobility in the Middle Ages, usually accompanied by a motto in Latin and painted on shields or helmets in tournaments, embroidered on clothing or on equine caparisons, or embodied in standards, brooches, paintings, tapestries, or other works of art. These emblems were meant to be expressive of the character, aspirations, and achievements of a particular person, rather than an entire family or lineage, and were often designed anew for each individual occasion. incunabulum A book, pamphlet, broadside, or other printed document that was produced during the earliest period of printing in Europe but prior to the widespread adoption of mass production techniques, i.e. generally in the few decades between the invention of the printing press and the year 1501. In the following decades it became increasingly common and inexpensive to print many copies of the same text, such that incunabula from the period immediately preceding the beginning of the 16th century are disproportionately rare and valuable to historians. Incunabula are printed by definition, either as block books or with movable type, and thus are distinct from manuscripts, which are handwritten. Industrial Age information history interdisciplinary The study or practice of a subject which applies the methods and approaches of several disciplines. For instance, while history, literature and archaeology are separate",
    "label": 0
  },
  {
    "text": "type, and thus are distinct from manuscripts, which are handwritten. Industrial Age information history interdisciplinary The study or practice of a subject which applies the methods and approaches of several disciplines. For instance, while history, literature and archaeology are separate disciplines, they may be combined in an interdisciplinary approach. interpretation The ensemble of procedures by which the historian–according to personal perspective, temperament, social conditioning, and conscious choice–imposes a pattern of meaning or significance on his subject; the process of selection, arrangement, accentuation, and synthesis of historical facts that establishes the personal stamp of an individual historian on an account of the past. interregnum (pl.) interregna A gap or discontinuity in the rule, administration, or activity of a government, organization, or social institution, especially in the rule of a monarchical dynasty; i.e. the period of time between the end of the reign of one monarch and the beginning of the reign of the next monarch, during which a monarch belonging to a different dynasty reigned, or during which no monarch reigned. The term usually refers to the temporary dissolution, usurpation, or replacement of the ruling dynasty, or of the monarchy itself, with another dynasty or a different form of government entirely, followed by the eventual restoration of the original dynasty or government, and also generally implies a period of widespread social unrest, civil wars or wars of succession, or power vacuums in which foreign invaders come to prominence. interwar period In the history of the 20th century, the period between the end of World War I on 11 November 1918 and the beginning of World War II on 1 September 1939; more generally, the term may refer to the period between any two successive wars. Iron Age J Jacobean journal A scholarly periodical devoted to publishing academic writings, often related to",
    "label": 0
  },
  {
    "text": "the beginning of World War II on 1 September 1939; more generally, the term may refer to the period between any two successive wars. Iron Age J Jacobean journal A scholarly periodical devoted to publishing academic writings, often related to a particular historical theme. Julian calendar L lacuna A gap in a manuscript, inscription, or text. landscape history Also called landscape archaeology. The study of the ways in which humanity has changed the physical appearance and landscapes of the surrounding environment in the past, and how they continue to change in the present. late modern period Laurasia A supercontinent that existed from the Jurassic to Early Tertiary after splitting from Pangaea. It was composed of Laurentia, Baltica, and Avalonia (what is now North America, Scandinavia, Greenland, and Western and Central Europe), and eventually fragmented into Eurasia and North America in the Tertiary with the opening of the North Atlantic Ocean. Laurentia A separate continental plate that existed from the Late Precambrian to Silurian, consisting of the major part of what is now North America, northwest Ireland, Scotland, Greenland, and pieces of Norway and Russia. legend local history The study of the history of a small geographical area, of a local community, or of the local incidence of broader national or international trends. If undertaken with a view to casting light on larger historical questions, local history may be regarded as a branch of microhistory. longue durée An approach to the study of history popularized by the French Annales School which gives priority to long-term historical processes and phenomena, concentrating on all-but-permanent or slowly evolving structures from which broad patterns and trends can be interpreted, in contrast to the more traditional focus on the lives of specific individuals and specific events that occurred at specific points in time. lore See folklore.",
    "label": 0
  },
  {
    "text": "all-but-permanent or slowly evolving structures from which broad patterns and trends can be interpreted, in contrast to the more traditional focus on the lives of specific individuals and specific events that occurred at specific points in time. lore See folklore. M macrohistory The study of large, long-term trends in world history, undertaken in order to uncover ultimate patterns that cut across the more specific details of diverse historical cultures. manuscript Any document written by hand, as opposed to one that is printed, typed, or reproduced in some other way. manuscriptology The study of history and literature through the use and interpretation of handwritten documents. The term is similar to codicology but is primarily used among historians of South Asia, especially India, because many of the historical manuscripts produced there are not considered codices in the strictest sense. microhistory The intensive historical investigation of a small and narrow unit of research (e.g. a specific event, community, or individual person, even an object or idea), generally undertaken with a view to casting light on broader historical questions. Local history may be considered a branch of microhistory. Middle Ages Also called the medieval period. The period in the history of Europe and the Near East lasting from approximately the 5th century to the 15th century AD, usually considered to have begun with the collapse of the Western Roman Empire c. AD 476 and to have ended with the transition to the Renaissance and the discovery of the Americas in the late 1400s. The Middle Ages can be seen as part of the broader post-classical period of world history, and as the middle of the three traditional divisions of Western history, preceded by classical antiquity and followed by the modern period. The medieval period itself is often subdivided into the Early, High, and Late",
    "label": 0
  },
  {
    "text": "post-classical period of world history, and as the middle of the three traditional divisions of Western history, preceded by classical antiquity and followed by the modern period. The medieval period itself is often subdivided into the Early, High, and Late Middle Ages. migration The movement of human beings from one place to another with the intention of settling, permanently or temporarily, at a new location. Human migrations have been defining components of the history of every settled place and a major driver of economic, cultural, and linguistic exchange between populations, so historians often emphasize the importance of studying their causes, paths, and effects. military history The study of the history of armed conflict and its impact on society. It may range from the study of specific military actions and engagements to the much broader examination of warfare as a political tool. modern history Also called the modern period or modernity. modernity 1. The state of being modern, by any of various definitions of the term. 2. The historical period defined by modern history, with various starting and ending points but sometimes inclusive of the present day (i.e. contemporary history), especially when used generically to contrast the recent or current state of human civilization with previous eras. 3. The ensemble of sociocultural norms, attitudes, practices, ideas, and beliefs associated with this period, often with an emphasis on those originating in the Renaissance, the Enlightenment, the Industrial Age, and/or the early modern period. monograph A piece of writing, especially a book or an essay, that is the product of detailed, specialized research, often by a single author, on a particular subject or an aspect of a subject, e.g. a specific historical phenomenon, person, place, or event. myth mythology The collected body of myths shared by a culture or a group of people,",
    "label": 0
  },
  {
    "text": "often by a single author, on a particular subject or an aspect of a subject, e.g. a specific historical phenomenon, person, place, or event. myth mythology The collected body of myths shared by a culture or a group of people, or the academic study of such myths. N narrative history The practice of writing about history in a story-like form, using literary elements commonly found in storytelling to relate the course of actual historical events, such as a central theme or narrative arc and a final climax or resolution. Real historical figures may be presented as \"characters\" identifiable as protagonists or antagonists. national memory A form of collective memory shared by the people of a particular country or nation and defined by their common experiences, history, ethnicity, society, or culture. The idea is associated with nationalism and is an integral part of national identity. nationalism nationalization of history natural history A domain of inquiry involving organisms including animals, fungi, and plants in their natural environments which leans more towards observational than experimental methods of study. notaphily The study and collection of paper currency and banknotes. numismatics The study and collection of all forms of currency, including coins, tokens, paper money, medals, and other means of payment used to resolve debts and exchange goods. O official history A work of history which is sponsored, authorized, or endorsed by its subject, such as an authorized biography; or a narrative which is the accepted or conventional interpretation of historical events as formally proclaimed or endorsed by a government or institution, particularly as it is distinguished from alternative narratives or interpretations. one-place study A type of family history, local history, or microhistory which describes and analyzes the people or events living in or associated with a single place, such as a building, road, neighborhood,",
    "label": 0
  },
  {
    "text": "is distinguished from alternative narratives or interpretations. one-place study A type of family history, local history, or microhistory which describes and analyzes the people or events living in or associated with a single place, such as a building, road, neighborhood, village, or community, or any other geographic area, during a particular time period. This contrasts with studies united by other themes, such as a history of a specific family lineage, whose members may have been geographically dispersed, or of specific types of events which may have occurred in more than one place. onomastics Also onomatology. The study of the etymology, history, and use of proper names. oral history 1. The collection and study of historical information obtained from individuals or families via some form of oral or verbal communication (e.g. planned interviews, public speeches, or everyday conversation, or audiotapes or videotapes of these events), as opposed to information obtained from written documents or other non-verbal sources. Oral history strives to record and preserve knowledge that cannot be obtained in other ways (e.g. stories told by people who are illiterate, or passed down by cultures who do not have a writing system), particularly from people who directly participated in or observed past events first-hand. Knowledge transmitted orally is unique in that it often shares the tacit or subconscious perspectives, thoughts, and opinions of the speaker, which might otherwise be excluded from written accounts, along with nuances particular to unplanned, off-the-cuff conversation, where the speaker has not had time to prepare their responses and is unable to change them after the fact. 2. Any information gathered in this manner, or any work of history, written or otherwise, which records transcripts of orally communicated accounts. original order A concept in archival theory which proposes that a group of records should be maintained in",
    "label": 0
  },
  {
    "text": "2. Any information gathered in this manner, or any work of history, written or otherwise, which records transcripts of orally communicated accounts. original order A concept in archival theory which proposes that a group of records should be maintained in the same order as they were placed by their creator. origo gentis In medieval studies, the origin story of a particular person or group of people as recounted and interpreted by the person or people themselves, often detailing their chronological history, sometimes by combining actual events with myths and folklore. P paleography Also palaeography. The study of historic writing systems, especially very old or ancient ones, and the deciphering, dating, and authentication of historical manuscripts, with a focus on the forms, processes, and methods of writing, in particular the analysis of handwriting, rather than the textual contents of documents. Paleo-Tethys Ocean A large ocean that originated between eastern Gondwana, Siberia, Kazakhstan, and Baltica in the Ordovician and finally closed in the Jurassic. It was replaced by the Tethys Ocean as eastern Pangaea was assembled. palimpsest Pangaea Also spelled Pangea. A supercontinent that existed from the end of the Permian to the Jurassic, assembled from large continents like Euramerica, Gondwana, and Siberia, as well as smaller landmasses like the Cathaysian and Cimmerian Terranes. The name Pangaea is Greek for “all lands”. Pannotia A supercontinent that existed in the Late Precambrian and gave rise to the continents of Gondwana, Laurentia, Siberia, and Baltica in the Cambrian. Panthalassic Ocean Also called the Panthalassa. A vast ocean that existed from the Late Precambrian to the Jurassic, circling the globe and connecting to smaller oceans that developed throughout the Phanerozoic. past The entire set or any subset of events which happened previously in time. people's history Also called history from below. 1. A type of",
    "label": 0
  },
  {
    "text": "the Jurassic, circling the globe and connecting to smaller oceans that developed throughout the Phanerozoic. past The entire set or any subset of events which happened previously in time. people's history Also called history from below. 1. A type of historical narrative which attempts to account for historical events from the perspective of ordinary people rather than leaders or authority figures, using a bottom-up approach that rejects elite perspectives, instead emphasizing those of the poor, the disenfranchised, the oppressed, nonconformists, social or cultural minorities, and any group that otherwise exists on the margins of society. 2. History for and about the majority of the population, especially that which is highly accessible and relevant to the people as a whole, as opposed to history that is intended for or only accessible to well-educated audiences or serious scholars. periodization The process or study of categorizing the past into discrete, quantified, and named periods or blocks of time, e.g. the Bronze Age, the Middle Ages, the Victorian Era, etc. This is often done to facilitate the analysis of history and the causality that might have linked specific events, resulting in descriptive abstractions that provide convenient labels for periods of time with relatively unique or stable characteristics, though the time periods represented by these labels often overlap because their beginnings and ends are imprecisely defined. In reality, history is continuous and not generalized, and therefore all systems of periodization are more or less arbitrary. phaleristics The study of military orders, decorations, and medals. philately The study of postage stamps. philology The study of language in oral and written historical sources, in particular literary texts, involving the establishment of their authenticity and original form and the determination of their meaning. The discipline lies at the intersection of textual criticism, literary criticism, history, and linguistics. political",
    "label": 0
  },
  {
    "text": "in oral and written historical sources, in particular literary texts, involving the establishment of their authenticity and original form and the determination of their meaning. The discipline lies at the intersection of textual criticism, literary criticism, history, and linguistics. political history The study of past events, ideas, movements, and leaders in politics. popular history post-classical history precolonial history prehistory Also pre-literary history. The period of human history between the use of the first stone tools by hominin apes (c. 3.3 million years ago) and the invention of the earliest forms of writing (c. 5,000 years ago), the latter of which marks the beginning of conventional history. The distinction between prehistory and history – i.e. between those events that occurred before the advent of writing and those that occurred after – is important because the scientific study of prehistoric events relies on very different methods from those used to study historic events. In the absence of written records, prehistory can only be understood through the interpretation of physical artefacts, fossils, and preserved archaeological contexts, combined with inferences based on research from other disciplines of the natural sciences, in particular anthropology, evolutionary biology, and geology. The prehistoric period also does not have a universally consistent end date, because human populations invented or adopted writing at different times in different places. See also protohistory. presentism The application of present-day ideas and perspectives to depictions or interpretations of the past. primary source Material from or directly related to the past. The term usually refers to written records and documents created during the period that is being studied, such as diaries, letters, legal documents, accounts, photographs, and news reports, but may also in the broadest sense include cultural artefacts. Contrast secondary source. prosopography The study of collective biography; the examination of a historical group of",
    "label": 0
  },
  {
    "text": "is being studied, such as diaries, letters, legal documents, accounts, photographs, and news reports, but may also in the broadest sense include cultural artefacts. Contrast secondary source. prosopography The study of collective biography; the examination of a historical group of individuals, e.g. those in a common occupation, institution, or place, through a collective study of their lives. protohistory 1. A period between prehistory and history during which a particular civilization or culture has not yet developed writing but during which other contemporary cultures have already noted in their own writings the existence of the pre-literate culture. For example, the cultures of ancient Celtic and Germanic tribes are considered protohistoric when they began appearing in contemporary Greek and Roman sources. 2. The transition period between the advent of literacy in a society and the earliest surviving writings of the first historians to emerge from that society. provenance The chronology of the ownership, custody, or location of a historical object, document, or group of records. pseudohistory A type of pseudoscholarship that attempts to distort or misrepresent the historical record, often using methods resembling those in legitimate historical research and frequently in service to a particular political, religious, or personal agenda. Works of pseudohistory share some features with other types of pseudoscience, such as treating myths, legends, and other unreliable sources as literal historical truth; emphasizing historical sources that appear to support the pseudohistorical thesis while ignoring or dismissing those that contradict it; and conflating possibility with actuality, assuming that if something could have happened, then it did. psychohistory public history A range of activities undertaken by people with some training in the discipline of history, but who are generally working outside of specialized academic settings. Q quantitative history An approach to historical research that makes use of quantitative, statistical, and computer-based tools.",
    "label": 0
  },
  {
    "text": "of activities undertaken by people with some training in the discipline of history, but who are generally working outside of specialized academic settings. Q quantitative history An approach to historical research that makes use of quantitative, statistical, and computer-based tools. R radical history History practiced as a form of social protest; i.e. history written in conscious opposition to perceived social injustice and dedicated to the furtherance of progressive political and social change. Practitioners of radical history believe that historians are morally obligated to relate their research to the struggle for positive change and to use the study of the past for the betterment of the present and the future. From their standpoint, knowledge of the past is not valuable for its own sake but only insofar as it may be used to serve some social purpose. radiocarbon dating recorded history reenactment See historical reenactment. reference work A text, usually in the form of a dictionary or encyclopedia, which contains facts and information but typically not discussions. regnal year A year of the reign of a particular sovereign or monarch, with the date considered as an ordinal rather than a cardinal number, e.g. \"the third year in the reign of King Henry VIII\". Regnal dating systems were widely used in historical times to date specific events and official records, including documents of parliamentary sessions in the United Kingdom until 1963, when the Gregorian calendar was instead adopted as the formal dating convention. Renaissance respect des fonds An archival principle which proposes that collections of archival records should be ordered and preserved according to the administration, organization, individual, or entity by which they were created or from which they were received. retronym retrospective revisionist history Any approach to history in which a previously held interpretation of history or of an historical event",
    "label": 0
  },
  {
    "text": "according to the administration, organization, individual, or entity by which they were created or from which they were received. retronym retrospective revisionist history Any approach to history in which a previously held interpretation of history or of an historical event is revised. In the most general usage, every original historian may be said to be a revisionist historian, because the simple act of generating a new understanding of the past necessarily challenges or re-interprets the body of historical knowledge about a subject, though the term may also refer more specifically to re-interpretations of the mainstream or \"orthodox\" views on a particular time period or event, a practice known as historical revisionism, or, with the much more negative connotation of distorting the historical record in service of a political agenda, to historical negationism. revolution Rodinia A supercontinent that existed during the Late Precambrian before the supercontinent Pannotia, and the oldest supercontinent for which scientists have a good record. The name Rodinia is Russian for \"homeland\". Romanticism A cultural and intellectual movement of the late 18th to mid-19th centuries that emphasized emotion and sentiment rather than reason, predominantly among Western European cultures but also in other parts of the world. S saeculum A length of time approximately equal to the potential lifetime of a human being or, equivalently, to the time it takes to completely regenerate a human population with new individuals – that is, the duration between the moment at which an event occurs (such as the founding of a city) and the point in time at which every individual who was alive at the first moment has died. Scientific Revolution seal A device for making an impression, usually in wax, clay, or lead, or the impression so formed, which historically was commonly used to authenticate documents on the rationale that",
    "label": 0
  },
  {
    "text": "was alive at the first moment has died. Scientific Revolution seal A device for making an impression, usually in wax, clay, or lead, or the impression so formed, which historically was commonly used to authenticate documents on the rationale that a carefully crafted symbol or image would be difficult for counterfeiters to precisely replicate. second modernity secondary source Material created by somebody removed from the event being studied; i.e. someone who was contemporaneous with the event but not physically present to witness it, or who was working from a period of time after the event occurred. All historical textbooks, for example, are secondary sources. Contrast primary source. sensory history Siberia Also called Angaraland, Angara or Angarida. A separate continental plate that existed from the Latest Precambrian to the Carboniferous, composed of a large part of what is now central Russia, namely the modern region of Siberia. sigillography The study of the seals and symbols used to authenticate documents, variously made by impressing an image into wax, clay, lead, or another substance. social history A branch of history that studies human societies of the past, particularly social structures, hierarchies, and expectations and how they have changed over time, often by detailing the experiences of ordinary people in the past. Space Age statistics The study of the collection, organization, and interpretation of (historical) data. Stone Age The first of the three periods into which prehistory is traditionally divided, during which stone was widely used by early hominins to make tools with an edge, a point, or a percussion surface. It preceded the Bronze Age and the Iron Age but spanned a period of time far longer than either of them, usually considered to have begun as early as 3.4 million years ago and to have ended with the advent of metalworking and",
    "label": 0
  },
  {
    "text": "Bronze Age and the Iron Age but spanned a period of time far longer than either of them, usually considered to have begun as early as 3.4 million years ago and to have ended with the advent of metalworking and particularly copper smelting, which were adopted at different times in different parts of the world but generally between 4000 BCE and 2000 BCE, after which bronze became widespread and supplanted stone in many uses. stratigraphy In archaeology, a key concept in interpreting a site through establishing the relative chronology of its separate physical contexts. subaltern In postcolonial studies and critical theory, the colonial populations that are socially, politically, and/or geographically excluded from the hierarchy of power of an imperial colony and from the metropolitan homeland of the colonial empire, often deliberately in order to deny their agency and voices in colonial politics. subaltern studies T teleology A mode of historical interpretation that holds that events move towards a definite end state or goal. terminus ante quem (TAQ) The latest time at which a specific, punctual event could possibly have occurred, as indicated by placing the event relative to any other events whose dates are known with certainty. The concept establishes a limit after which an event could not have occurred based on logical expectations about the progression of a chronology, e.g. the decree of a law that is known to have been decreed by a specific monarch could not have occurred after the monarch's death. terminus post quem (TPQ) The earliest time at which a specific, punctual event could possibly have occurred, as indicated by placing the event relative to any other events whose dates are known with certainty. The concept establishes a limit before which an event could not have occurred based on logical expectations about the progression of",
    "label": 0
  },
  {
    "text": "have occurred, as indicated by placing the event relative to any other events whose dates are known with certainty. The concept establishes a limit before which an event could not have occurred based on logical expectations about the progression of a chronology, e.g. a battle in a which a specific person is known to have been killed could not have occurred before the person's date of birth (or any other securely dated event in the person's life). Tethys Ocean A small ocean that existed from the Triassic to the Jurassic. As Pangaea was split into Gondwana and Laurasia in the Jurassic, an arm developed westward called the Tethys Seaway or Tethys Sea. three-age system The periodization of human history into three time periods. The most common example is the division of prehistory into the Stone Age, Bronze Age, and Iron Age, though the concept may also refer to other tripartite divisions of historic time periods. time The indefinite continued progress of existence and events that occur in an apparently irreversible succession from the past, through the present, and into the future. timeline A list of historical events presented in chronological order, typically of a tabular or graphical design, especially in the form of a line labeled with specific dates or ranges of dates and the contemporaneous events that occurred on those dates; often the length of the line scales to the duration of time it represents, allowing viewers to quickly and easily comprehend the order of events and the relative amounts of time between them. timeliness The quality of punctuality and proximity to a historical event, as a means of assessing the reliability of a source. Timeliness is an important consideration in determining the reliability of historical records because records produced contemporaneously with an event are generally considered more accurate",
    "label": 0
  },
  {
    "text": "and proximity to a historical event, as a means of assessing the reliability of a source. Timeliness is an important consideration in determining the reliability of historical records because records produced contemporaneously with an event are generally considered more accurate than records produced at a later time. top-down approach An approach to historical scholarship that emphasizes the experiences and perspectives of elites and leaders, as opposed to average people. Contrast bottom-up approach. toponymy The study of placenames. transhistoricity The quality of a concept or entity that persists throughout human history and is not governed or defined by the frame of reference of a particular time and place. translatio imperii translatio studii An historiographical concept originating in the Middle Ages in which history is viewed as a linear succession of transfers of knowledge and learning from one place and time to another. For example, ancient Rome was commonly seen as having inherited the knowledge, ideas, and cultural values of the ancient Hellenistic civilizations which had preceded it. transnational history typescript A document that is typewritten, i.e. produced using a typewriter or a digital computer, as opposed to a manuscript, which is handwritten. typology In archaeology, the classification of artifacts, buildings, and field monuments according to their physical characteristics; an important tool for managing large quantities of archaeological data. U universal history A work that aims to present a complete history of all mankind as a whole, coherent unit, including all times, nations, peoples, and events in recorded history, insofar as a scientific treatment of them is possible. unwitting testimony Unintentional evidence provided by historical sources, e.g. by authors whose writings reveal the implicit or subconscious attitudes, beliefs, or preconceptions of the author or of the society or culture to which the author belongs, even when the author did not intend to",
    "label": 0
  },
  {
    "text": "provided by historical sources, e.g. by authors whose writings reveal the implicit or subconscious attitudes, beliefs, or preconceptions of the author or of the society or culture to which the author belongs, even when the author did not intend to do so. The interpretation of unwitting testimony by historians acknowledges that primary sources may contain valuable information about the past which is not explicit or deliberate. urban history V Victorian W warfare Also simply called war. Whig history A mode of historical interpretation which presents the past as an inevitable progression towards ever greater liberty and enlightenment; or, more broadly, any teleological or goal-directed narrative that assumes the inevitability of progress in human civilization. women's history The study of the role that women have played in history, with particular emphasis on the growth of women's rights, individual women and groups of women of historical significance, and the effects that historical events have had on women. Inherent in the discipline is the belief that more traditional approaches to history have minimized or ignored the contributions of women and the impacts of political, social, and technological change on women's lives; in this respect, women's history is often practiced as a form of historical revisionism, seeking to challenge the orthodox historical consensus and make it more inclusive. world history written history Y yuga See also Index of history articles Outline of history References General information \"A Glossary of Archival and Records Terminology\". Society of American Archivists. Retrieved 2013-02-24. USMP Glossary: Paleontology DCMI Metadata Terms",
    "label": 0
  },
  {
    "text": "The following outline is provided as an overview of and topical guide to history: History – discovery, collection, organization, and presentation of information about past events. History can also mean the period of time after writing was invented (the beginning of recorded history). Nature of history History can be described as all of the following: Academic discipline – body of knowledge given to – or received by – a disciple (student); a branch or sphere of knowledge, or field of study, that an individual has chosen to specialise in. one of the humanities – academic discipline that study the human condition, using methods that are primarily analytical, critical, or speculative, as distinguished from the mainly empirical approaches of the natural sciences. Field of science – widely recognized category of specialized expertise within science, and typically embodies its own terminology and nomenclature. Such a field will usually be represented by one or more scientific journals, where peer-reviewed research is published. There are many sociology-related scientific journals. Social science – field of academic scholarship that explores aspects of human society. Essence of history Chronology – science of arranging events in their order of occurrence in time, such as in historical timelines. Past – totality of events which occurred before a given point in time. The past is contrasted with and defined by the present and the future. The concept of the past is derived from the linear fashion in which human observers experience time, and is accessed through memory and recollection. The past is the domain of history. Time – measure in which events can be ordered from the past through the present into the future, and also the measure of durations of events and the intervals between them. Time is often referred to as the fourth dimension, along with the three",
    "label": 0
  },
  {
    "text": "which events can be ordered from the past through the present into the future, and also the measure of durations of events and the intervals between them. Time is often referred to as the fourth dimension, along with the three spatial dimensions. History describes what happened where, but also when (in time) those events took place. Historical disciplines Archaeology – study of past human cultures through the recovery, documentation and analysis of material remains and environmental data Archontology – study of historical offices and important positions in state, international, political, religious and other organizations and societies Art history – study of changes in and social context of art Chronology – locating events in time Cultural history – study of culture in the past Diplomatic history – study of the historical foreign policy and diplomacy of states History of science – study of the emergence and development of scientific inquiry Economic history – the study of economics in the past Environmental history – study of natural history and the human relationship with the natural world Futurology – study of the future: researches the medium to long-term future of societies and of the physical world Historiography – both the study of the methodology of historians and development of history as a discipline, and also to a body of historical work on a particular subject. The historiography of a specific topic covers how historians have studied that topic using particular sources, techniques, and theoretical approaches. History painting – painting of works of art having historical motifs or depicting great events Intellectual history LGBTQ history - study of LGBTQ people and their culture around the world Local history – study of history in a geographically local context Military history – study of warfare and wars in history Naval history – branch of military history",
    "label": 0
  },
  {
    "text": "history - study of LGBTQ people and their culture around the world Local history – study of history in a geographically local context Military history – study of warfare and wars in history Naval history – branch of military history devoted to warfare at sea or in bodies of water Paleography – study of ancient texts Philosophy of history – philosophical study of history and its discipline. Political history – study of past political events, ideas, movements, and leaders Public history – presentation of history to public audiences and other areas typically outside academia Psychohistory – study of the psychological motivations of historical events Social history – study of societies and social trends in the past Universal history – study of trends and dynamics in world history Urban history – historical nature of cities and towns, and the process of urbanization Women's history – study of the roles of women throughout history World history – study of global or transnational historical patterns Auxiliary sciences of history Auxiliary sciences of history – scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Auxiliary sciences of history include, but are not limited to: Archeology – study of ancient and historic sites and artifacts Chronology – study of the sequence of past events Cliometrics – systematic application of economic theory, econometric techniques, and other formal or mathematical methods to the study of history Codicology – study of books as physical objects Diplomatics – study and textual analysis of historical documents Epigraphy – study of ancient inscriptions Faleristics – study of military orders, decorations and medals Genealogy – study of family relationships Heraldry – study of armorial devices Numismatics – study of coins Onomastics – study of proper names Paleography – study of old handwriting Philately –",
    "label": 0
  },
  {
    "text": "Faleristics – study of military orders, decorations and medals Genealogy – study of family relationships Heraldry – study of armorial devices Numismatics – study of coins Onomastics – study of proper names Paleography – study of old handwriting Philately – study of postage stamps Philology – study of the language of historical sources Prosopography – investigation of a historical group of individuals through a collective study of their lives Radiocarbon dating – assignation of dates to artefacts from the distant past Sigillography – study of seals Statistics – study of the collection, organization, and interpretation of (historical) data Toponymy – study of place-names History by period History of Earth Human history List of decades, centuries, and millennia News History by chronology Chronology of the Universe Formation and evolution of the Solar System Geological history of Earth Evolutionary history of life Human history Universal history Ancient history Prehistory Classical antiquity Post-classical history Early Middle Ages High Middle Ages Late Middle Ages Modern history Early modern period Late modern period Contemporary history Pre-Columbian Mesoamerican chronology Renaissance Future history Ages of history Prehistoric Ages Stone Age Paleolithic Lower Paleolithic – (Homo, Stone tools, spread of Homo Erectus to Eurasia, control of fire, and later spears, pigments, constructed shelter) Middle Paleolithic – (Recent African origin of modern humans, Homo sapiens, Homo neanderthalensis; clothing, beads, burial, bedding, bone tools) Upper Paleolithic – (behavioral modernity, atlatl, domestication of dogs) Mesolithic – (microliths, bow, canoes) Neolithic – (domestication, nomadic pastoralism, agriculture, proto-cities) Stone Age Levant Tell Halaf Ubaid period Neolithic Europe – (Linear Pottery, Vinča culture) Neolithic China Neolithic South Asia Mehrgarh Paleo-Indians (Americas) Chalcolithic (Copper Age) – (Yamna culture, Corded Ware) Uruk period Europe – (Metallurgy in pre-Columbian Mesoamerica) Historic Ages Ancient Age Bronze Age Iron Age Postclassical Age (Middle Ages) Early Middle Ages High Middle",
    "label": 0
  },
  {
    "text": "China Neolithic South Asia Mehrgarh Paleo-Indians (Americas) Chalcolithic (Copper Age) – (Yamna culture, Corded Ware) Uruk period Europe – (Metallurgy in pre-Columbian Mesoamerica) Historic Ages Ancient Age Bronze Age Iron Age Postclassical Age (Middle Ages) Early Middle Ages High Middle Ages Late Middle Ages Modern Age Early modern Age Late Modern Age Contemporary Age Other Ages Axial Age Dark Age Viking Age Age of Discovery Age of Reason Age of Enlightenment Industrial Age Atomic Age Information Age Space Age Regional histories Regional history Ancient Egypt Babylonia India Classics Ancient Greece Ancient Rome Ancient China Mesoamerica History by continent and country List of histories by country Timeline of country and capital changes since 1001 CE Timeline of ancient country changes before 1001 CE Economic history by region Military history by region Eras by region Era Chinese Eras Japanese Eras Korean Eras Vietnamese Eras History by field History of art History of art History of the performing arts History of dance History of film History of music History of opera History of theatre History of visual arts History of architecture (timeline) History of design History of drawing History of film History of painting History of photography History of sculpture History of culture Cultural history History of archaeology (timeline) History of banking History of cooking History of games History of chess History of journalism History of literature History of money History of poetry History of rape History of sport History of mathematics History of mathematics (timeline) History of algebra History of arithmetic History of calculus History of geometry History of trigonometry History of logic History of statistics History of philosophy History of philosophy (timeline) History of ethics History of normative ethics History of meta-ethics History of humanism History of logic History of metaphysics History of transhumanism History of Western philosophy History of religions",
    "label": 0
  },
  {
    "text": "logic History of statistics History of philosophy History of philosophy (timeline) History of ethics History of normative ethics History of meta-ethics History of humanism History of logic History of metaphysics History of transhumanism History of Western philosophy History of religions History of religions (timeline) Axial Age Evolutionary origin of religions The Bible and history History of Buddhism (timeline) History of Christianity (timeline) Historical Jesus History of the Catholic Church History of Protestantism History of the Puritans History of creationism History of Hinduism History of Islam (timeline) History of Judaism History of Taoism History of Wicca History of science History of science History of science in general History of scientific method Theories/sociology of science Historiography History of pseudoscience By era History of science in early cultures History of science in Classical Antiquity History of science in the Middle Ages History of science in the Renaissance Scientific Revolution History of natural science History of biology History of biochemistry History of physical science History of nature History of astronomy (timeline) History of chemistry History of ecology History of geography History of geology (timeline) History of meteorology (timeline) History of tornado research History of oceanography History of physics History of social sciences History of the social sciences Business history Historiography History of anthropology History of archaeology (timeline) History of criminal justice History of economic thought History of education History of geography History of linguistics History of management History of marketing History of political science History of psychology (timeline) History of science and technology History of scientific method History of sociology (timeline) Legal history (history of law) History of technology History of technology Aviation history History of agricultural science History of agriculture History of architecture (timeline) History of artificial intelligence History of biotechnology History of cartography History of communication History of computer science History of",
    "label": 0
  },
  {
    "text": "of law) History of technology History of technology Aviation history History of agricultural science History of agriculture History of architecture (timeline) History of artificial intelligence History of biotechnology History of cartography History of communication History of computer science History of programming languages (timeline) History of software engineering History of electromagnetism History of engineering History of chemical engineering History of electrical engineering History of materials science History of measurement History of medicine History of transport Industrial history Military history List of battles List of wars Timeline of historic inventions History of interdisciplinary fields Classics History of ideas Methods and tools Prosopography – a methodological tool for the collection of all known information about individuals within a given period Historical revisionism – traditionally used in a completely neutral sense to describe the work or ideas of a historian who has revised a previously accepted view of a particular topic Historiography – study of historical methodology General concepts Annals Big History Centuries Chronicle Chronology Decades Era Evolution Family history Future Future history Genealogy Historian Historical classification Historical revisionism – reinterpretation of orthodox views on evidence, motivations, and decision-making processes surrounding a historical event. Though the word revisionism is sometimes used in a negative way, constant revision of history is part of the normal scholarly process of writing history. Historical thinking – scholastic reasoning skills applied to historical content, including chronological thinking, historical comprehension, historical analysis and interpretation, historical research capabilities, and historical issues analysis and decision making. History is written by the victors History of science and technology Timeline of historic inventions Timeline of electromagnetism and classical optics Timeline of mathematics Timeline of atomic and subatomic physics Identity Landscape history List of time periods Marxist historiography Millennium Mythology Narrative Oral tradition Oral history Palaeography Past Periodization Political history Prehistory Present Pseudohistory Social history",
    "label": 0
  },
  {
    "text": "of electromagnetism and classical optics Timeline of mathematics Timeline of atomic and subatomic physics Identity Landscape history List of time periods Marxist historiography Millennium Mythology Narrative Oral tradition Oral history Palaeography Past Periodization Political history Prehistory Present Pseudohistory Social history Social change Virtual history Historians Antiquity Greco-Roman world China Middle Ages Byzantine sphere Latin sphere Islamic world East Asia India Renaissance to early modern Renaissance Europe Early modern period Middle East and Islamic Empires East Asia Modern historians Historians flourishing post-1815, born post-1770 Historians born in the 19th century Historians born in the 20th century Lists Index of history articles Outline of archaeology Outline of classical studies Outline of medieval history Outline of the Renaissance List of historians List of timelines References Further reading Williams, H. S. (1907). The historians' history of the world. (ed., This is Book 1 of 25 Volumes; PDF version is available) Wells, H. G. (1921). The outline of history, being a plain history of life and mankind. (ed., This is Book 1 of multi-volume set.) External links Internet History Sourcebooks Project See also Internet History Sourcebooks Project. Collections of public domain and copy-permitted historical texts presented cleanly (without advertising or excessive layout) for educational use. WWW-VL: History Central Catalogue Archived 2006-06-21 at the Wayback Machine first history on the WWW, located at European University Institute BBC History Site History of things provides quality historical information about countries, sports, brands, music and many other facts and stuff.",
    "label": 0
  },
  {
    "text": "A biography, or simply bio, is a detailed description of a person's life. It involves more than just basic facts like education, work, relationships, and death; it portrays a person's experience of these life events. Unlike a profile or curriculum vitae (résumé), a biography presents a subject's life story, highlighting various aspects of their life, including intimate details of experience, and may include an analysis of the subject's personality. Biographical works are usually non-fiction, but fiction can also be used to portray a person's life. One in-depth form of biographical coverage is called legacy writing. Works in diverse media, from literature to film, form the genre known as biography. An authorized biography is written with the permission, cooperation, and at times, participation of a subject or a subject's heirs. An unauthorized biography is one written without such permission or participation. An autobiography is written by the person themselves, sometimes with the assistance of a collaborator or ghostwriter. History At first, biographical writings were regarded merely as a subsection of history with a focus on a particular individual of historical importance. The independent genre of biography as distinct from general history writing, began to emerge in the 18th century and reached its contemporary form at the turn of the 20th century. Historical biography Biography is the earliest literary genre in history. According to Egyptologist Miriam Lichtheim, writing took its first steps toward literature in the context of the private tomb funerary inscriptions. These were commemorative biographical texts recounting the careers of deceased high royal officials. The earliest biographical texts are from the 26th century BC. In the 21st century BC, another famous biography was composed in Mesopotamia about Gilgamesh. One of the five versions could be historical. From the same region a couple of centuries later, according to another famous biography,",
    "label": 0
  },
  {
    "text": "the 26th century BC. In the 21st century BC, another famous biography was composed in Mesopotamia about Gilgamesh. One of the five versions could be historical. From the same region a couple of centuries later, according to another famous biography, departed Abraham. He and his 3 descendants became subjects of ancient Hebrew biographies whether fictional or historical. Xenophon (c. 430 – 355/354 BC) wrote the biography of Cyrus, the founder of the Persian Empire. One of the earliest Roman biographers was Cornelius Nepos, who published his work Excellentium Imperatorum Vitae (\"Lives of outstanding generals\") in 44 BC. Longer and more extensive biographies were written in Greek by Plutarch, in his Parallel Lives, published about 80 A.D. In this work famous Greeks are paired with famous Romans, for example, the orators Demosthenes and Cicero, or the generals Alexander the Great and Julius Caesar; some fifty biographies from the work survive. Another well-known collection of ancient biographies is De vita Caesarum (\"On the Lives of the Caesars\") by Suetonius, written about AD 121 in the time of the emperor Hadrian. Meanwhile, in the eastern imperial periphery, Gospel described the life of Jesus. In the early Middle Ages (AD 400 to 1450), there was a decline in awareness of the classical culture in Europe. During this time, the only repositories of knowledge and records of the early history in Europe were those of the Roman Catholic Church. Hermits, monks, and priests used this historic period to write biographies. Their subjects were usually restricted to the church fathers, martyrs, popes, and saints. Their works were meant to be inspirational to the people and vehicles for conversion to Christianity (see Hagiography). One significant secular example of a biography from this period is the life of Charlemagne by his courtier Einhard. In Medieval Western India, there",
    "label": 0
  },
  {
    "text": "were meant to be inspirational to the people and vehicles for conversion to Christianity (see Hagiography). One significant secular example of a biography from this period is the life of Charlemagne by his courtier Einhard. In Medieval Western India, there was a Sanskrit Jain literary genre of writing semi-historical biographical narratives about the lives of famous persons called Prabandhas. Prabandhas were written primarily by Jain scholars from the 13th century onwards and were written in colloquial Sanskrit (as opposed to Classical Sanskrit). The earliest collection explicitly titled Prabandha- is Jinabhadra's Prabandhavali (1234 CE). In Medieval Islamic Civilization (c. AD 750 to 1258), similar traditional Muslim biographies of Muhammad and other important figures in the early history of Islam began to be written, beginning the Prophetic biography tradition. Early biographical dictionaries were published as compendia of famous Islamic personalities from the 9th century onwards. They contained more social data for a large segment of the population than other works of that period. The earliest biographical dictionaries initially focused on the lives of the prophets of Islam and their companions, with one of these early examples being The Book of The Major Classes by Ibn Sa'd al-Baghdadi. And then began the documentation of the lives of many other historical figures (from rulers to scholars) who lived in the medieval Islamic world. By the late Middle Ages, biographies became less church-oriented in Europe as biographies of kings, knights, and tyrants began to appear. The most famous of such biographies was Le Morte d'Arthur by Sir Thomas Malory. The book was an account of the life of the fabled King Arthur and his Knights of the Round Table. Following Malory, the new emphasis on humanism during the Renaissance promoted a focus on secular subjects, such as artists and poets, and encouraged writing in the",
    "label": 0
  },
  {
    "text": "the life of the fabled King Arthur and his Knights of the Round Table. Following Malory, the new emphasis on humanism during the Renaissance promoted a focus on secular subjects, such as artists and poets, and encouraged writing in the vernacular. Giorgio Vasari's Lives of the Artists (1550) was the landmark biography focusing on secular lives. Vasari made celebrities of his subjects, as the Lives became an early \"bestseller\". Two other developments are noteworthy: the development of the printing press in the 15th century and the gradual increase in literacy. Biographies in the English language began appearing during the reign of Henry VIII. John Foxe's Actes and Monuments (1563), better known as Foxe's Book of Martyrs, was essentially the first dictionary of the biography in Europe, followed by Thomas Fuller's The History of the Worthies of England (1662), with a distinct focus on public life. Influential in shaping popular conceptions of pirates, A General History of the Pyrates (1724), by Charles Johnson, is the prime source for the biographies of many well-known pirates. A notable early collection of biographies of eminent men and women in the United Kingdom was Biographia Britannica (1747–1766) edited by William Oldys. The American biography followed the English model, incorporating Thomas Carlyle's view that biography was a part of history. Carlyle asserted that the lives of great human beings were essential to understanding society and its institutions. While the historical impulse would remain a strong element in early American biography, American writers carved out a distinct approach. What emerged was a rather didactic form of biography, which sought to shape the individual character of a reader in the process of defining national character. Emergence of the genre The first modern biography, and a work that exerted considerable influence on the evolution of the genre, was James",
    "label": 0
  },
  {
    "text": "which sought to shape the individual character of a reader in the process of defining national character. Emergence of the genre The first modern biography, and a work that exerted considerable influence on the evolution of the genre, was James Boswell's The Life of Samuel Johnson, a biography of lexicographer and man-of-letters Samuel Johnson published in 1791. While Boswell's personal acquaintance with his subject only began in 1763, when Johnson was 54 years old, Boswell covered the entirety of Johnson's life by means of additional research. Itself an important stage in the development of the modern genre of biography, it has been claimed to be the greatest biography written in the English language. Boswell's work was unique in its level of research, which involved archival study, eye-witness accounts and interviews, its robust and attractive narrative, and its honest depiction of all aspects of Johnson's life and character – a formula which serves as the basis of biographical literature to this day. Biographical writing generally stagnated during the 19th century – in many cases there was a reversal to the more familiar hagiographical method of eulogizing the dead, similar to the biographies of saints produced in Medieval times. A distinction between mass biography and literary biography began to form by the middle of the century, reflecting a breach between high culture and middle-class culture. However, the number of biographies in print experienced a rapid growth, thanks to an expanding reading public. This revolution in publishing made books available to a larger audience of readers. In addition, affordable paperback editions of popular biographies were published for the first time. Periodicals began publishing a sequence of biographical sketches. Autobiographies became more popular, as with the rise of education and cheap printing, modern concepts of fame and celebrity began to develop. Autobiographies were written",
    "label": 0
  },
  {
    "text": "biographies were published for the first time. Periodicals began publishing a sequence of biographical sketches. Autobiographies became more popular, as with the rise of education and cheap printing, modern concepts of fame and celebrity began to develop. Autobiographies were written by authors, such as Charles Dickens (who incorporated autobiographical elements in his novels) and Anthony Trollope (his Autobiography appeared posthumously, quickly becoming a bestseller in London), philosophers, such as John Stuart Mill, churchmen – John Henry Newman – and entertainers – P. T. Barnum. Modern biography The sciences of psychology and sociology were ascendant at the turn of the 20th century and would heavily influence the new century's biographies. The demise of the \"great man\" theory of history was indicative of the emerging mindset. Human behavior would be explained through Darwinian theories. \"Sociological\" biographies conceived of their subjects' actions as the result of the environment, and tended to downplay individuality. The development of psychoanalysis led to a more penetrating and comprehensive understanding of the biographical subject, and induced biographers to give more emphasis to childhood and adolescence. Clearly these psychological ideas were changing the way biographies were written, as a culture of autobiography developed, in which the telling of one's own story became a form of therapy. The conventional concept of heroes and narratives of success disappeared in the obsession with psychological explorations of personality. British critic Lytton Strachey revolutionized the art of biographical writing with his 1918 work Eminent Victorians, consisting of biographies of four leading figures from the Victorian era: Cardinal Manning, Florence Nightingale, Thomas Arnold, and General Gordon. Strachey set out to breathe life into the Victorian era for future generations to read. Up until this point, as Strachey remarked in the preface, Victorian biographies had been \"as familiar as the cortège of the undertaker\", and wore",
    "label": 0
  },
  {
    "text": "Gordon. Strachey set out to breathe life into the Victorian era for future generations to read. Up until this point, as Strachey remarked in the preface, Victorian biographies had been \"as familiar as the cortège of the undertaker\", and wore the same air of \"slow, funereal barbarism.\" Strachey defied the tradition of \"two fat volumes ... of undigested masses of material\" and took aim at the four iconic figures. His narrative demolished the myths that had built up around these cherished national heroes, whom he regarded as no better than a \"set of mouth bungled hypocrites\". The book achieved worldwide fame due to its irreverent and witty style, its concise and factually accurate nature, and its artistic prose. In the 1920s and 1930s, biographical writers sought to capitalize on Strachey's popularity by imitating his style. This new school featured iconoclasts, scientific analysts, and fictional biographers and included Gamaliel Bradford, André Maurois, and Emil Ludwig, among others. Robert Graves (I, Claudius, 1934) stood out among those following Strachey's model of \"debunking biographies.\" The trend in literary biography was accompanied in popular biography by a sort of \"celebrity voyeurism\", in the early decades of the century. This latter form's appeal to readers was based on curiosity more than morality or patriotism. By World War I, cheap hard-cover reprints had become popular. The decades of the 1920s witnessed a biographical \"boom.\" American professional historiography gives a limited role to biography, preferring instead to emphasize deeper social and cultural influences. Political biographers historically incorporated moralizing judgments into their work, with scholarly biography being an uncommon genre before the mid-1920s. Allan Nevins was a major contributor in the 1930s to the multivolume Dictionary of American Biography. Nevins also sponsored a series of long political biographies. Later biographers sought to show how political figures balanced power",
    "label": 0
  },
  {
    "text": "uncommon genre before the mid-1920s. Allan Nevins was a major contributor in the 1930s to the multivolume Dictionary of American Biography. Nevins also sponsored a series of long political biographies. Later biographers sought to show how political figures balanced power and responsibility. However, many biographers found that their subjects were not as morally pure as they originally thought, and young historians after 1960 tended to be more critical. The exception is Robert Remini whose books on Andrew Jackson idolize its hero and fends off criticisms. The study of decision-making in politics is important for scholarly political biographers, who can take different approaches such as focusing on psychology/personality, bureaucracy/interests, fundamental ideas, or societal forces. However, most documentation favors the first approach, which emphasizes personalities. Biographers often neglect the voting blocs and legislative positions of politicians and the organizational structures of bureaucracies. A more promising approach is to locate a person's ideas through intellectual history, but this has become more difficult with the philosophical shallowness of political figures in recent times. Political biography can be frustrating and challenging to integrate with other fields of political history. The feminist scholar Carolyn Heilbrun observed that women's biographies and autobiographies began to change character during the second wave of feminist activism. She cited Nancy Milford's 1970 biography Zelda, as the \"beginning of a new period of women's biography, because \"[only] in 1970 were we ready to read not that Zelda had destroyed Fitzgerald, but Fitzgerald her: he had usurped her narrative.\" Heilbrun named 1973 as the turning point in women's autobiography, with the publication of May Sarton's Journal of a Solitude, for that was the first instance where a woman told her life story, not as finding \"beauty even in pain\" and transforming \"rage into spiritual acceptance,\" but acknowledging what had previously been forbidden to",
    "label": 0
  },
  {
    "text": "May Sarton's Journal of a Solitude, for that was the first instance where a woman told her life story, not as finding \"beauty even in pain\" and transforming \"rage into spiritual acceptance,\" but acknowledging what had previously been forbidden to women: their pain, their rage, and their \"open admission of the desire for power and control over one's life.\" Recent years In recent years, multimedia biography has become more popular than traditional literary forms. Along with documentary biographical films, Hollywood produced numerous commercial films based on the lives of famous people. The popularity of these forms of biography have led to the proliferation of TV channels dedicated to biography, including A&E, The Biography Channel, and The History Channel. CD-ROM and online biographies have also appeared. Unlike books and films, they often do not tell a chronological narrative: instead they are archives of many discrete media elements related to an individual person, including video clips, photographs, and text articles. Biography-Portraits were created in 2001, by the German artist Ralph Ueltzhoeffer. Media scholar Lev Manovich says that such archives exemplify the database form, allowing users to navigate the materials in many ways. General \"life writing\" techniques are a subject of scholarly study. In recent years, debates have arisen as to whether all biographies are fiction, especially when authors are writing about figures from the past. President of Wolfson College at Oxford University, Hermione Lee argues that all history is seen through a perspective that is the product of one's contemporary society and as a result, biographical truths are constantly shifting. So, the history biographers write about will not be the way that it happened; it will be the way they remembered it. Debates have also arisen concerning the importance of space in life-writing. Daniel R. Meister in 2017 argued that: Biography Studies",
    "label": 0
  },
  {
    "text": "history biographers write about will not be the way that it happened; it will be the way they remembered it. Debates have also arisen concerning the importance of space in life-writing. Daniel R. Meister in 2017 argued that: Biography Studies is emerging as an independent discipline, especially in the Netherlands. This Dutch School of biography is moving biography studies away from the less scholarly life writing tradition and towards history by encouraging its practitioners to utilize an approach adapted from microhistory. Biographical research Biographical research is defined by Miller as a research method that collects and analyses a person's whole life, or portion of a life, through the in-depth and unstructured interview, or sometimes reinforced by semi-structured interview or personal documents. It is a way of viewing social life in procedural terms, rather than static terms. The information can come from \"oral history, personal narrative, biography and autobiography\" or \"diaries, letters, memoranda and other materials\". The central aim of biographical research is to produce rich descriptions of persons or \"conceptualise structural types of actions\", which means to \"understand the action logics or how persons and structures are interlinked\". This method can be used to understand an individual's life within its social context or understand the cultural phenomena. Critical issues There are many largely unacknowledged pitfalls to writing good biographies, and these largely concern the relation between firstly the individual and the context, and, secondly, the private and public. Paul James writes: The problems with such conventional biographies are manifold. Biographies usually treat the public as a reflection of the private, with the private realm being assumed to be foundational. This is strange given that biographies are most often written about public people who project a persona. That is, for such subjects the dominant passages of the presentation of themselves in",
    "label": 0
  },
  {
    "text": "with the private realm being assumed to be foundational. This is strange given that biographies are most often written about public people who project a persona. That is, for such subjects the dominant passages of the presentation of themselves in everyday life are already formed by what might be called a 'self-biofication' process. Book awards Several countries offer an annual prize for writing a biography such as the: Drainie-Taylor Biography Prize – Canada National Biography Award – Australia Pulitzer Prize for Biography or Autobiography – United States Whitbread Prize for Best Biography – United Kingdom J. R. Ackerley Prize for Autobiography – United Kingdom Prix Goncourt de la Biographie – France See also Historiography Historiography of science Historiography of the United Kingdom Historiography of the United States Legal biography Letter collection Psychobiography Autobiography Notes References Further reading Gosse, Edmund William (1911). \"Biography\" . In Chisholm, Hugh (ed.). Encyclopædia Britannica. Vol. 3 (11th ed.). Cambridge University Press. pp. 952–954. Sidney Lee (1911), Principles of Biography, London: Cambridge University Press, Wikidata Q107333538 Solomon, Maynard (2001). \"Biography\". Grove Music Online. Oxford: Oxford University Press. doi:10.1093/gmo/9781561592630.article.41156. ISBN 978-1-56159-263-0. (subscription, Wikilibrary access, or UK public library membership required) External links \"Biography\", In Our Time, BBC Radio 4 discussion with Richard Holmes, Nigel Hamilton and Amanda Foreman (June 22, 2000).",
    "label": 0
  },
  {
    "text": "Environmental history is the study of human interaction with the natural world over time, emphasising the active role nature plays in influencing human affairs and vice versa. Environmental history first emerged in the United States out of the environmental movement of the 1960s and 1970s, and much of its impetus still stems from present-day global environmental concerns. The field was founded on conservation issues but has broadened in scope to include more general social and scientific history and may deal with cities, population or sustainable development. As all history occurs in the natural world, environmental history tends to focus on particular time-scales, geographic regions, or key themes. It is also a strongly multidisciplinary subject that draws widely on both the humanities and natural science. The subject matter of environmental history can be divided into three main components. The first, nature itself and its change over time, includes the physical impact of humans on the Earth's land, water, atmosphere and biosphere. The second category, how humans use nature, includes the environmental consequences of increasing population, more effective technology and changing patterns of production and consumption. Other key themes are the transition from nomadic hunter-gatherer communities to settled agriculture in the Neolithic Revolution, the effects of colonial expansion and settlements, and the environmental and human consequences of the Industrial and technological revolutions. Finally, environmental historians study how people think about nature – the way attitudes, beliefs and values influence interaction with nature, especially in the form of myths, religion and science. Origin of name and early works In 1967, Roderick Nash published Wilderness and the American Mind, a work that has become a classic text of early environmental history. In an address to the Organization of American Historians in 1969 (published in 1970) Nash used the expression \"environmental history\", although 1972 is",
    "label": 0
  },
  {
    "text": "Wilderness and the American Mind, a work that has become a classic text of early environmental history. In an address to the Organization of American Historians in 1969 (published in 1970) Nash used the expression \"environmental history\", although 1972 is generally taken as the date when the term was first coined. The 1959 book by Samuel P. Hays, Conservation and the Gospel of Efficiency: The Progressive Conservation Movement, 1890–1920, while being a major contribution to American political history, is now also regarded as a founding document in the field of environmental history. Hays is professor emeritus of History at the University of Pittsburgh. Alfred W. Crosby's book The Columbian Exchange (1972) is another key early work of environmental history. Historiography Brief overviews of the historiography of environmental history have been published by J. R. McNeill, Richard White, and J. Donald Hughes. In 2014 Oxford University Press published a volume of 25 essays in The Oxford Handbook of Environmental History. Definition There is no universally accepted definition of environmental history. In general terms it is a history that tries to explain why our environment is like it is and how humanity has influenced its current condition, as well as commenting on the problems and opportunities of tomorrow. Donald Worster's widely quoted 1988 definition states that environmental history is the \"interaction between human cultures and the environment in the past\". In 2001, J. Donald Hughes defined the subject as the \"study of human relationships through time with the natural communities of which they are a part in order to explain the processes of change that affect that relationship\". and, in 2006, as \"history that seeks understanding of human beings as they have lived, worked and thought in relationship to the rest of nature through the changes brought by time\". \"As a method,",
    "label": 0
  },
  {
    "text": "of change that affect that relationship\". and, in 2006, as \"history that seeks understanding of human beings as they have lived, worked and thought in relationship to the rest of nature through the changes brought by time\". \"As a method, environmental history is the use of ecological analysis as a means of understanding human history...an account of changes in human societies as they relate to changes in the natural environment\". Environmental historians are also interested in \"what people think about nature, and how they have expressed those ideas in folk religions, popular culture, literature and art\". In 2003, J. R. McNeill defined it as \"the history of the mutual relations between humankind and the rest of nature\". Subject matter Traditional historical analysis has over time extended its range of study from the activities and influence of a few significant people to a much broader social, political, economic, and cultural analysis. Environmental history further broadens the subject matter of conventional history. In 1988, Donald Worster stated that environmental history \"attempts to make history more inclusive in its narratives\" by examining the \"role and place of nature in human life\", and in 1993, that \"Environmental history explores the ways in which the biophysical world has influenced the course of human history and the ways in which people have thought about and tried to transform their surroundings\". The interdependency of human and environmental factors in the creation of landscapes is expressed through the notion of the cultural landscape. Worster also questioned the scope of the discipline, asking: \"We study humans and nature; therefore can anything human or natural be outside our enquiry?\" Environmental history is generally treated as a subfield of history. But some environmental historians challenge this assumption, arguing that while traditional history is human history – the story of people and",
    "label": 0
  },
  {
    "text": "can anything human or natural be outside our enquiry?\" Environmental history is generally treated as a subfield of history. But some environmental historians challenge this assumption, arguing that while traditional history is human history – the story of people and their institutions, \"humans cannot place themselves outside the principles of nature\". In this sense, they argue that environmental history is a version of human history within a larger context, one less dependent on anthropocentrism (even though anthropogenic change is at the center of its narrative). Dimensions J. Donald Hughes responded to the view that environmental history is \"light on theory\" or lacking theoretical structure by viewing the subject through the lens of three \"dimensions\": nature and culture, history and science, and scale. This advances beyond Worster's recognition of three broad clusters of issues to be addressed by environmental historians although both historians recognize that the emphasis of their categories might vary according to the particular study as, clearly, some studies will concentrate more on society and human affairs and others more on the environment. Themes Several themes are used to express these historical dimensions. A more traditional historical approach is to analyse the transformation of the globe's ecology through themes like the separation of man from nature during the Neolithic Revolution, imperialism and colonial expansion, exploration, agricultural change, the effects of the Industrial and technological revolution, and urban expansion. More environmental topics include human impact through influences on forestry, fire, climate change, sustainability and so on. According to Paul Warde, \"the increasingly sophisticated history of colonization and migration can take on an environmental aspect, tracing the pathways of ideas and species around the globe and indeed is bringing about an increased use of such analogies and 'colonial' understandings of processes within European history.\" The importance of the colonial enterprise in",
    "label": 0
  },
  {
    "text": "on an environmental aspect, tracing the pathways of ideas and species around the globe and indeed is bringing about an increased use of such analogies and 'colonial' understandings of processes within European history.\" The importance of the colonial enterprise in Africa, the Caribbean and Indian Ocean has been detailed by Richard Grove. Much of the literature consists of case-studies targeted at the global, national and local levels. Scale Although environmental history can cover billions of years of history over the whole Earth, it can equally concern itself with local scales and brief time periods. Many environmental historians are occupied with local, regional and national histories. Some historians link their subject exclusively to the span of human history – \"every time period in human history\" while others include the period before human presence on Earth as a legitimate part of the discipline. Ian Simmons's Environmental History of Great Britain covers a period of about 10,000 years. There is a tendency to difference in time scales between natural and social phenomena: the causes of environmental change that stretch back in time may be dealt with socially over a comparatively brief period. Although at all times environmental influences have extended beyond particular geographic regions and cultures, during the 20th and early 21st centuries anthropogenic environmental change has assumed global proportions, most prominently with climate change but also as a result of settlement, the spread of disease and the globalization of world trade. History The questions of environmental history date back to antiquity, including Hippocrates, the father of medicine, who asserted that different cultures and human temperaments could be related to the surroundings in which peoples lived in Airs, Waters, Places. Scholars as varied as Ibn Khaldun and Montesquieu found climate to be a key determinant of human behavior. During the Enlightenment, there was",
    "label": 0
  },
  {
    "text": "and human temperaments could be related to the surroundings in which peoples lived in Airs, Waters, Places. Scholars as varied as Ibn Khaldun and Montesquieu found climate to be a key determinant of human behavior. During the Enlightenment, there was a rising awareness of the environment and scientists addressed themes of sustainability via natural history and medicine. However, the origins of the subject in its present form are generally traced to the 20th century. In 1929 a group of French historians founded the journal Annales, in many ways a forerunner of modern environmental history since it took as its subject matter the reciprocal global influences of the environment and human society. The idea of the impact of the physical environment on civilizations was espoused by this Annales School to describe the long-term developments that shape human history by focusing away from political and intellectual history, toward agriculture, demography, and geography. Emmanuel Le Roy Ladurie, a pupil of the Annales School, was the first to really embrace, in the 1950s, environmental history in a more contemporary form. One of the most influential members of the Annales School was Lucien Febvre (1878–1956), whose 1922 book A Geographical Introduction to History is now a classic in the field. The most influential empirical and theoretical work in the subject has been done in the United States where teaching programs first emerged and a generation of trained environmental historians is now active. In the United States environmental history as an independent field of study emerged in the general cultural reassessment and reform of the 1960s and 1970s along with environmentalism, \"conservation history\", and a gathering awareness of the global scale of some environmental issues. This was in large part a reaction to the way nature was represented in history at the time, which \"portrayed the",
    "label": 0
  },
  {
    "text": "and 1970s along with environmentalism, \"conservation history\", and a gathering awareness of the global scale of some environmental issues. This was in large part a reaction to the way nature was represented in history at the time, which \"portrayed the advance of culture and technology as releasing humans from dependence on the natural world and providing them with the means to manage it [and] celebrated human mastery over other forms of life and the natural environment, and expected technological improvement and economic growth to accelerate\". Environmental historians intended to develop a post-colonial historiography that was \"more inclusive in its narratives\". Moral and political inspiration Moral and political inspiration to environmental historians has come from American writers and activists such as Henry Thoreau, John Muir, Aldo Leopold, and Rachel Carson. Environmental history \"frequently promoted a moral and political agenda although it steadily became a more scholarly enterprise\". Early attempts to define the field were made in the United States by Roderick Nash in \"The State of Environmental History\" and in other works by frontier historians Frederick Jackson Turner, James Malin, and Walter Prescott Webb, who analyzed the process of settlement. Their work was expanded by a second generation of more specialized environmental historians such as Alfred Crosby, Samuel P. Hays, Donald Worster, William Cronon, Richard White, Carolyn Merchant, J. R. McNeill, Donald Hughes, and Chad Montrie in the United States and Paul Warde, Sverker Sorlin, Robert A. Lambert, T.C. Smout, and Peter Coates in Europe. British Empire Although environmental history was growing rapidly as a field after 1970 in the United States, it only reached historians of the British Empire in the 1990s. Gregory Barton argues that the concept of environmentalism emerged from forestry studies, and emphasizes the British imperial role in that research. He argues that imperial forestry movement in",
    "label": 0
  },
  {
    "text": "States, it only reached historians of the British Empire in the 1990s. Gregory Barton argues that the concept of environmentalism emerged from forestry studies, and emphasizes the British imperial role in that research. He argues that imperial forestry movement in India around 1900 included government reservations, new methods of fire protection, and attention to revenue-producing forest management. The result eased the fight between romantic preservationists and laissez-faire businessmen, thus giving the compromise from which modern environmentalism emerged. In recent years numerous scholars cited by James Beattie have examined the environmental impact of the Empire. Beinart and Hughes argue that the discovery and commercial or scientific use of new plants was an important concern in the 18th and 19th centuries. The efficient use of rivers through dams and irrigation projects was an expensive but important method of raising agricultural productivity. Searching for more efficient ways of using natural resources, the British moved flora, fauna and commodities around the world, sometimes resulting in ecological disruption and radical environmental change. Imperialism also stimulated more modern attitudes toward nature and subsidized botany and agricultural research. Scholars have used the British Empire to examine the utility of the new concept of eco-cultural networks as a lens for examining interconnected, wide-ranging social and environmental processes. Current practice In the United States the American Society for Environmental History was founded in 1977 while the first institute devoted specifically to environmental history in Europe was established in 1991, based at the University of St. Andrews in Scotland. In 1986, the Dutch foundation for the history of environment and hygiene Net Werk was founded and publishes four newsletters per year. In the UK the White Horse Press in Cambridge has, since 1995, published the journal Environment and History which aims to bring scholars in the humanities and biological sciences",
    "label": 0
  },
  {
    "text": "hygiene Net Werk was founded and publishes four newsletters per year. In the UK the White Horse Press in Cambridge has, since 1995, published the journal Environment and History which aims to bring scholars in the humanities and biological sciences closer together in constructing long and well-founded perspectives on present day environmental problems and a similar publication Tijdschrift voor Ecologische Geschiedenis (Journal for Environmental History) is a combined Flemish-Dutch initiative mainly dealing with topics in the Netherlands and Belgium although it also has an interest in European environmental history. Each issue contains abstracts in English, French and German. In 1999 the Journal was converted into a yearbook for environmental history. In Canada the Network in Canadian History and Environment facilitates the growth of environmental history through numerous workshops and a significant digital infrastructure including their website and podcast. Communication between European nations is restricted by language difficulties. In April 1999 a meeting was held in Germany to overcome these problems and to co-ordinate environmental history in Europe. This meeting resulted in the creation of the European Society for Environmental History in 1999. Only two years after its establishment, ESEH held its first international conference in St. Andrews, Scotland. Around 120 scholars attended the meeting and 105 papers were presented on topics covering the whole spectrum of environmental history. The conference showed that environmental history is a viable and lively field in Europe and since then ESEH has expanded to over 400 members and continues to grow and attracted international conferences in 2003 and 2005. In 1999 the Centre for Environmental History was established at the University of Stirling. Some history departments at European universities are now offering introductory courses in environmental history and postgraduate courses in Environmental history have been established at the Universities of Nottingham, Stirling and Dundee and",
    "label": 0
  },
  {
    "text": "History was established at the University of Stirling. Some history departments at European universities are now offering introductory courses in environmental history and postgraduate courses in Environmental history have been established at the Universities of Nottingham, Stirling and Dundee and more recently a Graduierten Kolleg was created at the University of Göttingen in Germany. In 2009, the Rachel Carson Center for Environment and Society (RCC), an international, interdisciplinary center for research and education in the environmental humanities and social sciences, was founded as a joint initiative of Munich's Ludwig-Maximilians-Universität and the Deutsches Museum, with the generous support of the German Federal Ministry of Education and Research. The Environment & Society Portal (environmentandsociety.org) is the Rachel Carson Center's open access digital archive and publication platform. Related disciplines Environmental history prides itself in bridging the gap between the arts and natural sciences although to date the scales weigh on the side of science. A definitive list of related subjects would be lengthy indeed and singling out those for special mention a difficult task. However, those frequently quoted include, historical geography, the history and philosophy of science, history of technology and climate science. On the biological side there is, above all, ecology and historical ecology, but also forestry and especially forest history, archaeology and anthropology. When the subject engages in environmental advocacy it has much in common with environmentalism. With increasing globalization and the impact of global trade on resource distribution, concern over never-ending economic growth and the many human inequities environmental history is now gaining allies in the fields of ecological and environmental economics. Engagement with sociological thinkers and the humanities is limited but cannot be ignored through the beliefs and ideas that guide human action. This has been seen as the reason for a perceived lack of support from traditional historians.",
    "label": 0
  },
  {
    "text": "environmental economics. Engagement with sociological thinkers and the humanities is limited but cannot be ignored through the beliefs and ideas that guide human action. This has been seen as the reason for a perceived lack of support from traditional historians. Issues The subject has a number of areas of lively debate. These include discussion concerning: what subject matter is most appropriate; whether environmental advocacy can detract from scholarly objectivity; standards of professionalism in a subject where much outstanding work has been done by non-historians; the relative contribution of nature and humans in determining the passage of history; the degree of connection with, and acceptance by, other disciplines – but especially mainstream history. For Paul Warde the sheer scale, scope and diffuseness of the environmental history endeavour calls for an analytical toolkit \"a range of common issues and questions to push forward collectively\" and a \"core problem\". He sees a lack of \"human agency\" in its texts and suggest it be written more to act: as of information for environmental scientists; incorporation of the notion of risk; a closer analysis of what it is we mean by \"environment\"; confronting the way environmental history is at odds with the humanities because it emphasises the division between \"materialist, and cultural or constructivist explanations for human behaviour\". Global sustainability Many of the themes of environmental history inevitably examine the circumstances that produced the environmental problems of the present day, a litany of themes that challenge global sustainability including: population, consumerism and materialism, climate change, waste disposal, deforestation and loss of wilderness, industrial agriculture, species extinction, depletion of natural resources, invasive organisms and urban development. The simple message of sustainable use of renewable resources is frequently repeated and early as 1864 George Perkins Marsh was pointing out that the changes we make in the environment",
    "label": 0
  },
  {
    "text": "extinction, depletion of natural resources, invasive organisms and urban development. The simple message of sustainable use of renewable resources is frequently repeated and early as 1864 George Perkins Marsh was pointing out that the changes we make in the environment may later reduce the environment's usefulness to humans so any changes should be made with great care – what we would nowadays call enlightened self-interest. Richard Grove has pointed out that \"States will act to prevent environmental degradation only when their economic interests are threatened\". Advocacy It is not clear whether environmental history should promote a moral or political agenda. The strong emotions raised by environmentalism, conservation and sustainability can interfere with historical objectivity: polemical tracts and strong advocacy can compromise objectivity and professionalism. Engagement with the political process certainly has its academic perils although accuracy and commitment to the historical method is not necessarily threatened by environmental involvement: environmental historians have a reasonable expectation that their work will inform policy-makers. A recent historiographical shift has placed an increased emphasis on inequality as an element of environmental history. Imbalances of power in resources, industry, and politics have resulted in the burden of industrial pollution being shifted to less powerful populations in both the geographic and social spheres. A critical examination of the traditional environmentalist movement from this historical perspective notes the ways in which early advocates of environmentalism sought the aesthetic preservation of middle-class spaces and sheltered their own communities from the worst effects of air and water pollution, while neglecting the plight of the less privileged. Communities with less economic and sociopolitical power often lack the resources to get involved in environmental advocacy. Environmental history increasingly highlights the ways in which the middle-class environmental movement has fallen short and left behind entire communities. Interdisciplinary research now understands historic inequality",
    "label": 0
  },
  {
    "text": "economic and sociopolitical power often lack the resources to get involved in environmental advocacy. Environmental history increasingly highlights the ways in which the middle-class environmental movement has fallen short and left behind entire communities. Interdisciplinary research now understands historic inequality as a lens through which to predict future social developments in the environmental sphere, particularly with regard to climate change. The United Nations Department of Economic and Social Affairs cautions that a warming planet will exacerbate environmental and other inequalities, particularly with regard to: \"(a) increase in the exposure of the disadvantaged groups to the adverse effects of climate change; (b) increase in their susceptibility to damage caused by climate change; and (c) decrease in their ability to cope and recover from the damage suffered.\" As an interdisciplinary field that encompasses a new understanding of social justice dynamics in a rapidly changing global climate, environmental history is inherently advocative. Declensionist narratives Narratives of environmental history tend to be what scholars call \"declensionist\", that is, accounts of increasing decline under human activity. In other words, \"declensionist\" history is a form of the \"lost golden age\" narrative that has repeatedly appeared in human thought since ancient times. Presentism and culpability Under the accusation of \"presentism\" it is sometimes claimed that, with its genesis in the late 20th century environmentalism and conservation issues, environmental history is simply a reaction to contemporary problems, an \"attempt to read late twentieth century developments and concerns back into past historical periods in which they were not operative, and certainly not conscious to human participants during those times\". This is strongly related to the idea of culpability. In environmental debate blame can always be apportioned, but it is more constructive for the future to understand the values and imperatives of the period under discussion so that causes are",
    "label": 0
  },
  {
    "text": "This is strongly related to the idea of culpability. In environmental debate blame can always be apportioned, but it is more constructive for the future to understand the values and imperatives of the period under discussion so that causes are determined and the context explained. Environmental determinism For some environmental historians \"the general conditions of the environment, the scale and arrangement of land and sea, the availability of resources, and the presence or absence of animals available for domestication, and associated organisms and disease vectors, that makes the development of human cultures possible and even predispose the direction of their development\" and that \"history is inevitably guided by forces that are not of human origin or subject to human choice\". This approach has been attributed to American environmental historians Webb and Turner and, more recently to Jared Diamond in his book Guns, Germs, and Steel, where the presence or absence of disease vectors and resources such as plants and animals that are amenable to domestication that may not only stimulate the development of human culture but even determine, to some extent, the direction of that development. The claim that the path of history has been forged by environmental rather than cultural forces is referred to as environmental determinism while, at the other extreme, is what may be called cultural determinism. An example of cultural determinism would be the view that human influence is so pervasive that the idea of pristine nature has little validity – that there is no way of relating to nature without culture. Methodology Useful guidance on the process of doing environmental history has been given by Donald Worster, Carolyn Merchant, William Cronon and Ian Simmons. Worster's three core subject areas (the environment itself, human impacts on the environment, and human thought about the environment) are generally",
    "label": 0
  },
  {
    "text": "the process of doing environmental history has been given by Donald Worster, Carolyn Merchant, William Cronon and Ian Simmons. Worster's three core subject areas (the environment itself, human impacts on the environment, and human thought about the environment) are generally taken as a starting point for the student as they encompass many of the different skills required. The tools are those of both history and science with a requirement for fluency in the language of natural science and especially ecology. In fact methodologies and insights from a range of physical and social sciences is required, there seeming to be universal agreement that environmental history is indeed a multidisciplinary subject. Some key works Chakrabarti, Ranjan (ed), Does Environmental History Matter: Shikar, Subsistence, Sustenance and the Sciences (Kolkata: Readers Service, 2006) Chakrabarti, Ranjan (ed.), Situating Environmental History (New Delhi: Manohar, 2007) Cronon, William (ed), Uncommon Ground: Toward Reinventing Nature (New York: W.W. Norton & Company, 1995) Dunlap, Thomas R., Nature and the English Diaspora: Environment and History in the United States, Canada, Australia, and New Zealand . (New York/Cambridge: Cambridge University Press, 1999) Glacken, Clarence, Traces on the Rhodian Shore: Nature and Culture in Western Thought From Ancient Times to the End of the Nineteenth Century (Berkeley: University of California Press, 1967) Griffiths, Tom and Libby Robin (eds.), Ecology and Empire: The Environmental History of Settler Societies. (Keele: Keele University Press, 1997) Grove, Richard, Green Imperialism: Colonial Expansion, Tropical Island Edens and the Origins of Environmentalism, 1600–1860. (Cambridge University Press, 1995) Headrick, Daniel, Humans Versus Nature: A Global Environmental History. (New York: Oxford University Press, 2020) Hughes, J.D., An Environmental History of the World: Humankind's Changing Role in the Community of Life (Oxford: Routledge, 2001) Hughes, J.D., \"Global Environmental History: The Long View\", Globalizations, Vol. 2 No. 3, 2005, 293–208. LaFreniere, Gilbert",
    "label": 0
  },
  {
    "text": "Oxford University Press, 2020) Hughes, J.D., An Environmental History of the World: Humankind's Changing Role in the Community of Life (Oxford: Routledge, 2001) Hughes, J.D., \"Global Environmental History: The Long View\", Globalizations, Vol. 2 No. 3, 2005, 293–208. LaFreniere, Gilbert F., 2007. The Decline of Nature: Environmental History and the Western Worldview, Academica Press, Bethesda, MD ISBN 978-1933146409 MacKenzie, John M., Imperialism and the Natural World. (Manchester University Press, 1990) McCormick, John, Reclaiming Paradise: The Global Environmental Movement. (Bloomington: Indiana University Press, 1989) Rajan, Ravi S., Modernizing Nature: Forestry and Imperial Eco-Development, 1800–1950 (Oxford: Oxford University Press, 2006) Redclift, Michael R., Frontier: Histories of Civil Society and Nature (Cambridge, MA.: The MIT Press, 2006). Stevis, Dimitris, \"The Globalizations of the Environment\", Globalizations, Vol. 2 No. 3, 2005, 323–334. Williams, Michael, Deforesting the Earth: From Prehistory to Global Crisis. An Abridgement. (Chicago: University of Chicago Press, 2006) White, Richard, The Organic Machine: The Remaking of the Columbia River. (Hill and Wang, 1996) Worster, Donald, Nature's Economy: A Study of Ecological Ideals. (Cambridge University Press, 1977) Zeilinga de Boer, Jelle and Donald Theodore Sanders, Volcanoes in Human History, The Far-reaching Effects of Major Eruptions. (Princeton: Princeton University Press, 2002) ISBN 978-0691118383 Germinal works by region In 2004 a theme issue of Environment and History 10(4) provided an overview of environmental history as practiced in Africa, the Americas, Australia, New Zealand, China and Europe as well as those with global scope. J. Donald Hughes (2006) has also provided a global conspectus of major contributions to the environmental history literature. George Perkins Marsh, Man and Nature; or, Physical Geography as Modified by Human Action, ed. David Lowenthal (Cambridge, MA: Belknap Press of Harvard University Press, 1965 ) Africa Adams, Jonathan S. and Thomas McShane, The Myth of Wild Africa: Conservation without Illusion (Berkeley: University",
    "label": 0
  },
  {
    "text": "and Nature; or, Physical Geography as Modified by Human Action, ed. David Lowenthal (Cambridge, MA: Belknap Press of Harvard University Press, 1965 ) Africa Adams, Jonathan S. and Thomas McShane, The Myth of Wild Africa: Conservation without Illusion (Berkeley: University of California Press, 1996) 266p; covers 1900 to 1980s Anderson, David; Grove, Richard. Conservation in Africa: People, Policies & Practice (1988), 355pp Bolaane, Maitseo. \"Chiefs, Hunters & Adventurers: The Foundation of the Okavango/Moremi National Park, Botswana\". Journal of Historical Geography. 31.2 (April 2005): 241–259. Carruthers, Jane. \"Africa: Histories, Ecologies, and Societies\", Environment and History, 10 (2004), pp. 379–406; Cock, Jacklyn and Eddie Koch (eds.), Going Green: People, Politics, and the Environment in South Africa (Cape Town: Oxford University Press, 1991) Dovers, Stephen, Ruth Edgecombe, and Bill Guest (eds.), South Africa's Environmental History: Cases and Comparisons (Athens: Ohio University Press, 2003) Green Musselman, Elizabeth, \"Plant Knowledge at the Cape: A Study in African and European Collaboration\", International Journal of African Historical Studies, Vol. 36, 2003, 367–392 Jacobs, Nancy J., Environment, Power and Injustice: A South African History (Cambridge: Cambridge University Press, 2003) Maathai, Wangari, Green Belt Movement: Sharing the Approach and the Experience (New York: Lantern Books, 2003) McCann, James, Green Land, Brown Land, Black Land: An Environmental History of Africa, 1800–1990 (Portsmouth: Heinemann, 1999) Showers, Kate B. Imperial Gullies: Soil Erosion and Conservation in Lesotho (2005) 346pp Steyn, Phia, \"The lingering environmental impact of repressive governance: the environmental legacy of the apartheid-era for the new South Africa\", Globalizations, 2#3 (2005), 391–403 Antarctica Pyne, S.J., The Ice: A Journey to Antarctica. (University of Iowa Press, 1986). Canada Dorsey, Kurkpatrick. The Dawn of Conservation Diplomacy: U.S.-Canadian Wildlife Protection Treaties in the Progressive Era. (Washington: University of Washington Press, 1998) Loo, Tina. States of Nature: Conserving Canada's Wildlife in the Twentieth Century. (Vancouver:",
    "label": 0
  },
  {
    "text": "(University of Iowa Press, 1986). Canada Dorsey, Kurkpatrick. The Dawn of Conservation Diplomacy: U.S.-Canadian Wildlife Protection Treaties in the Progressive Era. (Washington: University of Washington Press, 1998) Loo, Tina. States of Nature: Conserving Canada's Wildlife in the Twentieth Century. (Vancouver: UBC Press, 2006) MacDowell, Laurel Sefton. Environmental History of Canada (UBC Press, 2012) excerpt Parr, Joy. Sensing Changes: Technologies, Environments, and the Everyday, 1953–2003. (Vancouver: UBC Press, 2010) Wynn, Graeme. Canada and Arctic North America: An Environmental History. (Santa Barbara: ABC-CLIO, 2007) United States Allitt, Patrick. A Climate of Crisis: America in the Age of Environmentalism (2014), wide-ranging scholarly history since 1950s excerpt Andrews, Richard N.L., Managing the Environment, Managing Ourselves: A History of American Environmental Policy (Yale University Press, 1999) Bates, J. Leonard. \"Fulfilling American Democracy: The Conservation Movement, 1907 to 1921\", The Mississippi Valley Historical Review, (1957) 44#1 pp. 29–57. in JSTOR Browning, Judkin and Timothy Silver. An Environmental History of the Civil War (2020) online review Brinkley, Douglas G. The Wilderness Warrior: Theodore Roosevelt and the Crusade for America, (2009) excerpt and text search Carson, Rachel, Silent Spring (Cambridge, Mass. : Riverside Press, 1962) Cawley, R. McGreggor. Federal Land, Western Anger: The Sagebrush Rebellion and Environmental Politics (1993), on conservatives Cronon, William, Changes in the Land: Indians, Colonists and the Ecology of New England (New York: Hill and Wang, 1983) Cronon, William, Nature's Metropolis: Chicago and the Great West (New York: W.W. Norton & Company, 1991) Dant, Sara. Losing Eden: An Environmental History of the American West. (U of Nebraska Press, 2023). online, also see online book review Flippen, J. Brooks. Nixon and the Environment (2000). Gottlieb, Robert, Forcing the Spring: The Transformation of the American Environmental Movement (Washington: Island Press, 1993) Hays, Samuel P. Conservation and the Gospel of Efficiency (1959), on Progressive Era. Hays, Samuel",
    "label": 0
  },
  {
    "text": "review Flippen, J. Brooks. Nixon and the Environment (2000). Gottlieb, Robert, Forcing the Spring: The Transformation of the American Environmental Movement (Washington: Island Press, 1993) Hays, Samuel P. Conservation and the Gospel of Efficiency (1959), on Progressive Era. Hays, Samuel P. Beauty, Health, and Permanence: Environmental Politics in the United States, 1955–1985 (1987), the standard scholarly history Hays, Samuel, Conservation and the Gospel of Efficiency: The Progressive Conservation Movement 1890–1920 (Cambridge, MA: Harvard University Press, 1959) Hays, Samuel P. A History of Environmental Politics since 1945 (2000), shorter standard history King, Judson. The Conservation Fight, From Theodore Roosevelt to the Tennessee Valley Authority (2009) Melosi, Martin V. Coping with Abundance: Energy and Environment in Industrial America (Temple University Press, 1985) Merchant, Carolyn. American environmental history: An introduction (Columbia University Press, 2007). Merchant, Carolyn. The Columbia guide to American environmental history (Columbia University Press, 2012). Merchant, Carolyn. The Death of Nature: Women, Ecology and the Scientific Revolution (New York: Harper & Row, 1980) Nash, Roderick. The Rights of Nature: A History of Environmental Ethics (Madison: University of Wisconsin Press, 1989) Nash, Roderick. Wilderness and the American Mind, (4th ed. 2001), the standard intellectual history Rice, James D. Nature and History in the Potomac Country: From Hunter-Gatherers to the Age of Jefferson (2009) Rothman, Hal K. (1998). The Greening of a Nation? Environmentalism in the United States since 1945. Fort Worth, TX: Harcourt Brace College Publishers. ISBN 0155028553. Sale, Kirkpatrick. The Green Revolution: The American Environmental Movement, 1962–1999 (New York: Hill & Wang, 1993) Scheffer, Victor B. The Shaping of Environmentalism in America (1991). Steinberg, Ted, Down to Earth: Nature's Role in American History (Oxford University Press, 2002) Stradling, David (ed), Conservation in the Progressive Era: Classic Texts (Washington: University of Washington Press, 2004), primary sources Strong, Douglas H. Dreamers & Defenders:",
    "label": 0
  },
  {
    "text": "America (1991). Steinberg, Ted, Down to Earth: Nature's Role in American History (Oxford University Press, 2002) Stradling, David (ed), Conservation in the Progressive Era: Classic Texts (Washington: University of Washington Press, 2004), primary sources Strong, Douglas H. Dreamers & Defenders: American Conservationists. (1988) online edition Archived 2007-12-01 at the Wayback Machine, good biographical studies of the major leaders Sussman, Glen, and Byron W. Daynes. \"Spanning the Century: Theodore Roosevelt, Franklin Roosevelt, Richard Nixon, Bill Clinton, and the Environment.\" White House Studies 4.3 (2004): 337-355. Turner, James Morton, \"The Specter of Environmentalism\": Wilderness, Environmental Politics, and the Evolution of the New Right. The Journal of American History 96.1 (2009): 123–47 online at History Cooperative Unger, Nancy C., Beyond Nature's Housekeepers: American Women in Environmental History. (New York: Oxford University Press, 2012) Worster, Donald, Under Western Skies: Nature and History in the American West (Oxford University Press, 1992) Latin America and the Caribbean Boyer, Christopher R. Political Landscapes: Forests, Conservation, and Community in Mexico. (Durham: Duke University Press 2015.) Dean, Warren. With Broadax and Firebrand: The Destruction of the Brazilian Atlantic Forest. (Berkeley: University of California Press, 1995) Funes Monzote, Reinaldo. From Rainforest to Cane Field in Cuba: An Environmental History since 1492. (2008) Matthews, Andrew S. Instituting Nature: Authority, Expertise, and Power in Mexican Forests. (Cambridge: Massachusetts Institute of Technology Press, 2011.) Melville, Elinor. A Plague of Sheep: Environmental Consequences of the Conquest of Mexico. (Cambridge: Cambridge University Press, 1994) Miller, Shawn William. An Environmental History of Latin America. (2007) Miller, Shawn William. Fruitless Trees: Portuguese Conservation and Brazil's Colonial Timber. Stanford: Stanford University Press 2000. Noss, Andrew and Imke Oetting. \"Hunter Self-Monitoring by the Izoceño-Guarani in the Bolivian Chaco\". Biodiversity & Conservation. 14.11 (2005): 2679–2693. Raffles, Hugh, et al. \"Further Reflections on Amazonian Environmental History: Transformations of Rivers and Streams\".",
    "label": 0
  },
  {
    "text": "Stanford: Stanford University Press 2000. Noss, Andrew and Imke Oetting. \"Hunter Self-Monitoring by the Izoceño-Guarani in the Bolivian Chaco\". Biodiversity & Conservation. 14.11 (2005): 2679–2693. Raffles, Hugh, et al. \"Further Reflections on Amazonian Environmental History: Transformations of Rivers and Streams\". Latin American Research Review. Vol. 38, Number 3, 2003: 165–187 Santiago, Myrna I. The Ecology of Oil: Environment, Labor, and the Mexican Revolution, 1900–1938. Cambridge: Cambridge University Press 2006. Simonian, Lane. Defending the Land of the Jaguar: A History of Conservation in Mexico. (Austin: University of Texas Press, 1995) Wakild, Emily. Revolutionary Parks: Conservation, Social Justice, and Mexico's National Parks, 1910–1940. Tucson: University of Arizona Press 2012. South and South East Asia Boomgaard, Peter, ed. Paper Landscapes: Explorations in the Environment of Indonesia (Leiden: KITLV Press, 1997) David, A. & Guha, R. (eds) 1995. Nature, Culture, Imperialism: Essays on the Environmental History of South Asia. Delhi, India: Oxford University Press. Fisher, Michael. An Environmental History of India: From Earliest Times to the Twenty-First Century (Cambridge UP, 2018) Gadgil, M. and R. Guha. This Fissured Land: An Ecological History of India (University of California Press, 1993) Grove, Richard, Vinita Damodaran, and Satpal Sangwan (eds.) Nature & the Orient: The Environmental History of South and Southeast Asia (Oxford University Press, 1998) Hill, Christopher V., South Asia: An Environmental History (Santa Barbara: ABC-Clio, 2008) Shiva, Vandana, Stolen Harvest: the Hijacking of the Global Food Supply (Cambridge MA: South End Press, 2000), ISBN 0-89608-608-9 Yok-shiu Lee and Alvin Y. So, Asia's Environmental Movements: Comparative Perspectives (Armonk: M.E. Sharpe, 1999) Iqbal, Iftekhar. The Bengal Delta: Ecology, State and Social Change, 1840–1943 (London: Palgrave Macmillan, 2010) East Asia Elvin, Mark & Ts'ui-jung Liu (eds.), Sediments of Time: Environment and Society in Chinese History (Cambridge University Press, 1998) Totman, Conrad D., The Green Archipelago: Forestry in Preindustrial",
    "label": 0
  },
  {
    "text": "Ecology, State and Social Change, 1840–1943 (London: Palgrave Macmillan, 2010) East Asia Elvin, Mark & Ts'ui-jung Liu (eds.), Sediments of Time: Environment and Society in Chinese History (Cambridge University Press, 1998) Totman, Conrad D., The Green Archipelago: Forestry in Preindustrial Japan (Berkeley: University of California Press, 1989) Totman, Conrad D., Pre-industrial Korea and Japan in Environmental Perspective (Leiden: Brill, 2004) Ts'ui-jung Liu, Sediments of Time: Environment and Society in Chinese History (Cambridge University Press, 1998) Liu, Ts'ui-jung and James Beattie, eds, Environment, Modernization and Development in East Asia: Perspectives from Environmental History (Basingstoke: Palgrave Studies in World Environmental History, 2016) Tull, Malcolm, and A. R. Krishnan. \"Resource Use and Environmental Management in Japan, 1890–1990\", in: J.R. McNeill (ed), Environmental History of the Pacific and the Pacific Rim (Aldershot Hampshire: Ashgate Publishing, 2001) Menzie, Nicholas, Forest and Land Management in Late Imperial China (London, Macmillan Press. 1994) Maohong, Bao, \"Environmental History in China\", Environment and History, Volume 10, Number 4, November 2004, pp. 475–499 Marks, R. B., Tigers, rice, silk and silt. Environment and economy in late imperial South China (Cambridge: Cambridge University Press, 1998) Perdue, Peter C., \"Lakes of Empire: Man and Water in Chinese History\", Modern China, 16 (January 1990): 119–29 Shapiro, Judith, Mao's War against Nature: Politics and the Environment in Revolutionary China (New York: Cambridge University Press. 2001) ISBN 978-0521786805 Middle East and North Africa McNeill, J. R. \"The Eccentricity of the Middle East and North Africa's Environmental History.\" Water on Sand: Environmental Histories of the Middle East and North Africa (2013): 27–50. Mikhail, Alan, ed. Water on sand: Environmental histories of the Middle East and North Africa. Oxford University Press, 2013. Dursun, Selçuk. \"A call for an environmental history of the Ottoman Empire and Turkey: Reflections on the fourth ESEH conference.\" New Perspectives on Turkey 37",
    "label": 0
  },
  {
    "text": "on sand: Environmental histories of the Middle East and North Africa. Oxford University Press, 2013. Dursun, Selçuk. \"A call for an environmental history of the Ottoman Empire and Turkey: Reflections on the fourth ESEH conference.\" New Perspectives on Turkey 37 (2007): 211–222. Dursun, Selçuk. \"Forest and the state: history of forestry and forest administration in the Ottoman Empire.\" Unpublished PhD. Sabanci University (2007). Mikhail, Alan. Nature and empire in Ottoman Egypt: An environmental history. Cambridge University Press, 2011. White, Sam. \"Rethinking disease in Ottoman history.\" International Journal of Middle East Studies 42, no. 4 (2010): 549–567. * Burke III, Edmund, \"The Coming Environmental Crisis in the Middle East: A Historical Perspective, 1750–2000 CE\" (April 27, 2005). UC World History Workshop. Essays and Positions from the World History Workshop. Paper 2. Tal, Alon, Pollution in a Promised Land: An Environmental History of Israel (Berkeley: University of California Press, 2002) Europe Brimblecombe, Peter and Christian Pfister, The Silent Countdown: Essays in European Environmental History (Berlin: Springer-Verlag, 1993) Crosby, Alfred W., Ecological Imperialism: The Biological Expansion of Europe, 900–1900 (Cambridge: Cambridge University Press, 1986) Christensen, Peter, Decline of Iranshahr: Irrigation and Environments in the History of the Middle East, 500 B.C. to 1500 A.D (Austin: University of Texas Press, 1993) Ditt, Karl, 'Nature Conservation in England and Germany, 1900–1970: Forerunner of Environmental Protection?', Contemporary European History 5:1–28. Ford, Caroline. \"The Sound of Paris: An Environmental History of Noise in the City of Light\" Environment and History (2025) v.31#1 pp.1–21 online Hughes, J. Donald, Pan's Travail: Environmental Problems of the Ancient Greeks and Romans (Baltimore: Johns Hopkins, 1994) Hughes, J. Donald, The Mediterranean. An Environmental History (Santa Barbara: ABC-Clio, 2005) Martí Escayol, Maria Antònia. La construcció del concepte de natura a la Catalunya moderna (Barcelona: Universitat Autonoma de Barcelona, 2004) Netting, Robert, Balancing on",
    "label": 0
  },
  {
    "text": "(Baltimore: Johns Hopkins, 1994) Hughes, J. Donald, The Mediterranean. An Environmental History (Santa Barbara: ABC-Clio, 2005) Martí Escayol, Maria Antònia. La construcció del concepte de natura a la Catalunya moderna (Barcelona: Universitat Autonoma de Barcelona, 2004) Netting, Robert, Balancing on an Alp: Ecological Change and Continuity in a Swiss Mountain Community (Cambridge University Press, 1981) Parmentier, Isabelle, dir., Ledent, Carole, coll., La recherche en histoire de l'environnement : Belgique, Congo, Rwanda, Burundi, Namur, 2010 (Coll. Autres futurs). Stephen J. Pyne, Vestal Fire. An Environmental History, Told through Fire, of Europe and Europe's Encounter with the World (Seattle, University of Washington Press, 1997) Richards, John F., The Unending Frontier: Environmental History of the Early Modern World (Berkeley: University of California Press, 2003) Whited, Tamara L. (ed.), Northern Europe. An Environmental History (Santa Barbara: ABC-Clio, 2005) Australia, New Zealand & Oceania Beattie, James, Empire and Environmental Anxiety: Health, Science, Art and Conservation in South Asia and Australasia, 1800–1920 (Basingstoke: Palgrave Macmillan, 2011) Beattie, James, Emily O'Gorman and Matt Henry, eds, Climate, Science and Colonization: Histories from Australia and New Zealand (New York: Palgrave Macmillan, 2014). Bennett, Judith Ann, Natives and Exotics: World War II and Environment in the Southern Pacific (Honolulu: University of Hawai'i Press, 2009) Bennett, Judith Ann, Pacific Forest: A History of Resource Control and Contest in Solomon Islands, c. 1800–1997 (Cambridge and Leiden: White Horse Press and Brill, 2000) Bridgman, H. A., \"Could climate change have had an influence on the Polynesian migrations?\", Palaeogeography, Palaeoclimatology, Palaeoecology, 41(1983) 193–206. Brooking, Tom and Eric Pawson, Environmental Histories of New Zealand (Oxford: Oxford University Press, 2002). Carron, L.T., A History of Forestry in Australia (Canberra, 1985). Cassels, R., \"The Role of Prehistoric Man in the Faunal Extinctions of New Zealand and other Pacific Islands\", in Martin, P. S. and Klein, R. G.",
    "label": 0
  },
  {
    "text": "Oxford University Press, 2002). Carron, L.T., A History of Forestry in Australia (Canberra, 1985). Cassels, R., \"The Role of Prehistoric Man in the Faunal Extinctions of New Zealand and other Pacific Islands\", in Martin, P. S. and Klein, R. G. (eds.) Quaternary Extinctions: A Prehistoric Revolution (Tucson, The University of Arizona Press, 1984) D'Arcy, Paul, The People of the Sea: Environment, Identity, and History in Oceania (Honolulu: University of Hawai'i Press, 2006) Dargavel, John (ed.), Australia and New Zealand Forest Histories. Short Overviews, Australian Forest History Society Inc. Occasional Publications, No. 1 (Kingston: Australian Forest History Society, 2005) Dovers, Stephen (ed), Essays in Australian Environmental History: Essays and Cases (Oxford: OUP, 1994). Dovers, Stephen (ed.), Environmental History and Policy: Still Settling Australia (South Melbourne: Oxford University Press, 2000). Flannery, Tim, The Future Eaters, An Ecological History of the Australian Lands and People (Sydney: Reed Books,1994) ISBN 0-8021-3943-4 Garden, Don, Australia, New Zealand, and the Pacific. An Environmental History (Santa Barbara: ABC-Clio, 2005) Hughes, J. Donald, \"Nature and Culture in the Pacific Islands\", Leidschrift, 21 (2006) 1, 129–144. Hughes, J. Donald, \"Tahiti, Hawaii, New Zealand: Polynesian impacts on Island Ecosystems\", in: An Environmental History of the World. Humankind\"s Changing Role in the Community of Life, (London & New York, Routledge, 2002) James Beattie, \"Environmental Anxiety in New Zealand, 1840–1941: Climate Change, Soil Erosion, Sand Drift, Flooding and Forest Conservation\", Environment and History 9(2003): 379–392 Knight, Catherine, New Zealand's Rivers: An Environmental History (Christchurch: Canterbury University Press, 2016). McNeill, John R., \"Of Rats and Men. A Synoptic Environmental History of the Island Pacific\", Journal of World History, Vol. 5, no. 2, 299–349 Pyne, Stephen, Burning Bush: A Fire History of Australia (New York, Henry Holt, 1991). Robin, Libby, Defending the Little Desert: The Rise of Ecological Consciousness in Australia (Melbourne: MUP, 1998)",
    "label": 0
  },
  {
    "text": "Pacific\", Journal of World History, Vol. 5, no. 2, 299–349 Pyne, Stephen, Burning Bush: A Fire History of Australia (New York, Henry Holt, 1991). Robin, Libby, Defending the Little Desert: The Rise of Ecological Consciousness in Australia (Melbourne: MUP, 1998) Robin, Libby, How a Continent Created a Nation (Sydney: University of New South Wales Press, 2007) Robin, Libby, The Flight of the Emu: A Hundred Years of Australian Ornithology 1901–2001, (Melbourne: Melbourne University Press, 2000) Smith, Mike, Hesse, Paul (eds.), 23 Degrees S: Archaeology and Environmental History of the Southern Deserts (Canberra: National Museum of Australia Press, 2005) Star, Paul, \"New Zealand Environmental History: A Question of Attitudes\", Environment and History 9(2003): 463–475 Young, Ann R.M, Environmental Change in Australia since 1788 (Oxford University Press, 2000) Young, David, Our Islands, Our Selves: A History of Conservation in New Zealand ( Dunedin: Otago University Press, 2004) United Kingdom Beinart, William and Lotte Hughes, Environment and Empire (Oxford, 2007). Clapp, Brian W., An Environmental History of Britain Since the Industrial Revolution (London, 1994). excerpt Grove, Richard, Green Imperialism: Colonial Expansion, Tropical Island Edens and the Origins of Environmentalism, 1600–1860 (Cambridge, 1994). Lambert, Robert, Contested Mountains (Cambridge, 2001). Mosley, Stephen, The Chimney of the World: A History of Smoke Pollution in Victorian and Edwardian Manchester (White Horse, 2001). Porter, Dale, The Thames Embankment: Environment, Technology, and Society in Victorian London, (University of Akron, 1998). Simmonds, Ian G., Environmental History of Great Britain from 10,000 Years Ago to the Present (Edinburgh, 2001). Sheail, John, An Environmental History of Twentieth-Century Britain (Basingstoke, 2002). Thorsheim, Peter, Inventing Pollution: Coal, Smoke, and Culture in Britain since 1800 (Ohio University, 2006). Future Environmental history, like all historical studies, shares the hope that through an examination of past events it may be possible to forge a more considered future.",
    "label": 0
  },
  {
    "text": "Inventing Pollution: Coal, Smoke, and Culture in Britain since 1800 (Ohio University, 2006). Future Environmental history, like all historical studies, shares the hope that through an examination of past events it may be possible to forge a more considered future. In particular a greater depth of historical knowledge can inform environmental controversies and guide policy decisions. The subject continues to provide new perspectives, offering cooperation between scholars with different disciplinary backgrounds and providing an improved historical context to resource and environmental problems. There seems little doubt that, with increasing concern for our environmental future, environmental history will continue along the path of environmental advocacy from which it originated as \"human impact on the living systems of the planet bring us no closer to utopia, but instead to a crisis of survival\" with key themes being population growth, climate change, conflict over environmental policy at different levels of human organization, extinction, biological invasions, the environmental consequences of technology especially biotechnology, the reduced supply of resources – most notably energy, materials and water. Hughes comments that environmental historians \"will find themselves increasingly challenged by the need to explain the background of the world market economy and its effects on the global environment. Supranational instrumentalities threaten to overpower conservation in a drive for what is called sustainable development, but which in fact envisions no limits to economic growth\". Hughes also notes that \"environmental history is notably absent from nations that most adamantly reject US, or Western influences\". Michael Bess sees the world increasingly permeated by potent technologies in a process he calls \"artificialization\" which has been accelerating since the 1700s, but at a greatly accelerated rate after 1945. Over the next fifty years, this transformative process stands a good chance of turning our physical world, and our society, upside-down. Environmental historians can \"play",
    "label": 0
  },
  {
    "text": "which has been accelerating since the 1700s, but at a greatly accelerated rate after 1945. Over the next fifty years, this transformative process stands a good chance of turning our physical world, and our society, upside-down. Environmental historians can \"play a vital role in helping humankind to understand the gale-force of artifice that we have unleashed on our planet and on ourselves\". Against this background \"environmental history can give an essential perspective, offering knowledge of the historical process that led to the present situation, give examples of past problems and solutions, and an analysis of the historical forces that must be dealt with\" or, as expressed by William Cronon, \"The viability and success of new human modes of existing within the constraints of the environment and its resources requires both an understanding of the past and an articulation of a new ethic for the future.\" Related journals Key journals in this field include: Environment and History Environmental History, co-published by the American Society for Environmental History and Forest History Society Global Environment: A Journal of History and Natural and Social Sciences International Review of Environmental History See also 2020s in environmental history Environmental history of the United States Environmental history of Latin America List of environmental history topics American Society for Environmental History Conservation Movement Conservation in the United States Ecosemiotics Network in Canadian History and Environment Rachel Carson Center for Environment and Society References Bibliography Global Asia & Middle East Europe and Russia Historiography Further reading Biasillo, Roberta, Claudio de Majo, eds. \"Storytelling and Environmental History: Experiences from Germany and Italy\", RCC Perspectives: Transformations in Environment and Society 2020, no. 2. doi.org/10.5282/rcc/9116. Hall, Marcus, and Patrick Kupper (eds.), \"Crossing Mountains: The Challenges of Doing Environmental History\", RCC Perspectives 2014, no. 4. doi.org/10.5282/rcc/6510. Mauch, Christof, \"Notes From the Greenhouse: Making",
    "label": 0
  },
  {
    "text": "and Italy\", RCC Perspectives: Transformations in Environment and Society 2020, no. 2. doi.org/10.5282/rcc/9116. Hall, Marcus, and Patrick Kupper (eds.), \"Crossing Mountains: The Challenges of Doing Environmental History\", RCC Perspectives 2014, no. 4. doi.org/10.5282/rcc/6510. Mauch, Christof, \"Notes From the Greenhouse: Making the Case for Environmental History\", RCC Perspectives 2013, no. 6. doi.org/10.5282/rcc/5661. External links Podcasts Jan W.Oosthoek podcasts on many aspects of the subject including interviews with eminent environmental historians Nature's Past: Canadian Environmental History Podcast features monthly discussions about the environmental history research community in Canada. EnvirohistNZ Podcast is a podcast that looks at the environmental history of New Zealand. Institutions & resources International Consortium of Environmental History Organizations (ICE-HO) Oosthoek, K.J.W. What is environmental history? Historiographies of different countries H-Environment web resource for students of environmental history American Society for Environmental History European Society for Environmental History Environmental History Now Environmental History Resources Environmental History Timeline Environmental History on the Internet Rachel Carson Center for Environment and Society and its Environment & Society Portal Forest History Society Australian and New Zealand Environmental History Network Brazilian Environmental History Network Centre for Environmental History at the Australian National University Network in Canadian History and the Environment Centre for World Environmental History, University of Sussex Croatian journal for environmental history in croatian, english, german and slovenian Environmental History Virtual Library Environmental History Top News Environmental History Mobile Application Project HistoricalClimatology.com Explores climate history, a form of environmental history. Climate History Network Network of climate historians. Environment & Society Portal Turkish Society for Environmental History Archived 2015-08-17 at the Wayback Machine Journals JSTOR: All Volumes and Issues - Browse - Environmental History [1996–2007 (Volumes 1–12)] JSTOR: All Volumes and Issues - Browse - Forest & Conservation History [1990–1995 (Volumes 34–39)] JSTOR: All Volumes and Issues - Browse - Environmental Review: ER [1976–1989 (Volumes",
    "label": 0
  },
  {
    "text": "All Volumes and Issues - Browse - Environmental History [1996–2007 (Volumes 1–12)] JSTOR: All Volumes and Issues - Browse - Forest & Conservation History [1990–1995 (Volumes 34–39)] JSTOR: All Volumes and Issues - Browse - Environmental Review: ER [1976–1989 (Volumes 1–13)] JSTOR: All Volumes and Issues - Browse - Environmental History Review [1990–1995 (Volumes 14–19)] JSTOR: All Volumes and Issues - Browse - Journal of Forest History [1974–1989 (Volumes 18–33)] JSTOR: All Volumes and Issues - Browse - Forest History [1957–1974 (Volumes 1–17)] Environment and History, Published by White Horse Press with British-based Editorial collective Archived 2015-09-17 at the Wayback Machine Environmental History, Co-published quarterly by the American Society for Environmental History and the (US) Forest History Society Global Environment: A Journal of History and Natural and Social Sciences, Published in New Zealand with special regard to the modern and contemporary ages Historia Ambiental Latinoamericana y Caribeña (HALAC) Journal of the North Atlantic Economic and Ecohistory: Research Journal for Economic and Environmental History (Croatia) Pacific Historical Review Arcadia: Explorations in Environmental History, published by the Rachel Carson Center for Environment and Society and ESEH Articles Think About Nature Videos Notes from the Field public television episodes on U.S. environmental history subjects",
    "label": 0
  },
  {
    "text": "A historical figure is a significant person in history, who may have made important cultural, social, political, scientific or technological impacts on humanity. They are often widely known for their achievements, whether favourably or unfavourably. The significance of such figures in human progress has been debated. Some think they play a crucial role, while others say they have little impact on the broad currents of thought and social change. The concept is generally used in the sense that the person really existed in the past, as opposed to being legendary. However, the legends that can grow up around historical figures may be hard to distinguish from fact. Sources are often incomplete and may be inaccurate, particularly those from early periods of history. Without a body of personal documents, the more subtle aspects of personality of a historical figure can only be deduced. With historical figures who were also religious figures attempts to separate fact from belief may be controversial. Significance The significance of historical figures has long been the subject of debate by philosophers. Hegel (1770–1831) considered that \"world-historical figures\" played a pivotal role in human progress, but felt that they were bound to emerge when change was needed. Thomas Carlyle (1795–1881) saw the study of figures such as Muhammad, William Shakespeare and Oliver Cromwell as key to understanding history. Herbert Spencer (1820–1903), an early believer in evolution and in the universality of natural law, felt that historical individuals were of little importance. Hegel's world-historical figure The German philosopher Hegel defined the concept of the world-historical figure, who embodied the ruthless advance of Immanuel Kant's World Spirit, often overthrowing outdated structures and ideas. To him, Napoleon was such a figure. Hegel proposed that a world-historical figure essentially posed a challenge, or thesis, and this would generate an antithesis, or opposing",
    "label": 0
  },
  {
    "text": "ruthless advance of Immanuel Kant's World Spirit, often overthrowing outdated structures and ideas. To him, Napoleon was such a figure. Hegel proposed that a world-historical figure essentially posed a challenge, or thesis, and this would generate an antithesis, or opposing force. Eventually a synthesis would resolve the conflict. Hegel viewed Julius Caesar as a world historical figure, who appeared at a stage when Rome had grown to the point it could no longer continue as a republican city state but had to become an empire. Caesar failed in his bid to make himself an emperor, and was assassinated, but the empire came into existence soon afterward, and Caesar's name has become synonymous with \"emperor\" in forms such as \"kaiser\" or \"czar\". Søren Kierkegaard, in his early essay The Concept of Irony, generally agrees with Hegel's views, such as his characterization of Socrates as a world-historical figure who acted as a destructive force on Greek received views of morality. In Hegel's view, Socrates broke down social harmony by questioning the meaning of concepts like \"justice\" and \"virtue\". Eventually, the Athenians condemned Socrates to death. But they could not stop the evolution of thought that Socrates had begun, which would lead to the concept of individual conscience. Hegel said of world-historical figures, It was theirs to know this nascent principle; the necessary, directly sequent step in progress, which their world was to take; to make this their aim, and to expend their energy in promoting it ... They die early like Alexander; they are murdered, like Caesar; transported to St. Helena, like Napoleon ... They are great men, because they willed and accomplished something great; not a mere fancy, a mere intention, but that which met the case and fell in with the needs of the age. However, Hegel, Thomas Carlyle and",
    "label": 0
  },
  {
    "text": "Napoleon ... They are great men, because they willed and accomplished something great; not a mere fancy, a mere intention, but that which met the case and fell in with the needs of the age. However, Hegel, Thomas Carlyle and others noted that the great historical figures were just representative men, expressions of the material forces of history. Essentially they have little choice about what they do. This is in conflict with the views of George Bancroft or Ralph Waldo Emerson, who praised self-reliance and individualism, and in conflict with Karl Marx and Friedrich Engels, who also felt that individuals can determine their destiny. Engels found that Hegel's system contained an \"internal and incurable contradiction\", resting as it does on both dialectical relativism and idealistic absolutism. Spencerian view The Scottish philosopher and evolutionist Herbert Spencer, who was highly influential in the latter half of the nineteenth century, felt that historical figures were relatively unimportant. He wrote to a friend, \"I ignore utterly the personal element in history, and, indeed, show little respect for history altogether as it is ordinarily conceived.\" He wrote, \"The births, deaths, and marriages of kings, and other like historic trivialities, are committed to memory, not because of any direct benefits that can possibly result from knowing them: but because society considers them parts of a good education.\" In his essay What Knowledge Is of Most Worth? he wrote: That which constitutes History, properly so called, is in great part omitted from works on the subject. Only of late years have historians commenced giving us, in any considerable quantity, the truly valuable information. As in past ages the king was everything and the people nothing; So, in past histories, the doings of the king fill the entire picture, to which the national life forms but an obscure",
    "label": 0
  },
  {
    "text": "considerable quantity, the truly valuable information. As in past ages the king was everything and the people nothing; So, in past histories, the doings of the king fill the entire picture, to which the national life forms but an obscure background. While only now, when the welfare of nations rather than of rulers is becoming the dominant idea, are historians beginning to occupy themselves with the phenomena of social progress. The thing it really concerns us to know is the natural history of society. Inevitability or determinism Taken to an extreme, one may consider that what Hegel calls the \"world spirit\" and T. S. Eliot calls \"those vast impersonal forces\" hold us in their grip. What happens is predetermined. Both Hegel and Marx advocated historical inevitability in contrast to the doctrine of contingency, allowing for alternative outcomes, that was advocated by Friedrich Nietzsche, Michel Foucault and others. However, Marx argued against the use of the \"historical inevitability\" argument when used to explain the destruction of early communes in Russia. As an orthodox Marxist, Vladimir Lenin believed in the ideas of history that Marx had developed, including the historical inevitability of capitalism followed by a transition to socialism. Despite this, Lenin also believed the transition could be effected faster by voluntary action. In 1936 Karl Popper published an influential paper on The Poverty of Historicism, published as a book in 1957, that attacked the doctrine of historical inevitability. The historian Isaiah Berlin, author of Historical Inevitability, also argued forcibly against this view, going as far as to say that some choices are entirely free and cannot be predicted scientifically. Berlin presented his views in a 1953 lecture at the London School of Economics, published soon afterwards. When speaking he referred to Ludwig Wittgenstein's views, but the published version speaks approvingly of",
    "label": 0
  },
  {
    "text": "are entirely free and cannot be predicted scientifically. Berlin presented his views in a 1953 lecture at the London School of Economics, published soon afterwards. When speaking he referred to Ludwig Wittgenstein's views, but the published version speaks approvingly of Karl Popper, which caused a stir among academics. Heroic view Thomas Carlyle has espoused the \"heroic view\" of history, famously saying in his essay on the Norse god Odin in his book On heroes, hero-worship, & the heroic in history that \"No great man lives in vain. The History of the world is but the Biography of great men ... We do not now call our great men Gods, nor admire without limit; ah no, with limit enough! But if we have no great men, or do not admire at all,— that were a still worse case.\" Carlyle's historical philosophy was based on the \"Great Man theory\", saying, \"Universal History, the history of what man has accomplished in the world ... [is] at bottom the History of the Great Men who have worked here.\" An extreme believer in individuality, he also believed that the masses of people should let themselves be guided by the great leaders of men. Talking of poets he said, That ideal outline of himself, which a man unconsciously shadows forth in his writings, and which, if rightly deciphered, will be truer than any other representation of him, it is the task of the Biographer to fill-up into an actual coherent figure, and to bring him to our experience, or at least our clear undoubted admiration, thereby to instruct and edify us in many ways. Conducted on such principles, the Biography of great men, especially of great Poets, that is, of men in the highest degree noble-minded and wise, might become one of the most dignified and",
    "label": 0
  },
  {
    "text": "to instruct and edify us in many ways. Conducted on such principles, the Biography of great men, especially of great Poets, that is, of men in the highest degree noble-minded and wise, might become one of the most dignified and valuable species of composition. More recently, in his 1943 book The Hero in History, the pragmatist scholar Sidney Hook asserts: That history is made by men and women is no longer denied except by some theologians and mystical metaphysicians. And even they are compelled indirectly to acknowledge this commonplace truth, for they speak of historical personages as 'instruments' of Providence, Justice, Reason, Dialectic, the Zeitgeist, or Spirit of the Times. Men agree more readily about the consequences of the use of 'instruments' in history than they do about the ultimate ends 'instruments' allegedly serve, or the first causes by which they are allegedly determined. Hook recognizes the relevance of the environment within which the \"great man\" or \"hero\" acted, but asserts that this can provide the backdrop but never the plot of the \"dramas of human history\". and distinguish life and species Ranking There have been rankings of the significance of major historical figures. For example, Cesar A. Hidalgo and colleagues at the MIT Media Lab has calculated the memorability of historical figures using data such as the number of language editions for which there are articles for each person, the pageviews received, and other factors. These lists are available at MIT's Pantheon project. Historical truth It is sometimes hard to discern whether apparently historical figures from the earliest periods did in fact exist, due to the lack of records. Even with more recent personages, stories or anecdotes about the person often accumulate that have no basis in fact. Although the external aspects of a historical figure may be well",
    "label": 0
  },
  {
    "text": "did in fact exist, due to the lack of records. Even with more recent personages, stories or anecdotes about the person often accumulate that have no basis in fact. Although the external aspects of a historical figure may be well documented, their inner nature can only be a subject of speculation. It can also not be only a subject of speculation as many historical figures such as Hitler explicitly articulated their thoughts and intentions. With religious figures, often the subjects of voluminous literature, separating \"fact\" from \"belief\" can be difficult if not impossible. Ancient figures With older texts it can be difficult to be sure whether a person in the text is, in fact, a historical figure. \"Wisdom literature\" from early middle-eastern cultures (such as the Book of Job), mainly consist of verbal expositions or discussions that must be considered the work of the author, rather than the character supposedly speaking. It may still be possible to identify a figure in such texts with a historical figure known from some other context, and the text may be taken as informative about this figure, even if not verified by an independent source. On the other hand, a text may include realistic settings and references to historical people, while the central character may or may not be a historical figure. Fables Napoleon spoke of history as being a fable which had been agreed upon:– \"la fable convenue qu'on appellera l'histoire\". Great figures of the past have stories told about them which grow in the telling, and so become myths and legends which may dominate or displace the more prosaic historical facts about them. For example, some ancient chroniclers said that the Emperor Nero fiddled while Rome burned, but Tacitus disputed this by saying the stories were just malicious rumours. Similarly, there is",
    "label": 0
  },
  {
    "text": "may dominate or displace the more prosaic historical facts about them. For example, some ancient chroniclers said that the Emperor Nero fiddled while Rome burned, but Tacitus disputed this by saying the stories were just malicious rumours. Similarly, there is no good evidence that Marie Antoinette ever said \"let them eat cake\", or that Lady Godiva rode naked through the streets of Coventry. Personality Thomas Carlyle pointed out that even to the person living it, every life \"remains in so many points unintelligible\". The historian must struggle when writing biographies, \"the very facts of which, to say nothing of the purport of them, we know not, and cannot know!\" Some psychologists have sought to understand the personalities of historical figures through clues about the way in which they were raised. However, this theoretical psychoanalytic approach is not supported empirically. An alternative approach, favored by psychobiographers such as William Runyan, is to explain the personality of the historical figure in terms of their life history. This approach has the advantage of recognizing that personality may evolve over time in response to events. Religious figures With historical religious figures, fact and belief may be difficult to disentangle. There are cultural differences in the treatment of historical figures. Thus the Chinese can recognise that Mencius or Confucius were historical individuals, while also endowing them with sanctity. In Hinduism, on the other hand, figures such as Krishna or Rama are seen by the followers as embodiments of gods. The Nirvana Sutra states: \"Do not rely on the man but on the Dharma.\" A teacher such as Gautama Buddha is thus treated almost exclusively as a lesser god rather than a historical figure. E. P. Sanders, author of The Historical Figure of Jesus, called Jesus of Nazareth \"one of the most important figures in human",
    "label": 0
  },
  {
    "text": "such as Gautama Buddha is thus treated almost exclusively as a lesser god rather than a historical figure. E. P. Sanders, author of The Historical Figure of Jesus, called Jesus of Nazareth \"one of the most important figures in human history\". Various writers have struggled to present \"historical\" views of Jesus, as opposed to views distorted by belief. When writing about this subject, a historian who relies only on sources other than the New Testament may be criticized for implying that it is not a sufficient source of information about the subject. The theologian Martin Kähler is known for his work Der sogenannte historische Jesus und der geschichtliche, biblische Christus (The so-called historical Jesus, and the historic, biblical Christ). He clearly distinguished between \"the Jesus of history\" and \"the Christ of faith\". Some historians openly admit bias, which may anyway be unavoidable. Paul Hollenback says he writes about the historical Jesus, \"...in order to overthrow, not simply correct, the mistake called Christianity.\" Another historian who has written about Jesus, Frederick Gaiser, says, \"historical investigation is part and parcel of biblical faith.\" Political appropriation A historical figure may be interpreted to support political aims. In France in the first half of the seventeenth century, there was an outpouring of writing about Joan of Arc, including seven biographies, three plays and an epic poem. Joan had become a symbol of national pride and the Catholic faith, helping unite a country that had been divided by the recent wars of religion. The reality of the historical Joan was subordinated to the need for a symbol of feminine strength, Christian virtue and resistance to the English. George Bernard Shaw, introducing his 1923 play Saint Joan, discussed representations of Joan by other authors. He felt that William Shakespeare's depiction in Henry VI, Part 1 was",
    "label": 0
  },
  {
    "text": "a symbol of feminine strength, Christian virtue and resistance to the English. George Bernard Shaw, introducing his 1923 play Saint Joan, discussed representations of Joan by other authors. He felt that William Shakespeare's depiction in Henry VI, Part 1 was constrained from making her a \"beautiful and romantic figure\" by political considerations. Voltaire's version in his poem La Pucelle d'Orléans was also flawed by Voltaire's biases and Friedrich Schiller's play Die Jungfrau von Orleans \"is not about Joan at all, and can hardly be said to pretend to be.\" A historical figure may be used to validate a politician's claim to authority, where the modern leader shapes and exploits the ideas associated with the historical figure, which they are presumed to have inherited. Thus, Jesse Jackson has frequently evoked the spirit of Martin Luther King Jr. Fidel Castro often presented himself as following the path defined by José Martí. Hugo Chávez of Venezuela frequently identified himself with the historical figure Simón Bolívar, the liberator of South America from Spanish rule. Georg Hegel believed in the role of the state in guaranteeing individual liberties, and his views were therefore rejected by the German Nazi Party, who considered him dangerously liberal and perhaps a proto-Marxist. On the other hand, Adolf Hitler identified himself as a Hegelian world historical figure, and justified his actions on this basis. In education In education, presenting information as if it were being told by a historical figure may give it greater impact. Since classical times, students have been asked to put themselves in the place of a historical figure as a way of bringing history to life. Historical figures are often represented in fiction, where fact and fancy are combined. In earlier traditions, before the rise of a critical historical tradition, authors took less care to be",
    "label": 0
  },
  {
    "text": "historical figure as a way of bringing history to life. Historical figures are often represented in fiction, where fact and fancy are combined. In earlier traditions, before the rise of a critical historical tradition, authors took less care to be as accurate when describing what they knew of historical figures and their actions, interpolating imaginary elements intended to serve a moral purpose to events. Plato used historical figures in his writing, but only to illustrate his points. Xenophon used Cyrus the Great in the same way. When Plato apparently quotes Socrates in The Republic, it is only to add dramatic effect to the presentation of his own thought. For this reason, Plato's writings on Socrates tell us little, at least directly, about Socrates. The historical figure is used only as a device for communicating Plato's ideas. In classical Rome, students of rhetoric had to master the suasoria — a form of declamation in which they wrote the soliloquy of a historical figure who was debating a critical course of action. For example, the poet Juvenal wrote a speech for the dictator Sulla, in which he was counselled to retire. The poet Ovid enjoyed this exercise more than the other final challenge — the controversia. The German philosopher Friedrich Nietzsche wrote an influential essay \"On the Uses and Disadvantages of History for Life\". He said \"the unhistorical and historical are necessary in equal measure for the health of an individual, of a people and of a culture.\" Nietzsche identifies three approaches to history, each with dangers. The monumental approach describes the glories of the past, often focusing on heroic figures like Elizabeth I of England, King Robert the Bruce or Louis Pasteur. By treating these figures as models, the student is tempted to consider that there can be nobody of such",
    "label": 0
  },
  {
    "text": "glories of the past, often focusing on heroic figures like Elizabeth I of England, King Robert the Bruce or Louis Pasteur. By treating these figures as models, the student is tempted to consider that there can be nobody of such stature today. The antiquarian view examines the past in minute and reverent detail, turning its back on the present. The critical approach challenges traditional views, even though they may be valid. Historical figures may today be simulated as animated pedagogical agents to teach history and foreign culture. An example is Freudbot, which acted the part of Sigmund Freud for psychology students. When a variety of simulated character types were tried as educational agents, students rated historical figures as the most engaging. There are gender differences in the perception of historical figures. When modern US schoolchildren were asked to roleplay or illustrate historical stereotypes, boys tended to focus upon male figures exclusively while girls showed more varied family groupings. In branding Using historical figures in marketing communications and in branding is a new area of marketing research but historical figures’ names were used to promote products as early as in the Middle Ages. Historical figure brand is using famous historical person in branding, for instance Mozartkugel, Chopin (vodka) or Café Einstein. Historical figure is a person who lived in the past and whose deeds exerted a significant impact on other people’s lives and consciousness. These figures are attributed with certain features that are a compilation of the actual values they proclaimed and the manner they were perceived by others. This perception evolves and subsequent generations read the biography of a given historical figure in their own way through their own knowledge and experience. In order to determine the popularity of the commercialisation of historical figures, a study was conducted at the",
    "label": 0
  },
  {
    "text": "evolves and subsequent generations read the biography of a given historical figure in their own way through their own knowledge and experience. In order to determine the popularity of the commercialisation of historical figures, a study was conducted at the beginning of 2014 on the number of trademark protection applications filed with the Patent Office of the Republic of Poland as a measure of entrepreneurs’ interest in this activity. The names of 300 most prominent Polish historical figures were considered. The study showed that over 21% of the names analysed were recorded in the trademark register. 1,033 trademark protection applications were filed for 64 names out of the 300 historical figures investigated [Aldona Lipka, 2015,]. The greatest number of trademark protection applications were recorded for Mieszko (295), followed by Nicolaus Copernicus (250), John III Sobieski (94) and Chopin (81). In art and literature Realist historical fiction There is a huge body of historical fiction, where the text includes both imaginary and factual elements. In early English literature, Robin Hood was a fictional character, but the historical King Richard I of England also appears. William Shakespeare wrote plays about people who were historical figures in his day, such as Julius Caesar. He did not present these people as pure history, but dramatised their lives as a commentary about the people and politics of his own time. Napoleon figured in Victor Hugo's 1862 classic Les Misérables. There are many more examples. The compiler of a survey of historical novels in the 1920s claimed that the \"appearance of reality ... is the great charm of the historical novel.\" He went on to assert, regarding novels about periods of which little is known, that \"the danger is that the very elements which add to our interest in the tale as such will go far",
    "label": 0
  },
  {
    "text": "charm of the historical novel.\" He went on to assert, regarding novels about periods of which little is known, that \"the danger is that the very elements which add to our interest in the tale as such will go far to mislead us in our conception of the period dealt with\". Traditionally the treatment of historical figures in fiction was realistic in style and respectful of fact. A historical novel would be true to the facts known about the period in which the novel is set, a biographical novel would follow the facts that are known about the protagonist's life, and a \"roman à clef\" would try to give an accurate interpretation of what is known about a public figure's private life. In each genre, the novelist would avoid introducing any elements that were clearly in conflict with the facts. A writer may be handicapped by his readers' preconceptions about a historical person, which may or may not be accurate, and the facts about the historical person may also conflict with the novelist's plot requirements. According to the Marxist philosopher György Lukács in his 1937 book on The Historical Novel, \"The 'world-historical individual' can only figure as a minor character in the [historical] novel because of the complexity and intricacy of the whole social-historical process.\" As Jacobs observes, the \"realist aesthetic\" of the historical novel \"assumes that a recognizable historical figure in fiction must not 'do things' its model did not do in real life; it follows that historical figures can be used only in very limited ways.\" The author of a traditional historical novel should therefore focus more on the people who have been lost to history. A novelist such as Sir Walter Scott or Leo Tolstoy (War and Peace) would describe historical events accurately. They would give rein",
    "label": 0
  },
  {
    "text": "of a traditional historical novel should therefore focus more on the people who have been lost to history. A novelist such as Sir Walter Scott or Leo Tolstoy (War and Peace) would describe historical events accurately. They would give rein to their imagination only in scenes that were not significant historically, when interactions with fictional characters could safely be introduced. Modern fiction More recently, however, starting with works such as The Confessions of Nat Turner, and Sophie's Choice by William Styron, the novelist has felt more free to introduce much larger amounts of purely imaginary detail about historical people. E. L. Doctorow illustrates this different attitude when discussing his book Ragtime: \"Certain details were so delicious that I was scrupulous about getting them right. Others ... demanded to be mythologized.\" This reflects a changing attitude about the distinction between \"fact\" and \"truth\", expressed by Ursule Molinaro when he makes his Cassandra say, \"I've come as close to the truth as facts would let me ... facts oppress the truth, which can breathe freely only in poetry & art.\" Other media Many films have depicted historical figures. Often the way in which the films interpret these figures and their times reflects the social and cultural values of the period in which the film was made. Historical figures are familiar to the general reader and so may be used in speculative fiction so that readers marvel at their appearance in novel settings or with a fresh perspective. For example, the time traveler The Doctor has encountered numerous historical figures such as Marco Polo and Queen Elizabeth I in his adventures. They appeared most frequently when the television series first started, as it was directed at children and the use of historical figures in historical settings was intended to be educational. See also",
    "label": 0
  },
  {
    "text": "Historical significance is a historiographical key concept that explores and seeks to explain the selection of particular social and cultural past events for remembrance by human societies. This element of selection involved in both ascribing and analyzing historical significance is one factor in making the discipline of history distinct from the past. Historians consider knowledge of dates and events within and between specific historical periods the primary content of history, also known as \"first-order knowledge\" or substantive concepts. In contrast, historical significance is an example of a subject specific secondary key concept or \"second-order knowledge\" also known as a meta-concept, or disciplinary concept, which is typically used to help organize knowledge within a subject area, frame suitable areas of inquiry, provide the framework upon which substantive knowledge can be built, and map learner progression within a subject discipline. Specifically with regards to historical significance, the way dates and events are chosen and ascribed relative significance is not fixed and can change over time according to which criteria were used to form the judgement of significance as well as how those criteria were chosen themselves in the first place. This aspect to significance has been described as: “a flexible relationship between us and the past”. Historical significance is often regarded as involving judging why a particular person or event is remembered and why another is not, it is this aspect of reasoned and evaluative judgement about historical significance that makes history writing differ from being simply a record of past events. \"as soon as we turn to questions of significance—of why something happened versus the mere fact of its happening—history becomes an act of judgment.\" This emphasis on exploring what has been deemed significant by certain societies in contrast to what has been left out of the historical record has led",
    "label": 0
  },
  {
    "text": "something happened versus the mere fact of its happening—history becomes an act of judgment.\" This emphasis on exploring what has been deemed significant by certain societies in contrast to what has been left out of the historical record has led to historical significance often being paired with the concept of historical silence, which looks at why and how certain social class, racial, and/or ethnic groups have not featured in the historical record or whose contributions have not been seen as significant at particular times, and in particular contexts. Thus historical significance is not an intrinsic or fixed property of a particular historical event but rather more of an assessment of who, why, and how that event was judged significant enough to be remembered. With this potential fluidity in mind, it therefore follows that any assessment of historical significance should not be seen as fixed or permanent. \"historical significance is not an enduring or unchanging characteristic of any particular event. It is a contingent quality that depends on the perspective from which that event is subsequently viewed.\" Relevance A key concept for the study of history and public life in most societies regardless of topic, historical significance makes judgements about what is important to be remembered about the past and why, through its reflections on historical aspects to contemporary culture and society including historical reputations, events, issues, monuments, and what is chosen to be emphasized in history writing itself. Examining what has been included and what has been left out of the historical record can be an effective tool for guiding students to understand how cultural background affects their perception of history. The teaching of how to assess what has been considered significant and what has been left out has been described as:\"surely a fundamental corner-stone of a liberal and democratic",
    "label": 0
  },
  {
    "text": "students to understand how cultural background affects their perception of history. The teaching of how to assess what has been considered significant and what has been left out has been described as:\"surely a fundamental corner-stone of a liberal and democratic education and a pre-requisite for effective citizenship.\"The relevance of historical significance can also be demonstrated by its near ubiquitous appearance in provincial, national and international history curriculums including, but not limited to: the Singapore National Curriculum, the English National Curriculum, the IB Diploma History Guide, the New Zealand National Curriculum, and the Australian National Curriculum. Definitions There are many definitions of historical significance. For example, UNESCO includes any site as a world heritage site, provided it: \"bear[s] a unique or at least exceptional testimony to a cultural tradition or to a civilization\". Other notable examples include the following, the International Baccalaureate Diploma History Guide which includes historical significance as one of its six historical concepts alongside the other five: Perspectives, Change, Continuity, Causation and Consequence. The IBO define historical significance as including:\"the record that has been preserved through evidence or traces of the past, and/or the aspects that someone has consciously decided to record and communicate\". This definition has overlaps with that provided by the Historical Thinking Project which includes significance as one of its six key concepts of historical thinking:\"A historical person or event can acquire significance if we, the historians, can link it to larger trends and stories that reveal something important for us today\". Criteria for assessing historical significance Historical significance is typically assessed by judging an event against pre-defined criteria and numerous criteria for assessing historical significance have been proposed. However, these criteria are always subjective, and therefore debatable. There can also be important differences between what is seen as significant in terms of the dominant",
    "label": 0
  },
  {
    "text": "Historicism is an approach in the study of phenomena, particularly social and cultural practices, including ideas and beliefs, which emphasizes understanding these phenomena through the historical processes by which they developed. The concept is widely applied in fields such as philosophy, anthropology, and sociology. This historical approach to explanation differs from and complements the approach known as functionalism, which seeks to explain a phenomenon, such as for example a social form, by providing reasoned arguments about how that social form fulfills some function in the structure of a society. In contrast, rather than taking the phenomenon as a given and then seeking to provide a justification for it from reasoned principles, the historical approach asks \"Where did this come from?\" and \"What factors led up to its creation?\"; that is, historical explanations often place a greater emphasis on the role of process and contingency. In philosophy, historicism is defined as the view that an object can be fully understood only in terms of its historical development, that its values can be explained by tracing their origins, and that its nature is comprehensively revealed through its evolutionary course. This perspective does not address critiques related to the historical fallacy. The term historism is an English translation of the German Historismus, but its usage has declined over time in favor of \"historicism.\" For example, James Mark Baldwin’s 1918 Dictionary of Philosophy and Psychology includes the entry \"historism\" but omits \"historicism,\" whereas Dagobert D. Runes’ 1942 Dictionary of Philosophy emphasizes \"historicism.\" In contemporary usage, historicism is employed to describe approaches that integrate both German and Italian (storicismo) traditions and that focus on the explanation of historical processes. Mundane historicism refers to the view that examining an idea or individual within its historical context yields more accurate and comprehensive results, combining empirical observations with",
    "label": 0
  },
  {
    "text": "German and Italian (storicismo) traditions and that focus on the explanation of historical processes. Mundane historicism refers to the view that examining an idea or individual within its historical context yields more accurate and comprehensive results, combining empirical observations with conceptual considerations. Methodological historicism argues that the social sciences and the natural sciences require different methods due to the distinct nature of their subject matters, and it evaluates the non-experimental character of history within this framework. Popperian historicism grounds the search for general laws of history in both the examination of historical records and abstract theoretical arguments; Hegel’s philosophy of history, based on a dialectical process, is often cited as a classical example of this approach. Epistemic historicism draws on historical and anthropological research to suggest that modes of reasoning and conceptions of rationality change over time; Ian Hacking adopts this historical perspective, while Hilary Putnam argues that attempts to formalize reasoning—such as Carnap’s project of inductive logic—have been unsuccessful and maintains that rationality is closely tied to value judgments. Historicism is often used to help contextualize theories and narratives, and may be a useful tool to help understand how social and cultural phenomena came to be. The historicist approach differs from individualist theories of knowledge such as strict empiricism and rationalism, which does not take into account traditions. Historicism can be reductionist, often tends to be, and is usually contrasted with theories that posit that historical changes occur entirely at random. David Summers, building on the work of E. H. Gombrich, defines historicism negatively, writing that it posits \"that laws of history are formulatable and that in general the outcome of history is predictable,\" adding \"the idea that history is a universal matrix prior to events, which are simply placed in order within that matrix by the historian.\" This",
    "label": 0
  },
  {
    "text": "laws of history are formulatable and that in general the outcome of history is predictable,\" adding \"the idea that history is a universal matrix prior to events, which are simply placed in order within that matrix by the historian.\" This approach, he writes, \"seems to make the ends of history visible, thus to justify the liquidation of groups seen not to have a place in the scheme of history\" and that it has led to the \"fabrication of some of the most murderous myths of modern times.\" History of the term The term historicism (Historismus) was coined by German philosopher Karl Wilhelm Friedrich Schlegel. Over time, what historicism is and how it is practiced have developed different and divergent meanings.According to Schlegel, Winckelmann’s approach to historicism marked the beginning of a new era in philosophy by recognizing the unique character and distinctiveness of antiquity. In contrast, other eighteenth-century philosophers distorted the true nature of the ancient world by reinterpreting it through philosophical concepts. Schlegel particularly warned against theoretical views that were not linked to specific individuals and lacked a historical foundation. The following year, Novalis, while outlining different methods, employed the term Historismus, although he did not assign a precise meaning to it in this context. Elements of historicism appear in the writings of French essayist Michel de Montaigne (1533–1592) and Italian philosopher G. B. Vico (1668–1744), and became more fully developed with the dialectic of Georg Wilhelm Friedrich Hegel (1770–1831), influential in 19th-century Europe. The writings of Karl Marx, influenced by Hegel, also occasionally include historicism. The term is also associated with the empirical social sciences and with the work of Franz Boas. Historicism tends to be hermeneutic because it values cautious, rigorous, and contextualized interpretation of information; or relativist, because it rejects notions of universal, fundamental and immutable",
    "label": 0
  },
  {
    "text": "also associated with the empirical social sciences and with the work of Franz Boas. Historicism tends to be hermeneutic because it values cautious, rigorous, and contextualized interpretation of information; or relativist, because it rejects notions of universal, fundamental and immutable interpretations. In the twentieth century, the Italian philosopher Benedetto Croce, the English thinker R. G. Collingwood, and the Spanish philosopher Jose Ortega y Gasset adopted historicism as an approach aimed at enhancing the understanding of human thought and experience. Karl Popper, in his work The Poverty of Historicism, addressed historicism in relation to Hegel and Marx’s attempts to justify authority through laws of historical development. The Soviet Encyclopedia presented the term from a Marxist-Leninist perspective, emphasizing the notion of “legitimate development.” In American literary studies, New Historicism emerged as a postmodern interpretive approach that foregrounds the specificity of historical and cultural contexts, while making limited reference to earlier European debates. Variants Hegelian Hegel viewed the realization of human freedom as the ultimate purpose of history, which could be achieved only through the creation of the perfect state. Historical progress toward this state would occur through a dialectical process: the tension between the purpose of humankind (freedom) and humankind's current condition would produce the attempt by humankind to change its condition to one more in accord with its nature. However, because humans are often not aware of the goal of humanity and history, the process of achieving freedom is necessarily one of self-discovery. Hegel saw progress toward freedom as conducted by the \"spirit\" (Geist), a seemingly supernatural force that directs all human actions and interactions. Yet Hegel makes clear that the spirit is a mere abstraction that comes into existence \"through the activity of finite agents\". Thus, Hegel's determining forces of history may not have a metaphysical nature, though many of",
    "label": 0
  },
  {
    "text": "human actions and interactions. Yet Hegel makes clear that the spirit is a mere abstraction that comes into existence \"through the activity of finite agents\". Thus, Hegel's determining forces of history may not have a metaphysical nature, though many of his opponents and interpreters have understood him as holding metaphysical and determinist views. Hegel's historicism also suggests that any human society and all human activities such as science, art, or philosophy, are defined by their history. Consequently, their essence can be sought only by understanding said history. The history of any such human endeavor, moreover, not only continues but also reacts against what has gone before; this is the source of Hegel's famous dialectic teaching usually summarized by the slogan \"thesis, antithesis, and synthesis\". (Hegel did not use these terms, although Johann Fichte did.) Hegel's famous aphorism, \"Philosophy is the history of philosophy\", describes it bluntly. Hegel's position is perhaps best illuminated when contrasted against the atomistic and reductionist opinion of human societies and social activities self-defining on an ad hoc basis through the sum of dozens of interactions. Yet another contrasting model is the persistent metaphor of a social contract. Hegel considers the relationship between individuals and societies as organic, not atomic: even their social discourse is mediated by language, and language is based on etymology and unique character. It thus preserves the culture of the past in thousands of half-forgotten metaphors. To understand why a person is the way he is, you must examine that person in his society: and to understand that society, you must understand its history, and the forces that influenced it. The Zeitgeist, the \"Spirit of the Age\", is the concrete embodiment of the most important factors that are acting in human history at any given time. This contrasts with teleological theories of activity,",
    "label": 0
  },
  {
    "text": "history, and the forces that influenced it. The Zeitgeist, the \"Spirit of the Age\", is the concrete embodiment of the most important factors that are acting in human history at any given time. This contrasts with teleological theories of activity, which suppose that the end is the determining factor of activity, as well as those who believe in a tabula rasa, or blank slate, opinion, such that individuals are defined by their interactions. These ideas can be interpreted variously. The Right Hegelians, working from Hegel's opinions about the organicism and historically determined nature of human societies, interpreted Hegel's historicism as a justification of the unique destiny of national groups and the importance of stability and institutions. Hegel's conception of human societies as entities greater than the individuals who constitute them influenced nineteenth-century romantic nationalism and its twentieth-century excesses. The Young Hegelians, by contrast, interpreted Hegel's thoughts on societies influenced by social conflict for a doctrine of social progress, and attempted to manipulate these forces to cause various results. Karl Marx's doctrine of \"historical inevitabilities\" and historical materialism is one of the more influential reactions to this part of Hegel's thought. Significantly, Karl Marx's theory of alienation argues that capitalism disrupts traditional relationships between workers and their work. Hegelian historicism is related to his ideas on the means by which human societies progress, specifically the dialectic and his conception of logic as representing the inner essential nature of reality. Hegel attributes the change to the \"modern\" need to interact with the world, whereas ancient philosophers were self-contained, and medieval philosophers were monks. In his History of Philosophy Hegel writes: In modern times things are very different; now we no longer see philosophic individuals who constitute a class by themselves. With the present day all difference has disappeared; philosophers are not monks,",
    "label": 0
  },
  {
    "text": "monks. In his History of Philosophy Hegel writes: In modern times things are very different; now we no longer see philosophic individuals who constitute a class by themselves. With the present day all difference has disappeared; philosophers are not monks, for we find them generally in connection with the world, participating with others in some common work or calling. They live, not independently, but in the relation of citizens, or they occupy public offices and take part in the life of the state. Certainly they may be private persons, but if so, their position as such does not in any way isolate them from their other relationship. They are involved in present conditions, in the world and its work and progress. Thus their philosophy is only by the way, a sort of luxury and superfluity. This difference is really to be found in the manner in which outward conditions have taken shape after the building up of the inward world of religion. In modern times, namely, on account of the reconciliation of the worldly principle with itself, the external world is at rest, is brought into order — worldly relationships, conditions, modes of life, have become constituted and organized in a manner which is conformable to nature and rational. We see a universal, comprehensible connection, and with that individuality likewise attains another character and nature, for it is no longer the plastic individuality of the ancients. This connection is of such power that every individuality is under its dominion, and yet at the same time can construct for itself an inward world. This opinion that entanglement in society creates an indissoluble bond with expression, would become an influential question in philosophy, namely, the requirements for individuality. It would be considered by Nietzsche, John Dewey and Michel Foucault directly, as well",
    "label": 0
  },
  {
    "text": "inward world. This opinion that entanglement in society creates an indissoluble bond with expression, would become an influential question in philosophy, namely, the requirements for individuality. It would be considered by Nietzsche, John Dewey and Michel Foucault directly, as well as in the work of numerous artists and authors. There have been various responses to Hegel's challenge. The Romantic period emphasized the ability of individual genius to transcend time and place, and use the materials from their heritage to fashion works which were beyond determination. The modern would advance versions of John Locke's infinite malleability of the human animal. Post-structuralism would argue that since history is not present, but only the image of history, that while an individual era or power structure might emphasize a particular history, that the contradictions within the story would hinder the very purposes that the history was constructed to advance. Anthropological In the context of anthropology and other sciences which study the past, historicism has a different meaning. Historical Particularism is associated with the work of Franz Boas. His theory used the diffusionist concept that there were a few \"cradles of civilization\" which grew outwards, and merged it with the idea that societies would adapt to their circumstances. The school of historicism grew in response to unilinear theories that social development represented adaptive fitness, and therefore existed on a continuum. While these theories were espoused by Charles Darwin and many of his students, their application as applied in social Darwinism and general evolution characterized in the theories of Herbert Spencer and Leslie White, historicism was neither anti-selection, nor anti-evolution, as Darwin never attempted nor offered an explanation for cultural evolution. However, it attacked the notion that there was one normative spectrum of development, instead emphasizing how local conditions would create adaptations to the local environment.",
    "label": 0
  },
  {
    "text": "neither anti-selection, nor anti-evolution, as Darwin never attempted nor offered an explanation for cultural evolution. However, it attacked the notion that there was one normative spectrum of development, instead emphasizing how local conditions would create adaptations to the local environment. Julian Steward refuted the viability of globally and universally applicable adaptive standards proposing that culture was honed adaptively in response to the idiosyncrasies of the local environment, the cultural ecology, by specific evolution. What was adaptive for one region might not be so for another. This conclusion has likewise been adopted by modern forms of biological evolutionary theory. The primary method of historicism was empirical, namely that there were so many requisite inputs into a society or event, that only by emphasizing the data available could a theory of the source be determined. In this opinion, grand theories are unprovable, and instead intensive field work would determine the most likely explanation and history of a culture, and hence it is named \"historicism\". This opinion would produce a wide range of definition of what, exactly, constituted culture and history, but in each case the only means of explaining it was in terms of the historical particulars of the culture itself. New Historicism Since the 1950s, when Jacques Lacan and Michel Foucault argued that each epoch has its own knowledge system, within which individuals are inexorably entangled, many post-structuralists have used historicism to describe the opinion that all questions must be settled within the cultural and social context in which they are raised. Answers cannot be found by appeal to an external truth, but only within the confines of the norms and forms that phrase the question. This version of historicism holds that there are only the raw texts, markings and artifacts that exist in the present, and the conventions used to",
    "label": 0
  },
  {
    "text": "truth, but only within the confines of the norms and forms that phrase the question. This version of historicism holds that there are only the raw texts, markings and artifacts that exist in the present, and the conventions used to decode them. This school of thought is sometimes given the name of New Historicism. The same term, new historicism is also used for a school of literary scholarship which interprets a poem, drama, etc. as an expression of or reaction to the power-structures of its society. Stephen Greenblatt is an example of this school. Modern Historicism Within the context of 20th-century philosophy, debates continue as to whether ahistorical and immanent methods were sufficient to understand the meaning (that is to say, \"what you see is what you get\" positivism) or whether context, background and culture are important beyond the mere need to decode words, phrases and references. While post-structural historicism is relativist in its orientation—that is, it sees each culture as its own frame of reference—a large number of thinkers have embraced the need for historical context, not because culture is self-referential, but because there is no more compressed means of conveying all of the relevant information except through history. This opinion is often seen as deriving from the work of Benedetto Croce. Recent historians using this tradition include Thomas Kuhn. Talcott Parsons criticized historicism as a case of idealistic fallacy in The Structure of Social Action (1937). Post-structuralism uses the term new historicism, which has some associations with both anthropology and Hegelianism. Christian Historicism Eschatological In Christianity, the term historicism refers to the confessional Protestant form of prophetical interpretation which holds that the fulfillment of biblical prophecy has occurred throughout history and continues to occur; as opposed to other methods which limit the time-frame of prophecy-fulfillment to the past",
    "label": 0
  },
  {
    "text": "historicism refers to the confessional Protestant form of prophetical interpretation which holds that the fulfillment of biblical prophecy has occurred throughout history and continues to occur; as opposed to other methods which limit the time-frame of prophecy-fulfillment to the past or to the future. Dogmatic and ecclesiastic There is also a particular opinion in ecclesiastical history and in the history of dogmas which has been described as historicist by Pope Pius XII in the encyclical Humani generis. \"They add that the history of dogmas consists in the reporting of the various forms in which revealed truth has been clothed, forms that have succeeded one another in accordance with the different teachings and opinions that have arisen over the course of the centuries.\" \"There is also a certain historicism, which attributing value only to the events of man's life, overthrows the foundation of all truth and absolute law, both on the level of philosophical speculations and especially to Christian dogmas.\" Critics Marxism Western Marxists such as Karl Korsch, Antonio Gramsci and the early Georg Lukacs emphasise the roots of Marx's thought in Hegel. They interpret Marxism as a historically relativist philosophy, which views ideas (including Marxist theory) as products of the historical epochs that create them. In this view, Marxism is not an objective social science, but rather a theoretical expression of the class consciousness of the working class within a historical process. This understanding of Marxism is strongly criticised by the structural Marxist Louis Althusser, who affirms that Marxism is an objective science, autonomous from interests of society and class. Marxism is, therefore, often associated with deterministic claims of future historical development, but these are not structural parts of Marxism as a style of critique which requires distinction between various critical registers, which at once develops an understanding of broad",
    "label": 0
  },
  {
    "text": "Marxism is, therefore, often associated with deterministic claims of future historical development, but these are not structural parts of Marxism as a style of critique which requires distinction between various critical registers, which at once develops an understanding of broad historical-geographical tensions without prophesying a specific outcome. Karl Popper Karl Popper used the term historicism in his influential books The Poverty of Historicism and The Open Society and Its Enemies, to mean: \"an approach to the social sciences which assumes that historical prediction is their primary aim, and which assumes that this aim is attainable by discovering the 'rhythms' or the 'patterns', the 'laws' or the 'trends' that underlie the evolution of history\". Popper condemned historicism along with the determinism and holism which he argued formed its basis, claiming that historicism had the potential to inform dogmatic, ideological beliefs not predicated upon facts that were falsifiable. In The Poverty of Historicism, he identified historicism with the opinion that there are \"inexorable laws of historical destiny\", an opinion he warned against. If this seems to contrast with what proponents of historicism argue for, in terms of contextually relative interpretation, this happens, according to Popper, only because such proponents are unaware of the type of causality they ascribe to history. Popper wrote with reference to Hegel's theory of history, which he criticized extensively. In The Open Society and Its Enemies, Popper attacks \"historicism\" and its proponents, among whom he identifies and singles out Hegel, Plato and Marx—calling them all \"enemies of the open society\". The objection he makes is that historicist positions, by claiming that there is an inevitable and deterministic pattern to history, evade the responsibility of the individual to make free contributions to the evolution of society, hence leading to totalitarianism. Throughout this work, he defines his conception of historicism",
    "label": 0
  },
  {
    "text": "by claiming that there is an inevitable and deterministic pattern to history, evade the responsibility of the individual to make free contributions to the evolution of society, hence leading to totalitarianism. Throughout this work, he defines his conception of historicism as: \"The central historicist doctrine—the doctrine that history is controlled by specific historical or evolutionary laws whose discovery would enable us to prophesy the destiny of man.\" As mentioned above, such characterizations of Marx in particular are not entirely accurate to Marx in his own right, and have drawn criticism from philosophers such as Lakatos for mischaracterizing the defense of induction in historical materialism. Other philosophers such as Walter Kaufmann have also been critical of Popper, calling his reading of Hegel a “myth,” “known largely through secondary sources…” Another of his targets is what he terms \"moral historicism\", the attempt to infer moral values from the course of history; in Hegel's words, that \"history is the world's court of justice\". Popper says that he does not believe \"that success proves anything or that history is our judge\". Futurism must be distinguished from prophecies that the right will prevail: these attempt to infer history from ethics, rather than ethics from history, and are therefore historicism in the normal sense rather than moral historicism. He also attacks what he calls \"Historism\", which he regards as distinct from historicism. By historism, he means the tendency to regard every argument or idea as completely accounted for by its historical context, as opposed to assessing it by its merits. Leo Strauss Leo Strauss used the term historicism and reportedly termed it the single greatest threat to intellectual freedom insofar as it denies any attempt to address injustice-pure-and-simple (such is the significance of historicism's rejection of \"natural right\" or \"right by nature\"). Strauss argued that historicism",
    "label": 0
  },
  {
    "text": "term historicism and reportedly termed it the single greatest threat to intellectual freedom insofar as it denies any attempt to address injustice-pure-and-simple (such is the significance of historicism's rejection of \"natural right\" or \"right by nature\"). Strauss argued that historicism \"rejects political philosophy\" (insofar as this stands or falls by questions of permanent, trans-historical significance) and is based on the belief that \"all human thought, including scientific thought, rests on premises which cannot be validated by human reason and which came from historical epoch to historical epoch.\" Strauss further identified R. G. Collingwood as the most coherent advocate of historicism in the English language. Countering Collingwood's arguments, Strauss warned against historicist social scientists' failure to address real-life problems—most notably that of tyranny—to the extent that they relativize (or \"subjectivize\") all ethical problems by placing their significance strictly in function of particular or ever-changing socio-material conditions devoid of inherent or \"objective\" \"value\". Similarly, Strauss criticized Eric Voegelin's abandonment of ancient political thought as guide or vehicle in interpreting modern political problems. In his books, Natural Right and History and On Tyranny, Strauss offers a complete critique of historicism as it emerges in the works of Hegel, Marx, and Heidegger. Many believe that Strauss also found historicism in Edmund Burke, Tocqueville, Augustine, and John Stuart Mill. Although it is largely disputed whether Strauss himself was a historicist, he often indicated that historicism grew out of and against Christianity and was a threat to civic participation, belief in human agency, religious pluralism, and, most controversially, an accurate understanding of the classical philosophers and religious prophets themselves. Throughout his work, he warns that historicism, and the understanding of progress that results from it, expose us to tyranny, totalitarianism, and democratic extremism. In a collection of his works by Kenneth Hart entitled Jewish Philosophy and",
    "label": 0
  },
  {
    "text": "The history of marriage is a branch of social history that concerns the sociocultural evolution of socially or legally recognized unions between people called spouses, from prehistoric to modern times. Research on the history of marriage crosses disciplines and cultures, aiming to understand the structure and function of the family from many viewpoints. The study of history has shown that marriage, like other family systems, is flexible, culturally diverse and adaptive to ecological and economical conditions. The history of marriage is often considered under history of the family or legal history. Prehistory Marriage likely originated among Stone Age humans as a way to organize childrearing and daily work. While the theory of a male provider — who hunted for and protected his wife and children — has largely been dismissed by modern anthropologists, there was a division of labor between early men and women. Nursing women were much less capable of hunting large game, and instead focused on gathering and processing plants and smaller game (contributing the vast majority of a clan's food). This specialization led to greater interdependence between males and females, and as techniques became more advanced, it took longer to teach them to children, incentivizing parents to stay together longer. Still, the nuclear family was not the defining social institution — no individual family had enough resources to be independent, so larger clans were crucial for a family's survival. Likewise, unmarried women did not lack the support of their community. People married outside of their immediate clan groups, and this allowed bands to forge relationships with one another. As societies began to develop larger surpluses, and began to settle into more agrarian, sedentary structures, marriage became more important. Wealthier families became less interested in sharing resources, and instead began to use marriage as a tool to consolidate",
    "label": 0
  },
  {
    "text": "another. As societies began to develop larger surpluses, and began to settle into more agrarian, sedentary structures, marriage became more important. Wealthier families became less interested in sharing resources, and instead began to use marriage as a tool to consolidate those resources within the family or between a few closely linked kin groups. Studies of genetic material have shown that endogamy became increasingly common among wealthier families. Ancient world As wealthy families accumulated more and more resources, inheritance rights became of greater concern, and marriage came to play an important role in many ancient societies. Both men and women came under more pressure from families to choose the \"correct\" spouse for the family's interests, and the distinction between \"legitimate\" and \"illegitimate\" children grew sharper. The earliest written records show that by the time writing was invented, marriage became the way most wealth and land changed hands. In wealthy families, marriage became the site of political intrigue. Factions of ruling circles fought over who had the right to legitimize marriage and bloodlines -- arguments which often had serious historical ramifications. Because women had the unique ability to bear a child with an \"impure\" bloodline, generally their sexual behavior was more strictly supervised. Some societies developed elaborate ideas about female sexual purity, especially in upper class society. The invention of the plow widened the division of labor between men and women. Plowing required greater physical strength and was less compatible with childcare, and therefore became associated with male labor. The belief, therefore, that men contributed more to a household's resources led to the proliferation of the dowry. The increase in warfare between emerging societies also raised men and lowered women in the social hierarchy — devaluing women and contributing to the idea that their presence in a family needed to be compensated",
    "label": 0
  },
  {
    "text": "the proliferation of the dowry. The increase in warfare between emerging societies also raised men and lowered women in the social hierarchy — devaluing women and contributing to the idea that their presence in a family needed to be compensated in some way. Over time, in many societies, women became seen as the properties of their fathers or husbands. Where women could no longer choose their own mates, men now needed to win over their fathers. Africa Ancient Egypt The primary purpose of marriage was to have more children and descendants of the family. Marriages were typically arranged by parents. Marriages in ancient Egypt were usually exclusive, but it also was not uncommon for a man of high economic status to have more than one wife, especially if his first wife was unable to have children of her own. Among the lower classes, husbands and wives worked together to manage businesses. Among the upper classes, women usually did not work outside the home, and instead supervised the servants of the household and her children's education. Generally, women made more family decisions and controlled more of the home than usual. Women had control over most of their property and could serve as legal persons who brought cases to the court. Even pharaohs frequently found themselves under control over their mothers. Widows gained more legal freedom after their husbands' deaths -- notably, the ability to buy and sell land. Although it was possible to divorce, it was difficult. Men often divorced their wives if they were unable to reproduce. After about 365 B.C, divorce laws strongly favored women. Europe Among ancient Germanic tribes, the bride and groom were roughly the same age and generally older than their Roman counterparts, at least according to Tacitus:The youths partake late of the pleasures of love,",
    "label": 0
  },
  {
    "text": "B.C, divorce laws strongly favored women. Europe Among ancient Germanic tribes, the bride and groom were roughly the same age and generally older than their Roman counterparts, at least according to Tacitus:The youths partake late of the pleasures of love, and hence pass the age of puberty unexhausted: nor are the virgins hurried into marriage; the same maturity, the same full growth is required: the sexes unite equally matched and robust, and the children inherit the vigor of their parents.Where Aristotle had set the prime of life at 37 years for men and 18 for women, the Visigothic Code of law in the 7th century placed the prime of life at 20 years for both men and women, after which both presumably married. Tacitus states that ancient Germanic brides were on average about 20 and were roughly the same age as their husbands. Tacitus, however, had never visited the German-speaking lands and most of his information on Germania comes from secondary sources. In addition, Anglo-Saxon women, like those of other Germanic tribes, are marked as women from the age of 12 and older, based on archaeological finds, implying that the age of marriage coincided with puberty. Classical Greece In ancient Greece, no specific civil ceremony was required for the creation of a marriage – only mutual agreement and the fact that the couple must regard each other as husband and wife accordingly. Marriage was primarily an economic arrangement, and romance had little to do with mate choice. Men usually married when they were in their 20s and women in their teens. It has been suggested that these ages made sense for the Greeks because men were generally done with military service or financially established by their late 20s, and marrying a teenage girl ensured ample time for her to bear",
    "label": 0
  },
  {
    "text": "teens. It has been suggested that these ages made sense for the Greeks because men were generally done with military service or financially established by their late 20s, and marrying a teenage girl ensured ample time for her to bear children. Married Greek women had few rights in ancient Greek society and were expected to take care of the house and children. Inheritance was more important than feelings: a woman whose father dies without male heirs could be forced to marry her nearest male relative – even if she had to divorce her husband first. Classical Rome There were several types of marriages in ancient Roman society. The traditional (\"conventional\") form called conventio in manum required a ceremony with witnesses and was also dissolved with a ceremony. In this type of marriage, a woman lost her family rights of inheritance of her old family and gained them with her new one. She now was subject to the authority of her husband. Marriages were economic affairs, with inheritance rights at the center of the institution. Romans were very concerned about their wives birthing illegitimate children who might lay claim to their wealth. Public displays of affection, such as kissing, were generally considered boorish. It was considered disgraceful to kiss one's wife in front of others. Under the free marriage known as sine manu, the wife remained a member of her original family; she stayed under the authority of her father, kept her family rights of inheritance with her old family and did not gain any with the new family. The minimum age of marriage for girls was 12. Post-Christian Europe From the early Christian era (30 to 325 CE), marriage was thought of as primarily a private matter, with no uniform religious or other ceremony being required. However, bishop Ignatius of",
    "label": 0
  },
  {
    "text": "age of marriage for girls was 12. Post-Christian Europe From the early Christian era (30 to 325 CE), marriage was thought of as primarily a private matter, with no uniform religious or other ceremony being required. However, bishop Ignatius of Antioch, writing around 110, exhorts, \"[I]t becomes both men and women who marry, to form their union with the approval of the bishop, that their marriage may be according to God, and not after their own lust.\" Far East Ancient China The mythological origin of Chinese marriage is a story about Nüwa and Fu Xi, whose marriage allowed them to create humanity. Marriage in Ancient China was secondary to one's existing familial ties. Parents took a prominent role in arranging and overseeing marriages. Parents of men could force their sons to divorce for a variety of reasons (e.g., if they didn't like his wife's work ethic, if they felt their son's attachment to his wife was a threat to his attachment to his birth family). Likewise, the ancient Chinese word for \"romantic love\" had a negative connotation and was often used to describe an illicit, unapproved relationship. Chinese writers discouraged women from confiding in her husband or even telling him about her day. A husband who showed open affection for his wife was seen as having weak character. Confucius defined a wife as “someone who submits to another.” A wife, according to Confucian philosophy, had to follow “the rule of the three obediences: while at home she obeys her father, after marriage she obeys her husband, after he dies she obeys her son.” Concubines and \"backup wives\" (a wife's sisters who would move in in case their services were needed) were common. Parents could force their son to take a concubine if his wife was not producing viable heirs. Women",
    "label": 0
  },
  {
    "text": "obeys her son.” Concubines and \"backup wives\" (a wife's sisters who would move in in case their services were needed) were common. Parents could force their son to take a concubine if his wife was not producing viable heirs. Women and men were married relatively young in Ancient China. For the women, it was soon after puberty and men were expected to marry around age twenty. Endogamy was relatively common among all classes. Generally, marrying one's maternal relatives was not thought of as incest. But people of the same surname were expected supposed to consult with their family trees prior to marriage to reduce the potential risk of unintentional incest through one's paternal line. This practice was enforced under the law. During the Han Dynasty (202 BCE - 220 CE) marriage included a number of mandatory steps. The most important of them was the presentation of betrothal gifts from the groom and his family to the bride and her family (though the money was expected to be used by the bride, and it was considered dishonorable for a family to spend their daughter's betrothal gift). The bride's family then countered with a dowry, which separated her from a common concubine. Once all the goods were exchanged, the bride was taken to the groom's ancestral home, where she was expected to obey her husband and live with his relatives. Women continued to belong to their husband's families even after their husbands died; if they had any children they stayed with his family. If the widow's birth family wanted her to marry again, they would often have to ransom her back from her deceased husband's family. Near & Middle East The first recorded evidence of marriage ceremonies uniting a man and a woman dates back to approximately 2350 BC, in ancient Mesopotamia.",
    "label": 0
  },
  {
    "text": "again, they would often have to ransom her back from her deceased husband's family. Near & Middle East The first recorded evidence of marriage ceremonies uniting a man and a woman dates back to approximately 2350 BC, in ancient Mesopotamia. Wedding ceremonies, as well as dowry and divorce, can be traced back to Mesopotamia and Babylonia. Historically there were many types of marriages, used for various purposes. A mut'ah (or temporary marriage) created a sexual outlet for men and women under certain circumstances (in particular, travel) without subjecting them to the otherwise harsh penalties for nonmarital sex. Babylonian Jews allowed some men entering a new town to request a \"wife for a day\". These temporary marriages did not themselves create any obligation between the spouses; however, if a child was born of the union, the child was considered legitimate and had inheritance rights. By the second millennium B.C., women were generally expected to be subordinate to their husbands. The practice of secluding women in special quarters to guard their chastity became widespread in the Middle East; it was also a way to signify that a family's wealth was great enough that their daughters and wives did not need to work. Mesopotamia Hammurabi's Code codified rules about marriage and property inheritance. The code required that marriages be consummated and that wives remain loyal to their husbands. Infidelity was punishable by death. However, husbands were legally required to care for their wives economically, and women were legally protected if their husbands failed to do so. Women's dowries also belonged to them until they had sons to claim that inheritance. Men were allowed to see concubines and even take second wives under some circumstances, but their first wives were always held in higher authority than subsequent wives. The right of divorce rested with",
    "label": 0
  },
  {
    "text": "they had sons to claim that inheritance. Men were allowed to see concubines and even take second wives under some circumstances, but their first wives were always held in higher authority than subsequent wives. The right of divorce rested with men; however, a woman's children were her own responsibility after divorce. Divorced women were free to marry whomever they liked, or return to their father's households. Widows were also permitted to remarry with permission from a judge. Assyrian Empire Generally, there was no legal distinction between men and women in the Old Assyrian period and they had more or less the same rights in society. Both men and women could inherit property, participated in trade, bought, owned, and sold houses and slaves, made their own last wills, and were allowed to divorce their partners. Bridal dowries belonged to the brides, not the husband, and it was inherited by her children after her death. Despite their equal legal standing, women faced greater social expectations. Men generally worked as agricultural laborers or tradesmen, whereas wives were expected to provide their husbands with garments and food. Although marriages were typically monogamous, husbands were allowed to buy a female slave in order to produce an heir if his wife was infertile -- though wives had the right to choose the slave. Husbands who were away on long trading journeys were allowed to take a second wife in one of the trading colonies, although they were strictly expected to provide for their second wives. Women's rights generally degraded during the Middle Assyrian period. Women's premarital chastity and post-marital loyalty were strictly supervised. Married women were required to wear veils, and married women who committed adultery could be sentenced to death. Men were given the right to punish their wives virtually however they saw fit. A",
    "label": 0
  },
  {
    "text": "chastity and post-marital loyalty were strictly supervised. Married women were required to wear veils, and married women who committed adultery could be sentenced to death. Men were given the right to punish their wives virtually however they saw fit. A raped woman could be forced to marry her rapist. However, it is not entirely clear how strictly these laws were enforced. Widows were guaranteed special privileges during the Middle Assyrian period. Widows who did not have any sons or relatives to support them were guaranteed support from the government. Hebrews According to ancient Hebrew tradition, a wife was seen as being property of high value and was, therefore, usually, carefully looked after. The Covenant Code orders \"If he take him another; her food, her clothing, and her duty of marriage, shall he not diminish (or lessen)\". The Talmud interprets this as a requirement for a man to provide food and clothing to, and have sex with, each of his wives. However, \"duty of marriage\" is also interpreted as whatever one does as a married couple, which is more than just sexual activity. And the term diminish, which means to lessen, shows the man must treat her as if he was not married to another. Early nomadic communities in the Middle East practiced a form of marriage known as beena, in which a wife would own a tent of her own, within which she retains complete independence from her husband. As a polygynous society, the Israelites did not have any laws that imposed marital fidelity on men. Adulterous married women, adulterous betrothed women, and the men who slept with them, however, were subject to the death penalty by the biblical laws against adultery. The literary prophets indicate that adultery was a frequent occurrence, despite these legal strictness's. Middle Ages East Asia",
    "label": 0
  },
  {
    "text": "betrothed women, and the men who slept with them, however, were subject to the death penalty by the biblical laws against adultery. The literary prophets indicate that adultery was a frequent occurrence, despite these legal strictness's. Middle Ages East Asia China Chinese family loyalty continued to supersede marital ties. At some points, the law permitted a man's father raping his wife. In the Ming period, marriage was considered solemn and according to the law written in the Ming Code, all commoners' marriages must follow the rules written in Duke Wen's Family Rules. Licensed marriage matchmakers played a key role in marriages. The rules stated that \"in order to arrange a marriage, an agent must come and deliver messages between the two families.\" Sometimes both families were influential and wealthy and the matchmaker bonded the two families into powerful households. Marriage was key in increasing the strength of clans. Japan As in China, marriage matches in Japan were expected to serve the ie, or the larger family, by perpetuating the family line, adding new adults to the family's labor force, forming alliances with other families, offloading female dependents. Almost all people were expected to marry and produce children. Marriage generally occurred between a small group of families of the same social class. When choosing spouses, families considered economic status and political views. Brides and grooms, and their families, were considered equals. Romance in marriage was discouraged, as it was thought to undermine filial piety. According to one proverb, \"Those who come together in passion stay together in tears.\" Sexual gratification was more acceptable outside of one's marriage, through concubines, geishas, or \"pleasure districts\". During the Heian period (794-1185), court life flourished and upward mobility was one of the primary goals of marriage among the upper classes. Marriage did not necessarily need",
    "label": 0
  },
  {
    "text": "more acceptable outside of one's marriage, through concubines, geishas, or \"pleasure districts\". During the Heian period (794-1185), court life flourished and upward mobility was one of the primary goals of marriage among the upper classes. Marriage did not necessarily need to be a permanent agreement. Aristocrats would exchange letters and poetry for a period of months or years before arranging to meet after dark. If a man saw the same woman for a period of three nights, they were considered married, and the wife's parents held a banquet for the couple. Typically, husbands arranged for their wives to move in with them in order to ensure legitimacy of their offspring. Some aristocratic wives, however, stayed in their fathers' homes. High-ranking men sometimes kept multiple wives or concubines, and their children with these concubines were considered just as legitimate. There was language to separate wives from concubines, and distinguish between their respective children, but children of concubines were just as entitled to inheritance as children of wives. Korea The practice of matrilocality in Korea started in the Goguryeo period and ended in the early Joseon period. The Korean saying that when a man gets married, he is \"entering jangga\" (the house of his father-in-law), stems from the Goguryeo period. Islamic World Both Muslim writers in the Islamic world were more approving of sexual passion between husband and wife than prominent thinkers in Europe; too much intimacy, however, could be an affront to God. Secular writers, meanwhile, believed love thrived best outside of a marriage. Sunni Muslims declared temporary mut'ah marriages unacceptable, though Shiite Muslims continued the practice. Medieval Europe Marriage continued to be a primarily economic arrangement in Medieval Europe. It was generally agreed upon that romantic passion had no place in a marriage. Most men could not run their farms",
    "label": 0
  },
  {
    "text": "though Shiite Muslims continued the practice. Medieval Europe Marriage continued to be a primarily economic arrangement in Medieval Europe. It was generally agreed upon that romantic passion had no place in a marriage. Most men could not run their farms or businesses alone, and needed a business partner and someone to assist with domestic household tasks. A wife's dowry was often the largest infusion of wealth a man would ever get. The average age at first marriage varied depending on local custom and economic concerns -- in better times, more people could afford to marry earlier and thus fertility rose; conversely, marriages were delayed or forgone when times were bad, thus restricting family size. The average age of marriage for most of Northwestern Europe from 1500 to 1800 was around 25 years of age. Most brides were in their early twenties and most grooms two or three years older. However, a substantial number of women married for the first time in their thirties and forties, particularly in urban areas, With few local exceptions, until 1545, Christian marriages in Europe were by mutual consent, declaration of intention to marry and upon the subsequent physical union of the parties. The couple would promise verbally to each other that they would be married to each other; the presence of a priest or witnesses was not required. However, in practice, marriages were arranged by families and neighbors, sometimes as early as birth, and these early pledges to marry were often used to ensure treaties between different royal families, nobles, and heirs of fiefdoms. The rise of the Catholic Church had a dramatic influence on marriage practices. There was no state involvement in marriage and personal status; instead, these issues were adjudicated in ecclesiastical courts. The Church began to get more invested in marriages. It",
    "label": 0
  },
  {
    "text": "rise of the Catholic Church had a dramatic influence on marriage practices. There was no state involvement in marriage and personal status; instead, these issues were adjudicated in ecclesiastical courts. The Church began to get more invested in marriages. It began dictating age requirements and advocating for free partner choice. In 1563 the Council of Trent, twenty-fourth session, required that a valid marriage must be performed by a priest before two witnesses. Women began taking the surname of their husbands beginning around the 12th century. Infidelity was common and even romanticized, especially for men. Men in particular were rarely discreet about their extramarital affairs. Eastern Europe Classical Celtic and Germanic cultures (which were not rigidly patriarchal) had a stronger influence in Eastern Europe and helped to offset the Judaeo-Roman patriarchal influence. Here, almost everyone was expected to marry, and earlier than in other parts of Europe (often in early adolescence). Pre-colonial Americas Marriage practices varied greatly among indigenous groups in the pre-colonial Americas. Many indigenous groups recognized same-sex marriage, wherein one partner typically took on the role of the opposite sex. Inuit families were known to have cospousal relationships, in which two sets of spouses freely had sexual relationships with each other's spouse. Expressing jealous about these sexual relationships was generally frowned upon. Children of cospouses saw one another as siblings. In the Nuxalk and the Kwakwaka'wakw societies of the Pacific Northwest, marriages were used entirely to establish connections between kin groups. Marriage contracts were sometimes drawn up to a person's dog or foot if no suitable human matches were available. In traditional Hopi marriages, a woman's family paid a dowry in cornmeal to the groom's family. In the northeast, individuals generally made their own marital decisions. Couples were freely allowed to part ways. People in the Innu society enjoyed",
    "label": 0
  },
  {
    "text": "available. In traditional Hopi marriages, a woman's family paid a dowry in cornmeal to the groom's family. In the northeast, individuals generally made their own marital decisions. Couples were freely allowed to part ways. People in the Innu society enjoyed a lot of sexual freedom. Legitimacy of children was not much of a concern, because most things belonged to the entire tribe. Pre-colonial Oceania Among the Aborigines in Australia, marriage was strictly managed by elders. Girls were typically assigned a husband during their childhoods. Early Modern period East Asia Edo Japan (1600-1868) By the Edo period, the expectations of filial loyalty during marriage had grown even stronger. The head of the household was responsible for arranging matches, and brides and grooms were expected not to even express a preference. Even after an 1871 law abolished the legal caste system, class discrimination was alive and well in spouse selection. Meiji Restoration (1868-1912) Public education became almost universal between 1872 and the early 1900s, and schools stressed the traditional concept of filial piety, first toward the nation, second toward the household, and last of all toward a person's own private interests. Love was thought to be inessential to marriage. Marriage under the Meiji Civil Code required the permission of the head of a household and of the parents for men under 30 and women under 25. In arranged marriages, most couples met beforehand at a formal introduction called an omiai (お見合い, lit. 'looking at one another'), although some would meet for the first time at the wedding ceremony. A visitor to Japan described the omiai as \"a meeting at which the lovers (if persons unknown to each other may be so styled) are allowed to see, sometimes even to speak to each other, and thus estimate each others' merits.\" However, their objections",
    "label": 0
  },
  {
    "text": "described the omiai as \"a meeting at which the lovers (if persons unknown to each other may be so styled) are allowed to see, sometimes even to speak to each other, and thus estimate each others' merits.\" However, their objections carried little weight. Courtship remained rare in Japan at this period. Boys and girls were separated in schools, in cinemas, and at social gatherings. Colleagues who began a romantic relationship could be dismissed. Parents sometimes staged an arranged marriage to legitimize a \"love match,\" but many others resulted in separation and sometimes suicide. Marriage, like other social institutions of this period, emphasized the subordinate inferiority of women to men. Women learned that as a daughter they ought to obey their father, as a wife their husband, as a widow their sons. Chastity in marriage was expected for women, and a law not repealed until 1908 allowed a husband to kill his wife and her lover if he found them in an adulterous act. The prostitution of women survived the periodic intrusion of puritanical ideals on Japan's less restrictive sexuality. Divorce laws, however, became more equal over time. The laws of the early Meiji period established several grounds on which a man could divorce: sterility, adultery, disobedience to parents-in-law, loquacity, larceny, jealousy, and disease. A wife, accompanied by a close male relative, could appeal for divorce if she had been deserted or imprisoned by her husband, or if he was profligate or mentally ill. The 1898 Meiji Civil Code established the principle of mutual consent, although the consent of women was still likely to be forced until the early 20th century, as women gradually gained access to education and financial independence. Marriage between a Japanese and non-Japanese person was not officially permitted until 14 March 1873. A foreign national was required",
    "label": 0
  },
  {
    "text": "still likely to be forced until the early 20th century, as women gradually gained access to education and financial independence. Marriage between a Japanese and non-Japanese person was not officially permitted until 14 March 1873. A foreign national was required to surrender their citizenship and acquire Japanese citizenship. Europe Marriage became an inflection point of the conflicting values of Catholic and Protestant Christian traditions. As part of the Protestant Reformation, the role of recording marriages and setting the rules for marriage passed to the state, reflecting Martin Luther's view that marriage was a \"worldly thing\". John Calvin and his Protestant colleagues reformulated Christian marriage by enacting the Marriage Ordinance of Geneva, which imposed \"The dual requirements of state registration and church consecration to constitute marriage\" for recognition. By the 17th century, many of the Protestant European countries had a state involvement in marriage. Conventional wisdom in Protestant communities was that, with effort, love would develop after a marriage. Marrying primarily for love was still discouraged — one saying claimed that \"he who marries for love has good nights and bad days\". As part of the Counter-Reformation, in 1563 the Council of Trent decreed that a Roman Catholic marriage would be recognized only if the marriage ceremony was officiated by a priest with two witnesses. The council also authorized a Catechism, issued in 1566, which defined marriage as \"The conjugal union of man and woman, contracted between two qualified persons, which obliges them to live together throughout life.\" England and Wales In England and Wales, under the Anglican Church, marriage by consent and cohabitation was valid until the passage of Marriage Act 1753. The act required a marriage ceremony to be officiated by an Anglican priest in the Anglican Church with two witnesses and registration. The act did not apply to",
    "label": 0
  },
  {
    "text": "consent and cohabitation was valid until the passage of Marriage Act 1753. The act required a marriage ceremony to be officiated by an Anglican priest in the Anglican Church with two witnesses and registration. The act did not apply to Jewish marriages or those of Quakers, whose marriages continued to be governed by their own customs. North America tktktk Late Modern period Europe In the seventeenth century, a series of political, economic, and social changes rearranged marital priorities. England and Wales Since 1837, civil marriages have been recognized as a legal alternative to church marriages under the Marriage Act 1836. In Germany, civil marriages were recognized in 1875. This law permitted a declaration of the marriage before an official clerk of the civil administration, when both spouses affirm their will to marry, to constitute a legally recognized valid and effective marriage, and allowed an optional private clerical marriage ceremony. 20th century East Asia China The New Marriage Law of 1950 radically changed Chinese marriage traditions, enforcing monogamy, equality of men and women, and choice in marriage; arranged marriages were the most common type of marriage in China until then. Starting October 2003, it became legal to marry or divorce without authorization from the couple's work units. Europe By the 20th century, free choice of spouse was the norm in Western Europe. In contemporary English common law, a marriage is a voluntary contract by a man and a woman, in which by agreement they choose to become husband and wife. Edvard Westermarck proposed that \"the institution of marriage has probably developed out of a primeval habit\". Since the late twentieth century, major social changes in Western countries have led to changes in the demographics of marriage, with the age of first marriage increasing, fewer people marrying, and more couples choosing to",
    "label": 0
  },
  {
    "text": "Local history is the study of history in a geographically local context, often concentrating on a relatively small local community. It incorporates cultural and social aspects of history. Local history is not merely national history writ small but a study of past events in a given geographical area which is based on a wide variety of documentary evidence and placed in a comparative context that is both regional and national. Historic plaques are one form of documentation of significant occurrences in the past and oral histories are another. Local history is often documented by local historical societies or groups that form to preserve a local historic building or other historic site. Many works of local history are compiled by amateur historians working independently or archivists employed by various organizations. An important aspect of local history is the publication and cataloguing of documents preserved in local or national records which relate to particular areas. In a number of countries a broader concept of local lore is known, which is a comprehensive study of everything pertaining to a certain locality. Both fields document places and people but differ in breadth: history prioritizes human events, while lore covers environmental and ethnographic elements. Sources and Methods Local history research uses many kinds of sources, such as government records, land and tax files, newspapers, maps, photos, buildings, oral histories, family papers, and community traditions. Historians compare nearby communities and track change over time in local institutions, economies, and social relations, often using both numbers, like census data, and stories to study patterns in work, migration, and politics. Local history, like public history, does not focus on one topic, because a historian can write local labor history, local women’s history, local business history, or local religious history. Instead, the field centers on professional and ethical questions",
    "label": 0
  },
  {
    "text": "and politics. Local history, like public history, does not focus on one topic, because a historian can write local labor history, local women’s history, local business history, or local religious history. Instead, the field centers on professional and ethical questions about who local history serves, where people can find it, and how historians work with a community whose members are both neighbors and subjects. These questions include what local history can do and how historians can build relationships that reveal overlooked sources and help their work matter in the community. Uses Local history supports school curricula, lifelong learning, and community-based projects that use historical materials to explore identity and place. Exhibitions, walking tours, public talks, and digital resources make research accessible to residents and visitors. Local history also informs planning and preservation decisions, heritage tourism, and discussions of commemoration and public memory. It can provide a setting for dialogue in communities that face contested histories by highlighting shared experiences and multiple viewpoints. Local contexts Local history tends to be less documented than other types, with fewer books and artifacts than that of a country or continent. Many local histories are recorded as oral tales or stories and so are more vulnerable than more well known issues. Artifacts of local history are often collected in local history museums, which may be housed in a historic house or other building. Individual historic sites are inherently local, although they may have national or world history importance as well. Many however have little overall historical impact but add depth to the local area. Australia In Australia, local history is focussed on specific cities and suburbs or country towns and regions. In cities, local history might concentrate on a CBD and its bordering suburbs, on a specific suburb or municipality, or on an agglomeration of",
    "label": 0
  },
  {
    "text": "In Australia, local history is focussed on specific cities and suburbs or country towns and regions. In cities, local history might concentrate on a CBD and its bordering suburbs, on a specific suburb or municipality, or on an agglomeration of suburbs and municipalities (because local government boundaries have changed over time). Outside the larger cities, local history often examines regional towns and surrounding areas. Records are typically stored at state libraries, public libraries, historical societies and public record offices. For example, the State Library of Victoria holds extensive local history records for Melbourne and other places in Victoria. Many other Melbourne libraries have local history collections, along with the Public Record Office Victoria and the Royal Historical Society of Victoria. In New South Wales, the Royal Australian Historical Society has studied local history as part of its remit since its founding in 1901. It holds local history records along with the State Library of NSW and other state and local libraries and archives. Historians have examined the ways local history has been written in Australia since the nineteenth century. Early on, the emphasis was on pioneer and settler history. The creative ways that local history contributed to making community has also been argued. Subsequently, local history, urban history, public history and heritage were closely connected in Australia. United Kingdom The British Association for Local History in the United Kingdom encourages and assists in the study of local history as an academic discipline and as a leisure activity by both individuals and groups. Most historic counties in England have record societies and archaeological and historical societies which coordinate the work of historians and other researchers concerned with that area. Local history in the UK took a long time to be accepted as an academic discipline. In the 18th and 19th centuries,",
    "label": 0
  },
  {
    "text": "and archaeological and historical societies which coordinate the work of historians and other researchers concerned with that area. Local history in the UK took a long time to be accepted as an academic discipline. In the 18th and 19th centuries, it was widely regarded as an antiquarian pursuit, suitable for country parsons. The Victoria History of the Counties of England project begun in 1899 in honour of Queen Victoria with the aim of creating an encyclopaedic history of each of the historic counties of England. The project is coordinated by the Institute of Historical Research at the University of London. The first academic post related to local history was at Reading University which appointed a research fellow in local history in 1908. There was a department of local history (but without a professor) at Leicester University from 1947. H. P. R. Finberg was the first Professor of English Local History. He was appointed by Leicester in 1964. Local history continues to be neglected as an academic subject within universities. Academic local historians are often found within a more general department of history or in continuing education. Local history is rarely taught as a separate subject in British schools. In 1908, a Board of Education circular had urged that schools should pay attention \"to the history of the town and district\" in which they were situated. In 1952, the Ministry of Education suggested schools should use local material to illustrate national themes. Within the current National Curriculum, pupils at level 4 are expected to \"show their knowledge and understanding of local, national and international history\". The Alan Ball Local History Awards were established in the 1980s to recognize outstanding contributions in local history publishing in the UK (both in print and in new media), and to encourage the publishing of such",
    "label": 0
  },
  {
    "text": "national and international history\". The Alan Ball Local History Awards were established in the 1980s to recognize outstanding contributions in local history publishing in the UK (both in print and in new media), and to encourage the publishing of such works by public libraries and local authorities. Local history can become a crucial component to policy-making and serve as a marketable resource and this is demonstrated in the case of Northern Ireland. Aside from its contribution to local development, local history is being used as a non-contentious meeting ground in addressing conflicting traditions by reinforcing shared past rather than adversarial political history. United States Local history in the United States examines the history of specific geographic areas, including regions, states, counties, cities, towns, and neighborhoods. The field became visible in American academic life in the 1970s, though non-academic historians had engaged in documenting local histories since the nineteenth century. Early works focused on settlement, the experiences of settlers, and the displacement of Native Americans. They often described political events, wars, and the contributions of local communities to national developments. Local history in the United States connects the experiences of communities to the broader national narrative. It explores how local responses to national and international events shape the country’s development. The interplay between localism and nationalism has influenced federal-state relations, sectionalism, and regionalism. Many pivotal events in American history are best understood through the lens of local responses to shared issues. Preservation and Organizations By the early twentieth century, local historians began to focus on preserving primary materials and oral histories, recognizing the risk of losing irreplaceable records and memories. Historical societies and individual historians collected documents, artifacts, and oral accounts to safeguard community heritage. The American Association for State and Local History, established in 1940, supports the preservation and interpretation",
    "label": 0
  },
  {
    "text": "recognizing the risk of losing irreplaceable records and memories. Historical societies and individual historians collected documents, artifacts, and oral accounts to safeguard community heritage. The American Association for State and Local History, established in 1940, supports the preservation and interpretation of local history across the country. Methodology and Trends In the late twentieth century, the field expanded to include more diverse voices and topics, influenced by social and political movements. Community studies and place-oriented research became common, allowing historians to test generalizations about the national experience through local case studies. Slavic countries In several Slavic countries there is a related study which may be translated from the local languages as \"country lore\" or \"local lore\". In addition to history, it also incorporates other local studies, such as local geography, nature, and ethnography. In Russia local lore is known as krayevedenie (ru:Краеведение). It is taught in primary schools. There are also local lore museums known as krayevedcheskie muzei. In modern Russia the concept of \"regional studies\" (ru:Регионоведение) is also considered. In Ukraine, the study of local history and regional ethnography is known as krayeznavstvo (uk:краєзнавство). The National Union of Local Lore Researchers of Ukraine is a professional society for researchers of ethnology and local studies in Ukraine. It was founded in 1925 and has 3,000 members in 17 chapters. The society has published its journal Краєзнавство since 1927. In Poland, the corresponding concept is called touring (pl:Krajoznawstwo), the term known since 1902. In modern Poland various organized krajoznawstwo activities are carried out by Polskie Towarzystwo Turystyczno-Krajoznawcze, roughly translated as \"Polish Tourist and Sightseeing Society\". Belgium The study of local history in Belgium began with the creation of learned societies. After Belgium became independent in 1830, people developed a stronger sense of national and regional identity. This feeling led to the founding",
    "label": 0
  },
  {
    "text": "and Sightseeing Society\". Belgium The study of local history in Belgium began with the creation of learned societies. After Belgium became independent in 1830, people developed a stronger sense of national and regional identity. This feeling led to the founding of groups such as circles, academies, and associations focused on local history. These organizations aimed to study the history of specific places and to protect both physical objects and cultural traditions from the past. Learned societies played a key role in researching, recording, and preserving the local history and heritage of communities across Belgium. See also Chorography English local history Family history Historic preservation Local museum Microhistory One-place study Rural history American urban history Local heritage book References Further reading Beckett, John (2007). Writing local history. Manchester: Manchester University Press. ISBN 978-1-84779-513-7. Celoria, Francis (1958) Teach Yourself Local History. London: English Universities Press Hey, David (1996) The Oxford Companion to Local and Family History. Oxford University Press. ISBN 978-0-19-104493-9 (as editor) Hey, David. (1997; published online 2003) The Oxford Dictionary of Local and Family History ISBN 9780198600800 Morrill, John. \"The Diversity of Local History.\" Historical Journal 24, no. 3 (1981): 717-29. online on England. Parker, Donald Dean (1944). Bertha E. Josephson (ed.). Local History: How to Gather it, Write it, and Publish it. New York, USA: Social Science Research Council – via HathiTrust. (fulltext) Richardson, John (1974) The Local Historian's Encyclopedia. New Barnet: Historical Publications Rogers, Alan (1972) This Was their World: approaches to local history. London: BBC (to accompany the BBC radio series broadcast Apr.–Jun. & Oct.–Dec. 1972) External links American Local History Network, United States. Local History from the National Archives, UK. Local History Trails from the BBC. Waverly Lowell. \"Doing Your Neighborhood History\". Preserving Your History. USA: Society of California Archivists. Library guides \"U.S. local history: a",
    "label": 0
  },
  {
    "text": "National memory is a form of collective memory defined by shared experiences and culture. It is an integral part to national identity. It represents one specific form of cultural memory, which makes an essential contribution to national group cohesion. Historically national communities have drawn upon commemorative ceremonies and monuments, myths and rituals, glorified individuals, objects, and events in their own history to produce a common narrative. According to Lorraine Ryan, national memory is based on the public's reception of national historic narratives and the ability of people to affirm the legitimacy of these narratives. Conflicting versions, dynamicity, manipulation and subjectivity National memory typically consists of a shared interpretation of a nation's past. Such interpretations can vary and sometimes compete. They can get challenged and augmented by a range of interest groups, fighting to have their histories acknowledged, documented and commemorated and reshape national stories. Often national memory is adjusted to offer a politicized vision of the past to make a political position appear consistent with national identity. Furthermore, it profoundly affects how historical facts are perceived and recorded and may circumvent or appropriate facts. A repertoire of discursive strategies functions to emotionalize national narrative and nationalize personal pasts. National memory has been used calculatedly by governments for dynastic, political, religious and cultural purposes since as early as the sixteenth century. Marketing of memory by the culture industry and its instrumentalisation for political purposes can both be seen as serious threats to the objective understanding of a nation's past. Lorraine Ryan notes that individual memory both shapes and is shaped by national memory, and that there is a competition between the dominant and individual memories of a nation. Hyung Park states that the nation is continuously revived, re-imagined, reconstituted, through shared memories among its citizens. National memories may also conflict with",
    "label": 0
  },
  {
    "text": "national memory, and that there is a competition between the dominant and individual memories of a nation. Hyung Park states that the nation is continuously revived, re-imagined, reconstituted, through shared memories among its citizens. National memories may also conflict with the other nations' collective memory. Role of the media Reports that are narrated in terms of national memory characterize the past in ways that merge the past, the present and the future into \"a single ongoing tale\". Pierre Nora argues that a \"democratisation of history\" allows for emancipatory versions of the past to surface: National memory cannot come into being until the historical framework of the nation has been shattered. It reflects the abandonment of the traditional channels and modes of transmission of the past and the desacralisation of such primary sites of initiation as the school, the family, the museum, and the monument: what was once the responsibility of these institutions has now flowed over into the public domain and been taken over by the media and tourist industry However, national history being passed on by the culture industry, such as by historical films, can be seen as serious threats to the objective understanding of a nation's past. International media Nations' memories can be shared across nations via media such as the Internet (through social media and other means of widespread communication) and news outlets. Effects and functions National memory can be a force of cohesion as well as division and conflict. It can foster constructive national reforms, international communities and agreements, dialogue as well as deepen problematic courses and rhetoric. Identity crisis can occur in a country due to large-scale negative events such as crime, terroristic attacks (on a national or international scale), war, and large changes made over a short period of time. The negative mood created",
    "label": 0
  },
  {
    "text": "and rhetoric. Identity crisis can occur in a country due to large-scale negative events such as crime, terroristic attacks (on a national or international scale), war, and large changes made over a short period of time. The negative mood created by these events will eventually find a way to be expressed. This crisis can also occur during periods of economic political uncertainty, which can lead to citizens becoming uncertain of and questioning their own identities or losing them altogether. New developments, processes, problems and events are often made sense of and contextualized by drawing from national memory. Critical national memory Critical history or historic memory cuts from national memory's tradition centric to national heritage and orients itself towards a specialized study of history in a more sociological manner. It has been proposed that the unthinkable ought not to be unmasked but that instead what made it thinkable should be reconstructed and that the difficulty of discussing the non-places or the bad places of national memory make it necessary to include forgetfulness and amnesia in the concept. The absence of belief in a shared past may be another factor. National memory may lead to questioning the nation as it is as well as its identity and imply a societal negotiation of what the country wishes to be as a nation. To understand the links between memory, forgetfulness, identity and the imaginary construction of the nation analysis of the discourse in the places of memory is fundamental as in all writings of national history an image of the nation is being restructured. See also References Further reading Les Lieux de Mémoire Koshar, Rudy J. (9 November 2000). Germany's Transient Pasts: Preservation and National Memory in the Twentieth Century. Univ of North Carolina Press. ISBN 9780807862629.",
    "label": 0
  },
  {
    "text": "Technology is the application of conceptual knowledge to achieve practical goals, especially in a reproducible way. The word technology can also mean the products resulting from such efforts, including both tangible tools such as utensils or machines, and intangible ones such as software. Technology plays a critical role in science, engineering, and everyday life. Technological advancements have led to significant changes in society. The earliest known technology is the stone tool, used during prehistory, followed by the control of fire—which in turn contributed to the growth of the human brain and the development of language during the Ice Age, according to the cooking hypothesis. The invention of the wheel in the Bronze Age allowed greater travel and the creation of more complex machines. More recent technological inventions, including the printing press, telephone, and the Internet, have lowered barriers to communication and ushered in the knowledge economy. While technology contributes to economic development and improves human prosperity, it can also have negative impacts like pollution and resource depletion, and can cause social harms like technological unemployment resulting from automation. As a result, philosophical and political debates about the role and use of technology, the ethics of technology, and ways to mitigate its downsides are ongoing. Etymology Technology is a term dating back to the early 17th century that meant 'systematic treatment' (from Greek Τεχνολογία, from the Greek: τέχνη, romanized: tékhnē, lit. 'craft, art' and -λογία (-logíā), 'study, knowledge'). It is predated in use by the Ancient Greek word τέχνη (tékhnē), used to mean 'knowledge of how to make things', which encompassed activities like architecture. Starting in the 19th century, continental Europeans started using the terms Technik (German) or technique (French) to refer to a 'way of doing', which included all technical arts, such as dancing, navigation, or printing, whether or not",
    "label": 0
  },
  {
    "text": "activities like architecture. Starting in the 19th century, continental Europeans started using the terms Technik (German) or technique (French) to refer to a 'way of doing', which included all technical arts, such as dancing, navigation, or printing, whether or not they required tools or instruments. At the time, Technologie (German and French) referred either to the academic discipline studying the \"methods of arts and crafts\", or to the political discipline \"intended to legislate on the functions of the arts and crafts.\" The distinction between Technik and Technologie is absent in English, and so both were translated as technology. The term was previously uncommon in English and mostly referred to the academic discipline, as in the Massachusetts Institute of Technology. In the 20th century, as a result of scientific progress and the Second Industrial Revolution, technology stopped being considered a distinct academic discipline and took on the meaning: the systemic use of knowledge to practical ends. History Prehistoric Tools were initially developed by hominids through observation and trial and error. Around 2 Mya (million years ago), they learned to make the first stone tools by hammering flakes off a pebble, forming a sharp hand axe. This practice was refined 75 kya (thousand years ago) into pressure flaking, enabling much finer work. The discovery of fire was described by Charles Darwin as \"possibly the greatest ever made by man\". Archaeological, dietary, and social evidence point to \"continuous [human] fire-use\" at least 1.5 Mya. Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten. The cooking hypothesis proposes that the ability to cook promoted an increase in hominid brain size, though some researchers find the evidence inconclusive. Archaeological evidence of hearths was",
    "label": 0
  },
  {
    "text": "nutrient value and broadening the number of foods that could be eaten. The cooking hypothesis proposes that the ability to cook promoted an increase in hominid brain size, though some researchers find the evidence inconclusive. Archaeological evidence of hearths was dated to 790 kya; researchers believe this is likely to have intensified human socialization and may have contributed to the emergence of language. Other technological advances made during the Paleolithic era include clothing and shelter. No consensus exists on the approximate time of adoption of either technology, but archaeologists have found archaeological evidence of clothing 90-120 kya and shelter 450 kya. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 kya, humans were constructing temporary wood huts. Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate out of Africa around 200 kya, initially moving to Eurasia. Neolithic The Neolithic Revolution (or First Agricultural Revolution) brought about an acceleration of technological innovation, and a consequent increase in social complexity. The invention of the polished stone axe was a major advance that allowed large-scale forest clearance and farming. This use of polished stone axes increased greatly in the Neolithic but was originally used in the preceding Mesolithic in some areas such as Ireland. Agriculture fed larger populations, and the transition to sedentism allowed for the simultaneous raising of more children, as infants no longer needed to be carried around by nomads. Additionally, children could contribute labor to the raising of crops more readily than they could participate in hunter-gatherer activities. With this increase in population and availability of labor came an increase in labor specialization. What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations,",
    "label": 0
  },
  {
    "text": "they could participate in hunter-gatherer activities. With this increase in population and availability of labor came an increase in labor specialization. What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war among adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role. The invention of writing led to the spread of cultural knowledge and became the basis for history, libraries, schools, and scientific research. Continuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge gold, copper, silver, and lead – native metals found in relatively pure form in nature. The advantages of copper tools over stone, bone and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 kya). Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4,000 BCE). The first use of iron alloys such as steel dates to around 1,800 BCE. Ancient After harnessing fire, humans discovered other forms of energy. The earliest known use of wind power is the sailing ship; the earliest record of a ship under sail is that of a Nile boat dating to around 7,000 BCE. From prehistoric times, Egyptians likely used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to",
    "label": 0
  },
  {
    "text": "earliest record of a ship under sail is that of a Nile boat dating to around 7,000 BCE. From prehistoric times, Egyptians likely used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and \"catch\" basins. The ancient Sumerians in Mesopotamia used a complex system of canals and levees to divert water from the Tigris and Euphrates rivers for irrigation. Archaeologists estimate that the wheel was invented independently and concurrently in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture), and Central Europe. Time estimates range from 5,500 to 3,000 BCE with most experts putting it closer to 4,000 BCE. The oldest artifacts with drawings depicting wheeled carts date from about 3,500 BCE. More recently, the oldest-known wooden wheel in the world as of 2024 was found in the Ljubljana Marsh of Slovenia; Austrian experts have established that the wheel is between 5,100 and 5,350 years old. The invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. The ancient Sumerians used a potter's wheel and may have invented it. A stone pottery wheel found in the city-state of Ur dates to around 3,429 BCE, and even older fragments of wheel-thrown pottery have been found in the same area. Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois and were first used in Mesopotamia and Iran in around 3,000 BCE. The oldest known constructed roadways are the stone-paved streets of the city-state",
    "label": 0
  },
  {
    "text": "that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois and were first used in Mesopotamia and Iran in around 3,000 BCE. The oldest known constructed roadways are the stone-paved streets of the city-state of Ur, dating to c. 4,000 BCE, and timber roads leading through the swamps of Glastonbury, England, dating to around the same period. The first long-distance road, which came into use around 3,500 BCE, spanned 2,400 km from the Persian Gulf to the Mediterranean Sea, but was not paved and was only partially maintained. In around 2,000 BCE, the Minoans on the Greek island of Crete built a 50 km road leading from the palace of Gortyn on the south side of the island, through the mountains, to the palace of Knossos on the north side of the island. Unlike the earlier road, the Minoan road was completely paved. Ancient Minoan private homes had running water. A bathtub virtually identical to modern ones was unearthed at the Palace of Knossos. Several Minoan private homes also had toilets, which could be flushed by pouring water down the drain. The ancient Romans had many public flush toilets, which emptied into an extensive sewage system. The primary sewer in Rome was the Cloaca Maxima; construction began on it in the sixth century BCE and it is still in use today. The ancient Romans also had a complex system of aqueducts, which were used to transport water across long distances. The first Roman aqueduct was built in 312 BCE. The eleventh and final ancient Roman aqueduct was built in 226 CE. Put together, the Roman aqueducts extended over 450 km, but less than 70 km of this was above ground and supported by arches. Pre-modern Innovations continued through the Middle Ages with the introduction",
    "label": 0
  },
  {
    "text": "Roman aqueduct was built in 226 CE. Put together, the Roman aqueducts extended over 450 km, but less than 70 km of this was above ground and supported by arches. Pre-modern Innovations continued through the Middle Ages with the introduction of silk production (in Asia and later Europe), the horse collar, and horseshoes. Simple machines (such as the lever, the screw, and the pulley) were combined into more complicated tools, such as the wheelbarrow, windmills, and clocks. A system of universities developed and spread scientific ideas and practices, including Oxford and Cambridge. The Renaissance era produced many innovations, including the introduction of the movable type printing press to Europe, which facilitated the communication of knowledge. Technology became increasingly influenced by science, beginning a cycle of mutual advancement. Modern Starting in the United Kingdom in the 18th century, the discovery of steam power set off the Industrial Revolution, which saw wide-ranging technological discoveries, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, and the widespread application of the factory system. This was followed a century later by the Second Industrial Revolution which led to rapid scientific discovery, standardization, and mass production. New technologies were developed, including sewage systems, electricity, light bulbs, electric motors, railroads, automobiles, and airplanes. These technological advances led to significant developments in medicine, chemistry, physics, and engineering. They were accompanied by consequential social change, with the introduction of skyscrapers accompanied by rapid urbanization. Communication improved with the invention of the telegraph, the telephone, the radio, and television. The 20th century brought a host of innovations. In physics, the discovery of nuclear fission in the Atomic Age led to both nuclear weapons and nuclear power. Analog computers were invented and asserted dominance in processing complex data. While the invention of vacuum tubes allowed for digital computing with",
    "label": 0
  },
  {
    "text": "physics, the discovery of nuclear fission in the Atomic Age led to both nuclear weapons and nuclear power. Analog computers were invented and asserted dominance in processing complex data. While the invention of vacuum tubes allowed for digital computing with computers like the ENIAC, their sheer size precluded widespread use until innovations in quantum physics allowed for the invention of the transistor in 1947, which significantly compacted computers and led the digital transition. Information technology, particularly optical fiber and optical amplifiers, allowed for simple and fast long-distance communication, which ushered in the Information Age and the birth of the Internet. The Space Age began with the launch of Sputnik 1 in 1957, and later the launch of crewed missions to the moon in the 1960s. Organized efforts to search for extraterrestrial intelligence have used radio telescopes to detect signs of technology use, or technosignatures, given off by alien civilizations. In medicine, new technologies were developed for diagnosis (CT, PET, and MRI scanning), treatment (like the dialysis machine, defibrillator, pacemaker, and a wide array of new pharmaceutical drugs), and research (like interferon cloning and DNA microarrays). Complex manufacturing and construction techniques and organizations are needed to make and maintain more modern technologies, and entire industries have arisen to develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have developed to support them, including engineering, medicine, and computer science; and other fields have become more complex, such as construction, transportation, and architecture. Impact Technological change is the largest cause of long-term economic growth. Throughout human history, energy production was the main constraint on economic development, and new technologies allowed humans to",
    "label": 0
  },
  {
    "text": "fields have become more complex, such as construction, transportation, and architecture. Impact Technological change is the largest cause of long-term economic growth. Throughout human history, energy production was the main constraint on economic development, and new technologies allowed humans to significantly increase the amount of available energy. First came fire, which made edible a wider variety of foods, and made it less physically demanding to digest them. Fire also enabled smelting, and the use of tin, copper, and iron tools, used for hunting or tradesmanship. Then came the agricultural revolution: humans no longer needed to hunt or gather to survive, and began to settle in towns and cities, forming more complex societies, with militaries and more organized forms of religion. Technologies have contributed to human welfare through increased prosperity, improved comfort and quality of life, and medical progress, but they can also disrupt existing social hierarchies, cause pollution, and harm individuals or groups. Recent years have brought about a rise in social media's cultural prominence, with potential repercussions on democracy, and economic and social life. Early on, the internet was seen as a \"liberation technology\" that would democratize knowledge, improve access to education, and promote democracy. Modern research has turned to investigate the internet's downsides, including disinformation, polarization, hate speech, and propaganda. Since the 1970s, technology's impact on the environment has been criticized, leading to a surge in investment in solar, wind, and other forms of clean energy. Social Jobs Since the invention of the wheel, technologies have helped increase humans' economic output. Past automation has both substituted and complemented labor; machines replaced humans at some lower-paying jobs (for example in agriculture), but this was compensated by the creation of new, higher-paying jobs. Studies have found that computers did not create significant net technological unemployment. Due to artificial intelligence being",
    "label": 0
  },
  {
    "text": "labor; machines replaced humans at some lower-paying jobs (for example in agriculture), but this was compensated by the creation of new, higher-paying jobs. Studies have found that computers did not create significant net technological unemployment. Due to artificial intelligence being far more capable than computers, and still being in its infancy, it is not known whether it will follow the same trend; the question has been debated at length among economists and policymakers. A 2017 survey found no clear consensus among economists on whether AI would increase long-term unemployment. According to the World Economic Forum's \"The Future of Jobs Report 2020\", AI is predicted to replace 85 million jobs worldwide, and create 97 million new jobs by 2025. From 1990 to 2007, a study in the U.S. by MIT economist Daron Acemoglu showed that an addition of one robot for every 1,000 workers decreased the employment-to-population ratio by 0.2%, or about 3.3 workers, and lowered wages by 0.42%. Concerns about technology replacing human labor however are long-lasting. As US president Lyndon Johnson said in 1964, \"Technology is creating both new opportunities and new obligations for us, opportunity for greater productivity and progress; obligation to be sure that no workingman, no family must pay an unjust price for progress.\" upon signing the National Commission on Technology, Automation, and Economic Progress bill. Security With the growing reliance of technology, there have been security and privacy concerns along with it. Billions of people use different online payment methods, such as WeChat Pay, PayPal, Alipay, and much more to help transfer money. Although security measures are placed, some criminals are able to bypass them. In March 2022, North Korea used Blender.io, a mixer which helped them to hide their cryptocurrency exchanges, to launder over $20.5 million in cryptocurrency, from Axie Infinity, and steal over",
    "label": 0
  },
  {
    "text": "measures are placed, some criminals are able to bypass them. In March 2022, North Korea used Blender.io, a mixer which helped them to hide their cryptocurrency exchanges, to launder over $20.5 million in cryptocurrency, from Axie Infinity, and steal over $600 million worth of cryptocurrency from the game's owner. Because of this, the U.S. Treasury Department sanctioned Blender.io, which marked the first time it has taken action against a mixer, to try to crack down on North Korean hackers. The privacy of cryptocurrency has been debated. Although many customers like the privacy of cryptocurrency, many also argue that it needs more transparency and stability. Environmental Technology can have both positive and negative effects on the environment. Environmental technology, describes an array of technologies which seek to reverse, mitigate or halt environmental damage to the environment. This can include measures to halt pollution through environmental regulations, capture and storage of pollution, or using pollutant byproducts in other industries. Other examples of environmental technology include deforestation and the reversing of deforestation. Emerging technologies in the fields of climate engineering may be able to halt or reverse global warming and its environmental impacts, although this remains highly controversial. As technology has advanced, so too has the negative environmental impact, with increased release of greenhouse gases, including methane, nitrous oxide and carbon dioxide, into the atmosphere, causing the greenhouse effect. This continues to gradually heat the earth, causing global warming and climate change. Measures of technological innovation correlates with a rise in greenhouse gas emissions. Pollution Pollution, the presence of contaminants in an environment that causes adverse effects, could have been present as early as the Inca Empire. They used a lead sulfide flux in the smelting of ores, along with the use of a wind-drafted clay kiln, which released lead into the atmosphere",
    "label": 0
  },
  {
    "text": "that causes adverse effects, could have been present as early as the Inca Empire. They used a lead sulfide flux in the smelting of ores, along with the use of a wind-drafted clay kiln, which released lead into the atmosphere and the sediment of rivers. Philosophy Philosophy of technology is a branch of philosophy that studies the \"practice of designing and creating artifacts\", and the \"nature of the things so created.\" It emerged as a discipline over the past two centuries, and has grown \"considerably\" since the 1970s. The humanities philosophy of technology is concerned with the \"meaning of technology for, and its impact on, society and culture\". Initially, technology was seen as an extension of the human organism that replicated or amplified bodily and mental faculties. Marx framed it as a tool used by capitalists to oppress the proletariat, but believed that technology would be a fundamentally liberating force once it was \"freed from societal deformations\". Second-wave philosophers like Ortega later shifted their focus from economics and politics to \"daily life and living in a techno-material culture\", arguing that technology could oppress \"even the members of the bourgeoisie who were its ostensible masters and possessors.\" Third-stage philosophers like Don Ihde and Albert Borgmann represent a turn toward de-generalization and empiricism, and considered how humans can learn to live with technology. Early scholarship on technology was split between two arguments: technological determinism, and social construction. Technological determinism is the idea that technologies cause unavoidable social changes. It usually encompasses a related argument, technological autonomy, which asserts that technological progress follows a natural progression and cannot be prevented. Social constructivists argue that technologies follow no natural progression, and are shaped by cultural values, laws, politics, and economic incentives. Modern scholarship has shifted towards an analysis of sociotechnical systems, \"assemblages of things,",
    "label": 0
  },
  {
    "text": "a natural progression and cannot be prevented. Social constructivists argue that technologies follow no natural progression, and are shaped by cultural values, laws, politics, and economic incentives. Modern scholarship has shifted towards an analysis of sociotechnical systems, \"assemblages of things, people, practices, and meanings\", looking at the value judgments that shape technology. Cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called \"technopolies\", societies that are dominated by an ideology of technological and scientific progress to the detriment of other cultural practices, values, and world views. Herbert Marcuse and John Zerzan suggest that technological society will inevitably deprive us of our freedom and psychological health. Ethics The ethics of technology is an interdisciplinary subfield of ethics that analyzes technology's ethical implications and explores ways to mitigate potential negative impacts of new technologies. There is a broad range of ethical issues revolving around technology, from specific areas of focus affecting professionals working with technology to broader social, ethical, and legal issues concerning the role of technology in society and everyday life. Prominent debates have surrounded genetically modified organisms, the use of robotic soldiers, algorithmic bias, and the issue of aligning AI behavior with human values. Technology ethics encompasses several key fields: Bioethics looks at ethical issues surrounding biotechnologies and modern medicine, including cloning, human genetic engineering, and stem cell research. Computer ethics focuses on issues related to computing. Cyberethics explores internet-related issues like intellectual property rights, privacy, and censorship. Nanoethics examines issues surrounding the alteration of matter at the atomic and molecular level in various disciplines including computer science, engineering, and biology. And engineering ethics deals with the professional standards of engineers, including software engineers and their moral responsibilities to the public. A wide branch of technology ethics is concerned with the ethics of artificial intelligence:",
    "label": 0
  },
  {
    "text": "including computer science, engineering, and biology. And engineering ethics deals with the professional standards of engineers, including software engineers and their moral responsibilities to the public. A wide branch of technology ethics is concerned with the ethics of artificial intelligence: it includes robot ethics, which deals with ethical issues involved in the design, construction, use, and treatment of robots, as well as machine ethics, which is concerned with ensuring the ethical behavior of artificially intelligent agents. Within the field of AI ethics, significant yet-unsolved research problems include AI alignment (ensuring that AI behaviors are aligned with their creators' intended goals and interests) and the reduction of algorithmic bias. Some researchers have warned against the hypothetical risk of an AI takeover, and have advocated for the use of AI capability control in addition to AI alignment methods. Other fields of ethics have had to contend with technology-related issues, including military ethics, media ethics, and educational ethics. Futures studies Futures studies is the study of social and technological progress. It aims to explore the range of plausible futures and incorporate human values in the development of new technologies. More generally, futures researchers are interested in improving \"the freedom and welfare of humankind\". It relies on a thorough quantitative and qualitative analysis of past and present technological trends, and attempts to rigorously extrapolate them into the future. Science fiction is often used as a source of ideas. Futures research methodologies include survey research, modeling, statistical analysis, and computer simulations. Existential risk Existential risk researchers analyze risks that could lead to human extinction or civilizational collapse, and look for ways to build resilience against them. Relevant research centers include the Cambridge Center for the Study of Existential Risk, and the Stanford Existential Risk Initiative. Future technologies may contribute to the risks of artificial general",
    "label": 0
  },
  {
    "text": "civilizational collapse, and look for ways to build resilience against them. Relevant research centers include the Cambridge Center for the Study of Existential Risk, and the Stanford Existential Risk Initiative. Future technologies may contribute to the risks of artificial general intelligence, biological warfare, nuclear warfare, nanotechnology, anthropogenic climate change, global warming, or stable global totalitarianism, though technologies may also help us mitigate asteroid impacts and gamma-ray bursts. In 2019 philosopher Nick Bostrom introduced the notion of a vulnerable world, \"one in which there is some level of technological development at which civilization almost certainly gets devastated by default\", citing the risks of a pandemic caused by bioterrorists, or an arms race triggered by the development of novel armaments and the loss of mutual assured destruction. He invites policymakers to question the assumptions that technological progress is always beneficial, that scientific openness is always preferable, or that they can afford to wait until a dangerous technology has been invented before they prepare mitigations. Emerging technologies Emerging technologies are novel technologies whose development or practical applications are still largely unrealized. They include nanotechnology, biotechnology, robotics, 3D printing, and blockchains. In 2005, futurist Ray Kurzweil claimed the next technological revolution would rest upon advances in genetics, nanotechnology, and robotics, with robotics being the most impactful of the three technologies. Genetic engineering will allow far greater control over human biological nature through a process called directed evolution. Some thinkers believe that this may shatter our sense of self, and have urged for renewed public debate exploring the issue more thoroughly; others fear that directed evolution could lead to eugenics or extreme social inequality. Nanotechnology will grant us the ability to manipulate matter \"at the molecular and atomic scale\", which could allow us to reshape ourselves and our environment in fundamental ways. Nanobots could be",
    "label": 0
  },
  {
    "text": "evolution could lead to eugenics or extreme social inequality. Nanotechnology will grant us the ability to manipulate matter \"at the molecular and atomic scale\", which could allow us to reshape ourselves and our environment in fundamental ways. Nanobots could be used within the human body to destroy cancer cells or form new body parts, blurring the line between biology and technology. Autonomous robots have undergone rapid progress, and are expected to replace humans at many dangerous tasks, including search and rescue, bomb disposal, firefighting, and war. Estimates on the advent of artificial general intelligence vary, but half of machine learning experts surveyed in 2018 believe that AI will \"accomplish every task better and more cheaply\" than humans by 2063, and automate all human jobs by 2140. This expected technological unemployment has led to calls for increased emphasis on computer science education and debates about universal basic income. Political science experts predict that this could lead to a rise in extremism, while others see it as an opportunity to usher in a post-scarcity economy. Movements Appropriate technology Some segments of the 1960s hippie counterculture grew to dislike urban living and developed a preference for locally autonomous, sustainable, and decentralized technology, termed appropriate technology. This later influenced hacker culture and technopaganism. Technological utopianism Technological utopianism refers to the belief that technological development is a moral good, which can and should bring about a utopia, that is, a society in which laws, governments, and social conditions serve the needs of all its citizens. Examples of techno-utopian goals include post-scarcity economics, life extension, mind uploading, cryonics, and the creation of artificial superintelligence. Major techno-utopian movements include transhumanism and singularitarianism. The transhumanism movement is founded upon the \"continued evolution of human life beyond its current human form\" through science and technology, informed by \"life-promoting principles",
    "label": 0
  },
  {
    "text": "uploading, cryonics, and the creation of artificial superintelligence. Major techno-utopian movements include transhumanism and singularitarianism. The transhumanism movement is founded upon the \"continued evolution of human life beyond its current human form\" through science and technology, informed by \"life-promoting principles and values.\" The movement gained wider popularity in the early 21st century. Singularitarians believe that machine superintelligence will \"accelerate technological progress\" by orders of magnitude and \"create even more intelligent entities ever faster\", which may lead to a pace of societal and technological change that is \"incomprehensible\" to us. This event horizon is known as the technological singularity. Major figures of techno-utopianism include Ray Kurzweil and Nick Bostrom. Techno-utopianism has attracted both praise and criticism from progressive, religious, and conservative thinkers. Anti-technology backlash Technology's central role in our lives has drawn concerns and backlash. The backlash against technology is not a uniform movement and encompasses many heterogeneous ideologies. The earliest known revolt against technology was Luddism, a pushback against early automation in textile production. Automation had resulted in a need for fewer workers, a process known as technological unemployment. Between the 1970s and 1990s, American terrorist Ted Kaczynski carried out a series of bombings across America and published the Unabomber Manifesto denouncing technology's negative impacts on nature and human freedom. The essay resonated with a large part of the American public. It was partly inspired by Jacques Ellul's The Technological Society. Some subcultures, like the off-the-grid movement, advocate a withdrawal from technology and a return to nature. The ecovillage movement seeks to reestablish harmony between technology and nature. Relation to science and engineering Engineering is the process by which technology is developed. It often requires problem-solving under strict constraints. Technological development is \"action-oriented\", while scientific knowledge is fundamentally explanatory. Polish philosopher Henryk Skolimowski framed it like so: \"science concerns itself",
    "label": 0
  },
  {
    "text": "science and engineering Engineering is the process by which technology is developed. It often requires problem-solving under strict constraints. Technological development is \"action-oriented\", while scientific knowledge is fundamentally explanatory. Polish philosopher Henryk Skolimowski framed it like so: \"science concerns itself with what is, technology with what is to be.\" The direction of causality between scientific discovery and technological innovation has been debated by scientists, philosophers and policymakers. Because innovation is often undertaken at the edge of scientific knowledge, most technologies are not derived from scientific knowledge, but instead from engineering, tinkering and chance. For example, in the 1940s and 1950s, when knowledge of turbulent combustion or fluid dynamics was still crude, jet engines were invented through \"running the device to destruction, analyzing what broke [...] and repeating the process\". Scientific explanations often follow technological developments rather than preceding them. Many discoveries also arose from pure chance, like the discovery of penicillin as a result of accidental lab contamination. Since the 1960s, the assumption that government funding of basic research would lead to the discovery of marketable technologies has lost credibility. Probabilist Nassim Taleb argues that national research programs that implement the notions of serendipity and convexity through frequent trial and error are more likely to lead to useful innovations than research that aims to reach specific outcomes. Despite this, modern technology is increasingly reliant on deep, domain-specific scientific knowledge. In 1975, there was an average of one citation of scientific literature in every three patents granted in the U.S.; by 1989, this increased to an average of one citation per patent. The average was skewed upwards by patents related to the pharmaceutical industry, chemistry, and electronics. A 2021 analysis shows that patents that are based on scientific discoveries are on average 26% more valuable than equivalent non-science-based patents. Other animal",
    "label": 0
  },
  {
    "text": "patent. The average was skewed upwards by patents related to the pharmaceutical industry, chemistry, and electronics. A 2021 analysis shows that patents that are based on scientific discoveries are on average 26% more valuable than equivalent non-science-based patents. Other animal species The use of basic technology is also a feature of non-human animal species. Tool use was once considered a defining characteristic of the genus Homo. This view was supplanted after discovering evidence of tool use among chimpanzees and other primates, dolphins, and crows. For example, researchers have observed wild chimpanzees using basic foraging tools, pestles, levers, using leaves as sponges, and tree bark or vines as probes to fish termites. West African chimpanzees use stone hammers and anvils for cracking nuts, as do capuchin monkeys of Boa Vista, Brazil. Tool use is not the only form of animal technology use; for example, beaver dams, built with wooden sticks or large stones, are a technology with \"dramatic\" impacts on river habitats and ecosystems. In popular culture The relationship of humanity with technology has been explored in science-fiction literature, for example in Brave New World, A Clockwork Orange, Nineteen Eighty-Four, Isaac Asimov's essays, and movies like Minority Report, Total Recall, Gattaca, and Inception. It has spawned the dystopian and futuristic cyberpunk genre, which juxtaposes futuristic technology with societal collapse, dystopia or decay. Notable cyberpunk works include William Gibson's Neuromancer novel, and movies like Blade Runner, and The Matrix. See also References Citations Sources Further reading Gribbin, John, \"Alone in the Milky Way: Why we are probably the only intelligent life in the galaxy\", Scientific American, vol. 319, no. 3 (September 2018), pp. 94–99. \"Is life likely to exist elsewhere in the [Milky Way] galaxy? Almost certainly yes, given the speed with which it appeared on Earth. Is another technological civilization likely",
    "label": 0
  },
  {
    "text": "The following outline is provided as an overview of and topical guide to technology: Technology – collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technology. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments. Components of technology Knowledge – Awareness of facts or being competent Engineering – Applied science and research Process – Series of activities Science – Systematic endeavour to gain knowledge Skill – Ability to carry out a task Tool – Object used to achieve a goal Weapon – Implement or device used to inflict damage, harm, or kill Utensil – Tool used for food preparation Equipment – Items required to exercise a certain activity Invention – Novel device, material or technical process Machinery – Powered mechanical device Structure – Arrangement of interrelated elements in an object/system, or the object/system itself Building – Structure, typically with a roof and walls, standing more or less permanently in one place Road – Land route Bridge – Structure built to span physical obstacles Canal – Artificial channel for water Dam – Barrier that stops or restricts the flow of surface or underground streams Man-made systems – Interrelated entities that form a whole Infrastructure – Facilities and systems serving society Public utility – Entity which operates public service infrastructure Branches of technology Aerospace – flight or transport above the surface of the Earth. Space exploration – the physical investigation of the space more than 100 km above the Earth by either crewed or uncrewed spacecraft. General aviation Aeronautics Astronautics Aerospace engineering Applied physics – physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between \"pure\"",
    "label": 0
  },
  {
    "text": "the Earth by either crewed or uncrewed spacecraft. General aviation Aeronautics Astronautics Aerospace engineering Applied physics – physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between \"pure\" physics and engineering. Agriculture – cultivation of plants, animals, and other living organisms. Fishing – activity of trying to catch fish. Fish are normally caught in the wild. Techniques for catching fish include hand gathering, spearing, netting, angling and trapping. Fisheries – a fishery is an entity engaged in raising or harvesting fish which is determined by some authority to be a fishery. According to the FAO, a fishery is typically defined in terms of the \"people involved, species or type of fish, area of water or seabed, method of fishing, class of boats, purpose of the activities or a combination of the foregoing features\". Fishing industry – industry or activity concerned with taking, culturing, processing, preserving, storing, transporting, marketing or selling fish or fish products. It is defined by the FAO as including recreational, subsistence and commercial fishing, and the harvesting, processing, and marketing sectors. Forestry – art and science of tree resources, including plantations and natural stands. The main goal of forestry is to create and implement systems that allow forests to continue a sustainable provision of environmental supplies and services. Organic gardening and farming Sustainable agriculture Communication Books Telecommunication – the transfer of information at a distance, including signaling, telegraphy, telephony, telemetry, radio, television, and data communications. Radio – Aural or encoded telecommunications. Internet – the global system of interconnected computer networks that use the standard Internet Protocol Suite (TCP/IP). Technology of television Television broadcasting – Visual and aural telecommunications. Computing – any goal-oriented activity requiring, benefiting from, or creating computers. Computing includes designing and building hardware",
    "label": 0
  },
  {
    "text": "system of interconnected computer networks that use the standard Internet Protocol Suite (TCP/IP). Technology of television Television broadcasting – Visual and aural telecommunications. Computing – any goal-oriented activity requiring, benefiting from, or creating computers. Computing includes designing and building hardware and software systems; processing, structuring, and managing various kinds of information; doing scientific research on and with computers; making computer systems behave intelligently; creating and using communications and entertainment media; and more. Computer engineering – discipline that integrates several fields of electrical engineering and computer science required to develop computer systems, from designing individual microprocessors, personal computers, and supercomputers, to circuit design. Computers – general purpose devices that can be programmed to carry out a finite set of arithmetic or logical operations. Since a sequence of operations can be readily changed, computers can solve more than one kind of problem. Computer science – the study of the theoretical foundations of information and computation and of practical techniques for their implementation and application in computer systems. Artificial intelligence – intelligence of machines and the branch of computer science that aims to create it. Natural language processing Object recognition – in computer vision, this is the task of finding a given object in an image or video sequence. Cryptography – the technology to secure communications in the presence of third parties. Human-computer interaction Information technology – the acquisition, processing, storage and dissemination of vocal, pictorial, textual and numerical information by a microelectronics-based combination of computing and telecommunications. Software engineering – the systematic approach to the development, operation, maintenance, and retirement of computer software. Programming – the process of designing, writing, testing, debugging, and maintaining the source code of computer programs. Software development – development of a software product, which entails computer programming (process of writing and maintaining the source code), but also",
    "label": 0
  },
  {
    "text": "software. Programming – the process of designing, writing, testing, debugging, and maintaining the source code of computer programs. Software development – development of a software product, which entails computer programming (process of writing and maintaining the source code), but also encompasses a planned and structured process from the conception of the desired software to its final manifestation. Web design and web development Software – one or more computer programs and data held in the storage of the computer for one or more purposes. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system. Free software – software that can be used, studied, and modified without restriction. Search engines – information retrieval systems designed to help find information stored on a computer system. Internet – the global system of interconnected computer networks that use the standard Internet Protocol Suite (TCP/IP). World Wide Web Computer industry Apple Inc. – manufacturer and retailer of computers, hand-held computing devices, and related products and services. Google – Google Inc. and its Internet services including Google Search. Construction – building or assembly of any physical structure. Design – the art and science of creating the abstract form and function for an object or environment. Architecture – the art and science of designing buildings. Electronics – Electronics comprises the physics, engineering, technology and applications that deal with the emission, flow and control of electrons in vacuum and matter. Energy – In physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object. Energy development – ongoing effort to provide abundant, efficient, and accessible energy resources through knowledge, skills, and construction. Energy storage – the storage of a form of energy that",
    "label": 0
  },
  {
    "text": "in order to perform work on, or to heat, the object. Energy development – ongoing effort to provide abundant, efficient, and accessible energy resources through knowledge, skills, and construction. Energy storage – the storage of a form of energy that can then be used later. Nuclear technology – the technology and application of the spontaneous and induced reactions of atomic nuclei. Wind energy – wind energy is the use of wind to provide the mechanical power through wind turbines to turn electric generators and traditionally to do other work, like milling or pumping. Solar energy – Solar energy is radiant light and heat from the Sun that is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, solar thermal energy, solar architecture, molten salt power plants and artificial photosynthesis. Engineering – the application of science, mathematics, and technology to produce useful goods and systems. Chemical engineering – the technology and application of chemical processes to produce useful materials. Computer engineering – Computer engineering (CE) is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Control engineering – Control engineering or control systems engineering is an engineering discipline that applies automatic control theory to design systems with desired behaviors in control environments. Electrical engineering – the technology and application of electromagnetism, including electricity, electronics, telecommunications, computers, electric power, magnetics, and optics. Climate engineering – the large-scale manipulation of a specific process central to controlling Earth’s climate for the purpose of obtaining a specific benefit. Software engineering – the technology and application of a systematic approach to the development, operation, maintenance, and retirement of computer software. Firefighting – act of extinguishing fires. A firefighter fights fires to prevent destruction of life, property and the environment. Firefighting is",
    "label": 0
  },
  {
    "text": "– the technology and application of a systematic approach to the development, operation, maintenance, and retirement of computer software. Firefighting – act of extinguishing fires. A firefighter fights fires to prevent destruction of life, property and the environment. Firefighting is a professional technical skill. Forensic science – application of a broad spectrum of sciences to answer questions of interest to a legal system. This may be in relation to a crime or a civil action. Health Biotechnology – applied biology that involves the use of living organisms and bioprocesses in engineering, technology, medicine and other fields requiring bioproducts. Ergonomics – the study of designing equipment and devices that fit the human body, its movements, and its cognitive abilities. Hydrology – The study of the movement, distribution, and quality of water on Earth and other planets, including the hydrologic cycle, water resources and environmental watershed sustainability. Industry – production of an economic good or service. Automation – use of machinery to replace human labor. Industrial machinery Machines – devices that perform or assist in performing useful work. Manufacturing – use of machines, tools and labor to produce goods for use or sale. The term may refer to a range of human activity, from handicraft to high tech, but is most commonly applied to industrial production, in which raw materials are transformed into finished goods on a large scale. Robotics – deals with the design, construction, operation, structural disposition, manufacture and application of robots. Object recognition Information science Cartography – the study and practice of making maps. Combining science, aesthetics, and technique, cartography builds on the premise that reality can be modeled in ways that communicate spatial information effectively. Library science – technology related to libraries and the information fields. Military science – the study of the technique, psychology, practice and other",
    "label": 0
  },
  {
    "text": "cartography builds on the premise that reality can be modeled in ways that communicate spatial information effectively. Library science – technology related to libraries and the information fields. Military science – the study of the technique, psychology, practice and other phenomena which constitute war and armed conflict. Mining – extraction of mineral resources from the earth. Nanotechnology – The study of manipulating matter on an atomic and molecular scale. Generally, nanotechnology deals with structures sized between 1 and 100 nanometre in at least one dimension, and involves developing materials or devices possessing at least one dimension within that size. Prehistoric technology – technologies that emerged before recorded history (i.e., before the development of writing). Quantum technology Sustainability – capacity to endure. In ecology, the word describes how biological systems remain diverse and productive over time. Long-lived and healthy wetlands and forests are examples of sustainable biological systems. For humans, sustainability is the potential for long-term maintenance of well being, which has environmental, economic, and social dimensions. Transport – the transfer of people or things from one place to another. Rail transport – means of conveyance of passengers and goods by way of wheeled vehicles running on rail tracks consisting of steel rails installed on sleepers/ties and ballast. Vehicles – mechanical devices for transporting people or things. Automobiles – human-guided powered land-vehicles. Bicycles – human-powered land-vehicles with two or more wheels. Motorcycles – single-track, engine-powered, motor vehicles. They are also called motorbikes, bikes, or cycles. Vehicle components Tires – ring-shaped coverings that fit around wheel rims Technology by region Science and technology in Africa Science and technology in Algeria Science and technology in Angola Science and technology in Morocco Science and technology in South Africa Science and technology in Asia Science and technology in Bangladesh Science and technology in China Science",
    "label": 0
  },
  {
    "text": "technology in Africa Science and technology in Algeria Science and technology in Angola Science and technology in Morocco Science and technology in South Africa Science and technology in Asia Science and technology in Bangladesh Science and technology in China Science and technology in India Science and technology in Indonesia Science and technology in Iran Science and technology in Israel Science and technology in Japan Science and technology in Malaysia Science and technology in Pakistan Science and technology in the Philippines Science and technology in Russia Science and technology in Turkey Science and technology in Europe Science and technology in Albania Science and technology in Belgium Science and technology in Brussels Science and technology in Flanders Science and technology in Wallonia Science and technology in Bulgaria Science and technology in France Science and technology in Germany Science and technology in Hungary Science and technology in Iceland Science and technology in Italy Science and technology in Portugal Science and technology in Romania Science and technology in Russia Science and technology in Spain Science and technology in Switzerland Science and technology in Ukraine Science and technology in the United Kingdom Science and technology in North America Science and technology in Canada Science and technology in the United States Science and technology in Jamaica Science and technology in South America Science and technology in Argentina Science and technology in Colombia Science and technology in Venezuela History of technology History of technology Timelines of technology Man vs. Technology Technology museum History of technology by period Prehistoric technology (outline) Control of fire by early humans Ancient technology – c. 800 BCE – 476 CE Ancient Egyptian technology Ancient Greek technology – c. 800 BCE – 146 BCE Ancient Roman technology – c. 753 BCE – 476 CE Science and technology of the Han dynasty – 206",
    "label": 0
  },
  {
    "text": "technology – c. 800 BCE – 476 CE Ancient Egyptian technology Ancient Greek technology – c. 800 BCE – 146 BCE Ancient Roman technology – c. 753 BCE – 476 CE Science and technology of the Han dynasty – 206 BCE – 220 CE Science and technology of the Tang dynasty – 618–907 Science and technology of the Song dynasty – 960–1279 CE Medieval technology – 5th to 15th century Byzantine technology – 5th to 15th century Islamic Golden Age – 8th to 13th century Science and technology in the Ottoman Empire – 14th to 20th century Industrial Revolution – 18th to 19th century Second Industrial Revolution – 1820–1914 Technology during World War I – 1914–1918 Technology during World War II – 1939–1945 Allied technological cooperation during World War II American military technology during World War II German military technology during World War II 1970s in science and technology 1980s in science and technology 1990s in science and technology 2000s in science and technology 2010s in science and technology Technological ages Stone Age – Prehistoric period before metal tools Bronze Age – Historical period (c. 3300–1200 BCE) Iron Age – Archaeological period The Renaissance – European cultural period of the 14th to 17th centuries Industrial Age – Period of human history from the mid 18th to late 20th centuries Information Age – Industrial shift to information technology Media about the history of technology Connections – documentary television series and 1978 book (\"Connections\" based on the series) created, written and presented by science historian James Burke. It took an interdisciplinary approach to the history of science and invention and demonstrated how various discoveries, scientific achievements, and historical world events were built from one another successively in an interconnected way to bring about particular aspects of modern technology. There were 3 seasons",
    "label": 0
  },
  {
    "text": "to the history of science and invention and demonstrated how various discoveries, scientific achievements, and historical world events were built from one another successively in an interconnected way to bring about particular aspects of modern technology. There were 3 seasons produced, and they aired in 1978, 1994, and 1997. The Day the Universe Changed – documentary television series written and presented by science historian James Burke, originally broadcast in 1985 by the BBC. The series' primary focus is on the effect of advances in science and technology on western society in its philosophical aspects. Ran for one season, in 1986. History of technology by region History of science and technology in the Mediterranean Ancient Greek technology Ancient Roman technology Timeline of Polish science and technology History of science and technology in Africa History of science and technology in Asia History of science and technology in China Science and technology of the Han dynasty Science and technology of the Tang dynasty Science and technology of the Song dynasty History of science and technology in the People's Republic of China History of science and technology in the Indian subcontinent Science and technology in ancient India History of science and technology in Korea Science and Technology in the Ottoman Empire Science and technology in the Soviet Union History of science and technology in North America United States technological and industrial history History of science and technology in Mexico Technological and industrial history of Canada Technological and industrial history of 20th-century Canada Technological and industrial history of 21st-century Canada Technological and industrial history of the People's Republic of China Technological and industrial history of the United States History of technology by field History of invention History of aerospace History of artificial intelligence History of agriculture History of agricultural science History of architecture, timeline History",
    "label": 0
  },
  {
    "text": "the People's Republic of China Technological and industrial history of the United States History of technology by field History of invention History of aerospace History of artificial intelligence History of agriculture History of agricultural science History of architecture, timeline History of biotechnology History of cartography History of chemical engineering History of communication History of computing, timeline History of computer science History of computing hardware History of the graphical user interface History of hypertext, timeline History of the Internet, Internet phenomena History of the World Wide Web History of operating systems History of programming languages, timeline History of software engineering History of electrical engineering History of energy development History of engineering History of industry History of library and information science History of microscopy History of manufacturing History of the factory History of mass production History of materials science, timeline History of measurement History of medicine History of motor and engine technology History of military science History of transport, timeline History of biotechnology Timeline of biotechnology History of display technology History of film technology History of information technology auditing History of military technology History of nanotechnology History of science and technology History of web syndication technology Timeline of agriculture and food technology Timeline of clothing and textiles technology Timeline of communication technology Timeline of diving technology Timeline of heat engine technology Timeline of hypertext technology Timeline of lighting technology Timeline of low-temperature technology Timeline of materials technology Timeline of medicine and medical technology Timeline of microscope technology Timeline of motor and engine technology Timeline of particle physics technology Timeline of photography technology Timeline of rocket and missile technology Timeline of telescope technology Timeline of telescopes, observatories, and observing technology Timeline of temperature and pressure measurement technology Timeline of time measurement technology Timeline of transportation technology Hypothetical technology Potential technology of the future",
    "label": 0
  },
  {
    "text": "Timeline of rocket and missile technology Timeline of telescope technology Timeline of telescopes, observatories, and observing technology Timeline of temperature and pressure measurement technology Timeline of time measurement technology Timeline of transportation technology Hypothetical technology Potential technology of the future includes: Hypothetical technology Femtotechnology – hypothetical term used in reference to structuring of matter on the scale of a femtometer, which is 10−15 m. This is a smaller scale in comparison to nanotechnology and picotechnology which refer to 10−9 m and 10−12 m respectively. Work in the femtometer range involves manipulation of excited energy states within atomic nuclei (see nuclear isomer) to produce metastable (or otherwise stabilized) states with unusual properties. Philosophy of technology Philosophy of technology – Studies of the nature of technology Appropriate technology – Concept in the philosophy and politics of technology Instrumental and intrinsic value – Philosophical concept Jacques Ellul – French sociologist, technology critic, and Christian anarchist Paradigm – Set of distinct concepts or thought patterns Posthumanism – Class of philosophies Precautionary principle – Risk management strategy Singularitarianism – Belief in an incipient technological singularity Techno-progressivism – Stance of active support for the convergence of technological and social change Technocentrism – Value system centered on technology and its ability to control and protect the environment Technocracy – Form of government ruled by experts Technocriticism – Branch of critical theory Technological determinism – Reductionist theory Technoethics – Ethical questions specific to the technology agePages displaying short descriptions of redirect targets Technological evolution – Theory describing technology development Techno-nationalism – Way of understanding how technology affects the society and culture of a nation Technological singularity – Hypothetical event Technology readiness level – Method for estimating the maturity of technologies Technorealism – Attempt to expand the middle ground between techno-utopianism and Neo-Luddism Theories of technology – Factors that",
    "label": 0
  },
  {
    "text": "society and culture of a nation Technological singularity – Hypothetical event Technology readiness level – Method for estimating the maturity of technologies Technorealism – Attempt to expand the middle ground between techno-utopianism and Neo-Luddism Theories of technology – Factors that shape technological innovation Transhumanism – Philosophical movement High tech – Most advanced technology available Strategy of technology Strategy of Technology – Doctrine on how countries use technology to their advantage Human enhancement – Natural, artificial, or technological alteration of the human body Science – Systematic endeavour to gain knowledge Technology – Use of knowledge for practical goals Technology management – Design and control of technology products Technology integration – Use of technology tools in general content areas in education Technology intelligence Technology life cycle – Development, ascent, maturity, and decline of new technologies Technology roadmap – Planning technique Advancement of technology DARPA – Technology research and development agency of the U.S. Department of Defense Emerging technologies – Technologies whose development, practical applications, or both are still largely unrealized List of emerging technologies – New technologies actively in development Horizon scanning – Methodology in futures studies Hypothetical technology – Technology that does not exist yet Innovation – Practical implementation of improvements Invention – Novel device, material or technical process Inventor – Novel device, material or technical processPages displaying short descriptions of redirect targets Research and development – General term for activities in connection with corporate or governmental innovation Technological superpowers – State with extensive power or influence over much of the world Technological transitions – Describes how technological innovations occur and are incorporated into society Politics of technology Politics and technology AI takeover Accelerating change Format war Information privacy IT law PEST analysis Robot rights Technological singularity Technological sovereignty Economics of technology Energy accounting Nanosocialism Post-scarcity economy Technocracy Technocapitalism Technological diffusion",
    "label": 0
  },
  {
    "text": "are incorporated into society Politics of technology Politics and technology AI takeover Accelerating change Format war Information privacy IT law PEST analysis Robot rights Technological singularity Technological sovereignty Economics of technology Energy accounting Nanosocialism Post-scarcity economy Technocracy Technocapitalism Technological diffusion Technology acceptance model Technology lifecycle Technology transfer Technology education Technology education Technology museums Technoseum – Technology museum in Mannheim, Germany Technology organizations Science and technology think tanks Battelle Memorial Institute Cicada 3301 Council for Scientific and Industrial Research Edge Foundation, Inc. Eudoxa Federation of American Scientists Free Software Foundation GTRI Office of Policy Analysis and Research Information Technology and Innovation Foundation Institute for Science and International Security Institute for the Encouragement of Scientific Research and Innovation of Brussels Keck Institute for Space Studies Kestrel Institute Malaysian Industry-Government Group for High Technology Pakistan Council of Scientific and Industrial Research Piratbyrån RAND Corporation Regional Center for Renewable Energy and Energy Efficiency Res4Med Richard Dawkins Foundation for Reason and Science Swecha Wau Holland Foundation Technology media For historical treatments, see Media about the history of technology, above Technology journalism Books on technology Engines of Creation Technology periodicals Engadget TechCrunch Wired Websites The Verge Fictional technology Fictional technology In Death technology Technology in Star Trek Technology in Star Wars Technology of Robotech List of technology in the Dune universe Persons influential in technology List of engineers List of inventors List of scientists See also Outline of applied science Further reading Ambrose, Stanley H. (2 March 2001). \"Paleolithic Technology and Human Evolution\" (PDF). Science. 291 (5509): 1748–53. Bibcode:2001Sci...291.1748A. doi:10.1126/science.1059487. PMID 11249821. S2CID 6170692. Archived from the original (PDF) on 14 June 2007. Retrieved 10 March 2007. Huesemann, M.H., and J.A. Huesemann (2011). Technofix: Why Technology Won’t Save Us or the Environment, New Society Publishers, ISBN 0865717044. Kremer, Michael (1993). \"Population Growth and Technological Change: One",
    "label": 0
  },
  {
    "text": "the original (PDF) on 14 June 2007. Retrieved 10 March 2007. Huesemann, M.H., and J.A. Huesemann (2011). Technofix: Why Technology Won’t Save Us or the Environment, New Society Publishers, ISBN 0865717044. Kremer, Michael (1993). \"Population Growth and Technological Change: One Million B.C. to 1990\". Quarterly Journal of Economics. 108 (3): 681–716. doi:10.2307/2118405. JSTOR 2118405.. Kevin Kelly. What Technology Wants. New York, Viking Press, 14 October 2010, hardcover, 416 pages. ISBN 978-0670022151 Mumford, Lewis. (2010). Technics and Civilization. University of Chicago Press, ISBN 0226550273. Rhodes, Richard. (2000). Visions of Technology: A Century of Vital Debate about Machines, Systems, and the Human World. Simon & Schuster, ISBN 0684863111. Teich, A.H. (2008). Technology and the Future. Wadsworth Publishing, 11th edition, ISBN 0495570524. Wright, R.T. (2008). Technology. Goodheart-Wilcox Company, 5th edition, ISBN 1590707184. References External links Technology news BBC on technology Bloomberg on technology MIT Technology Review New York Times technology section Wired Miscellaneous topics Note: these topics need to be placed in the outline above. Some may be irrelevant and those should be removed. New sections may be needed in the outline to provide a suitable place for some of these items. Annotations by way of short descriptions may help decide where a link should go. Acoustic resonance technology – Inspection technology for measuring thickness Advanced steam technology – Evolution of steam power beyond mainstream mid-20th-century implementations Applications of nanotechnology – Uses for technology on very small scales Architectural technology – Application of technology to the design and architecture of buildings Assisted reproductive technology – Methods to achieve pregnancy by artificial or partially artificial means Assistive technology – Assistive devices for people with disabilities Assistive technology in sport Automatic box-opening technology – Mechanical process Barcode technology in healthcare Baseball telecasts technology – Chronological list Bead probe technology – Technique used for in-circuit testing",
    "label": 0
  },
  {
    "text": "artificial means Assistive technology – Assistive devices for people with disabilities Assistive technology in sport Automatic box-opening technology – Mechanical process Barcode technology in healthcare Baseball telecasts technology – Chronological list Bead probe technology – Technique used for in-circuit testing Beam lead technology – Technology used to deposit metal beams onto integrated circuits for connecting them Biomedical technology Biotechnology – Use of living systems and organisms to develop or make useful products Biotechnology in pharmaceutical manufacturing Bleeding edge technology – Technologies whose development, practical applications, or both are still largely unrealizedPages displaying short descriptions of redirect targets Braille technology – Type of assistive technology Brain technology – Technology that employs latest findings in neuroscience CASY cell counting technology – Cell counting system Calm technology – Type of information technology Ceramic mixing technology BOINC client–server technology – BOINC volunteer computing client–server structure Chirotechnology – Separation of a sample of a chiral compound into its enantiomersPages displaying short descriptions of redirect targets Circuit rider (technology) – Technology assistance organization for non-profits Civic technology – How government use of telecom and computers interacts with the people Clean coal technology – Combustible sedimentary rock composed primarily of carbonPages displaying short descriptions of redirect targets Clean technology – Any process, product, or service that reduces negative environmental impacts Close coupled field technology Clothing technology – Technology involving the manufacturing and innovation of clothing materials Coal upgrading technology – Solid fossil fuelPages displaying short descriptions of redirect targets Community technology Computer technology for developing areas – Donation of technology to developing areas Construction collaboration technology Contesting technology – Tools for amateur radio sport Cultural technology – South Korean media marketing system Cycling probe technology Cytotechnology – Microscopic interpretation of cells to detect cancer and other abnormalities DNA nanotechnology – Design and manufacture of artificial nucleic acid",
    "label": 0
  },
  {
    "text": "technology – Tools for amateur radio sport Cultural technology – South Korean media marketing system Cycling probe technology Cytotechnology – Microscopic interpretation of cells to detect cancer and other abnormalities DNA nanotechnology – Design and manufacture of artificial nucleic acid structures Dance technology – Application of information technology application of modern information technology in activities related to dance: in dance education, choreography, performance, and research. Demand flow technology – Strategy for business processes Design technology – Use of technology for product design Digital newspaper technology Digital scent technology – Study of smelling things through a computer Domestic technology – Usage of applied science in houses Downhole oil–water separation technology Dual-use technology – Technology that can be used for both peaceful and military purposes Dynamic video memory technology Ecotechnology Educational technology – Use of technology in education to enhance learning and teaching Electric transportation technology – Vehicle propelled fully or mostly by electricityPages displaying short descriptions of redirect targets Electrical engineering technology Electrofluidic display technology – Paper-like display technologyPages displaying short descriptions of redirect targets Electron-beam technology – Devices using beams of free electrons Electrothermal-chemical technology – Plasma ignition of projectile propellent Enabling technology Energy applications of nanotechnology Energy security and renewable technology Energy technology – Methods bringing energy into productionPages displaying short descriptions of redirect targets Entertainment technology Environmental biotechnology – Biotechnology applied to the natural environment Environmental technology – Technical and technological processes for protection of the environment Expandable tubular technology Field-induced polymer electroluminescent technology – Electroluminescent light source Food technology – Academic discipline regarding the preparation of foods Frame technology (software engineering) – System manufacturing custom software G-Technology – Professional storage products by Sandisk and Western DigitalPages displaying short descriptions of redirect targets General-purpose technology – Technologies that can affect an entire economy at large scale Genetic use",
    "label": 0
  },
  {
    "text": "Frame technology (software engineering) – System manufacturing custom software G-Technology – Professional storage products by Sandisk and Western DigitalPages displaying short descriptions of redirect targets General-purpose technology – Technologies that can affect an entire economy at large scale Genetic use restriction technology – Methods for controlling the use of GMOs Germinal choice technology – Liberal use of reprogenetics in human enhancementPages displaying short descriptions of redirect targets Gerontechnology – Academic and professional field Goal-line technology – Electronic aid to determine if a goal has been scored in association football Green nanotechnology – Environmentally-oriented nanotechnology Greenfish recirculation technology Group technology – Manufacturing technique Gustatory technology – Engineering discipline dealing with gustatory representation Haptic technology – Any form of interaction involving touch Headspace technology – Technique of gas chromatography Health information technology – Information technology applied to health and health care Health technology – Application of organized knowledge and skills to solve health problems Heart nanotechnology – Application of molecular-scale engineering to heart medicine High-technology swimwear fabric – Scientifically advanced materials used for swimwearPages displaying short descriptions of redirect targets Holiday lighting technology – Decorative lighting for festivities Human performance technology Hurdle technology – Food production safety method Hybridoma technology – Method for producing lots of identical antibodies Hyper-interactive teaching technology – Type of interactionPages displaying short descriptions of redirect targets Hypoxic air technology for fire prevention Imaging technology – Representation or reproduction of an object's formPages displaying short descriptions of redirect targets Immersive technology – Perception of being physically present in a non-physical worldPages displaying short descriptions of redirect targets Induction plasma technology – Type of high temperature plasma generated by electromagnetic inductionPages displaying short descriptions of redirect targets Industrial technology – Use of technology to increase the efficiency of production Information and communications technology – Extensional term for information technology",
    "label": 0
  },
  {
    "text": "plasma technology – Type of high temperature plasma generated by electromagnetic inductionPages displaying short descriptions of redirect targets Industrial technology – Use of technology to increase the efficiency of production Information and communications technology – Extensional term for information technology Information and communications technology in Kosovo Information processing technology and aging Information technology – Computer-based technology Internet services technology IsaKidd refining technology Keyboard technology – Hardware technology of keyboards Language technology Large-screen television technology – Technology rapidly developed in the late 1990s and 2000s Lithic technology – Ancient production techniques Low technology – Simple technology Marine technology – Technologies used in marine environments Mature technology Mechanical engineering technology Membrane technology – Transport of substances between two fractions with the help of permeable membranes Metamaterials surface antenna technology Microblade technology – Period of technological development Microtechnology – Technology with features near one micrometre Military technology – Application of technology for use in warfare Mobile technology – Technology used for cellular communication Molecular nanotechnology Music instrument technology – Device for making musical soundsPages displaying short descriptions of redirect targets Music technology – Use of technology by musicians NX technology – Desktop virtualization and application delivery software Nana technology – Microchip-based technology for aiding older adults Nanobiotechnology – Intersection of nanotechnology and biology Nanotechnology – Technology with features near one nanometer Near-infrared signature management technology – Camouflage designed to work at multiple frequencies, not just visible lightPages displaying short descriptions of redirect targets Neurotechnology – Technology that interfaces with the nervous system to monitor or modify neural function Non-profit technology – Technology used by non-profit organizations Nuclear technology – Technology that involves the reactions of atomic nuclei Omniview technology Open-source-appropriate technology – Appropriate technology from the open-design movementPages displaying short descriptions of redirect targets Orphaned technology – Technology abandoned by the original developers",
    "label": 0
  },
  {
    "text": "by non-profit organizations Nuclear technology – Technology that involves the reactions of atomic nuclei Omniview technology Open-source-appropriate technology – Appropriate technology from the open-design movementPages displaying short descriptions of redirect targets Orphaned technology – Technology abandoned by the original developers Orthodontic technology – Specialty of dental technology Particle technology – Science of the processing of particles and powders Performance acceleration technology Persuasive technology – Technology designed to persuade Phage-ligand technology Phonetic search technology Photoimageable thick-film technology Phytotechnology – Study of plant-based technology Picotechnology – Technology with features near one picometer Plasma deep drilling technology Point-to-point laser technology Positioning technology – Technology for determining an object's location in spacePages displaying short descriptions of redirect targets Presentation technology – Tools used for conveying information Primatte chromakey technology Process analytical technology – Type of technology Pull technology – Style of network communication where requests are sent by the client Pumpable ice technology – Type of technology to produce and use fluids or secondary refrigerants Push technology – Method of network communication where requests are sent by the publisher Quantum technology – Technological development using the laws of quantum mechanicsPages displaying short descriptions of redirect targets Radio access technology – Physical connection method for a radio communication network Rapid transit technology – High-capacity public transportPages displaying short descriptions of redirect targets Reproductive technology – Uses of technology in human and animal reproduction Resolution enhancement technology – Printing technology Rotary technology Rubber technology Search engine technology – System to help searching for informationPages displaying short descriptions of redirect targets Self-drying concrete technology Semantic technology – Technology to help machines understand data Site-specific recombinase technology – Genome engineering tools Social technology – Technology that enables social interactions Soft energy technology – Simple designs reliant on renewables Space technology – Technology developed for use in Space exploration",
    "label": 0
  },
  {
    "text": "Technology to help machines understand data Site-specific recombinase technology – Genome engineering tools Social technology – Technology that enables social interactions Soft energy technology – Simple designs reliant on renewables Space technology – Technology developed for use in Space exploration Speech technology Stealth technology – Military technology to make personnel and material less visible Subsea (technology) – Technology of submerged operations in the sea Surface-mount technology – Method for producing electronic circuits Suspension array technology Tamper-evident technology – Device or process that makes unauthorized access to the protected object easily detected Telepresence technology – Technology used in deep sea exploration Tennis technology – Technological advance in tennis Terahertz spectroscopy and technology – Molecule investigation techniquePages displaying short descriptions of redirect targets Terotechnology – Engineering term Thick film technology Thick-film dielectric electroluminescent technology – Optical and electrical phenomenonPages displaying short descriptions of redirect targets Through-hole technology – Circuit board manufacturing technique Time release technology – Mechanism that delivers a drug with a delay after its administrationPages displaying short descriptions of redirect targets Travel technology – Use of IT or ICT in the travel industry Trenchless technology – Underground construction that minimizes surface disruption Use of technology in treatment of mental disorders Vehicle safety technology – Special technology developed to ensure the safety and security of automobiles Video sensor technology Visual technology Wearable technology – Clothing and accessories incorporating computer and advanced electronic technologies Wet nanotechnology Wire rope spooling technology – Technology to prevent wire rope getting snagged when spooled on a drum Workflow technology X-Wind technology – Form of wind-powered mechanical or electrical generationPages displaying short descriptions of redirect targets Xpress technology Advanced technology – Most advanced technology available Appropriate technology – Concept in the philosophy and politics of technology Alternative technology Al Gore and information technology Assistive technology service provider",
    "label": 0
  },
  {
    "text": "or electrical generationPages displaying short descriptions of redirect targets Xpress technology Advanced technology – Most advanced technology available Appropriate technology – Concept in the philosophy and politics of technology Alternative technology Al Gore and information technology Assistive technology service provider Best available technology – Approved environmental solutions Biotechnology and genetic engineering in Bangladesh Biotechnology consulting Biotechnology industry in China Brazilian science and technology Buddhist influences on print technology Bullying in information technology – Form of workplace bullying Chief technology officer – Officer in charge of technical operations Community technology center – A public place where people can access digital technologiesPages displaying short descriptions of redirect targets Comparison of display technology Competitions and prizes in biotechnology Consumer adoption of technological innovations Corporate governance of information technology – Subset discipline of corporate governance Critique of technology Democratization of technology – Spread and access to technology Differential technological development – Strategy of technology governance Directive on the legal protection of biotechnological inventions – European Union directive in the field of patent law Drexler–Smalley debate on molecular nanotechnology Economic and Technological Development Zones – Zones in China for foreign direct investmentPages displaying short descriptions of redirect targets Educational technology in Saudi Arabia Ethics of technology – Ethical questions specific to the technology age Environmental impact of nanotechnology Etiquette in technology – Code of behavior for use of the Internet Fail-safes in nanotechnology Glossary of legal terms in technology Goans in science and technology Health impact of nanotechnology Health technology assessment – Field of policy analysis Hydrogen energy vision and technology roadmap – Chinese government plan Impact of nanotechnology Industrial applications of nanotechnology Information technology architecture Information technology audit – Examination of an information system Information technology consulting – Field that focuses on advising businesses on how best to use information technology Information technology controls",
    "label": 0
  },
  {
    "text": "plan Impact of nanotechnology Industrial applications of nanotechnology Information technology architecture Information technology audit – Examination of an information system Information technology consulting – Field that focuses on advising businesses on how best to use information technology Information technology controls Information technology in Bangladesh Information technology in India Information technology in Morocco Information technology in Pakistan Information technology management Information technology operations – Planning and operation of data centersPages displaying short descriptions of redirect targets Information technology outsourcing – Contracting internal tasks to an external organizationPages displaying short descriptions of redirect targets Information technology planning Information technology security audit – Independent examination of knowledge protection mechanismsPages displaying short descriptions of redirect targets Information technology specialist – United States Army PositionsPages displaying short descriptions of redirect targets Instrumental conception of technology – Philosophical conceptPages displaying short descriptions of redirect targets Investment-specific technological progress – Investment in new equipment List of DNA nanotechnology research groups List of United States technological universities List of advertising technology companies List of archaic technological nomenclature List of atheists in science and technology List of biotechnology companies List of computer technology code names List of cultural, intellectual, philosophical and technological revolutions List of information technology acronyms List of institutions using the term \"institute of technology\" or \"polytechnic\" List of nanotechnology organizations List of philosophers of technology List of science and technology articles by continent List of steam technology patents List of technology centers List of largest technology companies by revenue Marx's notebooks on the history of technology Medical technology assessment Mobile technology in Africa Muslim women in science and technology Nanotechnology education – Learning and teaching related to nanotechnology Nanotechnology in fiction – Fictional uses of nanotechnology Next generation of display technology Operations and technology management Participatory technology development Radio and television technology in Turkey Regulation of",
    "label": 0
  },
  {
    "text": "science and technology Nanotechnology education – Learning and teaching related to nanotechnology Nanotechnology in fiction – Fictional uses of nanotechnology Next generation of display technology Operations and technology management Participatory technology development Radio and television technology in Turkey Regulation of nanotechnology Religious response to assisted reproductive technology – Challenges for traditional communities Response time (technology) – Time a given technological system takes to respond to an input Science, technology and society – Academic fieldPages displaying short descriptions of redirect targets Science, technology, society and environment education Sexism in the technology industry Social construction of technology – Theory in science and technology studies Social shaping of technology Societal impact of nanotechnology Technological alliance Technological apartheid Technological applications of superconductivity Technological change – Process of invention, innovation and diffusion of technology or processes Technological convergence – Aspect of technological change Technological determinism – Reductionist theory Technological dualism Technological escalation Technological evolution – Theory describing technology development Technological fix – Attempt at using engineering or technology to solve a problem Technological history of the Roman military Technological innovation system Technological momentum Technological nationalism – Way of understanding how technology affects the society and culture of a nationPages displaying short descriptions of redirect targets Technological paradigm – Technological stage Technological rationality Technological revolution – Period of rapid technological change Technological self-efficacy Technological singularity – Hypothetical event Technological somnambulism Technological studies – School subject taught in Scotland Technological theory of social production Technological transitions – Describes how technological innovations occur and are incorporated into society Technological unemployment – Unemployment caused by technological change Technological utopianism – Any ideology based on the premise that advances in technology could bring a utopia Technology acceptance model – Information systems theory Technology adoption lifecycle – Sociological modelPages displaying short descriptions of redirect targets Technology alignment Technology and Construction Court",
    "label": 0
  },
  {
    "text": "– Any ideology based on the premise that advances in technology could bring a utopia Technology acceptance model – Information systems theory Technology adoption lifecycle – Sociological modelPages displaying short descriptions of redirect targets Technology alignment Technology and Construction Court – Division of the English High Court of JusticePages displaying short descriptions of redirect targets Technology and Culture Technology and Entertainment Software – Video game brandPages displaying short descriptions of redirect targets Technology and Livelihood Education – Learning area of the Philippine curriculum Technology and society Technology and the Character of Contemporary Life: A Philosophical Inquiry Technology assessment – Research area dealing with trends in science and technology and related social developments Technology aware design Technology brokering Technology company – Company focused on technology Technology demonstration – Showcasing an idea for new technology Technology doping – Practice of gaining a competitive advantage using sports equipment Technology dynamics – Scientific field Technology education – Education in processes and use of technology Technology evangelist – Person who gains significant acclaim for a given technology Technology for Improved Learning Outcomes Technology for peace Technology forecasting – Predicting the future of technology Technology gap – International trade theory Technology governance – Governance of the development of technology Technology policy – Form of policy Technology readiness level – Method for estimating the maturity of technologies Technology roadmap – Planning technique Technology scouting – Element of technology management Technology shock Technology stack – Set of software subsystems or components needed to create a complete platformPages displaying short descriptions of redirect targets Technology strategy – Strategy of technology use Technology support net Technology trajectory Technology-enhanced active learning Technology transfer – Process of disseminating technology Technology transfer in computer science – Subclass of technology transfer Technology treadmill Technology tree – Diagram used in strategy computer games Unified theory",
    "label": 0
  },
  {
    "text": "technology use Technology support net Technology trajectory Technology-enhanced active learning Technology transfer – Process of disseminating technology Technology transfer in computer science – Subclass of technology transfer Technology treadmill Technology tree – Diagram used in strategy computer games Unified theory of acceptance and use of technology – Information systems user theory The Beatles' recording technology Technology CAD – Branch of electronic design automation Techno-lodge – Type of business park in India Techno-organic virus – Fictional virus Techno-progressivism – Stance of active support for the convergence of technological and social change Techno-thriller – Thriller sub-genre with high level of technical detail Technoavia – Russian aircraft manufacturer Technobabble – Jargon-sounding nonsense Technobots Technoboy – Italian hardstyle DJ (born 1970) Technocapitalism – Changes in capitalism associated with the emergence of new technology sectors Technocentrism – Value system centered on technology and its ability to control and protect the environment Technoclash – 1993 video game Technocosmos Technocracy – Form of government ruled by experts Technocracy movement – 1930s North American social movement Technocrane – Telescopic camera crane Technocriticism – Branch of critical theory Technoculture Technodelic Technodiktator Technodon – 1993 studio album by Yellow Magic Orchestra Technoetic Technofile Technoflash – 1972 Canadian TV series Technogaianism – Favor of technology to fight environmental threats Technographic segmentation Technogypsie Technolangue/Easy – French syntactic parsing project Technoliberalism – Political ideology Technolibertarianism – Techno-political ideology rejecting all forms of government regulation Technologic – 2005 song by Daft Punk Technologie – American alternative rock bandPages displaying short descriptions of redirect targets Technometrics Technomimetics Technopaganism – Merging of neopaganism and magical ritual with digital technologies Technoparade – Parade of vehicles playing electronic dance music Technophilia – Strong enthusiasm for technology Technophobia – Fear or discomfort with technology Technophoby – Canadian musical projectPages displaying short descriptions of redirect targets Technopoly – 1992 book",
    "label": 0
  },
  {
    "text": "Criticism of technology is an analysis of adverse impacts of industrial and digital technologies. It is argued that, in all advanced industrial societies (not necessarily only capitalist ones), technology becomes a means of domination, control, and exploitation, or more generally something which threatens the survival of humanity. Some of the technology opposed by the most radical critics may include everyday household products, such as refrigerators, computers, and medication. However, criticism of technology comes in many shades. Overview Some authors such as Chellis Glendinning and Kirkpatrick Sale consider themselves Neo-Luddites and hold that technological progress has had a negative impact on humanity. Their work focused on seeking meaning out of technological change, specifically wrestling with the question of \"how tools and their affordances change and alter the fabric of everyday life.\" Ellul, for instance, maintained that when people assert that technology is an instrument of freedom or the means to achieve historical destiny or the execution of divine vocation, it results in the glorification and sanctification of Technique so that it becomes that which gives meaning and value to life rather than mere ensemble of materials. This is echoed by rhetorical critics who cite the way technological discourse damages institutions and individuals who make up those institutions due to its idealization and capacity to define social hierarchies. In its most extreme, criticisms of technology produce analyses of technology as potentially leading to catastrophe. For instance, activist Naomi Klein described how technology is employed by capitalism in its commitment to a \"shock doctrine\", which promotes a series of crises so that speculative profit can be accumulated. There are theorists who also cite the 2008 financial crisis as well as the Chernobyl and Fukushima disasters to support their critique. Critiques also focus on specific issues such as how technology—through robotics, automation, and software—is",
    "label": 0
  },
  {
    "text": "profit can be accumulated. There are theorists who also cite the 2008 financial crisis as well as the Chernobyl and Fukushima disasters to support their critique. Critiques also focus on specific issues such as how technology—through robotics, automation, and software—is destroying people's jobs faster than it is creating them, contributing to the incidence of poverty and inequality. In the 1970s in the US, the critique of technology became the basis of a new political perspective called anarcho-primitivism, which was forwarded by thinkers such as Fredy Perlman, John Zerzan, and David Watson. They proposed differing theories about how it became an industrial society, and not capitalism as such, that was at the root of contemporary social problems. This theory was developed in the journal Fifth Estate in the 1970s and 1980s, and was influenced by the Frankfurt School, the Situationist International, Jacques Ellul and others. The critique of technology overlaps with the philosophy of technology but whereas the latter tries to establish itself as an academic discipline the critique of technology is basically a political project, not limited to academia. It features prominently in neo-Marxist (Herbert Marcuse and Andrew Feenberg), ecofeminism (Vandana Shiva) and in post development (Ivan Illich) See also Critical theory Deep ecology Development criticism The Failure of Technology Frankfurt School History of science and technology Luddite Medicalization Paradigm shift Postdevelopment theory Science, technology and society Social criticism Social effect of evolutionary theory Technology and society References Further reading Wendy Hui Kyong Chun, Control and freedom (2006) Donna J. Haraway, A Cyborg Manifesto (1985) Gilles Deleuze, Postscript on the Societies of Control (1992) Lewis Mumford, Technics and Civilization (1934) Layla AbdelRahim, Children's Literature, Domestication, and Social Foundation: Narratives of Civilization and Wilderness, Routledge, 2018 paperback ISBN 9781138547810; 2015 hardback ISBN 9780415661102 Michael Adas, Machines as the Measure of Men:",
    "label": 0
  },
  {
    "text": "Societies of Control (1992) Lewis Mumford, Technics and Civilization (1934) Layla AbdelRahim, Children's Literature, Domestication, and Social Foundation: Narratives of Civilization and Wilderness, Routledge, 2018 paperback ISBN 9781138547810; 2015 hardback ISBN 9780415661102 Michael Adas, Machines as the Measure of Men: Science, Technology, and Ideologies of Western Dominance, Cornell University Press 1990 Braun, Ernest (2009). Futile Progress: Technology’s Empty Promise, Routledge. Jacques Ellul, The Technological Society, Trans. John Wilkinson. New York: Knopf, 1964. London: Jonathan Cape, 1965. Rev. ed.: New York: Knopf/Vintage, 1967. with introduction by Robert K. Merton (professor of sociology, Columbia University). Jacques Ellul, The Technological Bluff, Trans. Geoffrey W. Bromiley. Grand Rapids: Eerdmans, 1990. Andrew Feenberg, Transforming Technology. A Critical Theory Revisited, Oxford University Press, 2nd edition 2002, ISBN 0-19-514615-8 - Feenberg offers a \"coherent starting point for anticapitalist technical politics\" to overcome what he considers to be the \"fatalism\" of Ellul, Heidegger, and other proponents of \"substantive\" theories of technology. Martin Heidegger, The Question Concerning Technology, and Other Essays, B&T 1982, ISBN 0-06-131969-4 Huesemann, Michael H., and Joyce A. Huesemann (2011). Technofix: Why Technology Won’t Save Us or the Environment, New Society Publishers, Gabriola Island, British Columbia, Canada, ISBN 0865717044. Derrick Jensen and George Draffan, Welcome to the Machine: Science, Surveillance, and the Culture of Control, Chelsea Green Publishing Company, 2004, ISBN 1-931498-52-0 Mander, Jerry (1992). In the Absence of the Sacred: The Failure of Technology and the Survival of the Indian Nations, Sierra Club Books. Postman, Neil (1993). Technopoly: The Surrender of Culture to Technology, Vintage. David Watson, Against the Megamachine, Brooklyn: Autonomedia, 1998, ISBN 1-57027-087-2 - The title essay is available online here Joseph Weizenbaum, Computer Power and Human Reason: From Judgement to Calculation, W.H.Freeman & Co Ltd, New Edition 1976 Langdon Winner, Autonomous Technology: Technics-Out-Of-Control as a Theme in Political Thought, MIT Press 1977,",
    "label": 0
  },
  {
    "text": "On July 17th 2024, it was announced at the State Opening of Parliament that the Labour government will introduce the Cyber Security and Resilience Bill (CS&R). The proposed legislation is intended to update the existing Network and Information Security Regulations 2018, known as UK NIS. CS&R will strengthen the UK's cyber defences and resilience to hostile attacks thus ensuring that the infrastructure and critical services relied upon by UK companies are protected by addressing vulnerabilities, while ensuring the digital economy can deliver growth. The legislation will expand the remit of the existing regulations and put regulators on a stronger footing, as well as increasing the reporting requirements placed on businesses to help build a better picture of cyber threats. Its aim is to strengthen UK cyber defences, ensuring that the critical infrastructure and digital services which companies rely on are secure. The Bill will extend and apply UK-wide. The new laws are part of the Government's pledge to enhance and strengthen UK cyber security measures and protect the digital economy. CS&R will introduce a comprehensive regulatory framework designed to enforce stringent cyber security measures across various sectors. This framework will include mandatory compliance with established cyber security standards and practices to ensure essential cyber safety measures are being implemented. Ultimately, businesses will need to demonstrate their adherence to these standards through regular audits and reporting. Also included in the legislation are potential cost recovery mechanisms to provide resources to regulators and provide powers to proactively investigate potential vulnerabilities. The 'Cyber Security and Resilience (Network and Information Systems) Bill' was given its 1st reading in the UK Parliament on 12 November 2025. Key facts The key facts from the King's Speech are: i) The current UK NIS cyber security regulations play an essential role in safeguarding the UK’s critical national infrastructure",
    "label": 0
  },
  {
    "text": "its 1st reading in the UK Parliament on 12 November 2025. Key facts The key facts from the King's Speech are: i) The current UK NIS cyber security regulations play an essential role in safeguarding the UK’s critical national infrastructure by placing security duties on industry involved in the delivery of essential services. These regulations cover the five sectors of transport, energy, drinking water, health and digital infrastructure, as well as some digital services including online marketplaces, online search engines, and cloud computing services. 12 regulators are responsible for implementing the present regulations. ii) Hostile cyber actors are increasingly targeting UK critical sectors and supply chains. Recent serious high-profile attacks impacting London hospitals and the Ministry of Defence, as well as ransomware attacks on the British Library and Royal Mail, have highlighted that UK services and institutions are vulnerable to attack. iii) The impacts of a cyber attack on these sectors pose severe risks to UK citizens, core services and the economy at large. For example, as a result of the ransomware attack affecting the NHS in England in June , 3,396 outpatient appointments and 1,255 elective procedures were postponed across King's College Hospital, Guy’s Hospital and St Thomas’ Hospital, all in South London. It has been estimated that the cost of cybercrime in the UK in 2023 was $320 billion, near £225 billion. iv) The National Cyber Security Centre (NCSC) assess that the increased threat from hostile states and state-sponsored actors continues to escalate. At a recent speech at CyberUK, NCSC CEO Felicity Oswald warned that providers of essential services in the UK cannot afford to ignore these threats. v) 2 UK NIS Post-Implementation Reviews found that the original regulations are having a positive impact, but that progress has not been fast enough. In 2022 the review found that",
    "label": 0
  },
  {
    "text": "services in the UK cannot afford to ignore these threats. v) 2 UK NIS Post-Implementation Reviews found that the original regulations are having a positive impact, but that progress has not been fast enough. In 2022 the review found that they \"are a vital framework in raising wider UK resilience against network and information systems security threats\", but updates are required to keep pace with growing threats. Just over half of the operators of essential services have updated or strengthened existing policies and processes since the inception of the UK NIS Regulations in 2018, which were introduced after EU NIS Directive 2016/1148. Consequences It will introduce compulsory ransomware reporting so that the authorities can better understand the threat and \"alert us to potential attacks by expanding the type and nature of incidents that regulated entities must report.\" While this information collection is likely to increase resilience to attacks, the administrative burden for businesses from this reporting might well bring with it additional costs as well as the original cyber incident's expense. As modern business practices are interconnected, organisations must ensure that their partners and suppliers also adhere to the standards set by the CS&R. In the EU, the original Network and Information Security Directive (NIS Directive 2016/1148) is being updated to Directive 2022/2555, known as EU NIS 2. EU NIS 2 introduces wide-reaching changes to the existing EU cyber security laws for network and information systems. The CS&R should bring the existing UK NIS regulations 2018 to a framework similar to that of the EU. The Bill as yet has no information on any punishments for non-compliance or what the data regulators' demands from an organisation that has experienced a cyber security incident will be. It was announced in April 2025 by Peter Kyle, UK Secretary of State for Department",
    "label": 0
  },
  {
    "text": "no information on any punishments for non-compliance or what the data regulators' demands from an organisation that has experienced a cyber security incident will be. It was announced in April 2025 by Peter Kyle, UK Secretary of State for Department for Science, Innovation and Technology, that there would be £100,000 a day fines for failing to act against relevant threats. Reaction Jon Ellison, NCSC Director of National Resilience, said that the proposed bill was \"a landmark moment tackling the growing threat to the UK's critical systems\". He continued that it will be \"a crucial step towards a more comprehensive regulatory regime, fit for our volatile world\". Former head of the NCSC Ciaran Martin along with other experts welcomed the legislative proposal. On social media, he wrote that the proposed legislation seemed sensible, with mandatory reporting requirements being significant and positive steps. A representative of the CyberUp Campaign Matt Hull said that the organisation is looking forward to the Government updating UK cyber resilience and in particular the Computer Misuse Act 1990. Any updates to this Act would help cyber professionals protect the U.K., safeguard the digital economy and unlock the potential growth within the cybersecurity industry. Cyber security and resilience policy statement In April 2025, the CS&R Policy Statement was published, which outlines the confirmed and proposed measures to be included in the bill. Quoting: \"The digital revolution is transforming our Critical National Infrastructure (CNI) and our essential public services. It offers an extraordinary opportunity – to make our people and our country better off. However, it may also bring new and dangerous vulnerabilities... In this Policy Statement, I set out legislative proposals for this Bill. I also acknowledge that the cyber landscape moves exponentially – a lot can happen in a short space of time. This statement proposes several",
    "label": 0
  },
  {
    "text": "new and dangerous vulnerabilities... In this Policy Statement, I set out legislative proposals for this Bill. I also acknowledge that the cyber landscape moves exponentially – a lot can happen in a short space of time. This statement proposes several additional measures to tackle the threats that we are facing now.\" The legislation aims to strengthen the UK's cyber defenses and secure critical infrastructure and essential digital services, thereby enhancing CNI protection. The statement details plans to expand the regulatory framework to cover more entities, empower regulators and improve oversight. This includes enhancing incident reporting, augmenting the ICO's information-gathering capabilities and improving regulators’ cost recovery mechanisms. The bill also addresses the need for an adaptable regulatory framework to keep pace with the ever-evolving cyber landscape. CS&R seeks to broaden the scope of organizations required to improve their risk assessments, strengthening cybersecurity measures for approximately 1000 organizations. These measures will increase data protection and network security and are likely to include data center operators and managed service providers . The proposals also include giving regulators more tools to enhance security standards, mandating detailed incident reporting and granting the government powers to update regulatory frameworks as threats and technology evolve. The statement provides a detailed overview of the changes to the Cyber Essentials program, including updates to the software definition, vulnerability fixes and terminology related to remote working. The Cyber Essentials Plus test specification will be updated with new verification pointers, verification of segregation by sub-set and verification of sampling. The statement also outlines the steps organizations will need to take to achieve Cyber Essentials certification in 2025 and onwards. These include changes to IT infrastructure requirements, such as the introduction of passwordless authentication. Schedule The Bill will proceed through seven stages of the legislative process which happens in both houses of",
    "label": 0
  },
  {
    "text": "Cyberpunk is a subgenre of science fiction set in a dystopian future. It is characterized by its focus on a combination of \"low-life and high tech\". It features a range of futuristic technological and scientific achievements, including artificial intelligence and cyberware, which are juxtaposed with societal collapse, dystopia or decay. A significant portion of cyberpunk can be traced back to the New Wave science fiction movement of the 1960s and 1970s. During this period, prominent writers such as Philip K. Dick, Michael Moorcock, Roger Zelazny, John Brunner, J. G. Ballard, Philip José Farmer and Harlan Ellison explored the impact of technology, drug culture, and the sexual revolution. These authors diverged from the utopian inclinations prevalent in earlier science fiction. Comics exploring cyberpunk themes began appearing as early as Judge Dredd, first published in 1977. Released in 1984, William Gibson's influential debut novel Neuromancer helped solidify cyberpunk as a genre, drawing influence from punk subculture and early hacker culture. Frank Miller's Ronin is an example of a cyberpunk graphic novel. Other influential cyberpunk writers included Bruce Sterling and Rudy Rucker. The Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series Akira, with its 1988 anime film adaptation (also directed by Otomo) later popularizing the subgenre. Early films in the genre include Ridley Scott's 1982 film Blade Runner, one of several of Philip K. Dick's works that have been adapted into films (in this case, Do Androids Dream of Electric Sheep?). The \"first cyberpunk television series\" was the TV series Max Headroom from 1987, playing in a futuristic dystopia ruled by an oligarchy of television networks, and where computer hacking played a central role in many story lines. More recently, the animated series Batman Beyond (1999–2001) is considered a noteworthy example of the cyberpunk genre. The films",
    "label": 0
  },
  {
    "text": "futuristic dystopia ruled by an oligarchy of television networks, and where computer hacking played a central role in many story lines. More recently, the animated series Batman Beyond (1999–2001) is considered a noteworthy example of the cyberpunk genre. The films Johnny Mnemonic (1995) and New Rose Hotel (1998), both based upon short stories by William Gibson, flopped commercially and critically, while Robocop (1987), Total Recall (1990), Judge Dredd (1995), and The Matrix trilogy (1999–2003) were more successful cyberpunk films. Newer cyberpunk media includes Tron: Ares (2025) and Tron: Legacy (2010), sequels to the original Tron (1982); Blade Runner 2049 (2017), a sequel to the original 1982 film; Dredd (2012), which was not a sequel to the original movie; Ghost in the Shell (2017), a live-action adaptation of the original manga; Alita: Battle Angel (2019), based on the 1990s Japanese manga Battle Angel Alita; the 2018 Netflix TV series Altered Carbon, based on Richard K. Morgan's 2002 novel of the same name; and the video game Cyberpunk 2077 (2020) and original net animation (ONA) miniseries Cyberpunk: Edgerunners (2022), both based on R. Talsorian Games' 1988 tabletop role-playing game Cyberpunk. Background Lawrence Person has attempted to define the content and ethos of the cyberpunk literary movement stating: Classic cyberpunk characters were marginalized, alienated loners who lived on the edge of society in generally dystopic futures where daily life was impacted by rapid technological change, an ubiquitous datasphere of computerized information, and invasive modification of the human body. Cyberpunk plots often involve conflict between artificial intelligence, hackers, and megacorporations, and tend to be set in a near-future Earth, rather than in the far-future settings or galactic vistas found in novels such as Isaac Asimov's Foundation or Frank Herbert's Dune. The settings are usually post-industrial dystopias but tend to feature extraordinary cultural ferment and",
    "label": 0
  },
  {
    "text": "set in a near-future Earth, rather than in the far-future settings or galactic vistas found in novels such as Isaac Asimov's Foundation or Frank Herbert's Dune. The settings are usually post-industrial dystopias but tend to feature extraordinary cultural ferment and the use of technology in ways never anticipated by its original inventors (\"the street finds its own uses for things\"). Much of the genre's atmosphere echoes film noir, and written works in the genre often use techniques from detective fiction. There are sources who view that cyberpunk has shifted from a literary movement to a mode of science fiction due to the limited number of writers and its transition to a more generalized cultural formation. History and origins The origins of cyberpunk are rooted in the New Wave science fiction movement of the 1960s and 1970s, where New Worlds, under the editorship of Michael Moorcock, began inviting and encouraging stories that examined new writing styles, techniques, and archetypes. Reacting to conventional storytelling, New Wave authors attempted to present a world where society coped with a constant upheaval of new technology and culture, generally with dystopian outcomes. Writers like Roger Zelazny, J. G. Ballard, Philip José Farmer, Samuel R. Delany, and Harlan Ellison often examined the impact of drug culture, technology, and the ongoing sexual revolution, drawing themes and influence from experimental literature of Beat Generation authors such as William S. Burroughs, and art movements like Dadaism. Ballard, a notable critic of literary archetypes in science fiction, instead employs metaphysical and psychological concepts, seeking greater relevance to readers of the day. Ballard's work is considered have had a profound influence on cyberpunk's development, as evidenced by the term \"Ballardian\" becoming used to ascribe literary excellence amongst science fiction social circles. Ballard, along with Zelazny and others continued the popular development of",
    "label": 0
  },
  {
    "text": "Ballard's work is considered have had a profound influence on cyberpunk's development, as evidenced by the term \"Ballardian\" becoming used to ascribe literary excellence amongst science fiction social circles. Ballard, along with Zelazny and others continued the popular development of \"realism\" within the genre. Delany's 1968 novel Nova, considered a forerunner of cyberpunk literature, includes neural implants, a now popular cyberpunk trope for human computer interfaces. Philip K. Dick's novel, Do Androids Dream of Electric Sheep?, first published in 1968, shares common dystopian themes with later works by Gibson and Sterling, and is praised for its \"realist\" exploration of cybernetic and artificial intelligence ideas and ethics. Etymology The term \"cyberpunk\" first appeared as the title of a short story by Bruce Bethke, written in 1980 and published in Amazing Stories in 1983. The name was picked up by Gardner Dozois, editor of Isaac Asimov's Science Fiction Magazine, and popularized in his editorials. Bethke says he made two lists of words, one for technology, one for troublemakers, and experimented with combining them variously into compound words, consciously attempting to coin a term that encompassed both punk attitudes and high technology. He described the idea thus: The kids who trashed my computer; their kids were going to be Holy Terrors, combining the ethical vacuity of teenagers with a technical fluency we adults could only guess at. Further, the parents and other adult authority figures of the early 21st Century were going to be terribly ill-equipped to deal with the first generation of teenagers who grew up truly \"speaking computer\". Afterward, Dozois began using this term in his own writing, most notably in a 1984 Washington Post article where he said \"About the closest thing here to a self-willed esthetic 'school' would be the purveyors of bizarre hard-edged, high-tech stuff, who have on",
    "label": 0
  },
  {
    "text": "using this term in his own writing, most notably in a 1984 Washington Post article where he said \"About the closest thing here to a self-willed esthetic 'school' would be the purveyors of bizarre hard-edged, high-tech stuff, who have on occasion been referred to as 'cyberpunks' — Sterling, Gibson, Shiner, Cadigan, Bear.\" Also in 1984, William Gibson's novel Neuromancer was published, delivering a glimpse of a future encompassed by what became an archetype of cyberpunk \"virtual reality\", with the human mind being fed light-based worldscapes through a computer interface. Some, perhaps ironically including Bethke himself, argued at the time that the writers whose style Gibson's books epitomized should be called \"Neuromantics\", a pun on the name of the novel plus \"New Romantics\", a term used for a New Wave pop music movement that had just occurred in Britain, but this term did not catch on. Bethke later paraphrased Michael Swanwick's argument for the term: \"the movement writers should properly be termed neuromantics, since so much of what they were doing was clearly imitating Neuromancer\". Sterling was another writer who played a central role, often consciously, in the cyberpunk genre, variously seen as either keeping it on track, or distorting its natural path into a stagnant formula. In 1986, he edited a volume of cyberpunk stories called Mirrorshades: The Cyberpunk Anthology, an attempt to establish what cyberpunk was, from Sterling's perspective. In the subsequent decade, the motifs of Gibson's Neuromancer became formulaic, climaxing in the satirical extremes of Neal Stephenson's Snow Crash in 1992. Bookending the cyberpunk era, Bethke himself published a novel in 1995 called Headcrash, like Snow Crash a satirical attack on the genre's excesses. Fittingly, it won an honor named after cyberpunk's spiritual founder, the Philip K. Dick Award. It satirized the genre in this way: ...full of",
    "label": 0
  },
  {
    "text": "a novel in 1995 called Headcrash, like Snow Crash a satirical attack on the genre's excesses. Fittingly, it won an honor named after cyberpunk's spiritual founder, the Philip K. Dick Award. It satirized the genre in this way: ...full of young guys with no social lives, no sex lives and no hope of ever moving out of their mothers' basements ... They're total wankers and losers who indulge in Messianic fantasies about someday getting even with the world through almost-magical computer skills, but whose actual use of the Net amounts to dialing up the scatophilia forum and downloading a few disgusting pictures. You know, cyberpunks. Style and ethos Primary figures in the cyberpunk movement include William Gibson, Neal Stephenson, Bruce Sterling, Bruce Bethke, Pat Cadigan, Rudy Rucker, and John Shirley. Philip K. Dick (author of Do Androids Dream of Electric Sheep?, from which the film Blade Runner was adapted) is also seen by some as prefiguring the movement. Blade Runner can be seen as a quintessential example of the cyberpunk style and theme. Video games, board games, and tabletop role-playing games, such as Cyberpunk 2020 and Shadowrun, often feature storylines that are heavily influenced by cyberpunk writing and movies. Beginning in the early 1990s, some trends in fashion and music were also labeled as cyberpunk. Cyberpunk is also featured prominently in anime and manga (Japanese cyberpunk), with Akira, Ghost in the Shell and Cowboy Bebop being among the most notable. Setting Cyberpunk writers tend to use elements from crime fiction—particularly hardboiled detective fiction and film noir—and postmodernist prose to describe an often nihilistic underground side of an electronic society. The genre's vision of a troubled future is often called the antithesis of the generally utopian visions of the future popular in the 1940s and 1950s. Gibson defined cyberpunk's antipathy towards",
    "label": 0
  },
  {
    "text": "an often nihilistic underground side of an electronic society. The genre's vision of a troubled future is often called the antithesis of the generally utopian visions of the future popular in the 1940s and 1950s. Gibson defined cyberpunk's antipathy towards utopian science fiction in his 1981 short story \"The Gernsback Continuum\", which pokes fun at and, to a certain extent, condemns utopian science fiction. In some cyberpunk writing, much of the action takes place online, in cyberspace, blurring the line between actual and virtual reality. A typical trope in such work is a direct connection between the human brain and computer systems. Cyberpunk settings are dystopias with corruption, computers, and computer networks. The economic and technological state of Japan is a regular theme in the cyberpunk literature of the 1980s. Of Japan's influence on the genre, William Gibson said, \"Modern Japan simply was cyberpunk.\" Cyberpunk is often set in urbanized, artificial landscapes, and \"city lights, receding\" was used by Gibson as one of the genre's first metaphors for cyberspace and virtual reality. The cityscapes of Hong Kong has had major influences in the urban backgrounds, ambiance and settings in many cyberpunk works such as Blade Runner and Shadowrun. Ridley Scott envisioned the landscape of cyberpunk Los Angeles in Blade Runner to be \"Hong Kong on a very bad day\". The streetscapes of the Ghost in the Shell film were based on Hong Kong. Its director Mamoru Oshii felt that Hong Kong's strange and chaotic streets where \"old and new exist in confusing relationships\" fit the theme of the film well. Hong Kong's Kowloon Walled City is particularly notable for its disorganized hyper-urbanization and breakdown in traditional urban planning to be an inspiration to cyberpunk landscapes. During the British rule of Hong Kong, it was an area neglected by both the",
    "label": 0
  },
  {
    "text": "Hong Kong's Kowloon Walled City is particularly notable for its disorganized hyper-urbanization and breakdown in traditional urban planning to be an inspiration to cyberpunk landscapes. During the British rule of Hong Kong, it was an area neglected by both the British and Qing administrations, embodying elements of liberalism in a dystopian context. Portrayals of East Asia and Asians in Western cyberpunk have been criticized as Orientalist and promoting racist tropes playing on American and European fears of East Asian dominance; this has been referred to as \"techno-Orientalism\". Society and government Cyberpunk can be intended to disquiet readers and call them to action. It often expresses a sense of rebellion, suggesting that one could describe it as a type of cultural revolution in science fiction. In the words of author and critic David Brin: ...a closer look [at cyberpunk authors] reveals that they nearly always portray future societies in which governments have become wimpy and pathetic ...Popular science fiction tales by Gibson, Williams, Cadigan and others do depict Orwellian accumulations of power in the next century, but nearly always clutched in the secretive hands of a wealthy or corporate elite. Cyberpunk stories have also been seen as fictional forecasts of the evolution of the Internet. The earliest descriptions of a global communications network came long before the World Wide Web entered popular awareness, though not before traditional science-fiction writers such as Arthur C. Clarke and some social commentators such as James Burke began predicting that such networks would eventually form. Some observers cite that cyberpunk tends to marginalize sectors of society such as women and people of colour. It is claimed that, for instance, cyberpunk depicts fantasies that ultimately empower masculinity using fragmentary and decentered aesthetic that culminate in a masculine genre populated by male outlaws. Critics also note the absence",
    "label": 0
  },
  {
    "text": "such as women and people of colour. It is claimed that, for instance, cyberpunk depicts fantasies that ultimately empower masculinity using fragmentary and decentered aesthetic that culminate in a masculine genre populated by male outlaws. Critics also note the absence of any reference to Africa or black characters in the quintessential cyberpunk film Blade Runner, while other films reinforce stereotypes. Media Literature Minnesota writer Bruce Bethke coined the term in 1983 for his short story \"Cyberpunk\", which was published in an issue of Amazing Science Fiction Stories. The term was quickly appropriated as a label to be applied to the works of William Gibson, Bruce Sterling, Pat Cadigan and others. Of these, Sterling became the movement's chief ideologue, thanks to his fanzine Cheap Truth. John Shirley wrote articles on Sterling and Rucker's significance. John Brunner's 1975 novel The Shockwave Rider is considered by many to be the first cyberpunk novel with many of the tropes commonly associated with the genre, some five years before the term was popularized by Dozois. William Gibson with his novel Neuromancer (1984) is arguably the most famous writer connected with the term cyberpunk. He emphasized style, a fascination with surfaces, and atmosphere over traditional science-fiction tropes. Regarded as ground-breaking and sometimes as \"the archetypal cyberpunk work\", Neuromancer was awarded the Hugo, Nebula, and Philip K. Dick Awards. Count Zero (1986) and Mona Lisa Overdrive (1988) followed after Gibson's popular debut novel. According to the Jargon File, \"Gibson's near-total ignorance of computers and the present-day hacker culture enabled him to speculate about the role of computers and hackers in the future in ways hackers have since found both irritatingly naïve and tremendously stimulating.\" Early on, cyberpunk was hailed as a radical departure from science-fiction standards and a new manifestation of vitality. Shortly thereafter, some critics arose",
    "label": 0
  },
  {
    "text": "and hackers in the future in ways hackers have since found both irritatingly naïve and tremendously stimulating.\" Early on, cyberpunk was hailed as a radical departure from science-fiction standards and a new manifestation of vitality. Shortly thereafter, some critics arose to challenge its status as a revolutionary movement. These critics said that the science fiction New Wave of the 1960s was much more innovative as far as narrative techniques and styles were concerned. While Neuromancer's narrator may have had an unusual \"voice\" for science fiction, much older examples can be found: Gibson's narrative voice, for example, resembles that of an updated Raymond Chandler, as in his novel The Big Sleep (1939). Others noted that almost all traits claimed to be uniquely cyberpunk could in fact be found in older writers' works—often citing J. G. Ballard, Philip K. Dick, Harlan Ellison, Stanisław Lem, Samuel R. Delany, and even William S. Burroughs. For example, Philip K. Dick's works contain recurring themes of social decay, artificial intelligence, paranoia, and blurred lines between objective and subjective realities. The influential cyberpunk movie Blade Runner (1982) is based on his book, Do Androids Dream of Electric Sheep?. Humans linked to machines are found in Pohl and Kornbluth's Wolfbane (1959) and Roger Zelazny's Creatures of Light and Darkness (1968). In 1994, scholar Brian Stonehill suggested that Thomas Pynchon's 1973 novel Gravity's Rainbow \"not only curses but precurses what we now glibly dub cyberspace.\" Other important predecessors include Alfred Bester's two most celebrated novels, The Demolished Man and The Stars My Destination, as well as Vernor Vinge's novella True Names. Reception and impact Science-fiction writer David Brin describes cyberpunk as \"the finest free promotion campaign ever waged on behalf of science fiction\". It may not have attracted the \"real punks\", but it did ensnare many new readers, and",
    "label": 0
  },
  {
    "text": "True Names. Reception and impact Science-fiction writer David Brin describes cyberpunk as \"the finest free promotion campaign ever waged on behalf of science fiction\". It may not have attracted the \"real punks\", but it did ensnare many new readers, and it provided the sort of movement that postmodern literary critics found alluring. Cyberpunk made science fiction more attractive to academics, argues Brin; in addition, it made science fiction more profitable to Hollywood and to the visual arts generally. Although the \"self-important rhetoric and whines of persecution\" on the part of cyberpunk fans were irritating at worst and humorous at best, Brin declares that the \"rebels did shake things up. We owe them a debt.\" Fredric Jameson considers cyberpunk the \"supreme literary expression if not of postmodernism, then of late capitalism itself\". Cyberpunk further inspired many later writers to incorporate cyberpunk ideas into their own works, such as George Alec Effinger's When Gravity Fails. Wired magazine, created by Louis Rossetto and Jane Metcalfe, mixes new technology, art, literature, and current topics in order to interest today's cyberpunk fans, which Paula Yoo claims \"proves that hardcore hackers, multimedia junkies, cyberpunks and cellular freaks are poised to take over the world\". Film and television The film Blade Runner (1982) is set in 2019 in a dystopian future in which manufactured beings called replicants are slaves used on space colonies and are legal prey on Earth to various bounty hunters who \"retire\" (kill) them. Although Blade Runner was largely unsuccessful in its first theatrical release, it found a viewership in the home video market and became a cult film. Since the movie omits the religious and mythical elements of Dick's original novel (e.g. empathy boxes and Wilbur Mercer), it falls more strictly within the cyberpunk genre than the novel does. William Gibson later revealed",
    "label": 0
  },
  {
    "text": "and became a cult film. Since the movie omits the religious and mythical elements of Dick's original novel (e.g. empathy boxes and Wilbur Mercer), it falls more strictly within the cyberpunk genre than the novel does. William Gibson later revealed that upon first viewing the film, he was surprised at how the look of this film matched his vision for Neuromancer, a book he was then working on. The film's tone has since been the staple of many cyberpunk movies, such as The Matrix trilogy (1999–2003), which uses a wide variety of cyberpunk elements. A sequel to Blade Runner was released in 2017. The TV series Max Headroom (1987) is an iconic cyberpunk work, taking place in a futuristic dystopia ruled by an oligarchy of television networks. Computer hacking played a central role in many of the story lines. Max Headroom has been called \"the first cyberpunk television series\". The number of films in the genre has grown steadily since Blade Runner. Several of Philip K. Dick's works have been adapted to the silver screen. The films Johnny Mnemonic (1995) and New Rose Hotel (1998), both based on short stories by William Gibson, flopped commercially and critically. Other cyberpunk films include RoboCop (1987), Total Recall (1990), Hardware (1990), The Lawnmower Man (1992), 12 Monkeys (1995), Hackers (1995), and Strange Days (1995). Some cyberpunk films have been described as tech-noir, a hybrid genre combining neo-noir and science fiction or cyberpunk. Anime and manga The Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series Akira, with its 1988 anime film adaptation, which Otomo directed, later popularizing the subgenre. Akira inspired a wave of Japanese cyberpunk works, including manga and anime series such as Ghost in the Shell, Battle Angel Alita, and Cowboy Bebop. Other early Japanese cyberpunk",
    "label": 0
  },
  {
    "text": "1988 anime film adaptation, which Otomo directed, later popularizing the subgenre. Akira inspired a wave of Japanese cyberpunk works, including manga and anime series such as Ghost in the Shell, Battle Angel Alita, and Cowboy Bebop. Other early Japanese cyberpunk works include the 1982 film Burst City, and the 1989 film Tetsuo: The Iron Man. According to Paul Gravett, when Akira began to be published, cyberpunk literature had not yet been translated into Japanese, Otomo has distinct inspirations such as Mitsuteru Yokoyama's manga series Tetsujin 28-go (1956–1966) and Moebius. In contrast to Western cyberpunk which has roots in New Wave science fiction literature, Japanese cyberpunk has roots in underground music culture, specifically the Japanese punk subculture that arose from the Japanese punk music scene in the 1970s. The filmmaker Sogo Ishii introduced this subculture to Japanese cinema with the punk film Panic High School (1978) and the punk biker film Crazy Thunder Road (1980), both portraying the rebellion and anarchy associated with punk, and the latter featuring a punk biker gang aesthetic. Ishii's punk films paved the way for Otomo's seminal cyberpunk work Akira. Cyberpunk themes are widely visible in anime and manga. In Japan, where cosplay is popular and not only teenagers display such fashion styles, cyberpunk has been accepted and its influence is widespread. William Gibson's Neuromancer, whose influence dominated the early cyberpunk movement, was also set in Chiba, one of Japan's largest industrial areas, although at the time of writing the novel Gibson did not know the location of Chiba and had no idea how perfectly it fit his vision in some ways. The exposure to cyberpunk ideas and fiction in the 1980s has allowed it to seep into the Japanese culture. Cyberpunk anime and manga draw upon a futuristic vision which has elements in common with",
    "label": 0
  },
  {
    "text": "fit his vision in some ways. The exposure to cyberpunk ideas and fiction in the 1980s has allowed it to seep into the Japanese culture. Cyberpunk anime and manga draw upon a futuristic vision which has elements in common with Western science fiction and therefore have received wide international acceptance outside Japan. \"The conceptualization involved in cyberpunk is more of forging ahead, looking at the new global culture. It is a culture that does not exist right now, so the Japanese concept of a cyberpunk future, seems just as valid as a Western one, especially as Western cyberpunk often incorporates many Japanese elements.\" William Gibson is now a frequent visitor to Japan, and he came to see that many of his visions of Japan have become a reality: Modern Japan simply was cyberpunk. The Japanese themselves knew it and delighted in it. I remember my first glimpse of Shibuya, when one of the young Tokyo journalists who had taken me there, his face drenched with the light of a thousand media-suns—all that towering, animated crawl of commercial information—said, \"You see? You see? It is Blade Runner town.\" And it was. It so evidently was. Influence Akira (1982 manga) and its 1988 anime film adaptation have influenced numerous works in animation, comics, film, music, television and video games. Akira has been cited as a major influence on Hollywood films such as The Matrix, Chronicle, Looper, Midnight Special, and Inception, as well as cyberpunk-influenced video games such as Hideo Kojima's Snatcher and Metal Gear Solid, Valve's Half-Life series and Dontnod Entertainment's Remember Me. Akira has also influenced the work of musicians such as Kanye West, who paid homage to Akira in the \"Stronger\" music video, and Lupe Fiasco, whose album Tetsuo & Youth is named after Tetsuo Shima. The popular bike from",
    "label": 0
  },
  {
    "text": "Me. Akira has also influenced the work of musicians such as Kanye West, who paid homage to Akira in the \"Stronger\" music video, and Lupe Fiasco, whose album Tetsuo & Youth is named after Tetsuo Shima. The popular bike from the film, Kaneda's Motorbike, appears in Steven Spielberg's film Ready Player One, and CD Projekt's video game Cyberpunk 2077. Ghost in the Shell (1995) influenced a number of prominent filmmakers, most notably the Wachowskis in The Matrix (1999) and its sequels. The Matrix series took several concepts from the film, including the Matrix digital rain, which was inspired by the opening credits of Ghost in the Shell and a sushi magazine the wife of the senior designer of the animation, Simon Witheley, had in the kitchen at the time, and the way characters access the Matrix through holes in the back of their necks. Other parallels have been drawn to James Cameron's Avatar, Steven Spielberg's A.I. Artificial Intelligence, and Jonathan Mostow's Surrogates. James Cameron cited Ghost in the Shell as a source of inspiration, citing it as an influence on Avatar. The original video animation Megazone 23 (1985) has a number of similarities to The Matrix. Battle Angel Alita (1990) has had a notable influence on filmmaker James Cameron, who was planning to adapt it into a film since 2000. It was an influence on his TV series Dark Angel, and he is the producer of the 2019 film adaptation Alita: Battle Angel. Comics In 1975, artist Moebius collaborated with writer Dan O'Bannon on a story called The Long Tomorrow, published in the French magazine Métal Hurlant. One of the first works featuring elements now seen as exemplifying cyberpunk, it combined influences from film noir and hardboiled crime fiction with a distant sci-fi environment. Author William Gibson stated that Moebius'",
    "label": 0
  },
  {
    "text": "published in the French magazine Métal Hurlant. One of the first works featuring elements now seen as exemplifying cyberpunk, it combined influences from film noir and hardboiled crime fiction with a distant sci-fi environment. Author William Gibson stated that Moebius' artwork for the series, along with other visuals from Métal Hurlant, strongly influenced his 1984 novel Neuromancer. The series had a far-reaching impact in the cyberpunk genre, being cited as an influence on Ridley Scott's Alien (1979) and Blade Runner. Moebius expanded upon The Long Tomorrow's aesthetic with The Incal, a graphic novel collaboration with Alejandro Jodorowsky published from 1980 to 1988. The story centers around the exploits of a detective named John Difool in various science fiction settings, and while not confined to the tropes of cyberpunk, it features many elements of the genre. Moebius was one of the designers of Tron (1982), a movie that shows a world inside a computer. Concurrently with many other foundational cyberpunk works, DC Comics published Frank Miller's six-issue miniseries Rōnin from 1983 to 1984. The series, incorporating aspects of Samurai culture, martial arts films and manga, is set in a dystopian near-future New York. It explores the link between an ancient Japanese warrior and the apocalyptic, crumbling cityscape he finds himself in. The comic also bears several similarities to Akira, with highly powerful telepaths playing central roles, as well as sharing many key visuals. Rōnin would go on to influence many later works, including Samurai Jack and the Teenage Mutant Ninja Turtles, as well as video games such as Cyberpunk 2077. Two years later, Miller himself would incorporate several toned-down elements of Rōnin into his acclaimed 1986 miniseries The Dark Knight Returns, in which a retired Bruce Wayne once again takes up the mantle of Batman in a Gotham that is increasingly",
    "label": 0
  },
  {
    "text": "years later, Miller himself would incorporate several toned-down elements of Rōnin into his acclaimed 1986 miniseries The Dark Knight Returns, in which a retired Bruce Wayne once again takes up the mantle of Batman in a Gotham that is increasingly becoming more dystopian. Paul Pope's Batman: Year 100, published in 2006, also exhibits several traits typical of cyberpunk fiction, such as a rebel protagonist opposing a future authoritarian state, and a distinct retrofuturist aesthetic that makes callbacks to both The Dark Knight Returns and Batman's original appearances in the 1940s. Video games There are many cyberpunk video games. Popular series include the Megami Tensei series, Kojima's Snatcher and Metal Gear series, Deus Ex series, Syndicate series, and System Shock and its sequel. Other games, like Blade Runner, Ghost in the Shell, and the Matrix series, are based upon genre movies, or role-playing games (for instance the various Shadowrun games). Several RPGs called Cyberpunk exist: Cyberpunk, Cyberpunk 2020, Cyberpunk v3.0 and Cyberpunk Red written by Mike Pondsmith and published by R. Talsorian Games, and GURPS Cyberpunk, published by Steve Jackson Games as a module of the GURPS family of RPGs. Cyberpunk 2020 was designed with the settings of William Gibson's writings in mind, and to some extent with his approval, unlike the approach taken by FASA in producing the transgenre Shadowrun game and its various sequels, which mixes cyberpunk with fantasy elements such as magic and fantasy races such as orcs and elves. Both are set in the near future, in a world where cybernetics are prominent. Iron Crown Enterprises released an RPG named Cyberspace, which was out of print for several years until recently being re-released in online PDF form. CD Projekt Red released Cyberpunk 2077, a cyberpunk open world first-person shooter/role-playing video game (RPG) based on the tabletop RPG",
    "label": 0
  },
  {
    "text": "an RPG named Cyberspace, which was out of print for several years until recently being re-released in online PDF form. CD Projekt Red released Cyberpunk 2077, a cyberpunk open world first-person shooter/role-playing video game (RPG) based on the tabletop RPG Cyberpunk 2020, on December 10, 2020. In 1990, in a convergence of cyberpunk art and reality, the United States Secret Service raided Steve Jackson Games's headquarters and confiscated all their computers. Officials denied that the target had been the GURPS Cyberpunk sourcebook, but Jackson later wrote that he and his colleagues \"were never able to secure the return of the complete manuscript; [...] The Secret Service at first flatly refused to return anything – then agreed to let us copy files, but when we got to their office, restricted us to one set of out-of-date files – then agreed to make copies for us, but said \"tomorrow\" every day from March 4 to March 26. On March 26 we received a set of disks which purported to be our files, but the material was late, incomplete and well-nigh useless.\" Steve Jackson Games won a lawsuit against the Secret Service, aided by the new Electronic Frontier Foundation. This event has achieved a sort of notoriety, which has extended to the book itself as well. All published editions of GURPS Cyberpunk have a tagline on the front cover, which reads \"The book that was seized by the U.S. Secret Service!\" Inside, the book provides a summary of the raid and its aftermath. Cyberpunk has also inspired several tabletop, miniature and board games such as Necromunda by Games Workshop. Netrunner is a collectible card game introduced in 1996, based on the Cyberpunk 2020 role-playing game. Tokyo NOVA, debuting in 1993, is a cyberpunk role-playing game that uses playing cards instead of dice. Music",
    "label": 0
  },
  {
    "text": "such as Necromunda by Games Workshop. Netrunner is a collectible card game introduced in 1996, based on the Cyberpunk 2020 role-playing game. Tokyo NOVA, debuting in 1993, is a cyberpunk role-playing game that uses playing cards instead of dice. Music Invariably the origin of cyberpunk music lies in the synthesizer-heavy scores of cyberpunk films such as Escape from New York (1981) and Blade Runner (1982). Some musicians and acts have been classified as cyberpunk due to their aesthetic style and musical content. Often dealing with dystopian visions of the future or biomechanical themes, some fit more squarely in the category than others. Bands whose music has been classified as cyberpunk include Psydoll, Front Line Assembly, Clock DVA,Angelspit and Sigue Sigue Sputnik. Some musicians not normally associated with cyberpunk have at times been inspired to create concept albums exploring such themes. Albums such as the British musician and songwriter Gary Numan's Replicas, The Pleasure Principle and Telekon were heavily inspired by the works of Philip K. Dick. Kraftwerk's The Man-Machine and Computer World albums both explored the theme of humanity becoming dependent on technology. Nine Inch Nails' concept album Year Zero also fits into this category. Fear Factory concept albums are heavily based upon future dystopia, cybernetics, clash between man and machines, virtual worlds. Billy Idol's Cyberpunk drew heavily from cyberpunk literature and the cyberdelic counter culture in its creation. 1. Outside, a cyberpunk narrative fueled concept album by David Bowie, was warmly met by critics upon its release in 1995. Many musicians have also taken inspiration from specific cyberpunk works or authors, including Sonic Youth, whose albums Sister and Daydream Nation take influence from the works of Philip K. Dick and William Gibson respectively. Madonna's 2001 Drowned World Tour opened with a cyberpunk section, where costumes, asethetics and stage props",
    "label": 0
  },
  {
    "text": "or authors, including Sonic Youth, whose albums Sister and Daydream Nation take influence from the works of Philip K. Dick and William Gibson respectively. Madonna's 2001 Drowned World Tour opened with a cyberpunk section, where costumes, asethetics and stage props were used to accentuate the dystopian nature of the theatrical concert. Lady Gaga used a cyberpunk-persona and visual style for her sixth studio album Chromatica (2020). Vaporwave and synthwave are also influenced by cyberpunk. The former has been inspired by one of the messages of cyberpunk and is interpreted as a dystopian critique of capitalism in the vein of cyberpunk and the latter is more surface-level, inspired only by the aesthetic of cyberpunk as a nostalgic retrofuturistic revival of aspects of cyberpunk's origins. Social impact Art and architecture Writers David Suzuki and Holly Dressel describe the cafes, brand-name stores and video arcades of the Center Potsdamer Platz in the Potsdamer Platz public square of Berlin, Germany, as \"a vision of a cyberpunk, corporate urban future\". Society and counterculture Several subcultures have been inspired by cyberpunk fiction. These include the cyberdelic counter culture of the late 1980s and early 1990s. Cyberdelic, whose adherents referred to themselves as \"cyberpunks\", attempted to blend the psychedelic art and drug movement with the technology of cyberculture. Early adherents included Timothy Leary, Mark Frauenfelder and R. U. Sirius. The movement largely faded following the dot-com bubble implosion of 2000. Cybergoth is a fashion and dance subculture which draws its inspiration from cyberpunk fiction, as well as rave and Gothic subcultures. In addition, a distinct cyberpunk fashion of its own has emerged in recent years which rejects the raver and goth influences of cybergoth, and draws inspiration from urban street fashion, \"post apocalypse\", functional clothing, high tech sports wear, tactical uniform and multifunction. This fashion goes by",
    "label": 0
  },
  {
    "text": "of its own has emerged in recent years which rejects the raver and goth influences of cybergoth, and draws inspiration from urban street fashion, \"post apocalypse\", functional clothing, high tech sports wear, tactical uniform and multifunction. This fashion goes by names like \"tech wear\", \"goth ninja\" or \"tech ninja\". The Kowloon Walled City in Hong Kong, demolished in 1994, is often referenced as the model cyberpunk/dystopian slum as, given its poor living conditions at the time coupled with the city's political, physical, and economic isolation has caused many in academia to be fascinated by the ingenuity of its spawning. Cyberpunk derivatives As a wider variety of writers began to work with cyberpunk concepts, new subgenres of science fiction emerged, some of which could be considered as playing off the cyberpunk label, others which could be considered as legitimate explorations into newer territory. These focused on technology and its social effects in different ways. One prominent subgenre is \"steampunk,\" which is set in an alternate history Victorian era that combines anachronistic technology with cyberpunk's bleak film noir world view. The term was originally coined around 1987 as a joke to describe some of the novels of Tim Powers, James P. Blaylock, and K.W. Jeter, but by the time Gibson and Sterling entered the subgenre with their collaborative novel The Difference Engine the term was being used earnestly as well. Another subgenre is \"biopunk\" (cyberpunk themes dominated by biotechnology) from the early 1990s, a derivative style building on biotechnology rather than informational technology. In these stories, people are changed in some way not by mechanical means, but by genetic manipulation. Registered trademark status In the United States, the term \"Cyberpunk\" is a registered trademark owned by CD Projekt SA who obtained it from the previous owner R. Talsorian Games Inc. who originally",
    "label": 0
  },
  {
    "text": "not by mechanical means, but by genetic manipulation. Registered trademark status In the United States, the term \"Cyberpunk\" is a registered trademark owned by CD Projekt SA who obtained it from the previous owner R. Talsorian Games Inc. who originally registered it for its tabletop role-playing game. R. Talsorian Games currently used the trademark under license from CD Projekt SA for the tabletop role-playing game. Within the European Union, the \"Cyberpunk\" trademark is owned by two parties: CD Projekt SA for \"games and online gaming services\" (particularly for the video game adaptation of the former) and by Sony Music for use outside games. See also References Further reading Bould, Mark (2005). \"Cyberpunk\". A Companion to Science Fiction. John Wiley & Sons. pp. 217–231. doi:10.1002/9780470997055.ch15. ISBN 978-0-470-99705-5. O'Connell, Hugh Charles (2022). \"Cyberpunk\". In O'Donnell, Patrick; Burn, Stephen J.; Larkin, Lesley (eds.). The Encyclopedia of Contemporary American Fiction 1980–2020. John Wiley & Sons. pp. 1–11. doi:10.1002/9781119431732.ecaf0155. ISBN 978-1-119-43173-2. McFarlane Anna; Schmeink Lars; Murphy Graham, eds. (2019). The Routledge Companion to Cyberpunk Culture (1st ed.). New York: Routledge. doi:10.4324/9781351139885. ISBN 9781351139885. Murphy Graham; Schmeink Lars, eds. (2018). Cyberpunk and visual culture (1st ed.). New York: Routledge. ISBN 9781138062917. External links Cyberpunk in The Encyclopedia of Science Fiction The Cyberpunk Directory—Comprehensive directory of cyberpunk resources Cyberpunk Media Archive—Archive of cyberpunk media The Cyberpunk Project—A project dedicated toward maintaining a cyberpunk database, library, and other information",
    "label": 0
  },
  {
    "text": "De-extinction (also known as resurrection biology, or species revivalism) is the process of generating an organism that either resembles or is an extinct organism. There are several ways to carry out the process of de-extinction. Cloning is the most widely proposed method, although genome editing and selective breeding have also been considered. Similar techniques have been applied to certain endangered species, in hopes to boost their genetic diversity. The only method of the three that would provide an animal with the same genetic identity is cloning. There are benefits and drawbacks to the process of de-extinction ranging from technological advancements to ethical issues. Methods Cloning Cloning is a commonly suggested method for the potential restoration of an extinct species. It can be done by extracting the nucleus from a preserved cell from the extinct species and swapping it into an egg, without a nucleus, of that species' nearest living relative. The egg can then be inserted into a host from the extinct species' nearest living relative. This method can only be used when a preserved cell is available, meaning it would be most feasible for recently extinct species. Cloning offers the most direct route to an organism with the original species' nuclear genome, but it requires well-preserved viable donar cells. Cloning has been used by scientists since the 1950s. One of the most well known clones is Dolly the sheep. Dolly was born in the mid-1990s and lived normally until the abrupt midlife onset of health complications resembling premature aging, that led to her death. Other known cloned animal species include domestic cats, dogs, pigs, and horses. Genome editing Genome editing has been rapidly advancing with the help of the CRISPR/Cas systems, particularly CRISPR/Cas9. The CRISPR/Cas9 system was originally discovered as part of the bacterial immune system. Viral DNA that",
    "label": 0
  },
  {
    "text": "include domestic cats, dogs, pigs, and horses. Genome editing Genome editing has been rapidly advancing with the help of the CRISPR/Cas systems, particularly CRISPR/Cas9. The CRISPR/Cas9 system was originally discovered as part of the bacterial immune system. Viral DNA that was injected into the bacterium became incorporated into the bacterial chromosome at specific regions. These regions are called clustered regularly interspaced short palindromic repeats, otherwise known as CRISPR. Since the viral DNA is within the chromosome, it gets transcribed into RNA. Once this occurs, the Cas9 binds to the RNA. Cas9 can recognize the foreign insert and cleaves it. This discovery was very crucial because the Cas protein can now be viewed as a scissor in the genome editing process. By using cells from a closely related species to the extinct species, genome editing can play a role in the de-extinction process. Germ cells may be edited directly, so that the egg and sperm produced by the extant parent species will produce offspring of the extinct species, or somatic cells may be edited and transferred via somatic cell nuclear transfer. The result is an animal which is not completely the extinct species, but rather a hybrid of the extinct species and the closely related, non-extinct species. Because it is possible to sequence and assemble the genome of extinct organisms from highly degraded tissues, this technique enables scientists to pursue de-extinction in a wider array of species, including those for which no well-preserved remains exist. However, the more degraded and old the tissue from the extinct species is, the more fragmented the resulting DNA will be, making genome assembly more challenging. Genome editing does not require living cells from the exact same species and can target specific adaptive traits, but an organism created via this method can only ever be a",
    "label": 0
  },
  {
    "text": "the resulting DNA will be, making genome assembly more challenging. Genome editing does not require living cells from the exact same species and can target specific adaptive traits, but an organism created via this method can only ever be a proxy/hybrid. Back-breeding Back breeding is a form of selective breeding. As opposed to breeding animals for a trait to advance the species in selective breeding, back breeding involves breeding animals for an ancestral characteristic that may not be seen throughout the species as frequently. This method can recreate the traits of an extinct species, but the genome will differ from the original species. Back breeding, however, is contingent on the ancestral trait of the species still being in the population in any frequency. Back breeding is also a form of artificial selection by the deliberate selective breeding of domestic animals, in an attempt to achieve an animal breed with a phenotype that resembles a wild type ancestor, usually one that has gone extinct. Because back-breeding uses standard breeding techniques, no genetic reconstruction or preserved cells are required. The resulting offspring are phenotypic look-alikes, meaning that they are often lacking in behavioral, physiological, and ecological traits of the original species. Iterative evolution A natural process of de-extinction is iterative evolution. This occurs when a species becomes extinct, but then after some time a different species evolves into an almost identical creature. For example, the Aldabra rail was a flightless bird that lived on the island of Aldabra. It had evolved some time in the past from the flighted white-throated rail, but became extinct about 136,000 years ago due to an unknown event that caused sea levels to rise. About 100,000 years ago, sea levels dropped and the island reappeared, with no fauna. The white-throated rail recolonized the island, but soon evolved",
    "label": 0
  },
  {
    "text": "became extinct about 136,000 years ago due to an unknown event that caused sea levels to rise. About 100,000 years ago, sea levels dropped and the island reappeared, with no fauna. The white-throated rail recolonized the island, but soon evolved into a flightless species physically identical to the extinct species. No technological intervention is required for iterative evolution, but it lacks a controllable or timely method for restoration making it unpredictable over long time periods. Herbarium specimens for de-extincting plants Not all extinct plants have herbarium specimens that contain seeds. Of those that do, there is ongoing discussion on how to coax barely alive embryos back to life. Generally, plant material is better conserved than animal tissue. This means that if seeds are preserved, germination can produce living offspring that are identical to the historical specimen. However, many herbarium specimens do not contain viable seeds which complicate reintroduction. In-vitro fertilisation and artificial insemination In-vitro fertilisation and artificial insemination are assisted reproduction technology commonly used to treat infertility in humans. However, it has usage as a viable option for de-extinction in cases of functional extinction where all remaining individuals are of the same sex, incapable of naturally reproducing, or suffer from low genetic diversity such as the northern white rhinoceros, Yangtze giant softshell turtle, Hyophorbe amaricaulis, baiji, and vaquita. For example, viable embryos are created from preserved sperm from deceased males and ova from living females are implemented into a surrogate species. In-vitro fertillisation and artificial insemination can help preserve and restore genetic diversity to applicable near-extinct species where fresh gametes and tissues exist. However, cross-species surrogacy and embryo transfer present great biological and ethical challenges. Advantages of de-extinction The technologies being developed for de-extinction could lead to large advances in various fields: An advance in genetic technologies that are used",
    "label": 0
  },
  {
    "text": "and tissues exist. However, cross-species surrogacy and embryo transfer present great biological and ethical challenges. Advantages of de-extinction The technologies being developed for de-extinction could lead to large advances in various fields: An advance in genetic technologies that are used to improve the cloning process for de-extinction could be used to prevent endangered species from becoming extinct. By studying revived previously extinct animals, cures to diseases could be discovered. Revived species may support conservation initiatives by acting as \"flagship species\" to generate public enthusiasm and funds for conserving entire ecosystems. Prioritising de-extinction could lead to the improvement of current conservation strategies. Conservation measures would initially be necessary in order to reintroduce a species into the ecosystem, until the revived population can sustain itself in the wild. Reintroduction of an extinct species could also help improve ecosystems that had been destroyed by human development. It may also be argued that reviving species driven to extinction by humans is an ethical obligation. Disadvantages of de-extinction The reintroduction of extinct species could have a negative impact on existing species and their ecosystem. The extinct species' ecological niche may have been filled in its former habitat, thus making them an invasive species. This could lead to the extinction of other species due to competition for food or other competitive exclusion. It could also lead to the extinction of prey species if they have more predators in an environment that had few predators before the reintroduction of an extinct species. If a species has been extinct for a long period of time the environment they are introduced to could be wildly different from the one that they can survive in. The changes in the environment due to human development could mean that the species may not survive if reintroduced into that ecosystem. A species could",
    "label": 0
  },
  {
    "text": "are introduced to could be wildly different from the one that they can survive in. The changes in the environment due to human development could mean that the species may not survive if reintroduced into that ecosystem. A species could also become extinct again after de-extinction if the reasons for its extinction are still a threat. The woolly mammoth might be hunted by poachers just like elephants for their ivory and could go extinct again if this were to happen. Or, if a species is reintroduced into an environment with disease for which it has no immunity, the reintroduced species could be wiped out by a disease that current species can survive. De-extinction is also a very expensive process. Bringing back one species can cost millions of dollars. The money for de-extinction would most likely come from current conservation efforts. These efforts could be weakened if funding is taken from conservation and put into de-extinction. This would mean that critically endangered species would start to go extinct faster because there are no longer resources that are needed to maintain their populations. Also, since cloning techniques cannot perfectly replicate a species as it existed in the wild, the reintroduction of the species may not bring about positive environmental benefits. They may not have the same role in the food chain that they did before and therefore cannot restore damaged ecosystems. De-extinction also presents serious ethical challenges, particularly if applied to species with high cognitive abilities such as Neanderthals. If such beings were recreated, they might possess capacities for suffering and self-awareness, raising difficult questions about their rights and moral status. Without clear legal protections, they risk being treated as research tools or experimental assets rather than sentient individuals. Current candidate species for de-extinction Aurochs The aurochs (Bos primigenius) was widespread across",
    "label": 0
  },
  {
    "text": "raising difficult questions about their rights and moral status. Without clear legal protections, they risk being treated as research tools or experimental assets rather than sentient individuals. Current candidate species for de-extinction Aurochs The aurochs (Bos primigenius) was widespread across Eurasia, North Africa, and the Indian subcontinent during the Pleistocene, but only the European aurochs (B. p. primigenius) survived into historical times. This species is heavily featured in European cave paintings, such as Lascaux and Chauvet Cave in France, and was still widespread during the Roman era. Following the fall of the Roman Empire, overhunting of the aurochs by nobility caused its population to dwindle to a single population in the Jaktorów forest in Poland, where the last wild one died in 1627. However, because the aurochs is ancestral to most modern cattle breeds, it is possible for it to be brought back through selective or back breeding. The first attempt at this was by Heinz and Lutz Heck using modern cattle breeds, which resulted in the creation of Heck cattle. This breed has been introduced to nature preserves across Europe; however, it differs strongly from the aurochs in physical characteristics, and some modern attempts claim to try to create an animal that is nearly identical to the aurochs in morphology, behavior, and even genetics. There are several projects that aim to create a cattle breed similar to the aurochs through selectively breeding primitive cattle breeds over a course of twenty years to create a self-sufficient bovine grazer in herds of at least 150 animals in rewilded nature areas across Europe, for example the Tauros Programme and the separate Taurus Project. This organization is partnered with the organization Rewilding Europe to help revert some European natural ecosystems to their prehistoric form. A competing project to recreate the aurochs is the",
    "label": 0
  },
  {
    "text": "Europe, for example the Tauros Programme and the separate Taurus Project. This organization is partnered with the organization Rewilding Europe to help revert some European natural ecosystems to their prehistoric form. A competing project to recreate the aurochs is the Uruz Project by the True Nature Foundation, which aims to recreate the aurochs by a more efficient breeding strategy using genome editing, in order to decrease the number of generations of breeding needed and the ability to quickly eliminate undesired traits from the population of aurochs-like cattle. It is hoped that aurochs-like cattle will reinvigorate European nature by restoring its ecological role as a keystone species and bring back biodiversity that disappeared following the decline of European megafauna, as well as helping to bring new economic opportunities related to European wildlife viewing. Sometime in 2025, Tauros Programme and Rewilding Europe plan to release their aurochs into the wild in select areas of Europe and to have the species recognised as a protected wildlife species again. In 2026, these animals will be reintroduced to parts of the Scottish Highlands. Bush moa The bush moa, also known as the little bush moa or lesser moa (Anomalopteryx didiformis) is a slender species of moa slightly larger than a turkey that abruptly went extinct around 1400-1500AD following the arrival and proliferation of the Māori people in New Zealand, as well as the introduction of Polynesian dogs. Scientists at Harvard University assembled the first nearly complete genome of the species from toe bones, thus bringing the species a step closer to de-extinction. Trevor Mallard, a New Zealand politician, has also previously suggested bringing back a medium-sized species of moa. The proxy of the species will likely be the emu. Dire wolf There have been attempts to recreate the dire wolf (Aenocyon dirus) in modern times.",
    "label": 0
  },
  {
    "text": "New Zealand politician, has also previously suggested bringing back a medium-sized species of moa. The proxy of the species will likely be the emu. Dire wolf There have been attempts to recreate the dire wolf (Aenocyon dirus) in modern times. The first is a project called the Dire Wolf Project, a project begun in 1988 that aims to revive the species through backbreeding of domestic dogs, similar to the Quagga project. However, this project is not based in scientific method. In April 2025, Colossal Biosciences showcased three genetically modified wolf pups with the characteristics of the dire wolf: six-month-old males Romulus and Remus and two-month-old female Khaleesi. In-house scientists at Colossal analyzed the dire wolf genome, extracted from two ancient samples – a 13,000-year-old tooth and a 72,000-year-old ear bone. After comparing the genomes of gray wolves and dire wolves to identify the genetic differences responsible for the dire wolf's distinctive features, Colossal made edits to the genetic code of the gray wolf to replicate those traits. Domestic dogs were used as surrogate mothers for the pups. Colossal claims that these minor genetic modifications effectively revive dire wolves as a species, though \"no ancient dire wolf DNA was actually spliced into the gray wolf's genome\". Dodo The dodo (Raphus cucullatus) was a flightless bird endemic to the island of Mauritius in the Indian Ocean. Due to various factors such as the inability to feel fear caused by isolation from significant predators, predation from humans and introduced invasive species such as pigs, dogs, cats, rats, and crab-eating macaques, competition for food with invasive species, habitat loss, and the birds naturally slow reproduction, the species' numbers declined rapidly. The last widely accepted recorded sighting was in 1662. Since then, the bird has become a symbol for extinction and is often cited as",
    "label": 0
  },
  {
    "text": "with invasive species, habitat loss, and the birds naturally slow reproduction, the species' numbers declined rapidly. The last widely accepted recorded sighting was in 1662. Since then, the bird has become a symbol for extinction and is often cited as the primary example of man-made extinction. In January 2023, Colossal Biosciences announced their project to revive the dodo alongside their previously announced projects for reviving the woolly mammoth and thylacine in hopes of restoring biodiversity to Mauritius and changing the dodo's status as a symbol of extinction to de-extinction. Heath hen The heath hen (Tympanuchus cupido cupido) was a subspecies of greater prairie chicken endemic to the heathland barrens of coastal North America. It is even speculated that the pilgrims' first Thanksgiving featured this bird as the main course instead of wild turkey. Due to overhunting caused by its perceived abundancy, the population became extinct in mainland North America by 1870, leaving a population of 300 individuals left on Martha's Vineyard. Despite conservation efforts, the subspecies became extinct in 1932 following the disappearance and presumed death of Booming Ben, the final known member of the subspecies. In the summer of 2014, non-profit organisation, Revive & Restore held a meeting with the community of Martha's Vineyard to announce their project to revive the heath hen in hopes of restoring and maintaining the sandplain grasslands. On April 8, 2020, germs cells were collected from greater prairie chicken eggs at Texas A&M. Ivory-billed woodpecker The ivory-billed woodpecker (Campephilus principalis) is the largest woodpecker native to the United States, with an endemic subspecies in Cuba. The species numbers have declined since the late 1800s due to logging and hunting. Similarly to the northern white rhinoceros, the ivory-billed woodpecker may not be completely extinct, but rather functionally extinct, though the evidence suggests that the species",
    "label": 0
  },
  {
    "text": "Cuba. The species numbers have declined since the late 1800s due to logging and hunting. Similarly to the northern white rhinoceros, the ivory-billed woodpecker may not be completely extinct, but rather functionally extinct, though the evidence suggests that the species is 'very likely extinct'. In October 2024, Colossal Biosciences announced their non-profit Colossal Foundation, a foundation dedicated to conservation of extant species with their first projects being the Sumatran rhinoceros, vaquita, red wolf, pink pigeon, northern quoll, and ivory-billed woodpecker. Colossal plans to revive or rediscover the species through genome editing of its closest living relatives, such as the pileated woodpecker and using drones and AI to identify any potential remaining individuals in the wild. Maclear's rat The Maclear's rat (Rattus macleari), also known as the Christmas Island rat, was a large rat endemic to Christmas Island in the Indian Ocean. It is believed Maclear's rat might have been responsible for keeping the population of Christmas Island red crab in check. It is thought that the accidental introduction of black rats by the Challenger expedition infected the Maclear's rats with a disease (possibly a trypanosome), which resulted in the species' decline. The last recorded sighting was in 1903. In March 2022, researchers discovered the Maclear's rat shared about 95% of its genes with the living brown rat, thus sparking hopes in bringing the species back to life. Although scientists were mostly successful in using CRISPR technology to edit the DNA of the living species to match that of the extinct one, a few key genes were missing, which would mean resurrected rats would not be genetically pure replicas. Northern white rhinoceros The northern white rhinoceros or northern white rhino (Ceratotherium simum cottoni) is a subspecies of the white rhinoceros endemic to East and Central Africa south of the Sahara. Due",
    "label": 0
  },
  {
    "text": "resurrected rats would not be genetically pure replicas. Northern white rhinoceros The northern white rhinoceros or northern white rhino (Ceratotherium simum cottoni) is a subspecies of the white rhinoceros endemic to East and Central Africa south of the Sahara. Due to widespread and uncontrollable poaching and civil warfare in their former range, the subspecies' numbers dropped quickly over the course of the late 1900s and early 2000s. Unlike the majority of the potential candidates for de-extinction, the northern white rhinoceros is not extinct, but functionally extinct and is believed to be extinct in the wild with only two known female members left, Najin and Fatu who reside on the Ol Pejeta Conservancy in Kenya. The BioRescue Team in collaboration with Colossal Biosciences plan to implement 30 northern white rhinoceros embryos made from egg cells collected from Najin and Fatu and preserved sperm from dead male individuals into female southern white rhinoceros by the end of 2024. Passenger pigeon The passenger pigeon (Ectopistes migratorius) numbered in the billions before being wiped out due to unsustainable commercial hunting and habitat loss during the early 20th century. The non-profit Revive & Restore obtained DNA from the passenger pigeon from museum specimens and skins; however, this DNA is degraded because it is so old. For this reason, simple cloning would not be an effective way to perform de-extinction for this species because parts of the genome would be missing. Instead, Revive & Restore focuses on identifying mutations in the DNA that would cause a phenotypic difference between the extinct passenger pigeon and its closest living relative, the band-tailed pigeon. In doing this, they can determine how to modify the DNA of the band-tailed pigeon to change the traits to mimic the traits of the passenger pigeon. In this sense, the de-extinct passenger pigeon would",
    "label": 0
  },
  {
    "text": "living relative, the band-tailed pigeon. In doing this, they can determine how to modify the DNA of the band-tailed pigeon to change the traits to mimic the traits of the passenger pigeon. In this sense, the de-extinct passenger pigeon would not be genetically identical to the extinct passenger pigeon, but it would have the same traits. In 2015, the de-extinct passenger pigeon hybrid was forecast ready for captive breeding by 2025 and released into the wild by 2030. In October 2024, Revive & Restore collaborated with Applied Ecological Institute to simulate forest disturbances in the American state of Wisconsin to see how trees would react to the reintroduced passenger pigeons. The original 2025 goal was not met, with the new goal for reviving the species for captive breeding set for between 2029 and 2032. However, it could take decades for the species to be reintroduced to the wild. Quagga The quagga (Equus quagga quagga) is a subspecies of the plains zebra that was distinct in that it was striped on its face and upper torso, but its rear abdomen was a solid brown. It was native to South Africa, but was wiped out in the wild due to overhunting for sport, and the last individual died in 1883 in the Amsterdam Zoo. However, since it is technically the same species as the surviving plains zebra, it has been argued that the quagga could be revived through artificial selection. The Quagga Project aims to breed a similar form of zebra by selective breeding of plains zebras. This process is also known as back breeding. It also aims to release these animals onto the western Cape once an animal that fully resembles the quagga is achieved, which could have the benefit of eradicating introduced species of trees such as the Brazilian pepper",
    "label": 0
  },
  {
    "text": "as back breeding. It also aims to release these animals onto the western Cape once an animal that fully resembles the quagga is achieved, which could have the benefit of eradicating introduced species of trees such as the Brazilian pepper tree, Tipuana tipu, Acacia saligna, bugweed, camphor tree, stone pine, cluster pine, weeping willow and Acacia mearnsii. Steller's sea cow The Steller's sea cow was a sirenian endemic to Bering Sea between Russia and the United States but had a much larger range during the Pleistocene. First described by Georg Wilhelm Steller in 1741, it was hunted to extinction 27 years later due to its buoyancy making it an easy target for humans hunting it for its meat and fur in addition to an already low population caused by climate change. In 2021, the nuclear genome of the species was sequenced. In late 2022, a group of Russian scientists funded by Sergei Bachin began their project to revive and reintroduce the giant sirenian to its former range in the 18th century to restore its kelp forest ecosystem. Arctic Sirenia plans to revive the species through genome editing of the dugong, but they need an artificial womb to conceive a live animal due to lack of an adequate surrogate species. Thylacine The thylacine (Thylacinus cynocephalus), commonly known as the Tasmanian tiger, was native to the Australian mainland, Tasmania and New Guinea. It is believed to have become extinct in the 20th century. The thylacine had become extremely rare or extinct on the Australian mainland before British settlement of the continent. The last known thylacine died at the Hobart Zoo, on September 7, 1936. It is believed to have died as the result of neglect—locked out of its sheltered sleeping quarters, it was exposed to a rare occurrence of extreme Tasmanian weather:",
    "label": 0
  },
  {
    "text": "last known thylacine died at the Hobart Zoo, on September 7, 1936. It is believed to have died as the result of neglect—locked out of its sheltered sleeping quarters, it was exposed to a rare occurrence of extreme Tasmanian weather: extreme heat during the day and freezing temperatures at night. Official protection of the species by the Tasmanian government was introduced on July 10, 1936, roughly 59 days before the last known specimen died in captivity. In December 2017, it was announced in the journal Nature Ecology and Evolution that the full nuclear genome of the thylacine had been successfully sequenced, marking the completion of the critical first step toward de-extinction that began in 2008, with the extraction of the DNA samples from the preserved pouch specimen. The thylacine genome was reconstructed by using the genome editing method. The Tasmanian devil was used as a reference for the assembly of the full nuclear genome. Andrew J. Pask from the University of Melbourne has stated that the next step toward de-extinction will be to create a functional genome, which will require extensive research and development, estimating that a full attempt to resurrect the species may be possible as early as 2027. In August 2022, the University of Melbourne and Colossal Biosciences announced a partnership to accelerate de-extinction of the thylacine via genetic modification of one of its closest living relatives, the fat-tailed dunnart. In October 2024, Colossal claimed to have reconstructed a 99.9% complete genome of the thylacine from a well-preserved skull estimated to be 110 years old; however, the data has not yet been published. Woolly mammoth The existence of preserved soft tissue remains and DNA from woolly mammoths (Mammuthus primigenius) has led to the idea that the species could be recreated by scientific means. Two methods have been proposed",
    "label": 0
  },
  {
    "text": "has not yet been published. Woolly mammoth The existence of preserved soft tissue remains and DNA from woolly mammoths (Mammuthus primigenius) has led to the idea that the species could be recreated by scientific means. Two methods have been proposed to achieve this: The first would be to use the cloning process; however, even the most intact mammoth samples have had little usable DNA because of their conditions of preservation. There is not enough DNA intact to guide the production of an embryo. The second method would involve artificially inseminating an elephant egg cell with preserved sperm of the mammoth. The resulting offspring would be a hybrid of the mammoth and its closest living relative the Asian elephant. After several generations of cross-breeding these hybrids, an almost pure woolly mammoth could be produced. However, sperm cells of modern mammals are typically potent for up to 15 years after deep-freezing, which could hinder this method. Whether the hybrid embryo would be carried through the two-year gestation is unknown; in one case, an Asian elephant and an African elephant produced a live calf named Motty, but it died of defects at less than two weeks old. In 2008, a Japanese team found usable DNA in the brains of mice that had been frozen for 16 years. They hope to use similar methods to find usable mammoth DNA. In 2011, Japanese scientists announced plans to clone mammoths within six years. In March 2014, the Russian Association of Medical Anthropologists reported that blood recovered from a frozen mammoth carcass in 2013 would now provide a good opportunity for cloning the woolly mammoth. Another way to create a living woolly mammoth would be to migrate genes from the mammoth genome into the genes of its closest living relative, the Asian elephant, to create hybridized animals",
    "label": 0
  },
  {
    "text": "a good opportunity for cloning the woolly mammoth. Another way to create a living woolly mammoth would be to migrate genes from the mammoth genome into the genes of its closest living relative, the Asian elephant, to create hybridized animals with the notable adaptations that it had for living in a much colder environment than modern day elephants. This is currently being done by a team led by Harvard geneticist George Church. The team has made changes in the elephant genome with the genes that gave the woolly mammoth its cold-resistant blood, longer hair, and an extra layer of fat. According to geneticist Hendrik Poinar, a revived woolly mammoth or mammoth-elephant hybrid may find suitable habitat in the tundra and taiga forest ecozones. George Church has hypothesized the positive effects of bringing back the extinct woolly mammoth would have on the environment, such as the potential for reversing some of the damage caused by global warming. He and his fellow researchers predict that mammoths would eat the dead grass allowing the sun to reach the spring grass; their weight would allow them to break through dense, insulating snow in order to let cold air reach the soil; and their characteristic of felling trees would increase the absorption of sunlight. In an editorial condemning de-extinction, Scientific American pointed out that the technologies involved could have secondary applications, specifically to help species on the verge of extinction regain their genetic diversity. In March 2025, Colossal Biosciences, a startup founded by George Church with funding from Ben Lamm announced the birth of woolly mice. These mice, while they do not contain almost any mammoth genetic information – most of the edited genes are known mice fur genetic variants, and not woolly mammoth variants –, exhibit some of the key traits of the woolly",
    "label": 0
  },
  {
    "text": "mice. These mice, while they do not contain almost any mammoth genetic information – most of the edited genes are known mice fur genetic variants, and not woolly mammoth variants –, exhibit some of the key traits of the woolly mammoth, such as cold tolerance and long, shaggy, tawny-toned fur. Yangtze giant softshell turtle The Yangtze giant softshell turtle (Rafetus swinhoei) is a softshell turtle endemic to China and Vietnam and is possibly the largest living freshwater turtle. Due to various factors such as habitat loss, wildlife trafficking, trophy hunting, and the Vietnam War, the species population has been reduced to only three male individuals, rendering it functionally extinct similar to the northern white rhinoceros and ivory-billed woodpecker. There is one captive individual in Suzhou Zoo in China, and two wild individuals at Dong Mo Lake in Vietnam. Efforts to save the species from extinction through various means of assisted reproduction in captivity have been ongoing since 2009 by the Suzhou Zoo and Turtle Survival Alliance. Despite efforts to breed the turtles naturally, the eggs laid by the final known female were all infertile and unviable. In May 2015, artificial insemination was performed for the first time in the species. In July of the same year, the female laid 89 eggs, but like all previous natural attempts, they were all unviable. In April 2019, the female individual at the zoo died after another failed artificial insemination attempt. In 2020, a female was discovered in the wild, reigniting hope for the survival of the species. However, this individual was found dead in early 2023. Several searches across China and Vietnam are currently underway to locate female individuals to breed with the final known males, or to undergo artificial insemination. Further species considered for de-extinction A \"De-extinction Task Force\" was established in",
    "label": 0
  },
  {
    "text": "in early 2023. Several searches across China and Vietnam are currently underway to locate female individuals to breed with the final known males, or to undergo artificial insemination. Further species considered for de-extinction A \"De-extinction Task Force\" was established in April 2014 under the auspices of the Species Survival Commission (SSC) and charged with drafting a set of Guiding Principles on Creating Proxies of Extinct Species for Conservation Benefit to position the IUCN SSC on the rapidly emerging technological feasibility of creating a proxy of an extinct species. Avians Giant moa – The tallest birds to have ever lived. Both the northern and southern species became extinct by 1500 due to overhunting by the Māori in New Zealand. The extinction of these large avian herbivores left ecological vacancies, such as the browsing of tall understorey vegetation, seed‑dispersal of large forest plants, and nutrient cycling associated with large‑bodied birds Elephant bird – The heaviest birds to have ever lived, the elephant birds were driven to extinction by the early colonization of Madagascar. Ancient DNA has been obtained from the eggshells but may be too degraded for use in de-extinction. Carolina parakeet - One of the few native parrots to the United States, it was driven to extinction by destruction of its habitat, overhunting, competition from introduced honeybees, and persecution for crop damages and declared extinct following the death of its final known member, Incas in 1918. Hundreds of specimens with viable DNA still exist in museums around the world, making it a prime candidate for revival. In 2019, the full genome of the Carolina parakeet was sequenced. Great auk - A flightless bird native to the North Atlantic physically similar to penguins. The great auk went extinct in the 1800s due to overhunting by humans for food. The last two known",
    "label": 0
  },
  {
    "text": "of the Carolina parakeet was sequenced. Great auk - A flightless bird native to the North Atlantic physically similar to penguins. The great auk went extinct in the 1800s due to overhunting by humans for food. The last two known great auks lived on an island near Iceland and were clubbed to death by sailors. There have been no known sightings since. The great auk has been identified as a good candidate for de-extinction by Revive and Restore, a non-profit organization. Because the great auk is extinct it cannot be cloned, but its DNA can be used to alter the genome of its closest relative, the razorbill, and breed the hybrids to create a species that will be very similar to the original great auks. The plan is to introduce them back into their original habitat, which they would then share with razorbills and puffins, who are also at risk for extinction. This would help restore the biodiversity and restore that part of the ecosystem. Colossal Biosciences has also expressed interest in reviving the species. Imperial woodpecker – The largest woodpecker to have ever lived. Endemic to Mexico and possibly extinct, it has not been seen since 1956 due to habitat destruction and hunting. The Federal government of Mexico has considered the species extinct since 2001, 47 years after the last widely accepted sighting. However, they have conservation plans if the species is rediscovered or attempts at de-extinction are made. Cuban macaw – A colourful macaw that was endemic to Cuba and Isla de la Juventud, it was the only macaw species of the Antilles. It became extinct in the late 19th century due to overhunting, pet trade, and habitat loss. Labrador duck – A duck native to North America. it became extinct in the late 19th century due to",
    "label": 0
  },
  {
    "text": "macaw species of the Antilles. It became extinct in the late 19th century due to overhunting, pet trade, and habitat loss. Labrador duck – A duck native to North America. it became extinct in the late 19th century due to colonisation in their former range combined with an already naturally low population. It is also the first known endemic North American bird species to become extinct following the Columbian Exchange. Huia – A species of Callaeidae native to New Zealand. It became extinct in 1907 due to overhunting from both the Māori and European settlers, habitat loss, and predation from introduced invasive species. In 1999, students of Hastings Boys' High School proposed the idea of de-extinction of the huia, the school's emblem through cloning. The Ngāti Huia tribe approved of the idea and the de-extinction process would have been performed by the University of Otago with $100,000 funding from a Californian-based internet startup. However, due to the poor state of DNA in the specimens at Museum of New Zealand Te Papa Tongarewa, a complete huia genome could not be created, making this method of de-extinction improbable to succeed. Moho – An entire family and genus of songbirds that were endemic to the islands of Hawaii. The genus and family became extinct in 1987 following the extinction of its final living member, the Kauaʻi ʻōʻō. The reasons for the genus' decline were overhunting for their plumage, habitat loss caused by colonization of Hawaii, natural disasters, mosquito-borne diseases, and predation from introduced invasive species. Mammals Caribbean monk seal – A species of monk seal that was native to the Caribbean. It became extinct in 1952 due to poaching and starvation caused by overfishing of its natural prey. Bluebuck – A species of antelope that was native to Africa. The species was hunted",
    "label": 0
  },
  {
    "text": "monk seal that was native to the Caribbean. It became extinct in 1952 due to poaching and starvation caused by overfishing of its natural prey. Bluebuck – A species of antelope that was native to Africa. The species was hunted to extinction by 1799 or 1800 by Europeans, and the species had a naturally low population similar to the Labrador duck. In 2024, the nuclear genome of the species was sequenced by University of Potsdam and Colossal Biosciences. Colossal Biosciences has also expressed interest in reviving the species in the future. Pyrenean ibex – A subspecies of ibex that was native to the Pyrenees, the subspecies was declared extinct in early 2000 following the death of Celia, the endling of the subspecies after two centuries of overhunting and competition with livestock and introduced species. Celia was successfully cloned by Spanish scientists in 2003, but the clone died shortly after its birth due to a lung defect. 10 years later, The Aragon Hunting Federation in collaboration with CITA (Centre for Research and Food Technology of Aragon) began a second attempt to potentially revive the subspecies by verifying if Celia's frozen cells were still viable for future cloning attempts. As of April 2025, no further statements have come out from this project. Vaquita – The smallest cetacean to have ever lived; endemic to the upper Gulf of California in Mexico. The vaquita is not completely extinct, but functionally extinct with an estimate of 8 or less members left due to entanglement in gillnets meant to poach totoabas, a fish with a highly valued swim bladder on Asian black markets due to its perceived medicinal values. In October 2024, Colossal Biosciences launched their Colossal Foundation, a non-profit foundation dedicated to conservation of extant species with one of their first projects being the vaquita.",
    "label": 0
  },
  {
    "text": "valued swim bladder on Asian black markets due to its perceived medicinal values. In October 2024, Colossal Biosciences launched their Colossal Foundation, a non-profit foundation dedicated to conservation of extant species with one of their first projects being the vaquita. In addition to using technology to monitor the final remaining individuals, they aim to collect tissue samples from vaquitas in order to revive it if it does become extinct in the near future. Irish elk – The largest deer to have ever lived, formerly inhabiting Eurasia from present day Ireland to present day Siberia during the Pleistocene. It became extinct 5–10 thousand years ago due to suspected overhunting from humans. Cave lion – A species of Panthera related to the modern lion found throughout the Holarctic during the Pleistocene. It is estimated that the species died out 14-15 thousand years ago due to climate change and low genetic diversity. The discovery of well-preserved cubs in the Sakha Republic, Russia ignited a project to clone the animal. Cave hyena – A species or subspecies of hyena that was endemic to Eurasia during the Pleistocene. It is estimated that the species died out 31 thousand years ago due to competition with early humans and other carnivores and decreased availability of prey. Castoroides – An entire genus of giant beavers endemic to North America during the Pleistocene. It is unknown how the species died out, but some suggest that climate change and competition are factors. Beth Shapiro of Colossal Biosciences has expressed interest in reviving a species from this genus. Steppe bison – The ancestor species of all modern American bison, formerly endemic to Western Europe to eastern Beringia in North America during the Late Pleistocene. The discovery of the mummified steppe bison of 9,000 years ago could help people clone the ancient",
    "label": 0
  },
  {
    "text": "The ancestor species of all modern American bison, formerly endemic to Western Europe to eastern Beringia in North America during the Late Pleistocene. The discovery of the mummified steppe bison of 9,000 years ago could help people clone the ancient bison species back, even though the steppe bison would not be the first to be \"resurrected\". Russian and South Korean scientists are collaborating to clone steppe bison in the future using DNA preserved from an 8,000-year-old tail and wood bison as a surrogate species, which themselves have been introduced to Yakutia to fulfil a similar niche. Longhorn bison – Also known as the giant bison, a species of bison native to North America from Southern Canada to Mexico during the Late Pleistocene. It is estimated that the species died out 13,000 years ago, possibly due to pressure from early humans and overhunting. Ground sloths – An extremely diverse group of sloths native to the Americas during the Pleistocene with some growing to the size of modern elephants. Ground sloths died out on the mainland 11 thousand years ago, but relict populations in the Caribbean survived until about 4 thousand years ago like the final populations of woolly mammoths on Wrangel island. Ground sloths died out primarily due to climate change and some suspect that their size and slowness made them easy targets for early humans. The insular Caribbean populations were likely driven to extinction through overhunting. Woolly rhinoceros – A species of rhinoceros that was endemic to Northern Eurasia during the Pleistocene. It is believed to have become extinct as a result of both climate change and overhunting by early humans. In November 2023, scientists managed to sequence the woolly rhinoceros's genome from faeces of cave hyenas in addition to the existence of mummified specimens. Miracinonyx – Also known as",
    "label": 0
  },
  {
    "text": "a result of both climate change and overhunting by early humans. In November 2023, scientists managed to sequence the woolly rhinoceros's genome from faeces of cave hyenas in addition to the existence of mummified specimens. Miracinonyx – Also known as American cheetahs, an entire genus of felines that were native to North America during the Pleistocene. It is unknown how the genus went extinct, but some suggest that they died out for the same reasons as other North American megafauna; climate change, loss of prey, and competition with early humans and other carnivores. Machairodontinae – Commonly referred to as sabre-tooth cats or sabre-tooth tigers, an entire subfamily of feline apex predators that were widespread globally with the exceptions of Oceania and Antarctica from the middle Miocene to early Holocene. The final two genera of this subfamily, Smilodon and Homotherium, are estimated to have become extinct during the Quaternary extinction event 10–13 thousand years ago for the same reasons as other carnivorous megafauna. Despite one of their common names, the biggest challenge in the restoration of any species belonging to this subfamily is that they are too genetically distinct from modern big cats, such as tigers. In 2020, a mummified Homotherium latidens cub was discovered in Yakutia, Russia. Columbian mammoth – A species of mammoth that was endemic to North America across what are now the United States and northern Mexico. The species became extinct 12 thousand years ago during the Quaternary extinction event due to climate change, overhunting from early humans, and habitat loss. Mastodon – An entire genus of proboscideans that were native to North America from the Miocene to the early Holocene. Like the Columbian mammoth, the species became extinct about 11,795 to 11,345 years ago due to climate change, overhunting from early humans, and habitat loss. Arctodus",
    "label": 0
  },
  {
    "text": "proboscideans that were native to North America from the Miocene to the early Holocene. Like the Columbian mammoth, the species became extinct about 11,795 to 11,345 years ago due to climate change, overhunting from early humans, and habitat loss. Arctodus – An entire genus of short-faced bears endemic to North America during the Pleistocene. It is estimated that they became extinct 12 thousand years ago following the death of its final member, Arctodus simus due to climate change and low genetic diversity. Beth Shapiro of Colossal Biosciences has expressed interest in reviving one of the two species from the genus. Amphibians Gastric-brooding frog – A genus of ground-dwelling frogs that were native to Queensland, Australia. They became extinct in the mid-1980s primarily due to chytridiomycosis. They were unique in their reproductive strategy: after external fertilisation, the female swallowed the eggs, brooded the developing young within her stomach (ceasing gastric acid production), and gave birth via the mouth. In 2013, scientists in Australia successfully created a living embryo from non-living preserved genetic material, and hope that by using somatic-cell nuclear transfer methods, they can produce an embryo that can survive to the tadpole stage. This frog remains among the most advanced de‑extinction candidates within amphibians, offering a unique biological case study due to its unusual reproductive mode. A successfully revived population might inform broader amphibian conservation techniques. Insects Xerces blue – A species of butterfly that was native to the Sunset District of San Francisco in the American state of California. It is estimated that the species became extinct in the early 1940s due to urbanization of their former habitat. Similar species to the Xerces blue, such as Glaucopsyche lygdamus and the Palos Verdes blue, have been released into the Xerces blue's former range to substitute its ecological role. On April",
    "label": 0
  },
  {
    "text": "early 1940s due to urbanization of their former habitat. Similar species to the Xerces blue, such as Glaucopsyche lygdamus and the Palos Verdes blue, have been released into the Xerces blue's former range to substitute its ecological role. On April 15, 2024, non-profit organisation Revive & Restore announced the early stages of their plans to potentially revive the species. In April 2024, the conservation organisations California Academy of Sciences (CAS) and the Presidio Trust jointly released dozens of the closely related butterfly Glaucopsyche lygdamus (silvery blue) into restored sand‑dune habitat within the Presidio of San Francisco. Although this does not involve genome‑based resurrection of G. xerces, the initiative functions as an ecological surrogate: the silvery blue uses the same host plant, thrives in a similar micro‑climate (cool, foggy dunes) and fills the vacant pollinator/food‑web role of the extinct species. Plants Paschalococos – A genus of coccoid palm trees that were native to Easter Island, Chile. It is believed to have become extinct around 1650 due to overharvesting for its edible nuts and its subsequent disappearance from the pollen records. Any de‑extinction would require retrieving viable genetic material from sub‑fossil pollen or preserved endocarps, identifying a surrogate propagation system. Hyophorbe amaricaulis – A species of palm tree from the Arecales family that is native to the island of Mauritius. Unlike the majority of potential candidates, this palm is not completely extinct, but functionally extinct and is believed to be extinct in the wild with only one known specimen left in the Curepipe Botanic Gardens. In 2010, there was an attempt to revive the species through germination in vitro in which Isolated and growing embryos were extracted from seeds in tissue culture, but these seedlings only lived for three months. efforts centre on micro‑propagation, seed‑rescue and cloning, but the genetic bottleneck (single",
    "label": 0
  },
  {
    "text": "revive the species through germination in vitro in which Isolated and growing embryos were extracted from seeds in tissue culture, but these seedlings only lived for three months. efforts centre on micro‑propagation, seed‑rescue and cloning, but the genetic bottleneck (single survivor), high mortality of seedlings and historical habitat change remain major constraints. Successful de-extinctions Judean date palm The Judean date palm is a species of date palm native to the historical area Judea, believed to have become extinct around the 15th century as a result of climate change and human activity in the region. In 2005, preserved seeds recovered during excavations of Herod the Great's palace in the 1960s were given to Sarah Sallon of Bar-Ilan University, who had proposed germinating ancient seeds. Sallon then challenged her colleague, Elaine Solowey of the Center for Sustainable Agriculture at the Arava Institute for Environmental Studies, to attempt the germination. Solowey successfully revived several seeds using simple hydration with a household baby bottle warmer, along with standard fertiliser and growth hormones. The first plant to grow was named Methuselah, after Lamech's father, the oldest man mentioned in the Bible. Early plans in 2012 proposed crossbreeding the male plant with what was then thought to be its closest living relative, the Hayani date of Egypt, to produce fruit by 2022. Subsequent germination efforts, however, produced two female Judean date palms. By 2015, Methuselah was producing viable pollen, which was successfully used to pollinate female date palms. In June 2021, one of the female plants, Hannah, produced the first dates grown from an ancient Judean date palm lineage in millennia. The revived plants are currently cultivated at a Kibbutz in Ketura, Israel. Despite this being one of the oldest examples of a successful de-extinction, the Judea date palm continues to be a model for how",
    "label": 0
  },
  {
    "text": "date palm lineage in millennia. The revived plants are currently cultivated at a Kibbutz in Ketura, Israel. Despite this being one of the oldest examples of a successful de-extinction, the Judea date palm continues to be a model for how methods such as hydration and germination can be used to achieve revival. Floreana giant tortoise The Floreana giant tortoise (Chelonoidis niger niger) is a subspecies of Galápagos tortoise once endemic to Floreana Island, Ecuador. It is believed to have gone extinct by around 1850 due to overexploitation and habitat degradation, along with predation by invasive species such as feral livestock, rodents, and stray dogs and cats. A deliberate wildfire set in 1820 by Thomas Chappel, a crewman of the Essex, is also cited as contributing to the subspecies’ early decline. In 2012, hybrids of Floreana tortoises and the Volcán Wolf lineage were identified on Isabela Island. These animals were likely transported to the island or left there in the early 19th century, where they interbred with native tortoises. A breeding programme launched in 2017 aims to restore the subspecies by selectively back-breeding these hybrids to recover Floreana genetic traits. As of 2025, more than 400 Floreana giant tortoises have been hatched on Santa Cruz Island, with plans to release them onto Floreana Island once invasive species are fully extirpated. However, the IUCN has not updated the subspecies’ conservation status, as no genetically pure individuals were known at the time of its 2017 assessment, and the re-established population has not yet reproduced naturally in the wild. This is an example of de-extinction by hybrid recovery rather than by gene-editing. Unknown Commiphora In 2010, Sarah Sallon of the Arava Institute for Environmental Studies germinated a seed recovered in 1986 from excavations in a cave in the northern Judean Desert. The resulting plant,",
    "label": 0
  },
  {
    "text": "de-extinction by hybrid recovery rather than by gene-editing. Unknown Commiphora In 2010, Sarah Sallon of the Arava Institute for Environmental Studies germinated a seed recovered in 1986 from excavations in a cave in the northern Judean Desert. The resulting plant, named Sheba, reached maturity in 2024 and is believed to represent an entirely new species of Commiphora. The research team suggests that Sheba may correspond to the ancient tsori or Judean balsam—a historically significant medicinal plant known from descriptions in the Bible and other ancient sources, which are among the few surviving records of flora used in the region. This case of de-extinction illustrates how germ material can resurrect botanical lineages that were once thought to have been lost. York groundsel The York groundsel is a species of Senecio first identified in York, England in 1979 and last observed in the wild in 1991. A survey by the UK government advisory body Natural England later confirmed the species had gone extinct by 2000, with weedkiller use identified as a contributing factor. Seeds preserved in the Millennium Seed Bank were successfully germinated and the plant was reintroduced to York in 2023, marking the first documented case of an extinct species being revived and returned to its native habitat. This is a landmark for de-extinction by seed banking and re-introduction. It proves that plant species can be brought back to its natural range. Montreal melon The Montreal melon—also called the Montreal market muskmelon, Montreal nutmeg melon, and in French melon de Montréal (\"Melon of/from Montreal\")—is a Canadian melon cultivar traditionally grown in the Montreal region. Once considered a delicacy along the east coast of North America, it vanished from farms and was believed extinct by the 1920s due to regional urbanisation and its incompatibility with large-scale agribusiness. In 1996, seeds were found",
    "label": 0
  },
  {
    "text": "the Montreal region. Once considered a delicacy along the east coast of North America, it vanished from farms and was believed extinct by the 1920s due to regional urbanisation and its incompatibility with large-scale agribusiness. In 1996, seeds were found in a seed bank in Iowa, leading to the melon's successful reintroduction by local gardeners within its historical growing area. See also References Further reading O'Connor, M.R. (2015). Resurrection Science: Conservation, De-Extinction and the Precarious Future of Wild Things. New York: St. Martin's Press. ISBN 978-1-137-27929-3. Archived from the original on 2016-07-04. Shapiro, Beth (2015). How to Clone a Mammoth: The Science of De-Extinction. Princeton, NJ: Princeton University Press. ISBN 978-0-691-15705-4. Pilcher, Helen (2016). Bring Back the King: The New Science of De-extinction Archived 2021-05-07 at the Wayback Machine. Bloomsbury Press ISBN 9781472912251 External links TEDx DeExtinction March 15, 2013 conference sponsored by Revive and Restore project of the Long Now Foundation, supported by TEDx and hosted by the National Geographic Society, that helped popularize the public understanding of the science of de-extinction. Video proceedings, meeting report, and links to press coverage freely available. De-Extinction: Bringing Extinct Species Back to Life April 2013 article by Carl Zimmer for National Geographic magazine reporting on 2013 conference.",
    "label": 0
  },
  {
    "text": "Flyover is a feature on Apple Maps that allows users to view select areas in a 3D setting. Flyover also formerly allowed users to take \"tours\" of these locations through the City Tours feature, showcasing various landmarks in the area, however this feature was removed in 2025. Imagery is provided through the use of planes, which collect fine data on buildings. Flyover is available on all Apple devices which also support Maps, including iOS and macOS. Initially founded in 2012, along with Apple Maps itself, Flyover has expanded from its initial 10 locations in the US and Europe to now include over 300 cities, landmarks, and parks across all six inhabited continents through a series of multiple expansions, notably in 2014, 2019 and 2024. Upon release, Flyover was criticized for various visual issues, as part of a larger criticism of the overall release of Apple Maps, however, Flyover has also been praised for its detail and uniqueness. Similarly to Google Earth, the implementation of Flyover has been prevented in certain areas primarily due to security concerns over the collection of data in sensitive locations, notably in Norway in 2012. History Pre-2012 In anticipation of the creation of a 3D mapping service that would eventually become Maps, Apple acquired Poly9, a Canadian 3D mapping company, in mid-2010, two years before launching Flyover. Poly9, like Apple, was an early competitor to Google Earth, containing similar features to the company, particularly a 3D globe dubbed the \"Poly Globe\". In August of the same year, Apple was also revealed to be the buyer of C3 Technologies, a Sweden-based company which also specialized in 3D imagery, which the company collected aerially. C3 was originally built off military technology developed by Swedish defense company Saab AB, which allowed the company to collect higher-quality imagery than traditional",
    "label": 0
  },
  {
    "text": "C3 Technologies, a Sweden-based company which also specialized in 3D imagery, which the company collected aerially. C3 was originally built off military technology developed by Swedish defense company Saab AB, which allowed the company to collect higher-quality imagery than traditional civilian companies. In addition to Flyover, C3 also assisted with other Maps features, including traffic data and Wikipedia entries for cities and places of interest. The acquisition of both companies led to speculation that Apple would create a mapping service separate from Google Maps, as well as a mapping service which could compete with Google Earth. 2012–2015 Flyover was announced along with Apple Maps at the 2012 World Wide Developers Conference, and both were released in September 2012. Upon release, Flyover was only available in the iPhone 4s, iPad 2 and the 3rd generation iPad. The 10 initial cities that first received Flyover were Chicago, Copenhagen, Las Vegas, Los Angeles, Miami, Montreal, Sacramento, San Francisco, Seattle and Sydney. More cities would receive Flyover in later years, with Apple often adding multiple cities to Flyover at a time. In addition to this initial release, more cities were added later in 2012, including Toronto in Canada. In 2013, Flyover was added to Paris, as well as to smaller cities such as Indianapolis and Baltimore, totaling 16 cities in the United States, Germany, Canada, France and the UK, alongside coverage being expanded in several existing areas. In addition, Flyover was also added to London, Barcelona, Cologne, Rome, Madrid and Vancouver. Flyover was added to several cities and national parks in New Zealand, France, the United States, South Africa, Sweden and Japan, including Cape Town and two Arizona National Parks in 2014, with Marseille, the second-largest city in France receiving Flyover coverage later in the year as well. The capital of Japan, Tokyo, also",
    "label": 0
  },
  {
    "text": "the United States, South Africa, Sweden and Japan, including Cape Town and two Arizona National Parks in 2014, with Marseille, the second-largest city in France receiving Flyover coverage later in the year as well. The capital of Japan, Tokyo, also received Flyover that year, along with Wellington, New Zealand and Yosemite National Park. In addition to adding new areas, the coverage of existing cities with the feature was expanded. In San Francisco, Flyover was added across the San Francisco Bay to Berkeley. In mid-2014, the City Tours feature, which allows users to tour locations supported by the Flyover feature, was publicly released. Flyover was added to over 20 cities and locations in 2015 in various cities mostly across the US and Western Europe, including Canberra and Venice. Flyover was also added to Budapest, in Hungary and Prague in the Czech Republic later in the year. In Italy, the cities of Florence and Genoa received Flyover. Guadalajara, the largest city in Mexico to have the feature, also received Flyover. Japan also saw more cities with Flyover, with Nagoya, Osaka and Hiroshima being added, and Mannheim, Stuttgart and Neuschwanstein Castle in Germany also received Flyover that year. An additional 7 cities received Flyover in June 2015, those being the cities of Karlsruhe and Kiel in Germany, Braga in Portugal, San Juan in Puerto Rico, Cádiz and Jerez de la Frontera in Spain, and Kingston upon Hull in the United Kingdom. 2016–present 2016 saw Apple add Flyover to 20 additional locations in Australia, France, Switzerland, Germany, the US, the Netherlands, Japan, South Africa, Italy and Austria, including Adelaide in Australia and the Virgin Islands. January 2019 saw one of Apple's largest Flyover expansions, with the feature being added to 51 cities in France, Japan, the United States, and others, including Albuquerque, Cincinnati and",
    "label": 0
  },
  {
    "text": "Italy and Austria, including Adelaide in Australia and the Virgin Islands. January 2019 saw one of Apple's largest Flyover expansions, with the feature being added to 51 cities in France, Japan, the United States, and others, including Albuquerque, Cincinnati and Key West as well as the entirety of the nation of Monaco. In September 2020, Flyover was added to the city of Amsterdam, in the Netherlands. In June 2022, Flyover was added to Vienna in Austria. Another major rollout of Flyover occurred in 2024, with dozens of cities across North America, Australia, and Europe receiving the feature, including Washington D.C., Kansas City, and Brisbane, as well as smaller cities, including Chattanooga, Birmingham, Alabama and St. George. These updates continued into 2025, with Lisbon, Frankfurt, Le Havre, Dunkirk and several other cities receiving Flyover. In July 2025, Flyover was added to the cities of Brasília and Ouro Preto in Brazil, marking the first Brazilian and the first South American cities to receive Flyover. In November, Flyover was added to the cities of Plzeň in the Czech Republic, Stavanger in Norway, Logroño and Vitoria-Gasteiz in Spain, Lucerne in Switzerland, and Newbury in the United Kingdom. Use Apple collects the 3D data used for Flyover through the use of small military-grade drone-like planes equipped with cameras, which fly up close to buildings to create a more detailed map. The cameras are positioned at designated angles to capture rooftops, edges and sides of buildings, trees, and other visible objects. Flyover imagery additionally collects topographical data which also appears on the software. Once collected, 3D imagery can be generated within a period of hours. As of 2013, imagery used for Flyover is collected by C3 Technologies. With Flyover, certain locations – primarily big cities, landmarks and some national parks – can be viewed from a",
    "label": 0
  },
  {
    "text": "imagery can be generated within a period of hours. As of 2013, imagery used for Flyover is collected by C3 Technologies. With Flyover, certain locations – primarily big cities, landmarks and some national parks – can be viewed from a birds-eye perspective, as opposed to street level imagery provided by features such as Look Around on Apple Maps and Street View on Google Maps. The images provided are three-dimensional, photo-realistic, and users can alter the vantage point on the map through panning or zooming in or out. Flyover imagery has been viewed as similar to that of virtual reality, and it has theorized by some to be a possible precursor to a VR feature on Maps. City Tours Flyover City Tours were first released in 2014, however the feature was initially not accessible to Maps users. Shortly afterward, City Tours were leaked by a developer, who created a public video showcasing the feature in Paris. City Tours were officially announced by Apple at the 2014 World Wide Developers Conference, and were released with iOS 8. City Tours was a feature that allowed users to view various landmarks in a given city via a \"flying\" animation, a feature only available to cities that already contain Flyover 3D maps. City Tours was added as a feature to Apple Maps on iOS 8 on September 17, 2014, and in OS X Yosemite on October 16, 2014. City Tours were removed from Maps as part of the release of iOS 26 in September 2025. City Tours were accessed via a button available in the search history of Maps. Flyover imagery could be viewed manually without starting the tour. Most, but not all cities with Flyover allow tours of a given city, where the above virtual tour of various landmarks in the area will be",
    "label": 0
  },
  {
    "text": "search history of Maps. Flyover imagery could be viewed manually without starting the tour. Most, but not all cities with Flyover allow tours of a given city, where the above virtual tour of various landmarks in the area will be shown. Coverage Flyovers are available in over 350 cities, metropolitan areas, national parks and landmarks across 31 countries and all six inhabited continents, as of 2025. Flyover is currently available in the countries of Australia, Austria, the Bahamas, Belgium, Brazil, Canada, the Czech Republic, Denmark, Finland, France, Germany, Hungary, Ireland, Italy, Japan, Luxembourg, Mexico, Monaco, the Netherlands, New Zealand, Norway, Portugal, San Marino, Slovenia, South Africa, Spain, Sweden, Switzerland, the United Kingdom, the United States, and Vatican City. Reception Upon initial release in 2012, Flyover received mixed reception among news outlets. Flyover was described positively by several news organizations, including NBC, Yahoo News and the New York Times. The BBC News in the United Kingdom described Flyover as giving Apple an \"edge\" over Google Maps, where no 3D imagery was directly available, while USA Today referred to Flyover as a \"gee-whiz\" feature. Visual errors Similar to Maps itself, Flyover saw many visual issues upon release, a notable one containing a massive glitch on the Brooklyn and Anzac bridges in New York and Sydney and the Hoover Dam in Nevada, showing \"plunges\" on all structures. Flyover was also criticized for its early imagery, as Apple was initially unable to properly capture smaller or lower objects or buildings, resulting in structures such as trees having a blocky or otherwise low-quality appearance. Flyover additionally had glitches whilst interacting with other Maps features, including road markings, which would tend to follow the tops of 3D buildings rather than remain focused on the designated road, which particularly became an issue for bridges. Google Earth, which",
    "label": 0
  },
  {
    "text": "additionally had glitches whilst interacting with other Maps features, including road markings, which would tend to follow the tops of 3D buildings rather than remain focused on the designated road, which particularly became an issue for bridges. Google Earth, which also hosted 3D imagery on their mapping site, was noted to be experiencing similar issues with their image rendering. As a result of these issues, Flyover was initially criticized for its low quality, as well as being parodied online. Flyover was described by The Atlantic, an American magazine, as \"painfully slow, sort of useless\", in a 2012 publication. This was part of a larger series of technical issues with Apple Maps, most of which have since been fixed. Shortly after its initial release, Apple began to improve upon their initial criticized imagery. Security concerns In 2013, the Norwegian government denied a request by Apple to add Flyover to Oslo, the Norwegian capital, citing security concerns arising from the amount of detail given to buildings, information which could potentially be used to facilitate and carry out terrorist attacks. Øyvind Mandt, who worked for the National Security Authority, stated that \"We do not want it [Flyover] to be shot with such a high degree of precision that it could be used to identify areas that require special shielding.\" Concern was specifically elicited regarding the capturing of government and military buildings, where outside photography is normally not permitted, alongside increased security inside and around government buildings, primarily in response to the 2011 Norway attacks. Another spokesperson for the National Security Authority claimed that the level of detail used by Apple to collect data exceeded the amount allowed in particular areas of Oslo. The decision was met with criticism from Fabian Stang, then mayor of Oslo, who called for the Norwegian government to allow",
    "label": 0
  },
  {
    "text": "the level of detail used by Apple to collect data exceeded the amount allowed in particular areas of Oslo. The decision was met with criticism from Fabian Stang, then mayor of Oslo, who called for the Norwegian government to allow Apple to add the feature, citing the possibility of increased tourism from the addition, also stating that Apple had already been permitted to conduct aerial surveys in \"all Western capitals.\" A spokesperson for the National Security Authority of Norway stated that obtaining the needed imagery from a \"Norwegian supplier\" or from the Norwegian Mapping Authority could potentially assist Apple in adding the feature, as said companies are required to produce their imagery in a lower resolution. The US embassy in Norway also stated its support for Apple in adding the feature. Outside of Apple, other companies including Nokia were permitted to survey Oslo for 3D imagery. Flyover was eventually added to Oslo in late 2023, 11 years after the initial incident. Similar security concerns may have also prevented Apple from adding Flyover to other potentially sensitive areas, including Washington D.C., among others. See also 3D modeling Detailed City Experience Pre-installed iOS apps Notes References External links Locations with Flyover Tours (Apple Inc.)",
    "label": 0
  },
  {
    "text": "A foldable smartphone (also called a foldable phone or simply foldable) is a smartphone with a folding form factor. While folding designs have been used previously in clamshell or \"flip phone\" models, the term \"foldable\" now generally refers to a newer style featuring flexible displays. Some variants of the concept instead use multiple touchscreen panels connected by a hinge. Concepts for such devices date back as early as Nokia's \"Morph\" concept in 2008 and a concept presented by Samsung Electronics in 2013, which was part of a broader set of ideas using flexible OLED displays. The first commercially available folding smartphones with OLED displays began to appear in 2018. Some devices fold on a vertical axis into a wider, tablet-like form while remaining usable in a smaller, folded state. The display may either wrap around to the back of the device when folded, as seen with the Royole FlexPai and Huawei Mate X, or use a booklet-style design, where the larger, folded screen is located inside and a smaller screen on the cover allows interaction without opening the device, as with the Samsung Galaxy Fold series. Horizontally folding smartphones have also been produced, typically using a clamshell form factor. The first generation of commercially released foldable smartphones faced concerns over durability and their high prices, and they are largely regarded as a gimmick. In 2023, only around 1% of worldwide smartphone ownership were foldable smartphones. History In 2006, Polymer Vision showed a roll-able concept and a foldable smartphone, the Readius (zh), at the Mobile World Congress (MWC) which also serves as a reader. In 2008, Nokia presented animated concepts of a flexible device it dubbed \"Morph\", which had a tri-fold design that could be bended into various forms, such as a large unfolded device, a feature phone-sized unit, and a",
    "label": 0
  },
  {
    "text": "as a reader. In 2008, Nokia presented animated concepts of a flexible device it dubbed \"Morph\", which had a tri-fold design that could be bended into various forms, such as a large unfolded device, a feature phone-sized unit, and a smart wristband. In a 2019 retrospective on the concept, CNET noted that Morph could be considered a forerunner to the first wave of commercially produced folding phones, as well as a showcase of future possibilities. In 2011, Kyocera released a dual-touchscreen Android smartphone known as the Echo, which featured a pair of 3.5-inch touchscreens. When folded, the top screen continued to face the user while covering the secondary screen. Two individual apps could be shown on the displays, a single app could span across them, while specific apps also featured \"optimized\" two-pane layouts. Two years later, NEC released the Medias W in Japan. Unlike the Echo, the secondary screen could be folded behind the phone. The camera rotated with the screen so that the same sensor could face both forward and rear In 2017, ZTE released the Axon M with a similar hinge to the Medias W. ZTE stated that the more powerful hardware of modern smartphones, and improvements to multitasking and tablet support on Android, helped to improve this experience. The development of thin, flexible OLED displays enabled the possibility for new designs and form factors. During its Consumer Electronics Show keynote in 2013, Samsung presented several concepts—codenamed Youm - for smartphones incorporating flexible displays. One such concept was a smartphone that could fold outward into a single, uninterrupted tablet-sized display. The first Youm concept to make it to a production model was the Galaxy Note Edge—a phablet with a portion of the screen that sloped over the right-hand bezel. Speculation surrounding the development of folding phones using OLED",
    "label": 0
  },
  {
    "text": "tablet-sized display. The first Youm concept to make it to a production model was the Galaxy Note Edge—a phablet with a portion of the screen that sloped over the right-hand bezel. Speculation surrounding the development of folding phones using OLED displays began to emerge more rapidly in 2018. In January 2018, it was reported that LG Electronics had obtained a design patent for a folding smartphone. Later in June, it was reported that Microsoft had been developing a similar device as part of its Surface line, codenamed \"Andromeda\" (itself a spiritual successor to a dual-screen booklet tablet prototype Microsoft had been exploring in the late-2000s known as Courier), while Samsung was also said to be developing such a device. In November 2018, the Chinese startup Royole released the first commercially available foldable smartphone with an OLED display, the Royole Flexpai. It featured a single 7.8-inch display that folds outwards, leaving the display exposed when folded. Later that month at its developers' conference, Samsung officially teased a prototype of its folding smartphone, which would be produced \"in the coming months\". The prototype used a booklet-style layout, with an \"InfinityFlex\" display located on the inside of the device, and a smaller \"cover\" screen on the front of the device to allow access when the screen is closed. At a concurrent developers' summit, Android VP of engineering Dave Burke stated that the next version of the platform would provide enhancements and guidance relevant to folding devices, leveraging existing features. In January 2019, Xiaomi CEO Lin Bin published a video on Sina Weibo, featuring him demonstrating a prototype smartphone with two flaps capable of being folded inward. Samsung officially unveiled the Galaxy Fold during its media event at Mobile World Congress in February 2019. Alongside the Galaxy Fold, the convention also saw other foldable",
    "label": 0
  },
  {
    "text": "him demonstrating a prototype smartphone with two flaps capable of being folded inward. Samsung officially unveiled the Galaxy Fold during its media event at Mobile World Congress in February 2019. Alongside the Galaxy Fold, the convention also saw other foldable phones being unveiled or teased, such as the Huawei Mate X, and TCL presenting various prototype concepts featuring its \"DragonHinge\" technology (including a bracelet-styled device). LG did not unveil a folding device, citing a desire to focus more on re-gaining market share in the smartphone market. It did, however, unveil a \"Dual Screen\" case accessory for its LG V50 smartphone—a folio-styled case containing a secondary display panel inside. Other companies expressed interest in the concept, or have received patents on designs (such as hinge implementations and overall designs) relating to foldable phones. Motorola Mobility had received patents for a horizontal folding smartphone reminiscent of clamshell feature phones. In April 2019, the impending launch of the Galaxy Fold was met with quality concerns from critics, after widespread reports of review units experiencing varying forms of display failure (in some instances caused by accidental removal of a plastic layer meant to protect the screen in lieu of glass, along with other failures). Samsung indefinitely postponed the device's release, stating that it needed time to investigate the failures and improve the device's durability. Huawei also delayed its Huawei Mate X, with the company citing its desire to take a \"cautious\" approach due to the Samsung Galaxy Fold. In November 2019, Motorola unveiled its horizontal-folding Razr—inspired by its former Razr feature phone line released on 6 February 2020. Samsung also announced a similar device known as the Galaxy Z Flip. Huawei announced the Mate Xs on 24 February 2020 as a hardware revision of the original Mate X; it was released in \"global markets\"",
    "label": 0
  },
  {
    "text": "on 6 February 2020. Samsung also announced a similar device known as the Galaxy Z Flip. Huawei announced the Mate Xs on 24 February 2020 as a hardware revision of the original Mate X; it was released in \"global markets\" outside China in March 2020. The device features a more durable display, improved hinge function and a redesigned cooling system, as well as the newer Kirin 990 5G SoC and Android 10 with EMUI 10. Samsung later revealed the Samsung Galaxy Z Fold 2 in September 2020. On 25 February 2021, Huawei released the Huawei Mate X2. In March 2021, Xiaomi Technology announced the Xiaomi Mi MIX Fold. In August 2021, the Samsung Galaxy Z Fold 3 and Samsung Galaxy Z Flip 3 were released. On 15 December 2021, OPPO announced the OPPO Find N. On 11 April 2022, Vivo introduced the Vivo X Fold. On 11 August 2022, Xiaomi released the Xiaomi MIX Fold 2. The Samsung Galaxy Z Fold 4 and Samsung Galaxy Z Flip 4 were announced at the August 2022 edition of Galaxy Unpacked. The Galaxy Z Fold 4 was released on 25 August 2022, and the Galaxy Z Flip 4 was released on 26 August 2022. Motorola Mobility launched the Moto Razr 2022 on 11 August 2022. It was only available to the Chinese market but there were speculations that it could become available to other world markets at a later date. In June 2023, Motorola Mobility announced the new Razr (2023) and Razr+ (2023) to the U.S. market. The Huawei Mate XT is the world's first double-folding, or tri-fold foldable smartphone, released in September 2024. The device can be used with a case that has a kickstand, and a foldable keyboard with a built in trackpad to provide a desktop PC-like experience. On 1",
    "label": 0
  },
  {
    "text": "world's first double-folding, or tri-fold foldable smartphone, released in September 2024. The device can be used with a case that has a kickstand, and a foldable keyboard with a built in trackpad to provide a desktop PC-like experience. On 1 December 2025, Samsung announced their Samsung Galaxy Z TriFold, a tri-folding phone that was released on 12 December 2025, and will release to the US market on Q1 2026. Components Display materials Foldable smartphones typically use flexible, plastic OLED displays rather than glass (such as Corning's Gorilla Glass product, which is used in the majority of mid and high-end smartphones). Plastic displays are naturally capable of sustaining the required bend radius for a foldable smartphone, but they are more susceptible to blemishes and scratches than traditional glass smartphone displays. Although Corning does produce a flexible glass product known as Willow Glass, the company states that its manufacturing process requires use of a salt solution—thus making it unsuitable for electronic displays because the salt can damage the transistors used in OLED panels (which are built directly on the panel). Nonetheless, the company stated in March 2019 that it was in the process of developing a flexible glass suitable for smartphones, which would be 1 mm (0.039 in) thick and have a 5 mm (0.20 in) bend radius. Samsung marketed its Galaxy Z Flip as featuring 30 μm (0.0012 in)-thick \"ultra-thin glass\" with a plastic layer similar to the Galaxy Fold, manufactured by Samsung with materials from Schott AG, which is \"produced using an intensifying process to enhance its flexibility and durability\", and injected with a \"special material up to an undisclosed depth to achieve a consistent hardness\". A stress test by YouTube channel JerryRigEverything showed the screen was scratched when rubbed with a pick with a Mohs rating of 2 (in",
    "label": 0
  },
  {
    "text": "Gerontechnology, also called gerotechnology is an inter- and multidisciplinary academic as well as a professional field that combines various disciplines of gerontology and technology. Sustainability of an aging society depends on our effectiveness in creating technological environments, including assistive technology and inclusive design, for innovative and independent living and social participation of older adults in any state of health, comfort as well as safety. In short, gerontechnology concerns matching technological environments to health, housing, mobility, communication, leisure, work and also the personality/individual dispositions of older people. Gerontechnology is most frequently identified as a subset of HealthTech and is—since the 2010s—more commonly referred to as AgeTech or Agetech in Europe and the United States. Research outcomes form the basis for designers, builders, engineers, manufacturers, and those in the health professions (nursing, medicine, gerontology, geriatrics, environmental psychology, developmental psychology, etc.), to provide an optimum living environment for the widest range of ages. Description Gerontechnology is considered an adjunct to the promotion of human health and physical as well as emotional well-being. It pertains to both human development and aging with the aim to compress morbidity and to increase vitality and quality of life throughout the lifespan. It creates solutions to extend the working phase in society by maximizing the vital and productive years in the later years of life, which consequently reduces the cost of care. The overall framework of gerontechnology may be seen as a matrix of domains of human activity: (1) health & self-esteem, housing & activities of daily living, communication & governance, mobility & transport, work & leisure, as well as (2) technology interventions or impact levels (enhancement & satisfaction, prevention & engagement, compensation & assistance, care and care organisation). Underpinning all these elements are generic and applied evidence-based research findings that aid in the development of products and",
    "label": 0
  },
  {
    "text": "well as (2) technology interventions or impact levels (enhancement & satisfaction, prevention & engagement, compensation & assistance, care and care organisation). Underpinning all these elements are generic and applied evidence-based research findings that aid in the development of products and services. Gerontechnology has much in common with other interdisciplinary domains, such as Assistive Technology (for the compensation & assistance and the care support & care organisation rows of the matrix), Everybodytech, Technology for All (for example Technology 4 All.org) and Universal Design for the development of all products and services pertaining to gerontechnology. Gerontological design Gerontological design focuses on providing effective solutions to improve the way of life for aging people, through gerontological knowledge and design research methods to obtain a better understanding of individuals' preferences and requirements. Gerontological design also refers specifically to the study and practice of building design methods that support older users in the built environment. Some universities host professors, commonly in architecture or interior design departments, that specialize in the study and teaching of this design specialization. Not only does this include the examination of building design characteristics that impact older adults' physiological well-being, but it can also include the investigation of building design characteristics that impact informational needs (i.e. finding one's way around in a space) or social interaction needs (Campbell, 2012). Between 2008 and 2030, Singapore will witness an age profile shift in its population's history. In 2005, one in 12 residents was 65 years or older. By 2030, one in five residents will be 65 years or older. Studies show that in 2002, 7% of the world's population is aged 65 and above. By 2050, it is envisaged that the percentage could rise to nearly 17%. The ageing population and its impact on economics, politics, education and lifestyle is no longer an",
    "label": 0
  },
  {
    "text": "2002, 7% of the world's population is aged 65 and above. By 2050, it is envisaged that the percentage could rise to nearly 17%. The ageing population and its impact on economics, politics, education and lifestyle is no longer an isolated issue but a global concern. Products and services relevant to the \"silver industry\" or the \"mature market\" increasingly abound in the marketplace. The demand for designers with a keen sense for the aging population's needs who employ gerontological design process knowledge concomitantly rises. Creativity in Gerontechnology Creativity in gerontechnology focuses on integrating arts, design, and other artistic practices into technological interventions, with the goal of enhancing the quality of life for older adults. Researchers in this field aim to support cognitive health, emotional health, and social engagement within these aging populations. A 2023 study reviewed digital creative art inventions (DCAIs), which include online music, dance, and visual arts programs given through apps or video calls.The review identified multiple health benefits for older adults. Physiologically, DCAIs supported fine motor skills and overall physical engagement. Psychologically, they helped reduce loneliness, anxiety, and depression by acting as a form of mental health support. Socially, DCAIs promoted connections with peers and communities through digital platforms. Cognitively they stimulated memory, creativity and problem solving abilities. Older adults generally found DCAIs feasible and enjoyable. However, there were also barriers found in these technologies, specifically around the complex interfaces, and age related issues like vision, hearing, and impairments to mobility. The study overall found that DCAIs were not just good for recreational use, but they also served as an important tool within gerontechnology, supporting successful aging by enhancing physical, cognitive, psychological, and social well being. Another review conducted reviewed technology assisted creative arts activities for older adults living with mild cognitive impairment or dementia. The review",
    "label": 0
  },
  {
    "text": "served as an important tool within gerontechnology, supporting successful aging by enhancing physical, cognitive, psychological, and social well being. Another review conducted reviewed technology assisted creative arts activities for older adults living with mild cognitive impairment or dementia. The review saw various different health benefits. They engaged participants in music making, storytelling, and visual arts activities, again supporting fine motor skills and overall physical engagement. Participants in this showed improvements in memory call, attention and engagement during and after the activities. While these werenot large or permanent gains, the interventions appeared to help maintain cognitive functions while also stimulating mental activity. They also provided the older adults with enjoyment and emotional stimulation, reducing anxiety. Socially speaking, the activities facilitated engagement with peers and caregivers. The technology in this study was custom made and in the prototyping stage, but it highlighted the importance of designing technologies for older adults to ensure usability for creative works. It concluded that technology assisted creative arts are not just recreational but also serve as valuable components of gerontechnology, promoting successful aging. The future of designing gerontechnology for creative pursuits is centered on user centered design, particularly through design approaches that involve older adults in the development process of these technologies. A study by Zhao et al. showed that older adults value simplicity, positivity, and pro activity in these technologies. These adults wanted technologies that would allow them to do creative pursuits within the comfort and ease of their own homes. This shows the shift towards home based, personalized creative tools. Along with this, new technology that is being studied like virtual reality(VR), augmented reality(AR) and artificial intelligence(AI) could also help with creative engagement for older generations. All of these new technologies could come together to give the older generations immersive experiences and adaptive interfaces that",
    "label": 0
  },
  {
    "text": "that is being studied like virtual reality(VR), augmented reality(AR) and artificial intelligence(AI) could also help with creative engagement for older generations. All of these new technologies could come together to give the older generations immersive experiences and adaptive interfaces that can cater to more specific needs and preferences. For example, VR would allow for virtual art studios for hobbies like sculpture and painting, while AI can help with music composition and even storytelling. This would make creative activities both more accessible and engaging. Furthermore, this study spoke on the integration of social elements into creative gerontechnologies. Adults have spoken on their desires for platforms that would allow for social interaction as well as community building within creative pursuits. Designing these type of technologies would not just benefit individual creativity, but it could also create connections between adults. This would benefit the well being of older generations, as well as combating isolation to create a sense of belonging. The future of gerontechnology in creative pursuits is centered around developing platforms that are both inclusive and accessible. These platforms would empower older adults with the ability to continue creative ventures. With user centered design, specifically feedback from older generations, designers can create solutions for older generations with VR, AR, and AI. Doing so would result in improved cognitive, social, and emotional well being. Publications An international academic journal with delayed open-access, Gerontechnology , is published by the International Society for Gerontechnology (ISG) . A comprehensive volume titled Gerontechnology edited by Sunkyo Kwon has been published in 2016/2017. Applications Age technology (AgeTech/Agetech) has been used to enhance aspects of insurance, domiciliary care, residential and nursing homes, health care, and risk management. The services may originate from various independent service providers or the interconnection of devices and services enabled through open APIs. Commercial businesses",
    "label": 0
  },
  {
    "text": "been used to enhance aspects of insurance, domiciliary care, residential and nursing homes, health care, and risk management. The services may originate from various independent service providers or the interconnection of devices and services enabled through open APIs. Commercial businesses with an ageing component including the opportunities around the “Silver Economy” – providing services for the ‘wants’ of the older demographic, supporting independent living – addressing the ‘needs’ of the older demographic, longevity – extending healthy lifespan and geroscience. In the US, startup Aging2.0 launched in 2015 and has since organized 170 meet-up events, opened volunteer chapters in 30 countries and signed up 30 companies for its own accelerator program. Amongst these, SingFit “makes it easy for everyone to become a music therapist”, WalkJoy is a wearable sensor that measures a person’s gait and alerts caregivers when someone could be about to fall. Active Protective is a personal airbag that inflates to stop someone breaking their hip. And Vynca records a person’s dying wishes, so families aren’t unsure when the time comes. The company Honor, which connects seniors, caregivers and their families, recently raised $20 million, the biggest funding in the emerging category so far. In Europe, London-based AgeTech startup Birdie secured a €7 million Series A to help elderly adults live independently while independent living system Kraydel has raised over £1m in innovation and public sector grants to develop its smart device which sits on top of the TV, linking elderly people to their carers or family members, through their TV screens. Education The first ISG Masterclass for PhD students in 2006 produced a scheme to support gerontechnological research. See also Gero-Informatics Gerontology Biomechatronics References External links The International Society for Gerontechnology (ISG) Archived 2020-08-03 at the Wayback Machine The official LinkedIn discussion group of ISG Gero-tech Aging and",
    "label": 0
  },
  {
    "text": "The history of science and technology (HST) is a field of history that examines the development of the understanding of the natural world (science) and humans' ability to manipulate it (technology) at different points in time. This academic discipline also examines the cultural, economic, and political context and impacts of scientific practices; it likewise may study the consequences of new technologies on existing scientific fields. Academic study of history of science History of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science. Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends. History of the academic study of history of science Histories of science were originally written by practicing and retired scientists, starting primarily with William Whewell's History of the Inductive Sciences (1837), as a way to communicate the virtues of science to the public. Auguste Comte proposed that there should be a specific discipline to deal with the history of science. The development of the distinct academic discipline of the history of science and technology did not occur until the early 20th century. Historians have suggested that this was bound to the changing role of science during the same time period. After World War I, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world. In the decades since the end of World War II, history of science became an academic",
    "label": 0
  },
  {
    "text": "that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world. In the decades since the end of World War II, history of science became an academic discipline, with graduate schools, research institutes, public and private patronage, peer-reviewed journals, and professional societies. Formation of academic departments In the United States, a more formal study of the history of science as an independent discipline was initiated by George Sarton's publications, Introduction to the History of Science (1927) and the journal Isis (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress. The study of the history of science continued to be a small effort until the rise of Big Science after World War II. With the work of I. Bernard Cohen at Harvard University, the history of science began to become an established subdiscipline of history in the United States. In the United States, the influential bureaucrat Vannevar Bush, and the president of Harvard, James Conant, both encouraged the study of the history of science as a way of improving general knowledge about how science worked, and why it was essential to maintain a large scientific workforce. Universities with history of science and technology programs Argentina Buenos Aires Institute of Technology, Argentina, has been offering courses on History of the Technology and the Science. National Technological University, Argentina, has a complete history program on its offered careers. Australia The University of Sydney offers both undergraduate and postgraduate programmes in the History and Philosophy of Science, run by the Unit for",
    "label": 0
  },
  {
    "text": "Technology and the Science. National Technological University, Argentina, has a complete history program on its offered careers. Australia The University of Sydney offers both undergraduate and postgraduate programmes in the History and Philosophy of Science, run by the Unit for the History and Philosophy of Science, within the Science Faculty. Undergraduate coursework can be completed as part of either a Bachelor of Science or a Bachelor of Arts Degree. Undergraduate study can be furthered by completing an additional Honours year. For postgraduate study, the Unit offers both coursework and research-based degrees. The two course-work based postgraduate degrees are the Graduate Certificate in Science (HPS) and the Graduate Diploma in Science (HPS). The two research based postgraduate degrees are a Master of Science (MSc) and Doctor of Philosophy (PhD). Belgium University of Liège, has a Department called Centre d'histoire des Sciences et Techniques. Canada Carleton University Ottawa offer courses in Ancient Science and Technology in its Technology, Society and Environment program. University of Toronto has a program in History and Philosophy of Science and Technology. Huron University College offers a course in the History of Science which follows the development and philosophy of science from 10,000 BCE to the modern day. University of King's College in Halifax, Nova Scotia has a History of Science and Technology Program. France Nantes University has a dedicated Department called Centre François Viète. Paris Diderot University (Paris 7) has a Department of History and Philosophy of Science. A CNRS research center in History and Philosophy of Science SPHERE, affiliated with Paris Diderot University, has a dedicated history of technology section. Pantheon-Sorbonne University (Paris 1) has a dedicated Institute of History and Philosophy of Science and Technics. The École Normale Supérieure de Paris has a history of science department. Germany Technische Universität Berlin, has a program in",
    "label": 0
  },
  {
    "text": "history of technology section. Pantheon-Sorbonne University (Paris 1) has a dedicated Institute of History and Philosophy of Science and Technics. The École Normale Supérieure de Paris has a history of science department. Germany Technische Universität Berlin, has a program in the History of Science and Technology. The Deutsches Museum, 'German Museum' of Masterpieces of Science and Technology in Munich is one of the largest science and technology museums in the world in terms of exhibition space, with about 28,000 exhibited objects from 50 fields of science and technology. Greece The University of Athens has a Department of Philosophy and History of Science India History of science and technology is a well-developed field in India. At least three generations of scholars can be identified. The first generation includes D.D.Kosambi, Dharmpal, Debiprasad Chattopadhyay and Rahman. The second generation mainly consists of Ashis Nandy, Deepak Kumar, Dhruv Raina, S. Irfan Habib, Shiv Visvanathan, Gyan Prakash, Stan Lourdswamy, V.V. Krishna, Itty Abraham, Richard Grove, Kavita Philip, Mira Nanda and Rob Anderson. There is an emergent third generation that includes scholars like Abha Sur and Jahnavi Phalkey. Departments and Programmes The National Institute of Science, Technology and Development Studies had a research group active in the 1990s which consolidated social history of science as a field of research in India. Currently there are several institutes and university departments offering HST programmes. Jawaharlal Nehru University has an Mphil-PhD program that offers specialisation in Social History of Science. It is at the History of Science and Education group of the Zakir Husain Centre for Educational Studies (ZHCES) in the School of Social Sciences. Renowned Indian science historians Deepak Kumar and Dhruv Raina teach here. Also, *Centre for Studies in Science Policy has an Mphil-PhD program that offers specialization in Science, Technology, and Society along with various allied",
    "label": 0
  },
  {
    "text": "in the School of Social Sciences. Renowned Indian science historians Deepak Kumar and Dhruv Raina teach here. Also, *Centre for Studies in Science Policy has an Mphil-PhD program that offers specialization in Science, Technology, and Society along with various allied subdisciplines. Central University of Gujarat has an MPhil-PhD programme in Studies in Science, Technology & Innovation Policy at the Centre for Studies in Science, Technology & Innovation Policy (CSSTIP), where Social History of Science and Technology in India is a major emphasis for research and teaching. Banaras Hindu University has programs: one in History of Science and Technology at the Faculty of Science and one in Historical and Comparative Studies of the Sciences and the Humanities at the Faculty of Humanities. Andhra University has now set History of Science and Technology as a compulsory subject for all the First year B-Tech students. Israel Tel Aviv University. The Cohn Institute for the History and Philosophy of Science and Ideas is a research and graduate teaching institute within the framework of the School of History of Tel Aviv University. Bar-Ilan University has a graduate program in Science, Technology, and Society. Japan Kyoto University has a program in the Philosophy and History of Science. Tokyo Institute of Technology has a program in the History, Philosophy, and Social Studies of Science and Technology. The University of Tokyo has a program in the History and Philosophy of Science. Netherlands Utrecht University, has two co-operating programs: one in History and Philosophy of Science at the Faculty of Natural Sciences and one in Historical and Comparative Studies of the Sciences and the Humanities at the Faculty of Humanities. Poland Institute for the History of Science of the Polish Academy of Sciences offers PhD programmes and habilitation degrees in the fields of History of Science, Technology and Ideas.",
    "label": 0
  },
  {
    "text": "of the Sciences and the Humanities at the Faculty of Humanities. Poland Institute for the History of Science of the Polish Academy of Sciences offers PhD programmes and habilitation degrees in the fields of History of Science, Technology and Ideas. Russia Spain University of the Basque Country, offers a master's degree and PhD programme in History and Philosophy of Science and runs since 1952 THEORIA. International Journal for Theory, History and Foundations of Science. The university also sponsors the Basque Museum of the History of Medicine and Science, the only open museum of History of Science of Spain, that in the past offered also PhD courses. Universitat Autònoma de Barcelona, offers a master's degree and PhD programme in HST together with the Universitat de Barcelona. Universitat de València, offers a master's degree and PhD programme in HST together with the Consejo Superior de Investigaciones Científicas. Sweden Linköpings universitet, has a Science, Technology, and Society program which includes HST. Switzerland University of Bern, has an undergraduate and a graduate program in the History and Philosophy of Science. Ukraine State University of Infrastructure and Technologies, has a Department of Philosophy and History of Science and technology. United Kingdom University of Bristol has a masters and PhD program in the Philosophy and History of Science. University of Cambridge has an undergraduate course and a large masters and PhD program in the History and Philosophy of Science (including the History of Medicine). University of Durham has several undergraduate History of Science modules in the Philosophy department, as well as Masters and PhD programs in the discipline. University of Kent has a Centre for the History of the Sciences, which offers Masters programmes and undergraduate modules. University College London's Department of Science and Technology Studies offers undergraduate programme in History and Philosophy of Science, including",
    "label": 0
  },
  {
    "text": "the discipline. University of Kent has a Centre for the History of the Sciences, which offers Masters programmes and undergraduate modules. University College London's Department of Science and Technology Studies offers undergraduate programme in History and Philosophy of Science, including two BSc single honour degrees (UCAS V550 and UCAS L391), plus both major and minor streams in history, philosophy and social studies of science in UCL's Natural Sciences programme. The department also offers MSc degrees in History and Philosophy of Science and in the study of contemporary Science, Technology, and Society. An MPhil/PhD research degree is offered, too. UCL also contains a Centre for the History of Medicine. This operates a small teaching programme in History of Medicine. University of Leeds has both undergraduate and graduate programmes in History and Philosophy of Science in the Department of Philosophy. University of Manchester offers undergraduate modules and postgraduate study in History of Science, Technology and Medicine and is sponsored by the Wellcome Trust. University of Oxford has a one-year graduate course in 'History of Science: Instruments, Museums, Science, Technology' associated with the Museum of the History of Science. The London Centre for the History of Science, Medicine, and Technology – this Centre closed in 2013. It was formed in 1987 and ran a taught MSc programme, jointly taught by University College London's Department of Science and Technology Studies and Imperial College London. The Masters programme transferred to UCL. United States Academic study of the history of science as an independent discipline was launched by George Sarton at Harvard with his book Introduction to the History of Science (1927) and the Isis journal (founded in 1912). Sarton exemplified the early 20th century view of the history of science as the history of great men and great ideas. He shared with many of his",
    "label": 0
  },
  {
    "text": "to the History of Science (1927) and the Isis journal (founded in 1912). Sarton exemplified the early 20th century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress. The History of Science was not a recognized subfield of American history in this period, and most of the work was carried out by interested Scientists and Physicians rather than professional Historians. With the work of I. Bernard Cohen at Harvard, the history of Science became an established subdiscipline of history after 1945. Arizona State University's Center for Biology and Society offers several paths for MS or PhD students who are interested in issues surrounding the history and philosophy of the science. California Institute of Technology offers courses in the History and Philosophy of Science to fulfill its core humanities requirements. Case Western Reserve University has an undergraduate interdisciplinary program in the History and Philosophy of Science and a graduate program in the History of Science, Technology, Environment, and Medicine (STEM). Cornell University offers a variety of courses within the Science and Technology course. Georgia Institute of Technology has an undergraduate and graduate program in the History of Technology and Society. Harvard University has an undergraduate and graduate program in History of Science Indiana University offers undergraduate courses and a masters and PhD program in the History and Philosophy of Science. Johns Hopkins University has an undergraduate and graduate program in the History of Science, Medicine, and Technology. Lehigh University offers an undergraduate level STS concentration (founded in 1972) and a graduate program with emphasis on the History of Industrial America. Massachusetts Institute of Technology has a Science, Technology, and Society",
    "label": 0
  },
  {
    "text": "the History of Science, Medicine, and Technology. Lehigh University offers an undergraduate level STS concentration (founded in 1972) and a graduate program with emphasis on the History of Industrial America. Massachusetts Institute of Technology has a Science, Technology, and Society program which includes HST. Michigan State University offers an undergraduate major and minor in History, Philosophy, and Sociology of Science through its Lyman Briggs College. New Jersey Institute of Technology has a Science, Technology, and Society program which includes the History of Science and Technology Oregon State University offers a Masters and Ph.D. in History of Science through its Department of History. Princeton University has a program in the History of Science. Rensselaer Polytechnic Institute has a Science and Technology Studies department Rutgers has a graduate Program in History of Science, Technology, Environment, and Health. Stanford has a History and Philosophy of Science and Technology program. Stevens Institute of Technology has an undergraduate and graduate program in the History of Science. University of California, Berkeley offers a graduate degree in HST through its History program, and maintains a separate sub-department for the field. University of California, Los Angeles has a relatively large group History of Science and Medicine faculty and graduate students within its History department, and also offers an undergraduate minor in the History of Science. University of California, Santa Barbara has an interdisciplinary graduate program emphasis in Technology & Society through the Center for Information Technology & Society. University of Chicago offers a B.A. program in the History, Philosophy, and Social Studies of Science and Medicine as well as M.A. and Ph.D. degrees through its Committee on the Conceptual and Historical Studies of Science. University of Florida has a Graduate Program in 'History of Science, Technology, and Medicine' at the University of Florida provides undergraduate and graduate degrees.",
    "label": 0
  },
  {
    "text": "as M.A. and Ph.D. degrees through its Committee on the Conceptual and Historical Studies of Science. University of Florida has a Graduate Program in 'History of Science, Technology, and Medicine' at the University of Florida provides undergraduate and graduate degrees. University of Minnesota has a Ph.D. program in History of Science, Technology, and Medicine as well as undergraduate courses in these fields. University of Oklahoma has an undergraduate minor and a graduate degree program in History of Science. University of Pennsylvania has a program in History and Sociology of Science. University of Pittsburgh's Department of History and Philosophy of Science offers graduate and undergraduate courses. University of Puget Sound has a Science, Technology, and Society program, which includes the history of Science and Technology. University of Wisconsin–Madison has a program in History of Science, Medicine and Technology. It offers M.A. and Ph.D. degrees as well as an undergraduate major. Wesleyan University has a Science in Society program. Yale University has a program in the History of Science and Medicine. Prominent historians of the field See also the list of George Sarton medalists. Journals and periodicals Annals of Science The British Journal for the History of Science Centaurus Dynamis History and Technology (magazine) History of Science and Technology (journal) History of Technology (book series) Historical Studies in the Physical and Biological Sciences (HSPS) Historical Studies in the Natural Sciences (HSNS) HoST - Journal of History of Science and Technology ICON IEEE Annals of the History of Computing Isis Journal of the History of Biology Journal of the History of Medicine and Allied Sciences Notes and Records of the Royal Society Osiris Science & Technology Studies Science in Context Science, Technology, & Human Values Social History of Medicine Social Studies of Science Technology and Culture Transactions of the Newcomen Society Historia Mathematica",
    "label": 0
  },
  {
    "text": "Allied Sciences Notes and Records of the Royal Society Osiris Science & Technology Studies Science in Context Science, Technology, & Human Values Social History of Medicine Social Studies of Science Technology and Culture Transactions of the Newcomen Society Historia Mathematica Bulletin of the Scientific Instrument Society See also History of science History of technology Ancient Egyptian technology History of science and technology in China History of science and technology in Japan History of science and technology in France History of science and technology in the Indian subcontinent Mesopotamian science Productivity improving technologies (historical) Science and technology in Argentina Science and technology in Canada Science and technology in Iran Science and technology in the United States Science in the medieval Islamic world Science tourism Technological and industrial history of the United States Timeline of science and engineering in the Islamic world Professional societies The British Society for the History of Science (BSHS) History of Science Society (HSS) Newcomen Society Society for the History of Technology (SHOT) Society for the Social Studies of Science (4S) Scientific Instrument Society References Bibliography Historiography of science H. Floris Cohen, The Scientific Revolution: A Historiographical Inquiry, University of Chicago Press 1994 – Discussion on the origins of modern science has been going on for more than two hundred years. Cohen provides an excellent overview. Ernst Mayr, The Growth of Biological Thought, Belknap Press 1985 Michel Serres,(ed.), A History of Scientific Thought, Blackwell Publishers 1995 Companion to Science in the Twentieth Century, John Krige (Editor), Dominique Pestre (Editor), Taylor & Francis 2003, 941pp The Cambridge History of Science, Cambridge University Press Volume 4, Eighteenth-Century Science, 2003 Volume 5, The Modern Physical and Mathematical Sciences, 2002 History of science as a discipline J. A. Bennett, 'Museums and the Establishment of the History of Science at Oxford and Cambridge',",
    "label": 0
  },
  {
    "text": "Impairment detection technology (IDT) refers to tools and systems designed to assess whether an individual is functionally impaired at a given moment, regardless of the cause. Unlike drug and alcohol tests that detect substances or their metabolites in the body, IDTs evaluate real-time cognitive or physical performance to identify active impairment. These systems are relevant where safety is critical, such as workplaces and law enforcement. IDTs do not identify the specific cause of impairment, such as drug use, fatigue, or illness, but instead detect behavioral or physiological markers like slowed reaction time, poor coordination, or eye movement abnormalities. Interest in IDT has increased as workplaces and law enforcement agencies address the limitations of traditional drug testing methods. The legalization of cannabis has underscored challenges associated with metabolite-based tests, which can yield positive results days after use, even when the individual is no longer impaired. Studies indicate that tetrahydrocannabinol (THC) levels in blood or saliva do not reliably correlate with functional impairment, as frequent users may retain high THC concentrations without experiencing intoxication, and metabolites may remain detectable long after psychoactive effects have ended. Types IDTs assess an individual’s functional state in real time by monitoring cognitive performance or physiological responses. These technologies fall into several categories: Oculomotor These systems analyze involuntary eye movements and pupil responses, such as nystagmus or delayed reactions to detect impairment. Devices like Gaize’s VR headset use eye-tracking sensors and machine learning to detect intoxication based on ocular behavior. Psychomotor Digital tests measure reaction time, attention, and coordination to detect cognitive deficits due to fatigue or substances. The Psychomotor Vigilance Test (PVT), widely used in fatigue studies, has been shown to detect alertness lapses in professional drivers. Scientific validation for psychomotor tests is ongoing, but early peer-reviewed research is promising. For example, a 2021 study funded",
    "label": 0
  },
  {
    "text": "substances. The Psychomotor Vigilance Test (PVT), widely used in fatigue studies, has been shown to detect alertness lapses in professional drivers. Scientific validation for psychomotor tests is ongoing, but early peer-reviewed research is promising. For example, a 2021 study funded by the U.S. Department of Transportation validated the AlertMeter, a mobile-based psychomotor fatigue assessment tool, against the PVT and the Karolinska Sleepiness Scale (KSS). The results showed strong concurrent validity, indicating that AlertMeter effectively detects fatigue-related impairments in workplace settings. Physiological Wearables and in-vehicle systems detect drowsiness or impairment by monitoring eye blinks, head movement, or steering behavior. Some advanced devices use brain-scanning methods like functional near-infrared spectroscopy (fNIRS) to identify cannabis related changes in brain activity. From 2024, the EU mandates drowsiness detection systems in all new vehicles to enhance road safety. Applications Workplace safety IDTs are used in high-risk industries such as construction, mining, and transportation to assess workers' fitness-for-duty. Daily pre-shift screenings using tablets or wearable devices help prevent accidents by identifying impaired individuals before they begin safety-sensitive tasks. A 2021 National Safety Council report found that 82% of employers using IDTs saw improved safety outcomes. These tests are also considered less invasive, as they avoid bodily samples and focus on functional performance. States with cannabis legalization are pushing employers to demonstrate actual on-the-job impairment, making IDTs a useful compliance tool. Workplace safety is definitely a huge concern as impairment doesn't just come from drugs or alcohol. Fatigue, stress, illness, and substance use are what the National Safety Council reports are the most common impairments. All of these can contribute to the person working as it affects them physically and mentally. Because of these, so many incidents involve workers who aren't \"chemically\" impaired are still technically impaired as any effect on their health can become a liability",
    "label": 0
  },
  {
    "text": "these can contribute to the person working as it affects them physically and mentally. Because of these, so many incidents involve workers who aren't \"chemically\" impaired are still technically impaired as any effect on their health can become a liability and cause accidents in the workplace. So, workplaces need to start using impairment detection tool that look for different signs of impairment than just with alcohol or drugs. National Safety Council. \"Address Workplace Impairment with Technology.\" 2021 Law enforcement Law enforcement is adopting IDTs to support impaired driving enforcement. Standard tools like breathalyzers detect alcohol, but drugs lack equivalent roadside tests. New devices such as eye-tracking goggles (e.g., Gaize) and cognitive testing tablets offer objective indicators of impairment regardless of substance. These tools supplement field sobriety tests but are still under legal review for admissibility. The U.S. government is also mandating in-vehicle impairment detection. Under the HALT Act, NHTSA will require new cars to include systems such as driver-monitoring cameras and passive alcohol sensors by 2026 to prevent impaired driving. These technologies are projected to prevent over 10,000 fatalities annually. Other areas IDTs have applications in aviation, rail, healthcare, military, and sports, where cognitive or physical readiness is critical. Airlines may use alertness tests for pilots, hospitals may screen fatigued surgeons, and sports teams may use reaction-time tools to assess concussion-related impairment. These technologies serve as preventive tools wherever safety and peak performance are essential. Legality Impairment testing is legally distinct from traditional drug testing, as a result, these tests often fall outside the scope of U.S. drug testing regulations. In the United States, legislative changes such as California Assembly Bill 2188 limit employment decisions based on non-psychoactive cannabis metabolites, encouraging the adoption of functional impairment assessments. Most impairment detection tools do not collect biometric templates or personally identifiable data",
    "label": 0
  },
  {
    "text": "In the United States, legislative changes such as California Assembly Bill 2188 limit employment decisions based on non-psychoactive cannabis metabolites, encouraging the adoption of functional impairment assessments. Most impairment detection tools do not collect biometric templates or personally identifiable data to comply with privacy laws like the Biometric Information Privacy Act. Legal precedent such as The T.J. Hooper case suggests that employers may risk liability for failing to adopt reasonable safety measures like impairment testing in safety-sensitive environments. In Canada, workplace testing must align with human rights legislation, focusing on job relevance and minimal intrusiveness. The Canadian Supreme Court’s 2013 decision in Irving Pulp & Paper emphasized that random testing requires evidence of significant safety risk. Canadian privacy authorities also recommend limiting personal data collection during testing. Regulation In transportation law, the Infrastructure Investment and Jobs Act mandates impairment prevention technology in all new cars by 2026, though it leaves implementation to manufacturers. Similarly, the EU requires driver drowsiness detection in new vehicles, reflecting a trend toward tech-enabled crash prevention. For law enforcement, impairment tech must meet high standards (like Daubert) to be admissible in court. Devices like eye-tracking tools are under pilot testing, but as of 2025, no non-alcohol IDT has full legal acceptance in DUI cases. Legal validation is ongoing in states like Minnesota. In workplace, OSHA has not yet issued specific IDT standards, but employers must still ensure safety. Experts recommend using IDTs as screening tools within a broader protocol that includes second-step evaluations. Limitations IDTs promise real‑time safety benefits, but their performance varies and many products remain only partially validated. Reviews by the National Safety Council note that, while the scientific principles behind measures such as pupillary response or reaction‑time testing are established, most commercial devices lack peer‑reviewed evidence across populations and impairment sources. Consequently, false",
    "label": 0
  },
  {
    "text": "remain only partially validated. Reviews by the National Safety Council note that, while the scientific principles behind measures such as pupillary response or reaction‑time testing are established, most commercial devices lack peer‑reviewed evidence across populations and impairment sources. Consequently, false positives and false negatives remain a concern; results can be influenced by test environment, individual variability and learning effects. Two calibration approaches present trade‑offs. Baseline models compare each user to their own sober benchmark, improving sensitivity but requiring initial testing and periodic recalibration; they can also be “gamed” if a user deliberately underperforms when setting the baseline. Fixed cut‑off models are simpler but may misclassify atypical yet unimpaired individuals. IDTs are cause‑agnostic: a failed alertness or eye‑tracking test reveals functional deficit but not whether it stems from drug use, fatigue, illness or another factor. Follow‑up measures such as medical checks or toxicology are therefore still required for legal or disciplinary action. User acceptance and operational practicality also limit uptake. Tests must be quick, non‑intrusive and robust to industrial conditions; otherwise they face resistance or logistical hurdles. Regular maintenance and calibration (e.g., for camera‑based systems) add cost and complexity. Safety experts caution against over‑reliance on a single device. IDTs should complement, not replace, traditional fitness‑for‑duty management and supervisor judgement. Because impairment is multifaceted, no single metric captures every case; hybrid approaches that combine ocular, cognitive and physiological signals are viewed as the most promising long‑term solution. See also Fatigue detection software Field sobriety testing Driver drowsiness detection References External links Gaize - VR based impairment detection device AlertMeter - Real-time fatigue detection tool",
    "label": 0
  },
  {
    "text": "Moral outsourcing refers to placing responsibility for ethical decision-making on to external entities, often algorithms. The term is often used in discussions of computer science and algorithmic fairness, but it can apply to any situation in which one appeals to outside agents in order to absolve themselves of responsibility for their actions. In this context, moral outsourcing specifically refers to the tendency of society to blame technology, rather than its creators or users, for any harm it may cause. Definition The term \"moral outsourcing\" was first coined by Dr. Rumman Chowdhury, a data scientist concerned with the overlap between artificial intelligence and social issues. Chowdhury used the term to describe looming fears of a so-called “Fourth Industrial Revolution” following the rise of artificial intelligence. Moral outsourcing is often applied by technologists to shrink away from their part in building offensive products. In her TED Talk, Chowdhury gives the example of a creator excusing their work by saying they were simply doing their job. This is a case of moral outsourcing and not taking ownership for the consequences of creation. When it comes to AI, moral outsourcing allows for creators to decide when the machine is human and when it is a computer - shifting the blame and responsibility of moral plights off of the technologists and onto the technology. Conversations around AI and bias and its impacts require accountability to bring change. It is difficult to address these biased systems if their creators use moral outsourcing to avoid taking any responsibility for the issue. One example of moral outsourcing is the anger that is directed at machines for “taking jobs away from humans” rather than companies for employing that technology and jeopardizing jobs in the first place. The term \"moral outsourcing\" refers to the concept of outsourcing, or enlisting an",
    "label": 0
  },
  {
    "text": "the anger that is directed at machines for “taking jobs away from humans” rather than companies for employing that technology and jeopardizing jobs in the first place. The term \"moral outsourcing\" refers to the concept of outsourcing, or enlisting an external operation to complete specific work for another organization. In the case of moral outsourcing, the work of resolving moral dilemmas or making choices according to an ethical code is supposed to be conducted by another entity. Real-World Applications In the medical field, AI is increasingly involved in decision-making processes about which patients to treat, and how to treat them. The responsibility of the doctor to make informed decisions about what is best for their patients is outsourced to an algorithm. Sympathy is also noted to be an important part of medical practice; an aspect that artificial intelligence, glaringly, is missing. This form of moral outsourcing is a major concern in the medical community. Another field of technology in which moral outsourcing is frequently brought up is autonomous vehicles. California Polytechnic State University professor Keith Abney proposed an example scenario: \"Suppose we have some [troublemaking] teenagers, and they see an autonomous vehicle, they drive right at it. They know the autonomous vehicle will swerve off the road and go off a cliff, but should it?\" The decision of whether to sacrifice the autonomous vehicle (and any passengers inside) or the vehicle coming at it will be written into the algorithms defining the car's behavior. In the case of moral outsourcing, the responsibility of any damage caused by an accident may be attributed to the autonomous vehicle itself, rather than the creators who wrote protocol the vehicle will use to \"decide\" what to do. Moral outsourcing is also used to delegate the consequences of predictive policing algorithms to technology, rather than",
    "label": 0
  },
  {
    "text": "be attributed to the autonomous vehicle itself, rather than the creators who wrote protocol the vehicle will use to \"decide\" what to do. Moral outsourcing is also used to delegate the consequences of predictive policing algorithms to technology, rather than the creators or the police. There are many ethical concerns with predictive policing due to the fact that it results in the over-policing of low income and minority communities. In the context of moral outsourcing, the positive feedback loop of sending disproportionate police forces into minority communities is attributed to the algorithm and the data being fed into this system--rather than the users and creators of the predictive policing technology. Outside of Technology Religion Moral outsourcing is also commonly seen in appeals to religion to justify discrimination or harm. In his book What It Means to be Moral, sociologist Phil Zuckerman contradicts the popular religious notion that morality comes from God. Religion is oftentimes cited as a foundation for a moral stance without any tangible relation between the religious beliefs and personal stance. In these cases, religious individuals will \"outsource\" their personal beliefs and opinions by claiming that they are a result of their religious identification. This is seen where religion is cited as a factor for political beliefs, medical beliefs, and in extreme cases an excuse for violence. Manufacturing Moral outsourcing can also be seen in the business world in terms of manufacturing goods and avoiding environmental responsibility. Some companies in the United States will move their production process to foreign countries with more relaxed environmental policies to avoid the pollution laws that exist in the US. A study by the Harvard Business Review found that \"in countries with tight environmental regulation, companies have 29% lower domestic emissions on average. On the other hand, such a tightening in regulation",
    "label": 0
  },
  {
    "text": "Systems cause representational harm when they misrepresent a group of people in a negative manner. Representational harms include perpetuating harmful stereotypes about or minimizing the existence of a social group, such as a racial, ethnic, gender, or religious group. Machine learning algorithms often commit representational harm when they learn patterns from data that have algorithmic bias, and this has been shown to be the case with large language models. While preventing representational harm in models is essential to prevent harmful biases, researchers often lack precise definitions of representational harm and conflate it with allocative harm, an unequal distribution of resources among social groups, which is more widely studied and easier to measure. However, recognition of representational harms is growing and preventing them has become an active research area. Researchers have recently developed methods to effectively quantify representational harm in algorithms, making progress on preventing this harm in the future. Types Three prominent types of representational harm include stereotyping, denigration, and misrecognition. These subcategories present many dangers to individuals and groups. Stereotypes are oversimplified and usually undesirable representations of a specific group of people, usually by race and gender. This often leads to the denial of educational, employment, housing, and other opportunities. For example, the model minority stereotype of Asian Americans as highly intelligent and good at mathematics can be damaging professionally and academically. Representational harm happens when the representation of details teams improves damaging stereotypes, developing social exclusion and prejudice. This experience is particularly noticeable in the depiction of marginalised groups, containing people of color, women, LGBTQ+ people, and people with handicaps. Media depictions of these groups generally stop working to catch their array and intricacy. Instead, they are typically reduced to one-dimensional caricatures, which ultimately continue social prejudices. These organised depictions contribute to the help of hazardous stereotypes and",
    "label": 0
  },
  {
    "text": "with handicaps. Media depictions of these groups generally stop working to catch their array and intricacy. Instead, they are typically reduced to one-dimensional caricatures, which ultimately continue social prejudices. These organised depictions contribute to the help of hazardous stereotypes and the marginalisation of these locations. Denigration is the action of unfairly criticizing individuals. This frequently happens when the demeaning of social groups occurs. For example, when searching for \"Black-sounding\" names versus \"white-sounding\" ones, some retrieval systems bolster the false perception of criminality by displaying ads for bail-bonding businesses. A system may shift the representation of a group to be of lower social status, often resulting in a disregard from society. Research shows that hazardous depictions in the media can have substantial emotional and social impacts on both individuals and areas. Lawrence Bobo examined the issue of Ethnic stereotype in film, tv, and marketing. African Americans are commonly received duties specified by features such as \"violent tendencies,\" \"laziness,\" or being \"merely for contentment features.\" While these representations might appear varied externally, they stay to boost underlying frameworks of white prominence and racial inequality. As a circumstances, Black individuals are frequently represented as law offenders or in secondary roles, which adds to the support of Ethnic stereotype and Institutional racism. Misrecognition, or incorrect recognition, can display in many forms, including, but not limited to, erasing and alienating social groups, and denying people the right to self-identify. Erasing and alienating social groups involves the unequal visibility of certain social groups; specifically, systematic ineligibility in algorithmic systems perpetuates inequality by contributing to the underrepresentation of social groups. Not allowing people to self-identify is closely related as people's identities can be 'erased' or 'alienated' in these algorithms. Misrecognition causes more than surface-level harm to individuals: psychological harm, social isolation, and emotional insecurity can emerge from",
    "label": 0
  },
  {
    "text": "of social groups. Not allowing people to self-identify is closely related as people's identities can be 'erased' or 'alienated' in these algorithms. Misrecognition causes more than surface-level harm to individuals: psychological harm, social isolation, and emotional insecurity can emerge from this subcategory of representational harm. Quantification As the dangers of representational harm have become better understood, some researchers have developed methods to measure representational harm in algorithms. Modeling stereotyping is one way to identify representational harm. Representational stereotyping can be quantified by comparing the predicted outcomes for one social group with the ground-truth outcomes for that group observed in real data. For example, if individuals from group A achieve an outcome with a probability of 60%, stereotyping would be observed if it predicted individuals to achieve that outcome with a probability greater than 60%. The group modeled stereotyping in the context of classification, regression, and clustering problems, and developed a set of rules to quantitatively determine if the model predictions exhibit stereotyping in each of these cases. Other attempts to measure representational harms have focused on applications of algorithms in specific domains such as image captioning, the act of an algorithm generating a short description of an image. In a study on image captioning, researchers measured five types of representational harm. To quantify stereotyping, they measured the number of incorrect words included in the model-generated image caption when compared to a gold-standard caption. They manually reviewed each of the incorrectly included words, determining whether the incorrect word reflected a stereotype associated with the image or whether it was an unrelated error, which allowed them to have a proxy measure of the amount of stereotyping occurring in this caption generation. These researchers also attempted to measure demeaning representational harm. To measure this, they analyzed the frequency with which humans in the",
    "label": 0
  },
  {
    "text": "error, which allowed them to have a proxy measure of the amount of stereotyping occurring in this caption generation. These researchers also attempted to measure demeaning representational harm. To measure this, they analyzed the frequency with which humans in the image were mentioned in the generated caption. It was hypothesized that if the individuals were not mentioned in the caption, then this was a form of dehumanization. Examples One of the most notorious examples of representational harm was committed by Google in 2015 when an algorithm in Google Photos classified Black people as gorillas. Developers at Google said that the problem was caused because there were not enough faces of Black people in the training dataset for the algorithm to learn the difference between Black people and gorillas. Google issued an apology and fixed the issue by blocking its algorithms from classifying anything as a primate. In 2023, Google's photos algorithm was still blocked from identifying gorillas in photos. Another prevalent example of representational harm is the possibility of stereotypes being encoded in word embeddings, which are trained using a wide range of text. These word embeddings are the representation of a word as an array of numbers in vector space, which allows an individual to calculate the relationships and similarities between words. However, recent studies have shown that these word embeddings may commonly encode harmful stereotypes, such as the common example that the phrase \"computer programmer\" is oftentimes more closely related to \"man\" than it is to \"women\" in vector space. This could be interpreted as a misrepresentation of computer programming as a profession that is better performed by men, which would be an example of representational harm. Addressing representational harm Initiatives to minimise representational harm include advertising for even more inclusive and accurate portrayals of marginalised teams in",
    "label": 0
  },
  {
    "text": "computer programming as a profession that is better performed by men, which would be an example of representational harm. Addressing representational harm Initiatives to minimise representational harm include advertising for even more inclusive and accurate portrayals of marginalised teams in the media. Scholars and protestors recommend that the method to reducing representational injury depends on raising the selection of voices both behind and before the digital video camera. When marginalized groups are provided the chance to represent themselves, they can check traditional stereotypes and present their experiences additional authentically. Over the last few years, efforts to increase representation of people of color, women, and LGBTQ+ people in conventional media have made some progression. Films such as Selma, routed by Ava DuVernay, and tv series like Pose, developed by Ryan Murphy, have actually been extensively applauded for their nuanced and respectful representations of marginalised communities. These tasks existing complex individualities and stories that move past streamlined stereotypes. Self-representation is one more crucial method to addressing representational harm. By equipping marginalised locations to create their really own tales, media designers can effectively reduce the perpetuation of hazardous stereotypes. This procedure consists of both the manufacturing of media product by participants of these communities and proactively difficult typical media structures that have actually historically omitted them. References",
    "label": 0
  },
  {
    "text": "This list of science and technology awards for women is an index to articles about notable awards made to women for work in science and the STEM (Science, technology, engineering, and mathematics) fields generally. It includes awards for astronomy, space and atmospheric science; biology and medicine; chemistry; engineering; mathematics; neuroscience; physics; technology; and general or multiple fields. Astronomy, space, atmospheric science Annie Jump Cannon Award in Astronomy – annual award for outstanding contributions to astronomy by a woman within five years of earning a doctorate degree Peter B. Wagner Memorial Award for Women in Atmospheric Sciences – awarded annually since 1998, based on paper completion, to a woman studying for a Masters or PhD in atmospheric science at a university in the United States Biology and medicine Elizabeth Blackwell Medal, given by the American Medical Women's Association to a woman physician \"who has made the most outstanding contributions to the cause of women in the field of medicine\" Federation of American Societies for Experimental Biology (FASEB) Excellence in Science Award Group on Women in Medicine and Science Leadership Awards, Association of American Medical Colleges Margaret Oakley Dayhoff Award from the Biophysical Society, Rockville, Maryland – given to a woman who \"has achieved prominence for 'substantial contributions to science'\" and showing high promise in the early part of her career Pearl Meister Greengard Prize – established 2004 WICB Junior and Senior Awards from Women in Cell Biology (WICB) Chemistry ACS Award for Encouraging Women into Careers in the Chemical Sciences, sponsored by the Camille and Henry Dreyfus Foundation Garvan–Olin Medal – annual award that recognizes distinguished service to chemistry by women chemists Awards by the Iota Sigma Pi honorary society for women in chemistry: Agnes Fay Morgan Research Award Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research Centennial Award",
    "label": 0
  },
  {
    "text": "annual award that recognizes distinguished service to chemistry by women chemists Awards by the Iota Sigma Pi honorary society for women in chemistry: Agnes Fay Morgan Research Award Anna Louise Hoffman Award for Outstanding Achievement in Graduate Research Centennial Award for Excellence in Undergraduate Teaching Gladys Anderson Emerson Undergraduate Scholarship Members-at-Large Re-entry Award National Honorary Member Outstanding Young Women in Chemistry award. Undergraduate Excellence in Chemistry Violet Diller Professional Excellence Award Engineering Sharon Keillor Award for Women in Engineering Education Achievement Award of the Society of Women Engineers Young Woman Engineer of the Year Award Mathematics Awards by the Association for Women in Mathematics: Alice T. Schafer Prize – established 1991 Biographies of Contemporary Women in Mathematics Essay Contest – established in 2001 for biographical essays Emmy Noether Lectures – an honorary lecture award M. Gweneth Humphreys Award Louise Hay Award for Contributions to Mathematics Education – established 1991 Ruth I. Michler Memorial Prize Ruth Lyttle Satter Prize in Mathematics – established 1990 additional awards by the AWM Awards sponsored by the Kovalevskaia Fund in Mexico, Peru, and southern Africa Krieger–Nelson Prize for Distinguished Research by Women in Mathematics – established 1995 by the Canadian Mathematical Society Neuroscience Awards by the Society for Neuroscience: Bernice Grafstein Award for Outstanding Accomplishments in Mentoring – for dedication to mentoring women neuroscientists Louise Hanson Marshall Special Recognition Award – honors an individual who has significantly promoted the professional development of women in neuroscience through teaching, organizational leadership, public advocacy, or other efforts that are not necessarily related to research Mika Salpeter Lifetime Achievement Award – presented for outstanding career achievements in neuroscience who has also significantly promoted the professional advancement of women in neuroscience Patricia Goldman-Rakic Hall of Honor – posthumously recognizes a neuroscientist who has pursued career excellence and exhibited dedication and",
    "label": 0
  },
  {
    "text": "Award – presented for outstanding career achievements in neuroscience who has also significantly promoted the professional advancement of women in neuroscience Patricia Goldman-Rakic Hall of Honor – posthumously recognizes a neuroscientist who has pursued career excellence and exhibited dedication and advancement of women in neuroscience Physics Jocelyn Bell Burnell Medal and Prize, Institute of Physics Maria Goeppert-Mayer Award, American Physical Society Technology BlackBerry Women and Technology Awards WITI@UC Athena Awards – Awards recognize those who embody, encourage, and promote the inclusion of women in technology. Awardees are leaders who inspire others to pursue and persist in technical careers. Lori Bunch \"Recognized for Innovation in Cerner Millennium Implementation.\" She started with Cerner as an installation director then was promoted to senior project architect in 1997. She continues to be an innovator in the information technology industry. General or multiple fields Awards by the Association for Women in Science: Leadership Award Kirsten R. Lorentzen Award – for undergraduates in science Next Generation Award Pinnacle Award Amelia Earhart Fellowship Athena SWAN – a recognition scheme for UK universities working to advance and promote careers of women in science, engineering, and technology Scholarships for women in science by Brookhaven Women in Science: Renate W. Chasman Scholarship – awarded to a graduate student performing research at Brookhaven National Laboratory Gertrude S. Goldhaber Prize – awarded to a graduate student at Stony Brook University and/or performing thesis research at Brookhaven National Laboratory Edison Awards – Honoring excellence in innovation Edith D. Hendley Award – for a woman pursuing graduate studies at the University of Vermont Elizabeth Blackwell Award – given by Hobart and William Smith Colleges, established in 1958 Faculty for the Future Fellowships – awarded by the Schlumberger Foundation to women from developing and emerging economies who are preparing for PhD or post-doctoral study in",
    "label": 0
  },
  {
    "text": "Blackwell Award – given by Hobart and William Smith Colleges, established in 1958 Faculty for the Future Fellowships – awarded by the Schlumberger Foundation to women from developing and emerging economies who are preparing for PhD or post-doctoral study in the physical sciences and engineering Inspiring Women in Science Awards - awarded by Nature in partnership with The Estée Lauder Companies to recognise exceptional early career women researchers and initiatives that support girls or young women to engage with, enjoy and study STEM subjects or to increase the retention of women in STEM careers Katharine F. Erskine Award for Medicine and Science Kovalevskaia prizes sponsored by the Kovalevskaia Fund in Vietnam and Cuba L'Oréal-UNESCO Awards for Women in Science, aka the Helena Rubinstein Women in Science Awards L’Oréal Korea-UNESCO for Women in Science Award, a national variant of the L'Oréal-UNESCO Awards for Women in Science Margaret W. Rossiter History of Women in Science Prize – established in 1987 by the History of Science Society Maria Mitchell Women in Science Award – from the Maria Mitchell Association \"to recognize an individual whose efforts have encouraged the advancement of girls and women in the natural and physical sciences, mathematics, engineering, computer science and technology\" Mariafranca Morselli Award – for a woman pursuing undergraduate studies in science at the University of Vermont Outstanding Women in Science Award – established 2002 by the Korea Science and Engineering Foundation OWSD-Elsevier Foundation Awards for Early-Career Women Scientists in the Developing World Rachel Carson Prize – Norwegian prize for women environmentalists Robin Copeland Memorial Fellowship – fellowship from CRDF Global for women leaders in emerging countries Saruhashi Prize – an award for Japanese women researchers in the natural sciences South African Awards for Women in Science (SAWiSA) Tagea Brandt Rejselegat – Danish award for women who have",
    "label": 0
  },
  {
    "text": "Technological determinism is a reductionist theory in assuming that a society's technology progresses by following its own internal logic of efficiency, while determining the development of the social structure and cultural values. The term is believed to have originated from Thorstein Veblen (1857–1929), an American sociologist and economist. The most radical technological determinist in the United States in the 20th century was most likely Clarence Ayres who was a follower of Thorstein Veblen as well as John Dewey. William Ogburn was also known for his radical technological determinism and his theory on cultural lag. Origin The origins of technological determinism as a formal concept are often traced to Thorstein Veblen (1857–1929), an influential American sociologist and economist. Veblen, known for his work on social and economic issues, introduced ideas that portrayed technology as a powerful, autonomous force capable of shaping societal norms and structures. He argued that the development and use of machinery exerted an independent influence on human thought and behavior, notably asserting that \"the machine throws out anthropomorphic habits of thought.\" Historical Context and Influences During Veblen's time, rapid industrialization and advancements in technology were radically altering American society. Innovations in manufacturing and transportation, such as the assembly line and railroads, demonstrated technology's potential to reshape economic and social structures. These changes helped popularize the idea that technology could independently drive societal evolution, creating the conditions for Veblen's ideas to resonate widely. Influence of Karl Marx and Expansion by Clarence Ayres Although Veblen is credited with coining the core ideas behind technological determinism, the influence of Karl Marx on these ideas is also significant. Marx argued that technology drives historical change by shaping the \"material base\" of society. For instance, he suggested that the railway in colonial India would challenge and erode the caste system by introducing new",
    "label": 0
  },
  {
    "text": "on these ideas is also significant. Marx argued that technology drives historical change by shaping the \"material base\" of society. For instance, he suggested that the railway in colonial India would challenge and erode the caste system by introducing new economic activities and altering social hierarchies. Later, Clarence Ayres, a 20th-century economist inspired by Veblen, expanded on these ideas by introducing the concept of \"technological drag.\" According to Ayres, technology progresses as a dynamic, self-generating force, while traditional institutions often lag, resisting the transformative potential of technological change. Ayres' theory further solidified technological determinism, emphasizing the inevitable clash between technological progress and social conservatism. Explanation Technological determinism seeks to show technical developments, media, or technology as a whole, as the key mover in history and social change. It is a theory subscribed to by \"hyperglobalists\" who claim that as a consequence of the wide availability of technology, accelerated globalization is inevitable. Therefore, technological development and innovation become the principal motor of social, economic or political change. Technological determinism has been summarized as 'The belief in technology as a key governing force in society ...' (Merritt Roe Smith). 'The idea that technological development determines social change ...' (Bruce Bimber). It changes the way people think and how they interact with others and can be described as '...a three-word logical proposition: \"Technology determines history\"' (Rosalind H. Williams) . It is, '... the belief that social progress is driven by technological innovation, which in turn follows an \"inevitable\" course.' Technological determinism has been defined as an approach that identifies technology, or technological advances, as the central causal element in processes of social change. As technology is stabilized, its design tends to dictate users' behaviors, consequently stating that \"technological progress equals social progress.\" Key notions of this theory are separated into two parts, with",
    "label": 0
  },
  {
    "text": "as the central causal element in processes of social change. As technology is stabilized, its design tends to dictate users' behaviors, consequently stating that \"technological progress equals social progress.\" Key notions of this theory are separated into two parts, with the first being that the development of the technology itself may also be separate from social and political factors, arising from \"the ways of inventors, engineers, and designers following an internal, technical logic that has nothing to do with social relationships\". As technology changes, the ways in which it is utilized and incorporated into the daily lives of individuals within a culture consequently affect the ways of living, highlighting how technology ultimately determines societal growth through its influence on relations and ways of living within a culture. To illustrate, \"the invention of the wheel revolutionized human mobility, allowing humans to travel greater distances and carry greater loads with them\". This technological advancement also leads to interactions between different cultural groups, advanced trade, and thus impacts the size and relations both within and between different networks. Other examples include the invention of language, expanding modes of communication between individuals, the introduction of bookkeeping and written documentation, impacting the circulation of knowledge, and having streamlined effects on the socioeconomic and political systems as a whole. As Dusek (2006) notes, \"culture and society cannot affect the direction of technology...[and] as technology develops and changes, the institutions in the rest of society change, as does the art and religion of a society.\" Thus, technological determinism dictates that technological advances and social relations are inevitably tied, with the change of either affecting the other by consequence of normalization. This stance however ignores the social and cultural circumstances in which the technology was developed. Sociologist Claude Fischer (1992) characterized the most prominent forms of technological determinism",
    "label": 0
  },
  {
    "text": "tied, with the change of either affecting the other by consequence of normalization. This stance however ignores the social and cultural circumstances in which the technology was developed. Sociologist Claude Fischer (1992) characterized the most prominent forms of technological determinism as \"billiard ball\" approaches, in which technology is seen as an external force introduced into a social situation, producing a series of ricochet effects. Rather than acknowledging that a society or culture interacts with and even shapes the technologies that are used, a technological determinist view holds that \"the uses made of technology are largely determined by the structure of the technology itself, that is, that its functions follow from its form\" (Neil Postman). However, this is not the sole view of TD following Smith and Marx's (1998) notion of \"hard\" determinism, which states that once a technology is introduced into a culture what follows is the inevitable development of that technology. The other view follows what Smith and Marx (1998) dictate as \"soft\" determinism, where the development of technology is also dependent on social context, affecting how it is adopted into a culture, \"and, if the technology is adopted, the social context will have important effects on how the technology is used and thus on its ultimate impact\". For example, we could examine the spread of mass-produced knowledge through the role of the printing press in the Protestant Reformation. Because of the urgency from the protestant side to get the reform off the ground before the church could react, \"early Lutheran leaders, led by Luther himself, wrote thousands of anti-papal pamphlets in the Reformation's first decades and these works spread rapidly through reprinting in various print shops throughout central Europe\". Hard and soft determinism \"Soft determinism\", as the name suggests, is a more passive view of the way technology",
    "label": 0
  },
  {
    "text": "anti-papal pamphlets in the Reformation's first decades and these works spread rapidly through reprinting in various print shops throughout central Europe\". Hard and soft determinism \"Soft determinism\", as the name suggests, is a more passive view of the way technology interacts with socio-political situations. Soft determinists still subscribe to the fact that technology is the guiding force in our evolution but would maintain that we have a chance to make decisions regarding the outcomes of a situation. This is not to say that free will exists, but that the possibility for us to roll the dice and see what the outcome exists. A slightly different variant of soft determinism is the 1922 technology-driven theory of social change proposed by William Fielding Ogburn, in which society must adjust to the consequences of major inventions, but often does so only after a period of cultural lag. Criticism In his article \"Subversive Rationalization: Technology, Power and Democracy with Technology,\" Andrew Feenberg argues that technological determinism is not a very well founded concept by illustrating that two of the founding theses of determinism are easily questionable and in doing so calls for what he calls democratic rationalization (Feenberg 210–212). Another conflicting idea is that of technological somnambulism, a term coined by Winner in his essay \"Technology as Forms of Life\". Winner wonders whether or not we are simply sleepwalking through our existence with little concern or knowledge as to how we truly interact with technology. In this view, it is still possible for us to wake up and once again take control of the direction in which we are traveling (Winner 104). However, it requires society to adopt Ralph Schroeder's claim that, \"users don't just passively consume technology, but actively transform it\". In opposition to technological determinism are those who subscribe to the belief",
    "label": 0
  },
  {
    "text": "direction in which we are traveling (Winner 104). However, it requires society to adopt Ralph Schroeder's claim that, \"users don't just passively consume technology, but actively transform it\". In opposition to technological determinism are those who subscribe to the belief of social determinism and postmodernism. Social determinists believe that social circumstances alone select which technologies are adopted, with the result that no technology can be considered \"inevitable\" solely on its own merits. Technology and culture are not neutral and when knowledge comes into the equation, technology becomes implicated in social processes. The knowledge of how to create, enhance, and use technology is socially bound knowledge. Postmodernists take another view, suggesting that what is right or wrong is dependent on circumstance. They believe technological change can have implications on the past, present and future. Notable technological determinists Some interpret Karl Marx as advocating technological determinism, with such statements as \"The Handmill gives you society with the feudal lord: the steam-mill, society with the industrial capitalist\" (The Poverty of Philosophy, 1847), but others argue that Marx was not a determinist. Technological determinist Walter J. Ong reviews the societal transition from an oral culture to a written culture in his work Orality and Literacy: The Technologizing of the Word (1982). He asserts that this particular development is attributable to the use of new technologies of literacy (particularly print and writing,) to communicate thoughts which could previously only be verbalized. He furthers this argument by claiming that writing is purely context dependent as it is a \"secondary modelling system\" (8). Reliant upon the earlier primary system of spoken language, writing manipulates the potential of language as it depends purely upon the visual sense to communicate the intended information. Furthermore, the rather stagnant technology of literacy distinctly limits the usage and influence of knowledge, it",
    "label": 0
  },
  {
    "text": "primary system of spoken language, writing manipulates the potential of language as it depends purely upon the visual sense to communicate the intended information. Furthermore, the rather stagnant technology of literacy distinctly limits the usage and influence of knowledge, it unquestionably effects the evolution of society. In fact, Ong asserts that \"more than any other single invention, writing has transformed human consciousness\" (Ong 1982: 78). Media determinism as a form of technological determinism Media determinism is a form of technological determinism, a philosophical and sociological position which posits the power of the media to impact society. Two foundational media determinists are the Canadian scholars Harold Innis and Marshall McLuhan. One of the best examples of technological determinism in media theory is Marshall McLuhan's theory \"the medium is the message\" and the ideas of his mentor Harold Adams Innis. Both these Canadian theorists saw media as the essence of civilization. The association of different media with particular mental consequences by McLuhan and others can be seen as related to technological determinism. It is this variety of determinism that is referred to as media determinism. According to McLuhan, there is an association between communications media/technology and language; similarly, Benjamin Lee Whorf argues that language shapes our perception of thinking (linguistic determinism). For McLuhan, media is a more powerful and explicit determinant than is the more general concept of language. McLuhan was not necessarily a hard determinist. As a more moderate version of media determinism, he proposed that our use of particular media may have subtle influences on us, but more importantly, it is the social context of use that is crucial. See also Media ecology. Media determinism is a form of the popular dominant theory of the relationship between technology and society. In a determinist view, technology takes on an active life",
    "label": 0
  },
  {
    "text": "is the social context of use that is crucial. See also Media ecology. Media determinism is a form of the popular dominant theory of the relationship between technology and society. In a determinist view, technology takes on an active life of its own and is seen be as a driver of social phenomena. Innis believed that the social, cultural, political, and economic developments of each historical period can be related directly to the technology of the means of mass communication of that period. In this sense, like Dr. Frankenstein's monster, technology itself appears to be alive, or at least capable of shaping human behavior. However, it has been increasingly subject to critical review by scholars. For example, scholar Raymond Williams, criticizes media determinism and rather believes social movements define technological and media processes. With regard to communications media, audience determinism is a viewpoint opposed to media determinism. This is described as instead of media being presented as doing things to people; the stress is on the way people do things with media. Individuals need to be aware that the term \"deterministic\" is a negative one for many social scientists and modern sociologists; in particular they often use the word as a term of abuse. See also Instrumental conception of technology – Philosophical conceptPages displaying short descriptions of redirect targets Determinism – Philosophical view that events are determined by prior events Historical materialism – Marxist historiography History of science and technology Orthodox Marxism – Body of Marxist thought, prominent until World War I Philosophy of technology – Studies of the nature of technology Theory of the productive forces – Technological determinism in Marxism Path dependence Footnotes [as cited in Croteau, D. and Hoynes, M. (2003) Media Society: Industries, Images and Audiences (third edition), Pine Forge Press, Thousand Oaks pp. 305–306] References",
    "label": 0
  },
  {
    "text": "of technology Theory of the productive forces – Technological determinism in Marxism Path dependence Footnotes [as cited in Croteau, D. and Hoynes, M. (2003) Media Society: Industries, Images and Audiences (third edition), Pine Forge Press, Thousand Oaks pp. 305–306] References Further reading G.A. Cohen, Karl Marx's Theory of History: A Defence, Oxford and Princeton, 1978. Cowan, Ruth Schwartz (1983). More Work for Mother: The Ironies of Household Technology from the Open Hearth to the Microwave. New York: Basic Books. ISBN 978-0-465-04732-1. Croteau, David; Hoynes, William (2003). Media Society: Industries, Images and Audiences ((third edition) ed.). Thousand Oaks: Pine Forge Press. pp. 305–307. ISBN 9780761987734. Ellul, Jacques (1964). The Technological Society. New York: Alfred A. Knopf. Green, Lelia (2002). Technoculture. Crows Nest: Allen & Unwin. pp. 1–20. ISBN 9781865080482. Huesemann, Michael H., and Joyce A. Huesemann (2011). Technofix: Why Technology Won't Save Us or the Environment, New Society Publishers, Gabriola Island, British Columbia, Canada, ISBN 0865717044, 464 pp. Miller, Sarah (January 1997). \"Futures Work – Recognising the Social Determinants of Change\". Social Alternatives (vol.1, No.1 ed.). pp. 57–58. Murphie, Andrew; Potts, John (2003). \"1\". Culture and Technology. London: Palgrave. p. 21. Ong, Walter J (1982). Orality and Literacy: The Technologizing of the Word. New York: Methuen. Postman, Neil (1992). Technopoly: the Surrender of Culture to Technology. Vintage: New York. pp. 3–20. Roland, Alex. Once More into the Stirrups; Lynne White Jr, Medieval Technology and Social Change\" Classics Revisited. 574- 585. Sawyer, P.H. and R.H. Hilton. \"Technical Determinism\" Past & Present. April 1963: 90–100. Smith, Merritt Roe; Marx, Leo, eds. (1994). Does Technology Drive History? The Dilemma of Technological Determinism. Cambridge: MIT Press. ISBN 9780262691673. Staudenmaier, S.J., John M. (1985). \"The Debate over Technological Determinism\". Technology's Storytellers: Reweaving the Human Fabric. Cambridge: The Society for the History of Technology and the MIT",
    "label": 0
  },
  {
    "text": "Technology Drive History? The Dilemma of Technological Determinism. Cambridge: MIT Press. ISBN 9780262691673. Staudenmaier, S.J., John M. (1985). \"The Debate over Technological Determinism\". Technology's Storytellers: Reweaving the Human Fabric. Cambridge: The Society for the History of Technology and the MIT Press. pp. 134–148. Winner, Langdon (1977). Autonomous Technology: Technics-Out-of-Control as a Theme in Political Thought. Cambridge: MIT Press. ISBN 9780262230780. Winner, Langdon (1986). \"Do Artefacts Have Politics?\". The Whale and the Reactor. Chicago: University of Chicago Press. p. 26. Winner, Langdon. \"Technology as Forms of Life\". Readings in the Philosophy of Technology. David M. Kaplan. Oxford: Rowman & Littlefield, 2004. 103–113 Woolgar, Steve and Cooper, Geoff (1999). \"Do artefacts have ambivalence? Moses' bridges, Winner's bridges and other urban legends in S&TS\". Social Studies of Science 29 (3), 433–449. White, Lynn (1966). Medieval Technology and Social Change. New York: Oxford University Press. Furbank, P.N. \"The Myth of Determinism.\" Raritan. [City] Fall 2006: 79–87. EBSCOhost. Monroe Community College Library, Rochester, NY. 2 April 2007. Feenberg, Andrew. \"Democratic Rationalization\". Readings in the Philosophy of Technology. David M. Kaplan. Oxford: Rowman & Littlefield, 2004. 209–225 Chandler, Daniel. Technological or Media Determinism. 1995. 18 September 1995. <http://www.aber.ac.uk/media/Documents/tecdet/tecdet.html> External links Colin Rule, \"Is Technology Neutral?\" Archived 2007-08-19 at the Wayback Machine Megan McCormick, \"Technology as Neutral\" Daniel Chandler, \"Technological or Media Determinism\" Chris Kimble, \"Technological Determinism and Social Choice\" Vysotskyi, O., Deviatko, N., & Vysotska, O., \"Theory of technologies of geographical determinism in international relations\"",
    "label": 0
  },
  {
    "text": "Technology, society and life or technology and culture refers to the inter-dependency, co-dependence, co-influence, and co-production of technology and society upon one another. Evidence for this synergy has been found since humanity first started using simple tools. The inter-relationship has continued as modern technologies such as the printing press and computers have helped shape society. The first scientific approach to this relationship occurred with the development of tektology, the \"science of organization\", in early twentieth century Imperial Russia. In modern academia, the interdisciplinary study of the mutual impacts of science, technology, and society, is called science and technology studies. The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times have lessened physical barriers to communication and allowed humans to interact freely on a global scale, such as the printing press, telephone, and Internet. Technology has developed advanced economies, such as the modern global economy, and has led to the rise of a leisure class. Many technological processes produce by-products known as pollution, and deplete natural resources to the detriment of Earth's environment. Innovations influence the values of society and raise new questions in the ethics of technology. Examples include the rise of the notion of efficiency in terms of human productivity, and the challenges of bioethics. Philosophical debates have arisen over the use of technology, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticize the pervasiveness of technology, arguing that it harms the environment and alienates people. However, proponents of ideologies such as transhumanism and techno-progressivism view continued technological",
    "label": 0
  },
  {
    "text": "technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticize the pervasiveness of technology, arguing that it harms the environment and alienates people. However, proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition. Some scholarship approaches technology and society from a national-development perspective, as in Understanding Technology in the Context of National Development: Critical Reflections, which emphasizes how governance capacity, digital infrastructure, and institutional context mediate social outcomes. Pre-historical The importance of stone tools, circa 2.5 million years ago, is considered fundamental in the human development in the hunting hypothesis. Primatologist, Richard Wrangham, theorizes that the control of fire by early humans and the associated development of cooking was the spark that radically changed human evolution. Texts such as Guns, Germs, and Steel suggest that early advances in plant agriculture and husbandry fundamentally shifted the way that collective groups of individuals, and eventually societies, developed. Modern examples and effects Technology has taken a large role in society and day-to-day life. When societies know more about the development in a technology, they become able to take advantage of it. When an innovation achieves a certain point after it has been presented and promoted, this technology becomes part of the society. The use of technology in education provides students with technology literacy, information literacy, capacity for life-long learning, and other skills necessary for the 21st century workplace. Digital technology has entered each process and activity made by the social system. In fact, it constructed another worldwide communication system in addition to its origin. A 1982 study by The New York Times described a technology assessment study by the Institute for the Future, \"peering into the future of an electronic world.\" The study focused on the emerging",
    "label": 0
  },
  {
    "text": "communication system in addition to its origin. A 1982 study by The New York Times described a technology assessment study by the Institute for the Future, \"peering into the future of an electronic world.\" The study focused on the emerging videotex industry, formed by the marriage of two older technologies, communications, and computing. It estimated that 40 percent of American households will have two-way videotex service by the end of the century. By comparison, it took television 16 years to penetrate 90 percent of households from the time commercial service was begun. Since the creation of computers achieved an entire better approach to transmit and store data. Digital technology became commonly used for downloading music and watching movies at home either by DVDs or purchasing it online. Digital music records are not quite the same as traditional recording media. Obviously, because digital ones are reproducible, portable and free. Around the globe many schools have implemented educational technology in primary schools, universities and colleges. According to the statistics, in the early beginnings of 1990s the use of Internet in schools was, on average, 2–3%. Continuously, by the end of 1990s the evolution of technology increases rapidly and reaches to 60%, and by the year of 2008 nearly 100% of schools use Internet on educational form. According to ISTE researchers, technological improvements can lead to numerous achievements in classrooms. E-learning system, collaboration of students on project based learning, and technological skills for future results in motivation of students. Although these previous examples only show a few of the positive aspects of technology in society, there are negative side effects as well. Within this virtual realm, social media platforms such as Instagram, Facebook, and Snapchat have altered the way Generation Y culture is understanding the world and thus how they view themselves. In",
    "label": 0
  },
  {
    "text": "in society, there are negative side effects as well. Within this virtual realm, social media platforms such as Instagram, Facebook, and Snapchat have altered the way Generation Y culture is understanding the world and thus how they view themselves. In recent years, there has been more research on the development of social media depression in users of sites like these. \"Facebook Depression\" is when users are so affected by their friends' posts and lives that their own jealousy depletes their sense of self-worth. They compare themselves to the posts made by their peers and feel unworthy or monotonous because they feel like their lives are not nearly as exciting as the lives of others. Technology has a serious effect on youth's health. The overuse of technology is said to be associated with sleep deprivation which is linked to obesity and poor academic performance in the lives of adolescents. Economics and technological development In ancient history, economics began when spontaneous exchange of goods and services was replaced over time by deliberate trade structures. Makers of arrowheads, for example, might have realized they could do better by concentrating on making arrowheads and barter for other needs. Regardless of goods and services bartered, some amount of technology was involved—if no more than in the making of shell and bead jewelry. Even the shaman's potions and sacred objects can be said to have involved some technology. So, from the very beginnings, technology can be said to have spurred the development of more elaborate economies. Technology is seen as primary source in economic development. Technology advancement and economic growth are related to each other. The level of technology is important to determine the economic growth. It is the technological process which keeps the economy moving. In the modern world, superior technologies, resources, geography, and history",
    "label": 0
  },
  {
    "text": "advancement and economic growth are related to each other. The level of technology is important to determine the economic growth. It is the technological process which keeps the economy moving. In the modern world, superior technologies, resources, geography, and history give rise to robust economies; and in a well-functioning, robust economy, economic excess naturally flows into greater use of technology. Moreover, because technology is such an inseparable part of human society, especially in its economic aspects, funding sources for (new) technological endeavors are virtually illimitable. However, while in the beginning, technological investment involved little more than the time, efforts, and skills of one or a few men, today, such investment may involve the collective labor and skills of many millions. Most recently, because of the COVID-19 pandemic, the proportion of firms employing advanced digital technology in their operations expanded dramatically. It was found that firms that adopted technology were better prepared to deal with the pandemic's disruptions. Adaptation strategies in the form of remote working, 3D printing, and the use of big data analytics and AI to plan activities to adapt to the pandemic were able to ensure positive job growth. Funding Consequently, the sources of funding for large technological efforts have dramatically narrowed, since few have ready access to the collective labor of a whole society, or even a large part. It is conventional to divide up funding sources into governmental (involving whole, or nearly whole, social enterprises) and private (involving more limited, but generally more sharply focused) business or individual enterprises. Government funding for new technology The government is a major contributor to the development of new technology in many ways. In the United States alone, many government agencies specifically invest billions of dollars in new technology. In 1980, the UK government invested just over six million pounds",
    "label": 0
  },
  {
    "text": "is a major contributor to the development of new technology in many ways. In the United States alone, many government agencies specifically invest billions of dollars in new technology. In 1980, the UK government invested just over six million pounds in a four-year program, later extended to six years, called the Microelectronics Education Programme (MEP), which was intended to give every school in Britain at least one computer, software, training materials, and extensive teacher training. Similar programs have been instituted by governments around the world. Technology has frequently been driven by the military, with many modern applications developed for the military before they were adapted for civilian use. However, this has always been a two-way flow, with industry often developing and adopting a technology only later adopted by the military. Entire government agencies are specifically dedicated to research, such as America's National Science Foundation, the United Kingdom's scientific research institutes, America's Small Business Innovative Research effort. Many other government agencies dedicate a major portion of their budget to research and development. Private funding Research and development is one of the smallest areas of investments made by corporations toward new and innovative technology. Many foundations and other nonprofit organizations contribute to the development of technology. In the OECD, about two-thirds of research and development in scientific and technical fields is carried out by industry, and 98 percent and 10 percent, respectively, by universities and government. But in poorer countries such as Portugal and Mexico the industry contribution is significantly less. The U.S. government spends more than other countries on military research and development, although the proportion has fallen from about 30 percent in the 1980s to less than 10 percent. The 2009 founding of Kickstarter allows individuals to receive funding via crowdsourcing for many technology related products including both new physical",
    "label": 0
  },
  {
    "text": "and development, although the proportion has fallen from about 30 percent in the 1980s to less than 10 percent. The 2009 founding of Kickstarter allows individuals to receive funding via crowdsourcing for many technology related products including both new physical creations as well as documentaries, films, and web-series that focus on technology management. This circumvents the corporate or government oversight most inventors and artists struggle against but leaves the accountability of the project completely with the individual receiving the funds. Other economic considerations Appropriate technology, sometimes called \"intermediate\" technology, more of an economics concern, refers to compromises between central and expensive technologies of developed nations and those that developing nations find most effective to deploy given an excess of labour and scarcity of cash. Persuasion technology: In economics, definitions or assumptions of progress or growth are often related to one or more assumptions about technology's economic influence. Challenging prevailing assumptions about technology and its usefulness has led to alternative ideas like uneconomic growth or measuring well-being. These, and economics itself, can often be described as technologies, specifically, as persuasion technology. Technocapitalism Technological diffusion Technology acceptance model Technology life cycle Technology transfer Relation to science The relationship between science and technology can be complex. Science may drive technological development, by generating demand for new instruments to address a scientific question, or by illustrating technical possibilities previously unconsidered. An environment of encouraged science will also produce scientists and engineers, and technical schools, which encourages innovation and entrepreneurship that are capable of taking advantage of the existing science. In fact, it is recognized that \"innovators, like scientists, do require access to technical information and ideas\" and \"must know enough to recognize useful knowledge when they see it.\" Science spillover also contributes to greater technological diffusion. Having a strong policy contributing to basic science",
    "label": 0
  },
  {
    "text": "that \"innovators, like scientists, do require access to technical information and ideas\" and \"must know enough to recognize useful knowledge when they see it.\" Science spillover also contributes to greater technological diffusion. Having a strong policy contributing to basic science allows a country to have access to a strong a knowledge base that will allow them to be \"ready to exploit unforeseen developments in technology,\" when needed in times of crisis. For most of human history, technological improvements were arrived at by chance, trial and error, or spontaneous inspiration. Stokes referred to these innovators as \"'improvers of technology'…who knew no science and would not have been helped by it if they had.\" This idea is supported by Diamond who further indicated that these individuals are \"more likely to achieve a breakthrough if [they do] not hold the currently dominant theory in too high regard.\" Research and development directed towards immediate technical application is a relatively recent occurrence, arising with the Industrial Revolution and becoming commonplace in the 20th century. In addition, there are examples of economies that do not emphasize science research that have been shown to be technological leaders despite this. For example, the United States relied on the scientific output of Europe in the early 20th century, though it was regarded as a leader in innovation. Another example is the technological advancement of Japan in the latter part of the same century, which emphasized more applied science (directly applicable to technology). Though the link between science and technology has need for more clarity, what is known is that a society without sufficient building blocks to encourage this link are critical. A nation without emphasis on science is likely to eventually stagnate technologically and risk losing competitive advantage. The most critical areas for focus by policymakers are discouraging too",
    "label": 0
  },
  {
    "text": "a society without sufficient building blocks to encourage this link are critical. A nation without emphasis on science is likely to eventually stagnate technologically and risk losing competitive advantage. The most critical areas for focus by policymakers are discouraging too many protections on job security, leading to less mobility of the workforce, encouraging the reliable availability of sufficient low-cost capital for investment in R&D, by favorable economic and tax policies, and supporting higher education in the sciences to produce scientists and engineers. Sociological factors and effects Values The implementation of technology influences the values of a society by changing expectations and realities. The implementation of technology is also influenced by values. There are (at least) three major, interrelated values that inform, and are informed by, technological innovations: Mechanistic world view: Viewing the universe as a collection of parts (like a machine), that can be individually analyzed and understood. This is a form of reductionism that is rare nowadays. However, the \"neo-mechanistic world view\" holds that nothing in the universe cannot be understood by the human intellect. Also, while all things are greater than the sum of their parts (e.g., even if we consider nothing more than the information involved in their combination), in principle, even this excess must eventually be understood by human intelligence. That is, no divine or vital principle or essence is involved. Efficiency: A value, originally applied only to machines, but now applied to all aspects of society, so that each element is expected to attain a higher and higher percentage of its maximal possible performance, output, or ability. Social progress: The belief that there is such a thing as social progress, and that, in the main, it is beneficent. Before the Industrial Revolution, and the subsequent explosion of technology, almost all societies believed in a cyclical",
    "label": 0
  },
  {
    "text": "or ability. Social progress: The belief that there is such a thing as social progress, and that, in the main, it is beneficent. Before the Industrial Revolution, and the subsequent explosion of technology, almost all societies believed in a cyclical theory of social movement and, indeed, of all history and the universe. This was, obviously, based on the cyclicity of the seasons, and an agricultural economy's and society's strong ties to that cyclicity. Since much of the world is closer to their agricultural roots, they are still much more amenable to cyclicity than progress in history. This may be seen, for example, in Prabhat Rainjan Sarkar's modern social cycles theory. For a more westernized version of social cyclicity, see Generations: The History of America's Future, 1584 to 2069 (Paperback) by Neil Howe and William Strauss; Harper Perennial; Reprint edition (September 30, 1992); ISBN 0-688-11912-3, and subsequent books by these authors. Institutions and groups Technology often enables organizational and bureaucratic group structures that otherwise and heretofore were simply not possible. Examples of this might include: The rise of very large organizations: e.g., governments, the military, health and social welfare institutions, supranational corporations. The commercialization of leisure: sports events, products, etc. (McGinn) The almost instantaneous dispersal of information (especially news) and entertainment around the world. International Technology enables greater knowledge of international issues, values, and cultures. Due mostly to mass transportation and mass media, the world seems to be a much smaller place, due to the following: Globalization of ideas Embeddedness of values Population growth and control Environment Technology can provide understanding of and appreciation for the world around us, enable sustainability and improve environmental conditions but also degrade the environment and facilitate unsustainability. Some polities may conclude that certain technologies' environmental detriments and other risks to outweigh their benefits, especially if",
    "label": 0
  },
  {
    "text": "of and appreciation for the world around us, enable sustainability and improve environmental conditions but also degrade the environment and facilitate unsustainability. Some polities may conclude that certain technologies' environmental detriments and other risks to outweigh their benefits, especially if or once substitutive technologies have been or can be invented, leading to directed technological phase-outs such as the fossil fuel phase-out and the nuclear fission power phase-out. Most modern technological processes produce unwanted byproducts in addition to the desired products, which are known as waste and pollution. While material waste is often re-used in industrial processes, many processes lead to a release into the environment with negative environmental side effects, such as pollution and lack of sustainability. Development and technologies' implications Some technologies are designed specifically with the environment in mind, but most are designed first for financial or economic effects such as the free market's profit motive. The effects of a specific technology is often not only dependent on how it is used – e.g. its usage context – but also predetermined by the technology's design or characteristics, as in the theory of \"the medium is the message\" which relates to media-technologies in specific. In many cases, such predetermined or built-in implications may vary depending on factors of contextual contemporary conditions such as human biology, international relations and socioeconomics. However, many technologies may be harmful to the environment only when used in specific contexts or for specific purposes that not necessarily result from the nature of the technology. Values Historically, from the perspective of economic agent-centered responsibility, an increased, as of 2021 commonly theoretic and informal, value of healthy environments and more efficient productive processes may be the result of an increase in the wealth of society. Once people are able to provide for their basic needs, they can",
    "label": 0
  },
  {
    "text": "as of 2021 commonly theoretic and informal, value of healthy environments and more efficient productive processes may be the result of an increase in the wealth of society. Once people are able to provide for their basic needs, they can – and are often facilitated to – not only afford more environmentally destructive products and services, but could often also be able to put an – e.g. individual morality-motivated – effort into valuing less tangible goods such as clean air and water if product-, alternatives-, consequences- and services-information are adequate. From the perspective of systems science and cybernetics, economies (systems) have economic actors and sectors make decisions based upon a range of system-internal factors with structures – or sometimes forms of leveraging existing structures – that lead to other outcomes being the result of other architectures – or systems-level configurations of the existing designs – which are considered to be possible in the sense that such could be modeled, tested, priorly assessed, developed and studied. Negative effects on the environment The effects of technology on the environment are both obvious and subtle. The more obvious effects include the depletion of nonrenewable natural resources (such as petroleum, coal, ores, and precious metals), and the added pollution of air, water, and land. The more subtle effects may include long-term effects (e.g. global warming, deforestation, natural habitat destruction, and coastal wetland loss.) Pollution and energy requirements Each wave of technology creates a set of waste previously unknown by humans: toxic waste, radioactive waste, electronic waste, plastic waste, and space waste. Electronic waste creates direct environmental impacts through the production and maintaining the infrastructure necessary for using technology and indirect impacts by breaking barriers for global interaction through the use of information and communications technology. Certain usages of information technology and infrastructure maintenance consume",
    "label": 0
  },
  {
    "text": "direct environmental impacts through the production and maintaining the infrastructure necessary for using technology and indirect impacts by breaking barriers for global interaction through the use of information and communications technology. Certain usages of information technology and infrastructure maintenance consume energy that contributes global warming. This includes software-designs such as international cryptocurrencies and most hardware powered by nonrenewable sources. One of the main problems is the lack of societal decision-making processes – such as the contemporary economy and politics – that lead to sufficient implementation of existing as well as potential efficient ways to remove, recycle and prevent these pollutants on a large scale expediently. Digital technologies, however, are important in achieving the green transition and specifically, the SDGs and European Green Deal's environmental targets. Emerging digital technologies, if correctly applied, have the potential to play a critical role in addressing environmental issues. A few examples are: smart city mobility, precision agriculture, sustainable supply chains, environmental monitoring, and catastrophe prediction. Construction and shaping Choice Society also controls technology through the choices it makes. These choices not only include consumer demands; they also include: the channels of distribution, how do products go from raw materials to consumption to disposal; the cultural beliefs regarding style, freedom of choice, consumerism, materialism, etc.; the economic values we place on the environment, individual wealth, government control, capitalism, etc. According to Williams and Edge, the construction and shaping of technology includes the concept of choice (and not necessarily conscious choice). Choice is inherent in both the design of individual artifacts and systems, and in the making of those artifacts and systems. The idea here is that a single technology may not emerge from the unfolding of a predetermined logic or a single determinant, technology could be a garden of forking paths, with different paths potentially leading",
    "label": 0
  },
  {
    "text": "of those artifacts and systems. The idea here is that a single technology may not emerge from the unfolding of a predetermined logic or a single determinant, technology could be a garden of forking paths, with different paths potentially leading to different technological outcomes. This is a position that has been developed in detail by Judy Wajcman. Therefore, choices could have differing implications for society and for particular social groups. Autonomous technology In one line of thought, technology develops autonomously, in other words, technology seems to feed on itself, moving forward with a force irresistible by humans. To these individuals, technology is \"inherently dynamic and self-augmenting.\" Jacques Ellul is one proponent of the irresistibleness of technology to humans. He espouses the idea that humanity cannot resist the temptation of expanding our knowledge and our technological abilities. However, he does not believe that this seeming autonomy of technology is inherent. But the perceived autonomy is because humans do not adequately consider the responsibility that is inherent in technological processes. Langdon Winner critiques the idea that technological evolution is essentially beyond the control of individuals or society in his book Autonomous Technology. He argues instead that the apparent autonomy of technology is a result of \"technological somnambulism,\" the tendency of people to uncritically and unreflectively embrace and utilize new technologies without regard for their broader social and political effects. In 1980, Mike Cooley published a critique of the automation and computerisation of engineering work under the title \"Architect or Bee? The human/technology relationship\". The title alludes to a comparison made by Karl Marx, on the issue of the creative achievements of human imaginative power. According to Cooley \"\"Scientific and technological developments have invariably proved to be double-edged. They produced the beauty of Venice and the hideousness of Chernobyl; the caring therapies of",
    "label": 0
  },
  {
    "text": "Marx, on the issue of the creative achievements of human imaginative power. According to Cooley \"\"Scientific and technological developments have invariably proved to be double-edged. They produced the beauty of Venice and the hideousness of Chernobyl; the caring therapies of Rontgen's X-rays and the destruction of Hiroshima,\" Government Individuals rely on governmental assistance to control the side effects and negative consequences of technology. Supposed independence of government. An assumption commonly made about the government is that their governance role is neutral or independent. However, some argue that governing is a political process, so government will be influenced by political winds of influence. In addition, because government provides much of the funding for technological research and development, it has a vested interest in certain outcomes. Other point out that the world's biggest ecological disasters, such as the Aral Sea, Chernobyl, and Lake Karachay have been caused by government projects, which are not accountable to consumers. Liability. One means for controlling technology is to place responsibility for the harm with the agent causing the harm. Government can allow more or less legal liability to fall to the organizations or individuals responsible for damages. Legislation. A source of controversy is the role of industry versus that of government in maintaining a clean environment. While it is generally agreed that industry needs to be held responsible when pollution harms other people, there is disagreement over whether this should be prevented by legislation or civil courts, and whether ecological systems as such should be protected from harm by governments. Recently, the social shaping of technology has had new influence in the fields of e-science and e-social science in the United Kingdom, which has made centers focusing on the social shaping of science and technology a central part of their funding programs. See also References Sources",
    "label": 0
  },
  {
    "text": "has had new influence in the fields of e-science and e-social science in the United Kingdom, which has made centers focusing on the social shaping of science and technology a central part of their funding programs. See also References Sources Katz, Mark (2010). Capturing Sound: How Technology Has Changed Music. University of California Press. ISBN 9780520261051. McGinn, Robert E. (1991). Science, Technology, and Society. Englewood Cliffs, N.J.: Prentice-Hall. ISBN 0-13-794736-4. Puricelli, F (2011). \"Early Twentieth Century Transportation Technology and the Creation of Modern American Culture \" (PDF). Archived from the original (PDF) on 2014-11-03. Rückriem, F (2009). Digital technology and mediation: A challenge to activity theory. Learning and expanding with activity theory'. Cambridge University Press. ISBN 9780521760751. Williams, Robin; Edge, David (1996). \"What is the Social Shaping of Technology? (The Introduction to paper \"The Social Shaping of Technology\".)\". Research Policy 25. Archived from the original on September 17, 2006. Retrieved August 10, 2006. Further reading Adas, Michael (1989). Machines as the Measure of Men: Science, Technology, and Ideologies of Western Dominance. Ithaca: Cornell University Press. ISBN 0-8014-2303-1. Bereano, P. (1977). Technology as a Social and Political Phenomenon. Wiley & Sons, ISBN 0471068756. Castells, Manuel (2009). The Rise of the Network Society (2nd ed.). Oxford, UK.: Wiley-Blackwell. ISBN 978-1405196864. Cochrane, T; Bateman, R (2010). \"Smartphones give you wings: Pedagogical affordances of mobile Web 2.0\". Australasian Journal of Educational Technology. Archived from the original on 2016-03-15. Retrieved 2014-11-03. Dickson, D. (1977). Politics of Alternative Technology. Universe Publisher, ISBN 0876639171. Easton, T. (2011). Taking Sides: Clashing Views in Science, Technology, and Society. McGraw-Hill/Dushkin, ISBN 0078050278. Harrington, Jan L. (2008). Technology and Society. Sudbury, Massachusetts: Jones and Bartlett. ISBN 9781449673918. (Google Books preview) Huesemann, Michael H., and Joyce A. Huesemann (2011). Technofix: Why Technology Won't Save Us or the Environment, New Society Publishers, Gabriola",
    "label": 0
  },
  {
    "text": "ISBN 0078050278. Harrington, Jan L. (2008). Technology and Society. Sudbury, Massachusetts: Jones and Bartlett. ISBN 9781449673918. (Google Books preview) Huesemann, Michael H., and Joyce A. Huesemann (2011). Technofix: Why Technology Won't Save Us or the Environment, New Society Publishers, Gabriola Island, British Columbia, Canada, ISBN 0865717044, 464 pp. Andrey Korotayev, Artemy Malkov, and Daria Khaltourina. Introduction to Social Macrodynamics: Compact Macromodels of the World System Growth ISBN 5-484-00414-4 ] MacKenzie, D., and J. Wajcman. (1999). The Social Shaping of Technology. McGraw Hill Education, ISBN 0335199135. Mesthene, E.G. (1970). Technological Change: Its Impact on Man and Society. Harvard University Press, ISBN 0674872355. Mumford, L. (2010). Technics and Civilization. University of Chicago Press, ISBN 0226550273. Noble, David F. (1984), Forces of Production: A Social History of Industrial Automation, New York, New York, US: Knopf, ISBN 978-0-394-51262-4, LCCN 83048867. Postman, N. (1993). Technopoly: The Surrender of Culture to Technology. Vintage, ISBN 0679745408. Sclove, R.E. (1995). Democracy and Technology. The Guilford Press, ISBN 089862861X. Dan Senor and Saul Singer, Start-up Nation: The Story of Israel's Economic Miracle, Hachette Book Group, New York, (2009) ISBN 0-446-54146-X Shaw, Jeffrey M. (2014). Illusions of Freedom: Thomas Merton and Jacques Ellul on Technology and the Human Condition. Eugene, OR: Wipf and Stock. ISBN 978-1625640581. Sicilia, David B.; Wittner, David G. Strands of Modernization: The Circulation of Technology and Business Practices in East Asia, 1850–1920 (University of Toronto Press, 2021) online review Smil, Vaclav (1994). Energy in World History. Boulder: Westview Press. pp. 259–267. ISBN 0-8133-1901-3. Cited at Technology Chronology (accessed September 11, 2005). Volti, Rudi (2017). society and technological change. New York: Worth. p. 3. ISBN 9781319058258. Winston, Morton (2003). \"Children of invention\". In Morton Winston; Ralph Edelbach (eds.). Society, Ethics, and Technology (2nd ed.). Belmont, Calif.: Thomson/Wadsworth. ISBN 0-534-58540-X. Ruckenstein, Minna (2023). The Feel of Algorithms (University",
    "label": 0
  },
  {
    "text": "Technosignature or technomarker is any measurable property or effect that provides scientific evidence of past or present technology. Technosignatures are analogous to biosignatures, which signal the presence of life, whether intelligent or not. Some authors prefer to exclude radio transmissions from the definition, but such restrictive usage is not widespread. Jill Tarter has proposed that the search for extraterrestrial intelligence (SETI) be renamed \"the search for technosignatures\". Various types of technosignatures, such as radiation leakage from megascale astroengineering installations such as Dyson spheres, the light from an extraterrestrial ecumenopolis, or Shkadov thrusters with the power to alter the orbits of stars around the Galactic Center, may be detectable with hypertelescopes. Some examples of technosignatures are described in Paul Davies's 2010 book The Eerie Silence, although the terms \"technosignature\" and \"technomarker\" do not appear in the book. In February 2023, astronomers reported, after scanning 820 stars, the detection of 8 possible technosignatures for follow-up studies. Astroengineering projects A Dyson sphere, constructed by life forms dwelling in proximity to a Sun-like star, would cause an increase in the amount of infrared radiation in the star system's emitted spectrum. Hence, Freeman Dyson selected the title \"Search for Artificial Stellar Sources of Infrared Radiation\" for his 1960 paper on the subject. SETI has adopted these assumptions in its search, looking for such \"infrared heavy\" spectra from solar analogs. Since 2005, Fermilab has conducted an ongoing survey for such spectra, analyzing data from the Infrared Astronomical Satellite. Identifying one of the many infra-red sources as a Dyson sphere would require improved techniques for discriminating between a Dyson sphere and natural sources. Fermilab discovered 17 \"ambiguous\" candidates, of which four have been named \"amusing but still questionable\". Other searches also resulted in several candidates, which remain unconfirmed. In October 2012, astronomer Geoff Marcy, one of the",
    "label": 0
  },
  {
    "text": "a Dyson sphere and natural sources. Fermilab discovered 17 \"ambiguous\" candidates, of which four have been named \"amusing but still questionable\". Other searches also resulted in several candidates, which remain unconfirmed. In October 2012, astronomer Geoff Marcy, one of the pioneers of the search for extrasolar planets, was given a research grant to search data from the Kepler telescope, with the aim of detecting possible signs of Dyson spheres. Orbital paths, transit signatures, stellar activity and star-system composition Shkadov thrusters, with the hypothetical ability to change the orbital paths of stars in order to avoid various dangers to life such as cold molecular clouds or cometary impacts, would also be detectable in a similar fashion to the transiting extrasolar planets searched by Kepler. Unlike planets, though, the thrusters would appear to abruptly stop over the surface of a star rather than crossing it completely, revealing their technological origin. In addition, evidence of targeted extrasolar asteroid mining may also reveal extraterrestrial intelligence (ETI). Furthermore, it has been suggested that information could be hidden within the transit signatures of other planets. Advanced civilizations could \"cloak their presence, or deliberately broadcast it, through controlled laser emission\". Other characteristics proposed as potential technosignatures (or starting points for detection of clearer signatures) include peculiar orbital periods such as arranging planets in prime number patterns. Coronal and chromospheric activity on stars might be altered. Extraterrestrial civilizations may use free-floating planets (rogue planets) for interstellar transportation with a number of proposed possible technosignatures. Communication networks A study suggests that if ETs exist, they may have established communications network(s) and may already have probes in the Solar System whose communication may be detectable. Studies by John Gertz suggest flyby (scout) probes might intermittently surveil nascent planetary systems and permanent probes would communicate with a home base, potentially using",
    "label": 0
  },
  {
    "text": "network(s) and may already have probes in the Solar System whose communication may be detectable. Studies by John Gertz suggest flyby (scout) probes might intermittently surveil nascent planetary systems and permanent probes would communicate with a home base, potentially using triggers and conditions such as detection of electromagnetic leakage or biosignatures. They also suggest several strategies to detecting local ET probes such as detecting emitted optical messages. He also finds that due to interstellar networks of communications nodes, the search for deliberate interstellar signals – as is common in SETI – may be futile. The architecture may consist of nodes separated by sub-light-year distances and strung out between neighboring stars. It may also contain pulsars as beacons or nodes whose beams are modulated by mechanisms that could be searched for. Moreover, a study suggests prior searches wouldn't have detected cost-effective electromagnetic signal beacons. Planetary analysis Artificial heat and light Various astronomers, including Avi Loeb of the Harvard-Smithsonian Center for Astrophysics. Edwin L. Turner of Princeton University, and Thomas Beatty of the University of Wisconsin have proposed that artificial light from extraterrestrial planets, such as that originating from cities, industries, and transport networks, could be detected and signal the presence of an advanced civilization. Light and heat detected from planets must be distinguished from natural sources to conclusively prove the existence of intelligent life on a planet. For example, NASA's 2012 Black Marble experiment showed that significant stable light and heat sources on Earth, such as chronic wildfires in arid Western Australia, originate from uninhabited areas and are naturally occurring. Spectroscopic observations of exoplanet nightsides would be able to identify artificial lighting via its distinct spectroscopic signature. Work by astronomer Thomas Beatty has shown that the spectrally concentrated emission from sodium street lights would be distinguishable from natural sources using proposed",
    "label": 0
  },
  {
    "text": "observations of exoplanet nightsides would be able to identify artificial lighting via its distinct spectroscopic signature. Work by astronomer Thomas Beatty has shown that the spectrally concentrated emission from sodium street lights would be distinguishable from natural sources using proposed next generation space telescopes. The proposed Large Ultraviolet Optical Infrared Surveyor may be able to detect city lights twelve times those of Earth on Proxima b in 300 hours. Atmospheric analysis Atmospheric analysis of planetary atmospheres, as is already done on various Solar System bodies and in a rudimentary fashion on several hot Jupiter extrasolar planets, may reveal the presence of chemicals produced by technological civilizations. For example, atmospheric emissions from human technology use on Earth, including nitrogen dioxide and chlorofluorocarbons, are detectable from space. Artificial air pollution may therefore be detectable on extrasolar planets and on Earth via \"atmospheric SETI\" – including NO2 pollution levels and with telescopic technology close to today. Such technosignatures may consist not of the detection of the level of one specific chemical but simultaneous detections of levels of multiple specific chemicals in atmospheres. However, there remains a possibility of mis-detection; for example, the atmosphere of Titan has detectable signatures of complex chemicals that are similar to what on Earth are industrial pollutants, though not the byproduct of civilisation. Some SETI scientists have proposed searching for artificial atmospheres created by planetary engineering to produce habitable environments for colonisation by an ETI. Extraterrestrial artifacts, influence and spacecraft Spacecraft Interstellar spacecraft may be detectable from hundreds to thousands of light-years away through various forms of radiation, such as the photons emitted by an antimatter rocket or cyclotron radiation from the interaction of a magnetic sail with the interstellar medium. Such a signal would be easily distinguishable from a natural signal and could hence firmly establish the existence",
    "label": 0
  },
  {
    "text": "as the photons emitted by an antimatter rocket or cyclotron radiation from the interaction of a magnetic sail with the interstellar medium. Such a signal would be easily distinguishable from a natural signal and could hence firmly establish the existence of extraterrestrial life, were it to be detected. In addition, smaller Bracewell probes within the Solar System itself may also be detectable by means of optical or radio searches. Self-replicating spacecraft or their communications networks could potentially be detectable within the Solar system or in nearby star-based systems, if they are located there. Such technologies or their footprints could be in Earth's orbit, on the Moon or on the Earth. Satellites A less advanced technology, and one closer to humanity's current technological level, is the Clarke Exobelt proposed by Astrophysicist Hector Socas-Navarro of the Instituto de Astrofisica de Canarias. This hypothetical belt would be formed by all the artificial satellites occupying geostationary/geosynchronous orbits around an exoplanet. From early simulations it appeared that a very dense satellite belt, requiring only a moderately more-advanced civilization than ours, would be detectable with existing technology in the light curves from transiting exoplanets, but subsequent analysis has questioned this result, suggesting that exobelts detectable by current and upcoming missions will be very rare. Extraterrestrial influence or activity on Earth It has been suggested that once extraterrestrials arrive \"at a new home, such life will almost certainly create technosignatures (because it used technology to get there), and some fraction of them may also eventually give rise to a new biosphere\". Microorganism DNA may have been used for self-replicating messages. See also: DNA digital data storage On exoplanets Low- or high-albedo installations such as solar panels may also be detectable, albeit distinguishing artificial megastructures from high- and low-albedo natural environments (e.g., bright ice caps) may make it",
    "label": 0
  },
  {
    "text": "for self-replicating messages. See also: DNA digital data storage On exoplanets Low- or high-albedo installations such as solar panels may also be detectable, albeit distinguishing artificial megastructures from high- and low-albedo natural environments (e.g., bright ice caps) may make it unfeasible. Scientific projects searching for technosignatures One of the first attempts to search for Dyson Spheres was made by Vyacheslav Slysh from the Russian Space Research Institute in Moscow in 1985 using data from the Infrared Astronomical Satellite (IRAS). Another search for technosignatures, c. 2001, involved an analysis of data from the Compton Gamma Ray Observatory for traces of anti-matter, which, besides one \"intriguing spectrum probably not related to SETI\", came up empty. In 2005, Fermilab had an ongoing survey for such spectra by analyzing data from IRAS. Identifying one of the many infra-red sources as a Dyson Sphere would require improved techniques for discriminating between a Dyson Sphere and natural sources. Fermilab discovered 17 potential \"ambiguous\" candidates of which four have been named \"amusing but still questionable\". Other searches also resulted in several candidates, which are, however, unconfirmed. In a 2005 paper, Luc Arnold proposed a means of detecting planetary-sized artifacts from their distinctive transit light curve signature. He showed that such technosignature was within the reach of space missions aimed at detecting exoplanets by the transit method, as were Corot or Kepler projects at that time. The principle of the detection remains applicable for future exoplanets missions. In 2012, a trio of astronomers led by Jason Wright started a two-year search for Dyson Spheres, aided by grants from the Templeton Foundation. In 2013, Geoff Marcy received funding to use data from the Kepler Telescope to search for Dyson Spheres and interstellar communication using lasers, and Lucianne Walkowicz received funding to detect artificial signatures in stellar photometry. Starting in",
    "label": 0
  },
  {
    "text": "the Templeton Foundation. In 2013, Geoff Marcy received funding to use data from the Kepler Telescope to search for Dyson Spheres and interstellar communication using lasers, and Lucianne Walkowicz received funding to detect artificial signatures in stellar photometry. Starting in 2016, astronomer Jean-Luc Margot of UCLA has been searching for technosignatures with large radio telescopes. Vanishing stars In 2016, it was proposed that vanishing stars are a plausible technosignature. A pilot project searching for vanishing stars was carried out, finding one candidate object. In 2019, the Vanishing & Appearing Sources during a Century of Observations (VASCO) project began more general searches for vanishing and appearing stars, and other astrophysical transients They identified 100 red transients of \"most likely natural origin\", while analyzing 15% of the image data. In 2020, the VASCO collaboration started up a citizen science project, vetting through images of many thousands of candidate objects. The citizen science project is carried out in close collaboration with schools and amateur associations mainly in African countries. The VASCO project has been referred to as \"Perhaps the most general artefact search to date\". In 2021, VASCO's principal investigator Beatriz Villarroel received a L'Oreal-Unesco prize in Sweden for the project. In June 2021, the collaboration published the discovery of nine light sources seemingly appearing and vanishing simultaneously from archival plates taken in 1950. Villarroel's team also found three 16th magnitude stars which had vanished on plates exposed within one hour of each other on 19 July 1952. Organization of novel projects In June 2020, NASA was awarded their first SETI-specific grant in three decades. The grant funds the first NASA-funded search for technosignatures from advanced extraterrestrial civilizations other than radio waves, including the creation and population of an online technosignature library. A 2021 scientific review produced by the i.a. NASA-sponsored online workshop",
    "label": 0
  },
  {
    "text": "three decades. The grant funds the first NASA-funded search for technosignatures from advanced extraterrestrial civilizations other than radio waves, including the creation and population of an online technosignature library. A 2021 scientific review produced by the i.a. NASA-sponsored online workshop TechnoClimes 2020 classified possible optimal mission concepts for the search of technosignatures. It evaluates signatures based on a metric about the distance of humanity to the capacity of developing the signature's required technology – a comparison to contemporary human technology footprints, associated methods of detection and ancillary benefits of their search for other astronomy. The study's conclusions include a robust rationale for organizing missions for searching artifacts – including probes – within the Solar system. In 2021, astronomers proposed a sequence of \"verification checks for narrowband technosignature signals\" after concluding that technosignature candidate BLC1 could be the result of a form of local radiofrequency interference. It has been suggested that observatories on the Moon could be more successful. In 2022, scientists provided an overview of the capabilities of ongoing, recent, past, planned and proposed missions and observatories for detecting various alien technosignatures. Implications of detection Steven J. Dick states that there generally are no principles for dealing with successful SETI detections. Detections of technosignatures may have ethical implications, such as conveying information related to astroethical and related machine ethics ones (e.g., related to machines' applied ethical values), or include information about alien societies or histories or fates, which may vary depending on the type, prevalence and form of the detected signature's technology. Moreover, various types of information about detected technosignatures and their distribution or dissemination may have varying implications that may also depend on time and context. Beyond astronomy and astrobiology, detection would raise significant methodological questions about how we search for life. Research into detection windows suggests that advancing",
    "label": 0
  },
  {
    "text": "Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth, origin, evolution, and distribution of life. Central to biology are five fundamental themes: the cell as the basic unit of life, genes and heredity as the basis of inheritance, evolution as the driver of biological diversity, energy transformation for sustaining life processes, and the maintenance of internal stability (homeostasis). Biology examines life across multiple levels of organization, from molecules and cells to organisms, populations, and ecosystems. Subdisciplines include molecular biology, physiology, ecology, evolutionary biology, developmental biology, and systematics, among others. Each of these fields applies a range of methods to investigate biological phenomena, including observation, experimentation, and mathematical modeling. Modern biology is grounded in the theory of evolution by natural selection, first articulated by Charles Darwin, and in the molecular understanding of genes encoded in DNA. The discovery of the structure of DNA and advances in molecular genetics have transformed many areas of biology, leading to applications in medicine, agriculture, biotechnology, and environmental science. Life on Earth is believed to have originated over 3.7 billion years ago. Today, it includes a vast diversity of organisms—from single-celled archaea and bacteria to complex multicellular plants, fungi, and animals. Biologists classify organisms based on shared characteristics and evolutionary relationships, using taxonomic and phylogenetic frameworks. These organisms interact with each other and with their environments in ecosystems, where they play roles in energy flow and nutrient cycling. As a constantly evolving field, biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss. Etymology From Greek βίος (bíos) 'life', (from Proto-Indo-European root *gwei-, to",
    "label": 0
  },
  {
    "text": "biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss. Etymology From Greek βίος (bíos) 'life', (from Proto-Indo-European root *gwei-, to live) and λογία (logia) 'study of'. The compound appears in the title of Volume 3 of Michael Christoph Hanow's Philosophiae naturalis sive physicae dogmaticae: Geologia, biologia, phytologia generalis et dendrologia, published in 1766. The term biology in its modern sense appears to have been introduced independently by Thomas Beddoes (in 1799), Karl Friedrich Burdach (in 1800), Gottfried Reinhold Treviranus (Biologie oder Philosophie der lebenden Natur, 1802) and Jean-Baptiste Lamarck (Hydrogéologie, 1802). History The earliest of roots of science, which included medicine, can be traced to ancient Egypt and Mesopotamia in around 3000 to 1200 BCE. Their contributions shaped ancient Greek natural philosophy. Ancient Greek philosophers such as Aristotle (384–322 BCE) contributed extensively to the development of biological knowledge. He explored biological causation and the diversity of life. His successor, Theophrastus, began the scientific study of plants. Scholars of the medieval Islamic world who wrote on biology included al-Jahiz (781–869), Al-Dīnawarī (828–896), who wrote on botany, and Rhazes (865–925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought. Biology began to quickly develop with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop techniques of microscopic dissection and staining. Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838,",
    "label": 0
  },
  {
    "text": "new interest in entomology and helped to develop techniques of microscopic dissection and staining. Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells, continuing to support spontaneous generation. However, Robert Remak and Rudolf Virchow were able to reify the third tenet, and by the 1860s most biologists accepted all three tenets which consolidated into cell theory. Meanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735, and in the 1750s introduced scientific names for all his species. Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleable—even suggesting the possibility of common descent. Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who presented a coherent theory of evolution. The British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions. The basis for modern genetics began with the work of Gregor Mendel in 1865. This outlined the principles of biological inheritance. However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics. In the 1940s and early",
    "label": 0
  },
  {
    "text": "the principles of biological inheritance. However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics. In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s onwards, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. The Human Genome Project was launched in 1990 to map the human genome. Chemical basis Atoms and molecules All organisms are made up of chemical elements; oxygen, carbon, hydrogen, and nitrogen account for most (96%) of the mass of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting essentially all the remainder. Different elements can combine to form compounds such as water, which is fundamental to life. Biochemistry is the study of chemical processes within and relating to living organisms. Molecular biology is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions. Water Life arose from the Earth's first ocean, which formed some 3.8 billion years ago. Since then, water continues to be the most abundant molecule in every organism. Water is important to life because it is an effective solvent, capable of dissolving solutes such",
    "label": 0
  },
  {
    "text": "the Earth's first ocean, which formed some 3.8 billion years ago. Since then, water continues to be the most abundant molecule in every organism. Water is important to life because it is an effective solvent, capable of dissolving solutes such as sodium and chloride ions or other small molecules to form an aqueous solution. Once dissolved in water, these solutes are more likely to come in contact with one another and therefore take part in chemical reactions that sustain life. In terms of its molecular structure, water is a small polar molecule with a bent shape formed by the polar covalent bonds of two hydrogen (H) atoms to one oxygen (O) atom (H2O). Because the O–H bonds are polar, the oxygen atom has a slight negative charge and the two hydrogen atoms have a slight positive charge. This polar property of water allows it to attract other water molecules via hydrogen bonds, which makes water cohesive. Surface tension results from the cohesive force due to the attraction between molecules at the surface of the liquid. Water is also adhesive as it is able to adhere to the surface of any polar or charged non-water molecules. Water is denser as a liquid than it is as a solid (or ice). This unique property of water allows ice to float above liquid water such as ponds, lakes, and oceans, thereby insulating the liquid below from the cold air above. Water has the capacity to absorb energy, giving it a higher specific heat capacity than other solvents such as ethanol. Thus, a large amount of energy is needed to break the hydrogen bonds between water molecules to convert liquid water into water vapor. As a molecule, water is not completely stable as each water molecule continuously dissociates into hydrogen and hydroxyl ions before",
    "label": 0
  },
  {
    "text": "amount of energy is needed to break the hydrogen bonds between water molecules to convert liquid water into water vapor. As a molecule, water is not completely stable as each water molecule continuously dissociates into hydrogen and hydroxyl ions before reforming into a water molecule again. In pure water, the number of hydrogen ions balances (or equals) the number of hydroxyl ions, resulting in a pH that is neutral. Organic compounds Organic compounds are molecules that contain carbon bonded to another element such as hydrogen. With the exception of water, nearly all the molecules that make up each organism contain carbon. Carbon can form covalent bonds with up to four other atoms, enabling it to form diverse, large, and complex molecules. For example, a single carbon atom can form four single covalent bonds such as in methane, two double covalent bonds such as in carbon dioxide (CO2), or a triple covalent bond such as in carbon monoxide (CO). Moreover, carbon can form very long chains of interconnecting carbon–carbon bonds such as octane or ring-like structures such as glucose. The simplest form of an organic molecule is the hydrocarbon, which is a large family of organic compounds that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other elements such as oxygen (O), hydrogen (H), phosphorus (P), and sulfur (S), which can change the chemical behavior of that compound. Groups of atoms that contain these elements (O-, H-, P-, and S-) and are bonded to a central carbon atom or skeleton are called functional groups. There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group. In 1953, the Miller–Urey experiment showed that organic compounds could be",
    "label": 0
  },
  {
    "text": "are called functional groups. There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group. In 1953, the Miller–Urey experiment showed that organic compounds could be synthesized abiotically within a closed system mimicking the conditions of early Earth, thus suggesting that complex organic molecules could have arisen spontaneously in early Earth (see abiogenesis). Macromolecules Macromolecules are large molecules made up of smaller subunits or monomers. Monomers include sugars, amino acids, and nucleotides. Carbohydrates include monomers and polymers of sugars. Lipids are the only class of macromolecules that are not made up of polymers. They include steroids, phospholipids, and fats, largely nonpolar and hydrophobic (water-repelling) substances. Proteins are the most diverse of the macromolecules. They include enzymes, transport proteins, large signaling molecules, antibodies, and structural proteins. The basic unit (or monomer) of a protein is an amino acid. Twenty amino acids are used in proteins. Nucleic acids are polymers of nucleotides. Their function is to store, transmit, and express hereditary information. Cells Cell theory states that cells are the fundamental units of life, that all living things are composed of one or more cells, and that all cells arise from preexisting cells through cell division. Most cells are very small, with diameters ranging from 1 to 100 micrometers and are therefore only visible under a light or electron microscope. There are generally two types of cells: eukaryotic cells, which contain a nucleus, and prokaryotic cells, which do not. Prokaryotes are single-celled organisms such as bacteria, whereas eukaryotes can be single-celled or multicellular. In multicellular organisms, every cell in the organism's body is derived ultimately from a single cell in a fertilized egg. Cell structure Every cell is enclosed within a cell membrane that separates its cytoplasm from",
    "label": 0
  },
  {
    "text": "can be single-celled or multicellular. In multicellular organisms, every cell in the organism's body is derived ultimately from a single cell in a fertilized egg. Cell structure Every cell is enclosed within a cell membrane that separates its cytoplasm from the extracellular space. A cell membrane consists of a lipid bilayer, including cholesterols that sit between phospholipids to maintain their fluidity at various temperatures. Cell membranes are semipermeable, allowing small molecules such as oxygen, carbon dioxide, and water to pass through while restricting the movement of larger molecules and charged particles such as ions. Cell membranes also contain membrane proteins, including integral membrane proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer side of the cell membrane, acting as enzymes shaping the cell. Cell membranes are involved in various cellular processes such as cell adhesion, storing electrical energy, and cell signalling and serve as the attachment surface for several extracellular structures such as a cell wall, glycocalyx, and cytoskeleton. Within the cytoplasm of a cell, there are many biomolecules such as proteins and nucleic acids. In addition to biomolecules, eukaryotic cells have specialized structures called organelles that have their own lipid bilayers or are spatially units. These organelles include the cell nucleus, which contains most of the cell's DNA, or mitochondria, which generate adenosine triphosphate (ATP) to power cellular processes. Other organelles such as endoplasmic reticulum and Golgi apparatus play a role in the synthesis and packaging of proteins, respectively. Biomolecules such as proteins can be engulfed by lysosomes, another specialized organelle. Plant cells have additional organelles that distinguish them from animal cells such as a cell wall that provides support for the plant cell, chloroplasts that harvest sunlight energy to produce sugar, and vacuoles that provide storage and structural",
    "label": 0
  },
  {
    "text": "specialized organelle. Plant cells have additional organelles that distinguish them from animal cells such as a cell wall that provides support for the plant cell, chloroplasts that harvest sunlight energy to produce sugar, and vacuoles that provide storage and structural support as well as being involved in reproduction and breakdown of plant seeds. Eukaryotic cells also have cytoskeleton that is made up of microtubules, intermediate filaments, and microfilaments, all of which provide support for the cell and are involved in the movement of the cell and its organelles. In terms of their structural composition, the microtubules are made up of tubulin (e.g., α-tubulin and β-tubulin) whereas intermediate filaments are made up of fibrous proteins. Microfilaments are made up of actin molecules that interact with other strands of proteins. Metabolism All cells require energy to sustain cellular processes. Metabolism is the set of chemical reactions in an organism. The three main purposes of metabolism are: the conversion of food to energy to run cellular processes; the conversion of food/fuel to monomer building blocks; and the elimination of metabolic wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. Metabolic reactions may be categorized as catabolic—the breaking down of compounds (for example, the breaking down of glucose to pyruvate by cellular respiration); or anabolic—the building up (synthesis) of compounds (such as proteins, carbohydrates, lipids, and nucleic acids). Usually, catabolism releases energy, and anabolism consumes energy. The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, each step being facilitated by a specific enzyme. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous",
    "label": 0
  },
  {
    "text": "of steps into another chemical, each step being facilitated by a specific enzyme. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts—they allow a reaction to proceed more rapidly without being consumed by it—by reducing the amount of activation energy needed to convert reactants into products. Enzymes also allow the regulation of the rate of a metabolic reaction, for example in response to changes in the cell's environment or to signals from other cells. Cellular respiration Cellular respiration is a set of metabolic reactions and processes that take place in cells to convert chemical energy from nutrients into adenosine triphosphate (ATP), and then release waste products. The reactions involved in respiration are catabolic reactions, which break large molecules into smaller ones, releasing energy. Respiration is one of the key ways a cell releases chemical energy to fuel cellular activity. The overall reaction occurs in a series of biochemical steps, some of which are redox reactions. Although cellular respiration is technically a combustion reaction, it clearly does not resemble one when it occurs in a cell because of the slow, controlled release of energy from the series of reactions. Sugar in the form of glucose is the main nutrient used by animal and plant cells in respiration. Cellular respiration involving oxygen is called aerobic respiration, which has four stages: glycolysis, citric acid cycle (or Krebs cycle), electron transport chain, and oxidative phosphorylation. Glycolysis is a metabolic process that occurs in the cytoplasm whereby glucose is converted into two pyruvates, with two net molecules of ATP being produced at the same time. Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also",
    "label": 0
  },
  {
    "text": "metabolic process that occurs in the cytoplasm whereby glucose is converted into two pyruvates, with two net molecules of ATP being produced at the same time. Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also generates NADH and carbon dioxide. Acetyl-CoA enters the citric acid cycle, which takes places inside the mitochondrial matrix. At the end of the cycle, the total yield from 1 glucose (or 2 pyruvates) is 6 NADH, 2 FADH2, and 2 ATP molecules. Finally, the next stage is oxidative phosphorylation, which in eukaryotes, occurs in the mitochondrial cristae. Oxidative phosphorylation comprises the electron transport chain, which is a series of four protein complexes that transfer electrons from one complex to another, thereby releasing energy from NADH and FADH2 that is coupled to the pumping of protons (hydrogen ions) across the inner mitochondrial membrane (chemiosmosis), which generates a proton motive force. Energy from the proton motive force drives the enzyme ATP synthase to synthesize more ATPs by phosphorylating ADPs. The transfer of electrons terminates with molecular oxygen being the final electron acceptor. If oxygen were not present, pyruvate would not be metabolized by cellular respiration but undergoes a process of fermentation. The pyruvate is not transported into the mitochondrion but remains in the cytoplasm, where it is converted to waste products that may be removed from the cell. This serves the purpose of oxidizing the electron carriers so that they can perform glycolysis again and removing the excess pyruvate. Fermentation oxidizes NADH to NAD+ so it can be re-used in glycolysis. In the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis. This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is",
    "label": 0
  },
  {
    "text": "the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis. This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is called lactic acid fermentation. In strenuous exercise, when energy demands exceed energy supply, the respiratory chain cannot process all of the hydrogen atoms joined by NADH. During anaerobic glycolysis, NAD+ regenerates when pairs of hydrogen combine with pyruvate to form lactate. Lactate formation is catalyzed by lactate dehydrogenase in a reversible reaction. Lactate can also be used as an indirect precursor for liver glycogen. During recovery, when oxygen becomes available, NAD+ attaches to hydrogen from lactate to form ATP. In yeast, the waste products are ethanol and carbon dioxide. This type of fermentation is known as alcoholic or ethanol fermentation. The ATP generated in this process is made by substrate-level phosphorylation, which does not require oxygen. Photosynthesis Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's metabolic activities via cellular respiration. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water. In most cases, oxygen is released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis, which is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth. Photosynthesis has four stages: Light absorption, electron transport, ATP synthesis, and carbon fixation. Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to",
    "label": 0
  },
  {
    "text": "and carbon fixation. Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to a primary electron acceptor, a quinone designated as Q. In the second stage, electrons move from the quinone primary electron acceptor through a series of electron carriers until they reach a final electron acceptor, which is usually the oxidized form of NADP+, which is reduced to NADPH, a process that takes place in a protein complex called photosystem I (PSI). The transport of electrons is coupled to the movement of protons (or hydrogen) from the stroma to the thylakoid membrane, which forms a pH gradient across the membrane as hydrogen becomes more concentrated in the lumen than in the stroma. This is analogous to the proton-motive force generated across the inner mitochondrial membrane in aerobic respiration. During the third stage of photosynthesis, the movement of protons down their concentration gradients from the thylakoid lumen to the stroma through the ATP synthase is coupled to the synthesis of ATP by that same ATP synthase. The NADPH and ATPs generated by the light-dependent reactions in the second and third stages, respectively, provide the energy and electrons to drive the synthesis of glucose by fixing atmospheric carbon dioxide into existing organic carbon compounds, such as ribulose bisphosphate (RuBP) in a sequence of light-independent (or dark) reactions called the Calvin cycle. Cell signaling Cell signaling (or communication) is the ability of cells to receive, process, and transmit signals with its environment and with itself. Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of",
    "label": 0
  },
  {
    "text": "process, and transmit signals with its environment and with itself. Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of another cell or located deep inside a cell. There are generally four types of chemical signals: autocrine, paracrine, juxtacrine, and hormones. In autocrine signaling, the ligand affects the same cell that releases it. Tumor cells, for example, can reproduce uncontrollably because they release signals that initiate their own self-division. In paracrine signaling, the ligand diffuses to nearby cells and affects them. For example, brain cells called neurons release ligands called neurotransmitters that diffuse across a synaptic cleft to bind with a receptor on an adjacent cell such as another neuron or muscle cell. In juxtacrine signaling, there is direct contact between the signaling and responding cells. Finally, hormones are ligands that travel through the circulatory systems of animals or vascular systems of plants to reach their target cells. Once a ligand binds with a receptor, it can influence the behavior of another cell, depending on the type of receptor. For instance, neurotransmitters that bind with an ionotropic receptor can alter the excitability of a target cell. Other types of receptors include protein kinase receptors (e.g., receptor for the hormone insulin) and G protein-coupled receptors. Activation of G protein-coupled receptors can initiate second messenger cascades. The process by which a chemical or physical signal is transmitted through a cell as a series of molecular events is called signal transduction. Cell cycle The cell cycle is a series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA and some of its organelles, and the subsequent partitioning",
    "label": 0
  },
  {
    "text": "The cell cycle is a series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA and some of its organelles, and the subsequent partitioning of its cytoplasm into two daughter cells in a process called cell division. In eukaryotes (i.e., animal, plant, fungal, and protist cells), there are two distinct types of cell division: mitosis and meiosis. Mitosis is part of the cell cycle, in which replicated chromosomes are separated into two new nuclei. Cell division gives rise to genetically identical cells in which the total number of chromosomes is maintained. In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often followed by telophase and cytokinesis; which divides the cytoplasm, organelles and cell membrane of one cell into two new cells containing roughly equal shares of these cellular components. The different stages of mitosis all together define the mitotic phase of an animal cell cycle—the division of the mother cell into two genetically identical daughter cells. The cell cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. In contrast to mitosis, meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions. Homologous chromosomes are separated in the first division (meiosis I), and sister chromatids are separated in the second division (meiosis II). Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle.",
    "label": 0
  },
  {
    "text": "are separated in the first division (meiosis I), and sister chromatids are separated in the second division (meiosis II). Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle. Both are believed to be present in the last eukaryotic common ancestor. Prokaryotes (i.e., archaea and bacteria) can also undergo cell division (or binary fission). Unlike the processes of mitosis and meiosis in eukaryotes, binary fission in prokaryotes takes place without the formation of a spindle apparatus on the cell. Before binary fission, DNA in the bacterium is tightly coiled. After it has uncoiled and duplicated, it is pulled to the separate poles of the bacterium as it increases the size to prepare for splitting. Growth of a new cell wall begins to separate the bacterium (triggered by FtsZ polymerization and \"Z-ring\" formation). The new cell wall (septum) fully develops, resulting in the complete split of the bacterium. The new daughter cells have tightly coiled DNA rods, ribosomes, and plasmids. Sexual reproduction and meiosis Meiosis is a central feature of sexual reproduction in eukaryotes, and the most fundamental function of meiosis appears to be conservation of the integrity of the genome that is passed on to progeny by parents. Two aspects of sexual reproduction, meiotic recombination and outcrossing, are likely maintained respectively by the adaptive advantages of recombinational repair of genomic DNA damage and genetic complementation which masks the expression of deleterious recessive mutations. The beneficial effect of genetic complementation, derived from outcrossing (cross-fertilization) is also referred to as hybrid vigor or heterosis. Charles Darwin in his 1878 book The Effects of Cross and Self-Fertilization in the Vegetable Kingdom at the start of chapter XII noted \"The first and most important of the conclusions which may be drawn from the",
    "label": 0
  },
  {
    "text": "vigor or heterosis. Charles Darwin in his 1878 book The Effects of Cross and Self-Fertilization in the Vegetable Kingdom at the start of chapter XII noted \"The first and most important of the conclusions which may be drawn from the observations given in this volume, is that generally cross-fertilisation is beneficial and self-fertilisation often injurious, at least with the plants on which I experimented.\" Genetic variation, often produced as a byproduct of sexual reproduction, may provide long-term advantages to those sexual lineages that engage in outcrossing. Genetics Inheritance Genetics is the scientific study of inheritance. Mendelian inheritance, specifically, is the process by which genes and traits are passed on from parents to offspring. It has several principles. The first is that genetic characteristics, alleles, are discrete and have alternate forms (e.g., purple vs. white or tall vs. dwarf), each inherited from one of two parents. Based on the law of dominance and uniformity, which states that some alleles are dominant while others are recessive; an organism with at least one dominant allele will display the phenotype of that dominant allele. During gamete formation, the alleles for each gene segregate, so that each gamete carries only one allele for each gene. Heterozygotic individuals produce gametes with an equal frequency of two alleles. Finally, the law of independent assortment, states that genes of different traits can segregate independently during the formation of gametes, i.e., genes are unlinked. An exception to this rule would include traits that are sex-linked. Test crosses can be performed to experimentally determine the underlying genotype of an organism with a dominant phenotype. A Punnett square can be used to predict the results of a test cross. The chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with fruit flies,",
    "label": 0
  },
  {
    "text": "with a dominant phenotype. A Punnett square can be used to predict the results of a test cross. The chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with fruit flies, which established the sex linkage between eye color and sex in these insects. Genes and DNA A gene is a unit of heredity that corresponds to a region of deoxyribonucleic acid (DNA) that carries genetic information that controls form or function of an organism. DNA is composed of two polynucleotide chains that coil around each other to form a double helix. It is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell is collectively known as its genome. In eukaryotes, DNA is mainly in the cell nucleus. In prokaryotes, the DNA is held within the nucleoid. The genetic information is held within genes, and the complete assemblage in an organism is called its genotype. DNA replication is a semiconservative process whereby each strand serves as a template for a new strand of DNA. Mutations are heritable changes in DNA. They can arise spontaneously as a result of replication errors that were not corrected by proofreading or can be induced by an environmental mutagen such as a chemical (e.g., nitrous acid, benzopyrene) or radiation (e.g., x-ray, gamma ray, ultraviolet radiation, particles emitted by unstable isotopes). Mutations can lead to phenotypic effects such as loss-of-function, gain-of-function, and conditional mutations. Some mutations are beneficial, as they are a source of genetic variation for evolution. Others are harmful if they were to result in a loss of function of genes needed for survival. Gene expression Gene expression is the molecular process by which a genotype encoded in DNA gives rise to an observable phenotype in",
    "label": 0
  },
  {
    "text": "Others are harmful if they were to result in a loss of function of genes needed for survival. Gene expression Gene expression is the molecular process by which a genotype encoded in DNA gives rise to an observable phenotype in the proteins of an organism's body. This process is summarized by the central dogma of molecular biology, which was formulated by Francis Crick in 1958. According to the Central Dogma, genetic information flows from DNA to RNA to protein. There are two gene expression processes: transcription (DNA to RNA) and translation (RNA to protein). Gene regulation The regulation of gene expression by environmental factors and during different stages of development can occur at each step of the process such as transcription, RNA splicing, translation, and post-translational modification of a protein. Gene expression can be influenced by positive or negative regulation, depending on which of the two types of regulatory proteins called transcription factors bind to the DNA sequence close to or at a promoter. A cluster of genes that share the same promoter is called an operon, found mainly in prokaryotes and some lower eukaryotes (e.g., Caenorhabditis elegans). In positive regulation of gene expression, the activator is the transcription factor that stimulates transcription when it binds to the sequence near or at the promoter. Negative regulation occurs when another transcription factor called a repressor binds to a DNA sequence called an operator, which is part of an operon, to prevent transcription. Repressors can be inhibited by compounds called inducers (e.g., allolactose), thereby allowing transcription to occur. Specific genes that can be activated by inducers are called inducible genes, in contrast to constitutive genes that are almost constantly active. In contrast to both, structural genes encode proteins that are not involved in gene regulation. In addition to regulatory events involving the",
    "label": 0
  },
  {
    "text": "be activated by inducers are called inducible genes, in contrast to constitutive genes that are almost constantly active. In contrast to both, structural genes encode proteins that are not involved in gene regulation. In addition to regulatory events involving the promoter, gene expression can also be regulated by epigenetic changes to chromatin, which is a complex of DNA and protein found in eukaryotic cells. Genes, development, and evolution Development is the process by which a multicellular organism (plant or animal) goes through a series of changes, starting from a single cell, and taking on various forms that are characteristic of its life cycle. There are four key processes that underlie development: Determination, differentiation, morphogenesis, and growth. Determination sets the developmental fate of a cell, which becomes more restrictive during development. Differentiation is the process by which specialized cells arise from less specialized cells such as stem cells. Stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell. Cellular differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals, which are largely due to highly controlled modifications in gene expression and epigenetics. With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself. Thus, different cells can have very different physical characteristics despite having the same genome. Morphogenesis, or the development of body form, is the result of spatial differences in gene expression. A small fraction of the genes in an organism's genome called the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body",
    "label": 0
  },
  {
    "text": "the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Among the most important toolkit genes are the Hox genes. Hox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva. Evolution Evolutionary processes Evolution is a central organizing concept in biology. It is the change in heritable characteristics of populations over successive generations. In artificial selection, animals were selectively bred for specific traits. Given that traits are inherited, populations contain a varied mix of traits, and reproduction is able to increase any population, Darwin argued that in the natural world, it was nature that played the role of humans in selecting for specific traits. Darwin inferred that individuals who possessed heritable traits better adapted to their environments are more likely to survive and produce more offspring than other individuals. He further inferred that this would lead to the accumulation of favorable traits over successive generations, thereby increasing the match between the organisms and their environment. Speciation A species is a group of organisms that mate with one another and speciation is the process by which one lineage splits into two lineages as a result of having evolved independently from each other. For speciation to occur, there has to be reproductive isolation. Reproductive isolation can result from incompatibilities between genes as described by Bateson–Dobzhansky–Muller model. Reproductive isolation also tends to increase with genetic divergence. Speciation can occur when there are physical barriers that divide an ancestral species, a process known as allopatric speciation. Phylogeny A phylogeny is an evolutionary history",
    "label": 0
  },
  {
    "text": "as described by Bateson–Dobzhansky–Muller model. Reproductive isolation also tends to increase with genetic divergence. Speciation can occur when there are physical barriers that divide an ancestral species, a process known as allopatric speciation. Phylogeny A phylogeny is an evolutionary history of a specific group of organisms or their genes. It can be represented using a phylogenetic tree, a diagram showing lines of descent among organisms or their genes. Each line drawn on the time axis of a tree represents a lineage of descendants of a particular species or population. When a lineage divides into two, it is represented as a fork or split on the phylogenetic tree. Phylogenetic trees are the basis for comparing and grouping different species. Different species that share a feature inherited from a common ancestor are described as having homologous features (or synapomorphy). Phylogeny provides the basis of biological classification. This classification system is rank-based, with the highest rank being the domain followed by kingdom, phylum, class, order, family, genus, and species. All organisms can be classified as belonging to one of three domains: Archaea (originally Archaebacteria), Bacteria (originally eubacteria), or Eukarya (includes the fungi, plant, and animal kingdoms). History of life The history of life on Earth traces how organisms have evolved from the earliest emergence of life to present day. Earth formed about 4.5 billion years ago and all life on Earth, both living and extinct, descended from a last universal common ancestor that lived about 3.5 billion years ago. Geologists have developed a geologic time scale that divides the history of the Earth into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years. Each eon can be divided into eras, with the Phanerozoic eon",
    "label": 0
  },
  {
    "text": "into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years. Each eon can be divided into eras, with the Phanerozoic eon that began 539 million years ago being subdivided into Paleozoic, Mesozoic, and Cenozoic eras. These three eras together comprise eleven periods (Cambrian, Ordovician, Silurian, Devonian, Carboniferous, Permian, Triassic, Jurassic, Cretaceous, Tertiary, and Quaternary). The similarities among all known present-day species indicate that they have diverged through the process of evolution from their common ancestor. Biologists regard the ubiquity of the genetic code as evidence of universal common descent for all bacteria, archaea, and eukaryotes. Microbial mats of coexisting bacteria and archaea were the dominant form of life in the early Archean eon and many of the major steps in early evolution are thought to have taken place in this environment. The earliest evidence of eukaryotes dates from 1.85 billion years ago, and while they may have been present earlier, their diversification accelerated when they started using oxygen in their metabolism. Later, around 1.7 billion years ago, multicellular organisms began to appear, with differentiated cells performing specialised functions. Algae-like multicellular land plants are dated back to about 1 billion years ago, although evidence suggests that microorganisms formed the earliest terrestrial ecosystems, at least 2.7 billion years ago. Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event. Ediacara biota appear during the Ediacaran period, while vertebrates, along with most other modern phyla originated about 525 million years ago during the Cambrian explosion. During the Permian period, synapsids, including the ancestors of mammals, dominated the land,",
    "label": 0
  },
  {
    "text": "extinction event. Ediacara biota appear during the Ediacaran period, while vertebrates, along with most other modern phyla originated about 525 million years ago during the Cambrian explosion. During the Permian period, synapsids, including the ancestors of mammals, dominated the land, but most of this group became extinct in the Permian–Triassic extinction event 252 million years ago. During the recovery from this catastrophe, archosaurs became the most abundant land vertebrates; one archosaur group, the dinosaurs, dominated the Jurassic and Cretaceous periods. After the Cretaceous–Paleogene extinction event 66 million years ago killed off the non-avian dinosaurs, mammals increased rapidly in size and diversity. Such mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify. Diversity Bacteria and Archaea Bacteria are a type of cell that constitute a large domain of prokaryotic microorganisms. Typically a few micrometers in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste, and the deep biosphere of the Earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory. Archaea constitute the other domain of prokaryotic cells and were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), a term that has fallen out of use. Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as",
    "label": 0
  },
  {
    "text": "unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi. Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes, including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores. The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet. Archaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin. Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example. Eukaryotes Eukaryotes are hypothesized to",
    "label": 0
  },
  {
    "text": "gut, mouth, and on the skin. Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example. Eukaryotes Eukaryotes are hypothesized to have split from archaea, which was followed by their endosymbioses with bacteria (or symbiogenesis) that gave rise to mitochondria and chloroplasts, both of which are now part of modern-day eukaryotic cells. The major lineages of eukaryotes diversified in the Precambrian about 1.5 billion years ago and can be classified into eight major clades: alveolates, excavates, stramenopiles, plants, rhizarians, amoebozoans, fungi, and animals. Five of these clades are collectively known as protists, which are mostly microscopic eukaryotic organisms that are not plants, fungi, or animals. While it is likely that protists share a common ancestor (the last eukaryotic common ancestor), protists by themselves do not constitute a separate clade as some protists may be more closely related to plants, fungi, or animals than they are to other protists. Like groupings such as algae, invertebrates, or protozoans, the protist grouping is not a formal taxonomic group but is used for convenience. Most protists are unicellular; these are called microbial eukaryotes. Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae, which would exclude fungi and some algae. Plant cells were derived by endosymbiosis of a cyanobacterium into an early eukaryote about one billion years ago, which gave rise to chloroplasts. The first several clades that emerged following primary endosymbiosis were aquatic and most of the aquatic photosynthetic eukaryotic organisms are collectively described as algae, which is a term of convenience as not all algae are closely related. Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular",
    "label": 0
  },
  {
    "text": "collectively described as algae, which is a term of convenience as not all algae are closely related. Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular ancestor of Plantae. Unlike glaucophytes, the other algal clades such as red and green algae are multicellular. Green algae comprise three major clades: chlorophytes, coleochaetophytes, and stoneworts. Fungi are eukaryotes that digest foods outside their bodies, secreting digestive enzymes that break down large food molecules before absorbing them through their cell membranes. Many fungi are also saprobes, feeding on dead organic matter, making them important decomposers in ecological systems. Animals are multicellular eukaryotes. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. They have complex interactions with each other and their environments, forming intricate food webs. Viruses Viruses are submicroscopic infectious agents that replicate inside the cells of organisms. Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea. More than 6,000 virus species have been described in detail. Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity. The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction. Because viruses possess some but",
    "label": 0
  },
  {
    "text": "of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction. Because viruses possess some but not all characteristics of life, they have been described as \"organisms at the edge of life\", and as self-replicators. Ecology Ecology is the study of the distribution and abundance of life, the interaction between organisms and their environment. Ecosystems The community of living (biotic) organisms in conjunction with the nonliving (abiotic) components (e.g., water, light, radiation, temperature, humidity, atmosphere, acidity, and soil) of their environment is called an ecosystem. These biotic and abiotic components are linked together through nutrient cycles and energy flows. Energy from the sun enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals move matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes. Populations A population is the group of organisms of the same species that occupies an area and reproduce from generation to generation. Population size can be estimated by multiplying population density by the area or volume. The carrying capacity of an environment is the maximum population size of a species that can be sustained by that specific environment, given the food, habitat, water, and other resources that are available. The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability of resources and the cost of maintaining them. In",
    "label": 0
  },
  {
    "text": "environment, given the food, habitat, water, and other resources that are available. The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability of resources and the cost of maintaining them. In human populations, new technologies such as the Green revolution have helped increase the Earth's carrying capacity for humans over time, which has stymied the attempted predictions of impending population decline, the most famous of which was by Thomas Malthus in the 18th century. Communities A community is a group of populations of species occupying the same geographical area at the same time. A biological interaction is the effect that a pair of organisms living together in a community have on each other. They can be either of the same species (intraspecific interactions), or of different species (interspecific interactions). These effects may be short-term, like pollination and predation, or long-term; both often strongly influence the evolution of the species involved. A long-term interaction is called a symbiosis. Symbioses range from mutualism, beneficial to both partners, to competition, harmful to both partners. Every species participates as a consumer, resource, or both in consumer–resource interactions, which form the core of food chains or food webs. There are different trophic levels within any food web, with the lowest level being the primary producers (or autotrophs) such as plants and algae that convert energy and inorganic material into organic compounds, which can then be used by the rest of the community. At the next level are the heterotrophs, which are the species that obtain energy by breaking apart organic compounds from other organisms. Heterotrophs that consume plants are primary consumers (or herbivores) whereas heterotrophs that consume herbivores are secondary consumers (or carnivores). And those that eat secondary consumers are tertiary consumers and so on. Omnivorous",
    "label": 0
  },
  {
    "text": "by breaking apart organic compounds from other organisms. Heterotrophs that consume plants are primary consumers (or herbivores) whereas heterotrophs that consume herbivores are secondary consumers (or carnivores). And those that eat secondary consumers are tertiary consumers and so on. Omnivorous heterotrophs are able to consume at multiple levels. Finally, there are decomposers that feed on the waste products or dead bodies of organisms. On average, the total amount of energy incorporated into the biomass of a trophic level per unit of time is about one-tenth of the energy of the trophic level that it consumes. Waste and dead material used by decomposers as well as heat lost from metabolism make up the other ninety percent of energy that is not consumed by the next trophic level. Biosphere In the global ecosystem or biosphere, matter exists as different interacting compartments, which can be biotic or abiotic as well as accessible or inaccessible, depending on their forms and locations. For example, matter from terrestrial autotrophs are both biotic and accessible to other organisms whereas the matter in rocks and minerals are abiotic and inaccessible. A biogeochemical cycle is a pathway by which specific elements of matter are turned over or moved through the biotic (biosphere) and the abiotic (lithosphere, atmosphere, and hydrosphere) compartments of Earth. There are biogeochemical cycles for nitrogen, carbon, and water. Conservation Conservation biology is the study of the conservation of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is concerned with factors that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity. The concern stems from estimates suggesting that up to 50% of all species on the planet",
    "label": 0
  },
  {
    "text": "that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity. The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years, which has contributed to poverty, starvation, and will reset the course of evolution on this planet. Biodiversity affects the functioning of ecosystems, which provide a variety of services upon which people depend. Conservation biologists research and educate on the trends of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Organizations and citizens are responding to the current biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales. See also References Further reading External links OSU's Phylocode Biology Online – Wiki Dictionary MIT video lecture series on biology OneZoom Tree of Life Journal of the History of Biology (springer.com) Journal links PLOS ONE PLOS Biology A peer-reviewed, open-access journal published by the Public Library of Science Current Biology: General journal publishing original research from all areas of biology Biology Letters: A high-impact Royal Society journal publishing peer-reviewed biology papers of general interest Science: Internationally renowned AAAS science journal – see sections of the life sciences International Journal of Biological Sciences: A biological journal publishing significant peer-reviewed scientific papers Perspectives in Biology and Medicine: An interdisciplinary scholarly journal publishing essays of broad relevance",
    "label": 0
  },
  {
    "text": "A biologist is a scientist who conducts research in biology. Biologists are interested in studying life on Earth, whether it is an individual cell, a multicellular organism, or a community of interacting populations. They usually specialize in a particular branch (e.g., molecular biology, zoology, and evolutionary biology) of biology and have a specific research focus (e.g., studying malaria or cancer). Biologists who are involved in basic research have the aim of advancing knowledge about the natural world. They conduct their research using the scientific method, which is an empirical method for testing hypotheses. Their discoveries may have applications for some specific purpose such as in biotechnology, which has the goal of developing medically useful products for humans. In modern times, most biologists have one or more academic degrees such as a bachelor's degree, as well as an advanced degree such as a master's degree or a doctorate. Like other scientists, biologists can be found working in different sectors of the economy such as in academia, nonprofits, private industry, or government. History Francesco Redi, the founder of experimental biology, is recognized to be one of the greatest biologists of all time. Robert Hooke, an English natural philosopher, coined the term cell, suggesting plant structure's resemblance to honeycomb cells. Charles Darwin and Alfred Wallace independently formulated the theory of evolution by natural selection, which was described in detail in Darwin's book On the Origin of Species, which was published in 1859. In it, Darwin proposed that the features of all living things, including humans, were shaped by natural processes of descent with accumulated modification leading to divergence over long periods of time. The theory of evolution in its current form affects almost all areas of biology. Separately, Gregor Mendel formulated the principles of inheritance in 1866, which became the basis of modern",
    "label": 0
  },
  {
    "text": "accumulated modification leading to divergence over long periods of time. The theory of evolution in its current form affects almost all areas of biology. Separately, Gregor Mendel formulated the principles of inheritance in 1866, which became the basis of modern genetics. In 1953, James D. Watson and Francis Crick described the basic structure of DNA, the genetic material for expressing life in all its forms, building on the work of Maurice Wilkins and Rosalind Franklin, suggested that the structure of DNA was a double helix. Ian Wilmut led a research group that in 1996 first cloned a mammal from an adult somatic cell, a Finnish Dorset lamb named Dolly. Education An undergraduate degree in biology typically requires coursework in molecular and cellular biology, development, ecology, genetics, microbiology, anatomy, physiology, botany, zoology and paleontology. Additional requirements may include physics, chemistry (general, organic, and biochemistry), calculus, and statistics. Students who aspire to a research-oriented career usually pursue a graduate degree such as a master's or a doctorate (e.g., PhD) whereby they would receive training from a research head based on an apprenticeship model that has been in existence since the 1800s. Students in these graduate programs often receive specialized training in a particular subdiscipline of biology. Research Biologists who work in basic research formulate theories and devise experiments to advance human knowledge on life including topics such as evolution, biochemistry, molecular biology, neuroscience and cell biology. Biologists typically conduct laboratory experiments involving animals, plants, microorganisms or biomolecules. However, a small part of biological research also occurs outside the laboratory and may involve natural observation rather than experimentation. For example, a botanist may investigate the plant species present in a particular environment, while an ecologist might study how a forest area recovers after a fire. Biologists who work in applied research use instead",
    "label": 0
  },
  {
    "text": "natural observation rather than experimentation. For example, a botanist may investigate the plant species present in a particular environment, while an ecologist might study how a forest area recovers after a fire. Biologists who work in applied research use instead the accomplishments gained by basic research to further knowledge in particular fields or applications. For example, this applied research may be used to develop new pharmaceutical drugs, treatments and medical diagnostic tests. Biological scientists conducting applied research and product development in private industry may be required to describe their research plans or results to non-scientists who are in a position to veto or approve their ideas. These scientists must consider the business effects of their work. Swift advances in knowledge of genetics and organic molecules spurred growth in the field of biotechnology, transforming the industries in which biological scientists work. Biological scientists can now manipulate the genetic material of animals and plants, attempting to make organisms (including humans) more productive or resistant to disease. Basic and applied research on biotechnological processes, such as recombining DNA, has led to the production of important substances, including human insulin and growth hormone. Many other substances not previously available in large quantities are now produced by biotechnological means. Some of these substances are useful in treating diseases. Those working on various genome (chromosomes with their associated genes) projects isolate genes and determine their function. This work continues to lead to the discovery of genes associated with specific diseases and inherited health risks, such as sickle cell anemia. Advances in biotechnology have created research opportunities in almost all areas of biology, with commercial applications in areas such as medicine, agriculture, and environmental remediation. Specializations Most biological scientists specialize in the study of a certain type of organism or in a specific activity, although recent advances",
    "label": 0
  },
  {
    "text": "in almost all areas of biology, with commercial applications in areas such as medicine, agriculture, and environmental remediation. Specializations Most biological scientists specialize in the study of a certain type of organism or in a specific activity, although recent advances have blurred some traditional classifications. Geneticists study genetics, the science of genes, heredity, and variation of organisms. Neuroscientists study the nervous system. Developmental biologists study the process of development and growth of organisms Biochemists study the chemical composition of living things. They analyze the complex chemical combinations and reactions involved in metabolism, reproduction, and growth. Molecular biologists study the biological activity between biomolecules. Microbiologists investigate the growth and characteristics of microscopic organisms such as bacteria, algae, or fungi. Physiologists study life functions of plants and animals, in the whole organism and at the cellular or molecular level, under normal and abnormal conditions. Physiologists often specialize in functions such as growth, reproduction, photosynthesis, respiration, or movement, or in the physiology of a certain area or system of the organism. Biophysicists use experimental methods traditionally employed in physics to answer biological questions . Computational biologists apply the techniques of computer science, applied mathematics and statistics to address biological problems. The main focus lies on developing mathematical modeling and computational simulation techniques. By these means it addresses scientific research topics with their theoretical and experimental questions without a laboratory. Zoologists and wildlife biologists study animals and wildlife—their origin, behavior, diseases, and life processes. Some experiment with live animals in controlled or natural surroundings, while others dissect dead animals to study their structure. Zoologists and wildlife biologists also may collect and analyze biological data to determine the environmental effects of current and potential uses of land and water areas. Zoologists usually are identified by the animal group they study. For example, ornithologists study birds,",
    "label": 0
  },
  {
    "text": "and wildlife biologists also may collect and analyze biological data to determine the environmental effects of current and potential uses of land and water areas. Zoologists usually are identified by the animal group they study. For example, ornithologists study birds, mammalogists study mammals, herpetologists study reptiles and amphibians, ichthyologists study fish, cnidariologists study jellyfishes and entomologists study insects. Botanists study plants and their environments. Some study all aspects of plant life, including algae, lichens, mosses, ferns, conifers, and flowering plants; others specialize in areas such as identification and classification of plants, the structure and function of plant parts, the biochemistry of plant processes, the causes and cures of plant diseases, the interaction of plants with other organisms and the environment, the geological record of plants and their evolution. Mycologists study fungi, such as yeasts, mold and mushrooms, which are a separate kingdom from plants. Aquatic biologists study micro-organisms, plants, and animals living in water. Marine biologists study salt water organisms, and limnologists study fresh water organisms. Much of the work of marine biology centers on molecular biology, the study of the biochemical processes that take place inside living cells. Marine biology is a branch of oceanography, which is the study of the biological, chemical, geological, and physical characteristics of oceans and the ocean floor. (See the Handbook statements on environmental scientists and hydrologists and on geoscientists.) Paleontologists study the history of life on Earth through the analysis of fossils—preserved remains, traces, or impressions of past organisms found within rocks. Their work combines biological knowledge with geological methods, since interpreting fossils requires understanding the stratigraphy, sedimentology, and geochronology of the rock layers in which they occur. Paleontologists reconstruct ancient organisms, ecosystems, and environments; investigate patterns of evolution, extinction, and biodiversity through deep time; and analyze how geological processes influence the preservation",
    "label": 0
  },
  {
    "text": "requires understanding the stratigraphy, sedimentology, and geochronology of the rock layers in which they occur. Paleontologists reconstruct ancient organisms, ecosystems, and environments; investigate patterns of evolution, extinction, and biodiversity through deep time; and analyze how geological processes influence the preservation and distribution of fossils. Many specialize in particular groups, such as vertebrates, invertebrates, or plants, or in subfields such as taphonomy (the study of fossilization), paleoecology (ancient ecosystems), or biostratigraphy (dating and correlating rock layers using fossils). Evolutionary biologists investigate the evolutionary processes that produced the diversity of life on Earth, starting from a single common ancestor. These processes include natural selection, common descent, and speciation. Ecologists investigate the relationships among organisms and between organisms and their environments, examining the effects of population size, pollutants, rainfall, temperature, and altitude. Using knowledge of various scientific disciplines, ecologists may collect, study, and report data on the quality of air, food, soil, and water. Employment Biologists typically work regular hours but longer hours are not uncommon. Researchers may be required to work odd hours in laboratories or other locations (especially while in the field), depending on the nature of their research. Many biologists depend on grant money to fund their research. They may be under pressure to meet deadlines and to conform to rigid grant-writing specifications when preparing proposals to seek new or extended funding. Marine biologists encounter a variety of working conditions. Some work in laboratories; others work on research ships, and those who work underwater must practice safe diving while working around sharp coral reefs and hazardous marine life. Although some marine biologists obtain their specimens from the sea, many still spend a good deal of their time in laboratories and offices, conducting tests, running experiments, recording results, and compiling data. Biologists are not usually exposed to unsafe or unhealthy conditions.",
    "label": 0
  },
  {
    "text": "This is a list of encyclopedias as well as encyclopedic and biographical dictionaries published on the subject of biology in any language. Entries are in the English language unless specifically stated as otherwise. General biology Becher, Anne, Joseph Richey. American environmental leaders: From colonial times to the present. Grey House, 2008. ISBN 9781592371198. Butcher, Russell D., Stephen E. Adair, Lynn A. Greenwalt. America's national wildlife refuges: A complete guide. Roberts Rinehart Publishers in cooperation with Ducks Unlimited, 2003. ISBN 1570983798. Cullen, Katherine E. (2009). Encyclopedia of Life Science. Infobase Publishing. ISBN 978-0-8160-7008-4. Ecological Internet, Inc. EcoEarth.info: Environment portal and search engine. Ecological Internet, Inc. . Encyclopedia of Life Sciences. John Wiley & Sons. 25 May 2007. ISBN 978-0-470-06651-5. Encyclopedia of Life Sciences. Groves Dictionaries Incorporated. 1 November 2001. ISBN 978-1-56159-238-8. Friday, Adrian & Davis S. Ingram. The Cambridge Encyclopedia of Life Sciences. Cambridge, 1985. Gaither, Carl C., Alma E. Cavazos-Gaither, Andrew Slocombe. Naturally speaking: A dictionary of quotations on biology, botany, nature and zoology. Institute of Physics, 2001. ISBN 0750306815. Gibson, Daniel, National Audubon Society. Audubon guide to the national wildlife refuges. Southwest: Arizona, Nevada, New Mexico, Texas. St. Martin's Griffin, 2000. ISBN 0312207778. Goudie, Andrew, David J. Cuff. Encyclopedia of global change: Environmental change and human society. Oxford University Press, 2002. ISBN 0195108256. Gove, Doris. Audubon guide to the national wildlife refuges. Southeast : Alabama, Florida, Georgia, Kentucky, Mississippi, North Carolina, Puerto Rico, South Carolina, Tennessee, U.S. Virgin Islands. St. Martin's Griffin, 2000. ISBN 0312241283. Grassy, John. Audubon guide to the national wildlife refuges: Northern Midwest: Illinois, Indiana, Iowa, Michigan, Minnesota, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin. St. Martin's Griffin, c2000. ISBN 0312243154. Grassy, John. Audubon guide to the national wildlife refuges: Rocky Mountains: Colorado, Idaho, Montana, Utah, Wyoming. St. Martin's Griffin, 2000. ISBN 0312245742. Gray, Peter. Encyclopedia",
    "label": 0
  },
  {
    "text": "Michigan, Minnesota, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin. St. Martin's Griffin, c2000. ISBN 0312243154. Grassy, John. Audubon guide to the national wildlife refuges: Rocky Mountains: Colorado, Idaho, Montana, Utah, Wyoming. St. Martin's Griffin, 2000. ISBN 0312245742. Gray, Peter. Encyclopedia of the Biological Sciences. Krieger, 1981. Grinstein, Louise S., Carol A. Biermann, Rose K. Rose. Women in the biological sciences: A biobibliographic sourcebook. Greenwood Press, 1997. ISBN 0313291802. Hancock, John M., Marketa J. Zvelebil. Dictionary of bioinformatics and computational biology. Wiley-Liss, 2004. ISBN 0471436224. Hosansky, David. The environment A to Z. CQ Press, 2001. ISBN 1568025831. Laubach, René. Audubon guide to the national wildlife refuges. New England : Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont. St. Martin's Griffin, 2000. ISBN 0312204507. Mac Arthur, Loren, Debbie S. Miller. Audubon guide to the national wildlife refuges. Alaska and the Northwest: Alaska, Oregon, Washington. St. Martin's Griffin, 2000. ISBN 0312253729. Mac Arthur, Loren. Audubon guide to the national wildlife refuges. California & Hawaii : California, Hawaii. St. Martin's Griffin, 2000. ISBN 0312206895. Marinelli, Janet, Stephen K.-M. Tim, Brooklyn Botanic Garden. The Brooklyn Botanic Garden gardener's desk reference. Henry Holt, 1998. ISBN 0805050957. McAinish, T. F. Physics in Medicine & Biology Encyclopedia. Pergamon Press, 1986. Mongillo, John F.; Zierdt-Warshaw, Linda (2000). Encyclopedia of Environmental Science. University Rochester Press. ISBN 978-1-57356-147-1. Munn, R. E. Encyclopedia of global environmental change. Wiley, 2002. ISBN 0471977969. National Academy of Sciences. Biographical memoirs of the national academy of sciences. National Academy of Sciences. . Nature Publishing Group. Encyclopedia of life sciences. Nature Publishing Group, 2002. ISBN 1561592749. Niergenberg, William Aaron, Edward O. Wilson, Peter H. Raven. Encyclopedia of environmental biology. Academic Press, 1995. ISBN 0122267303. Nybakken, James Willard, William W. Broenkow, T. L. Vallier. Interdisciplinary encyclopedia of marine sciences. Grolier Academic Reference, 2003. ISBN 0717259463. O'Daly, Anne. Encyclopedia",
    "label": 0
  },
  {
    "text": "William Aaron, Edward O. Wilson, Peter H. Raven. Encyclopedia of environmental biology. Academic Press, 1995. ISBN 0122267303. Nybakken, James Willard, William W. Broenkow, T. L. Vallier. Interdisciplinary encyclopedia of marine sciences. Grolier Academic Reference, 2003. ISBN 0717259463. O'Daly, Anne. Encyclopedia of life sciences. Marshall Cavendish, 2004. ISBN 0761474420. Palmer, William, National Audubon Society. Audubon guide to the national wildlife refuges. South Central: Arkansas, Kansas, Louisiana, Missouri, Oklahoma. St. Martin's Griffin, 2000. ISBN 0312244878. Pilch, Richard F., Raymond A. Zilinskas. Encyclopedia of bioterrorism defense. Wiley-LISS, 2005. ISBN 0471467170. Polunin, Nicholas, Lynn M. Curme. World who is who and does what in environment and conservation. St. Martin's Press; Foundation for Environmental Conservation, 1997. ISBN 0312174489. Porter, Roy, Marilyn Bailey Ogilvie. The biographical dictionary of scientists. Oxford University Press, 2000. ISBN 0195216636. Ricciuti, Edward R. Audubon guide to the national wildlife refuges. Mid-Atlantic: Delaware, Maryland, New Jersey, New York, Pennsylvania, Virginia, West Virginia. St. Martin's Griffin, 2000. ISBN 0312204817. Roberts, Keith (2007). Handbook of plant science. John Wiley. ISBN 978-0-470-05723-0. Royal Society (Great Britain). Biographical memoirs of fellows of the royal society. Royal Society, 1955–. ISSN 0080-4606. Shearer, Benjamin F., Barbara Smith Shearer. Notable women in the life sciences: a biographical dictionary. Greenwood Press, 1996. ISBN 0313293023. Sterling, Keir B. Biographical dictionary of American and Canadian naturalists and environmentalists. Greenwood Press, 1997. ISBN 0313230471. U. S. Department of Energy, Office of Scientific and Technical Information. Science.gov. U. S. Dept. of Energy, Office of Scientific and Technical Information, 2002-. . Wexler, Philip, Bruce D. Anderson, Ann de Peyster. Encyclopedia of toxicology. Elsevier Academic, 2005. ISBN 0127453512. Lifeforms Association for Biodiversity Information. NatureServe explorer: An online encyclopedia of life. NatureServe. . Beacham, Walton, Frank V. Castronova, Suzanne Sessine. Beacham's guide to the endangered species of North America. Gale Group, 2001. ISBN 0787650285. Center for Applied",
    "label": 0
  },
  {
    "text": "ISBN 0127453512. Lifeforms Association for Biodiversity Information. NatureServe explorer: An online encyclopedia of life. NatureServe. . Beacham, Walton, Frank V. Castronova, Suzanne Sessine. Beacham's guide to the endangered species of North America. Gale Group, 2001. ISBN 0787650285. Center for Applied Biodiversity Science. Biodiversity hotspots. Conservation International. . Field Museum of Natural History. The encyclopedia of life: EOL. Encyclopedia of Life, 2007-. . Henry, Helen L., Anthony W. Norman. Encyclopedia of hormones. Academic Press, 2003. ISBN 0123411033. IUCN Invasive Species Specialist Group. Global invasive species database. IUCN Invasive Species Specialist Group. 2000-. . Levin, Simon A. Encyclopedia of biodiversity. Academic Press, 2001. ISBN 0122268652. Pagel, Mark D. Encyclopedia of evolution. Oxford University Press, 2002. ISBN 0195122003. . Polistes Foundation. Discover life. Polistes Corporation, 1999-. . Tree of Life Web Project. Tree of life. University of Arizona. . World Wildlife Fund. World wildlife fund. World Wildlife Fund. . Animals Audubon Society Encyclopedia of Animal Life. Clarkson N. Potter, 1982. Bekoff, Marc, Jane Goodall. Encyclopedia of animal behavior. Greenwood Press, 2004. ISBN 0313327459. Bruce, Jenni. The encyclopedia of animals: A complete visual guide. University of California Press, 2004. ISBN 0520244060. Burton, Maurice, Robert Burton. International wildlife encyclopedia. Marshall Cavendish, 2002. ISBN 0761472665. Encyclopedia of Animal Behavior. Facts on File, 1987. Encyclopedia of Animal Biology. Facts on File, 1987. Encyclopedia of Animal Ecology. Facts on File, 1987. Encyclopedia of Animal Evolution. Facts on File, 1987. Grzimek, Bernhard. Grzimek's Animal Life Encyclopedia. Van Nostrand, 1972–1975. Grzimek's Encyclopedia of Ethology. Van Nostrand, 1972–1977. The Illustrated Encyclopedia of Wildlife. Grey Castle Press, 1991. Knobil, Ernst and Jimmy D. Neill. Encyclopedia of reproduction. Academic Press, 1998. ISBN 0122270207. Macmillan Illustrated Animal Encyclopedia. Macmillan, 1984. Marshall Cavendish International Wildlife Encyclopedia. Marshall Cavendish, 1990. Nowak, Ronald M., David W. Macdonald, Roland W. Kays. Walker's carnivores of the world. Johns Hopkins",
    "label": 0
  },
  {
    "text": "D. Neill. Encyclopedia of reproduction. Academic Press, 1998. ISBN 0122270207. Macmillan Illustrated Animal Encyclopedia. Macmillan, 1984. Marshall Cavendish International Wildlife Encyclopedia. Marshall Cavendish, 1990. Nowak, Ronald M., David W. Macdonald, Roland W. Kays. Walker's carnivores of the world. Johns Hopkins University Press, 2005. ISBN 0801880335. The Oxford Companion to Animal Behaviour. Oxford, 1987. World Nature Encyclopedia. Raintree/Steck-Vaughn, 1989. Aquatic Banister, Keith & Andrew Campbell. The Encyclopedia of Aquatic Life. Facts on File, 1985. Dakin, Nick. Macmillan Book of the Marine Aquarium. Macmillan, 1993. The Encyclopedia of Marine Invertebrates. T. F. H. Publications, 1983. Folkens, Pieter A., Randall R. Reeves, National Audubon Society. Guide to marine mammals of the world. A. A. Knopf, 2002. ISBN 0375411410. George, David and Jennifer. Marine Life: An Illustrated Encyclopedia of Invertebrates in the Sea. Wiley, 1979. Halstead, Bruce. Dangerous Aquatic Animals of the World: A Color Atlas. Darwin Press, 1992. Sterba, Gunther. The Aquarium Encyclopedia. MIT Press, 1983. Stickney, Robert R. Encyclopedia of aquaculture. Wiley, 2000. ISBN 0471291013. Fishes Dr. Axelrod's Atlas of Freshwater Aquarium Fishes. T. F. H. Publications, 6th ed., 1991. Dr. Burgess's Mini-Atlas of Freshwater Aquarium Fishes. T. F. H. Publications, 1991. Eschmeyer, William N., Earl Stanndard Herald, Howard Hammann. A field guide to Pacific Coast fishes of North America: From the Gulf of Alaska to Baja, California. Houghton Mifflin, 1983. ISBN 0395331889. Froese, R., D. Pauly International Center for Living Aquatic Resources Management. FishBase: A global information system on fishes. FishBase, [2000?]-. . Gilbert, Carter Rowell, James D. Williams, National Audubon Society. National Audubon Society field guide to fishes. Alfred A. Knopf, 2002. ISBN 0375412247. Goldstein, Robert J., Rodney W. Harper, Richard Edwards. American aquarium fishes. Texas A&M University Press, 2000. ISBN 0890968802. Page, Lawrence M., Brooks M. Burr, Eugene C. Beckham III, National Audubon Society. A field guide to freshwater",
    "label": 0
  },
  {
    "text": "Knopf, 2002. ISBN 0375412247. Goldstein, Robert J., Rodney W. Harper, Richard Edwards. American aquarium fishes. Texas A&M University Press, 2000. ISBN 0890968802. Page, Lawrence M., Brooks M. Burr, Eugene C. Beckham III, National Audubon Society. A field guide to freshwater fishes: North America north of Mexico. Houghton Mifflin, 1991. ISBN 0395910919. Paxton, John R., William N. Eschmeyer, David Kirshner. Encyclopedia of fishes. Academic Press, 1998. ISBN 0125476655. Arthropods Insects Audubon Society Book of Insects. Abrams, 1983. Capinera, John L. Encyclopedia of entomology. Kluwer Academic, 2004. ISBN 0792386701. Johnson, Warren T., Howard H. Lyon, C. S. Koehler. Insects that feed on trees and shrubs. Comstock Publishing Associates, 1991. ISBN 0801426022. Marshall, S. A. Insects: Their natural history and diversity; With a photographic guide to insects of eastern North America. Firefly Books, 2006. ISBN 1552979008. Milne, Lorus Johnson, Margery Joan Greene Milne, Susan Rayfield. The Audubon Society field guide to North American insects and spiders. Knopf, 1980. ISBN 0394507630. O'Toole, Christopher. The Encyclopedia of Insects. Facts on File, 1986. Pyle, Robert Michael, Carol Nehring, Jane Opper, National Audubon Society. The Audubon Society field guide to North American butterflies. Knopf, 1981. ISBN 0394519140. Resh, Vincent H., Ring T. Cardé. Encyclopedia of insects. Academic Press, 2003. ISBN 0125869908. Solomon, J. D., U.S. Dept. of Agriculture, Forest Service. Guide to insect borers in North American broadleaf trees and shrubs. U.S. Dept. of Agriculture, Forest Service, 1995. Birds Alderfer, Jonathan K., National Geographic Society (U. S.). National Geographic complete birds of North America. National Geographic, 2006. ISBN 0792241754. American Ornithologists' Union, Academy of Natural Sciences of Philadelphia, Cornell University. The birds of North America. American Ornithologists' Union, 1992–2002. ISSN 1061-5466. Cambridge Encyclopedia of Ornithology. Cambridge, 1991. Campbell, Bruce and Elizabeth Lack. Dictionary of Birds. Buteo, rev. ed., 1985. Clark, William S., Brian K. Wheeler. A field",
    "label": 0
  },
  {
    "text": "Philadelphia, Cornell University. The birds of North America. American Ornithologists' Union, 1992–2002. ISSN 1061-5466. Cambridge Encyclopedia of Ornithology. Cambridge, 1991. Campbell, Bruce and Elizabeth Lack. Dictionary of Birds. Buteo, rev. ed., 1985. Clark, William S., Brian K. Wheeler. A field guide to hawks of North America. Houghton Mifflin, 2001. ISBN 0395670675. Cornell University. All about birds. Cornell Laboratory of Ornithology. . del Hoyo, Josep, Andrew Elliott, Jordi Sargatal. Handbook of the birds of the world. Lynx Edicions, 1992–2006. ISBN 8487334105. Dunn, Jon, Jonathan K. Alderfer, National Geographic Society (U.S.). National Geographic field guide to the birds of North America. National Geographic, 2006. ISBN 0792253140. Ferguson-Lees, David A. Christie, Kim Franklin. Raptors of the world. Houghton Mifflin, 2001. ISBN 0618127623. Fuller, Errol. Extinct birds. Comstock, 2001. ISBN 080143954X. Montagu, George (1802). Ornithological Dictionary; or Alphabetical Synopsis of British Birds, London: J. White. Montagu, George (1813). Supplement to the Ornithological Dictionary, Or Synopsis of British Birds. S. Woolmer, Exeter. p. 1. Perrins, Christopher. Illustrated Encyclopedia of Birds. Prentice Hall, 1991. Perrins, Christopher and Alex Middleton. Encyclopedia of Birds. Facts on File, 1985. Robbins, Chandler S., Bertel Bruun, Herbert Spencer Zim. Birds of North America: A guide to field identification. St. Martin's Press, 2001. ISBN 1582380910. Sibley, David. The Sibley field guide to birds of eastern North America. Alfred A. Knopf, 2003. ISBN 067945120X. Sibley, David. The Sibley field guide to birds of western North America. Alfred A. Knopf, 2003. ISBN 0679451218. Sibley, David, Chris Elphick, John B. Dunning, National Audubon Society. The Sibley guide to bird life and behavior. Alfred A. Knopf, 2001. ISBN 0679451234. Sibley, David. The Sibley guide to birds. Alfred A. Knopf, 2000. ISBN 0679451226. Terres, John K. Audubon Society Encyclopedia of North American Birds. Knopf, 1991. Endangered species Endangered Wildlife of the World. Marshall Cavendish, 1993. The Grolier",
    "label": 0
  },
  {
    "text": "2001. ISBN 0679451234. Sibley, David. The Sibley guide to birds. Alfred A. Knopf, 2000. ISBN 0679451226. Terres, John K. Audubon Society Encyclopedia of North American Birds. Knopf, 1991. Endangered species Endangered Wildlife of the World. Marshall Cavendish, 1993. The Grolier World Encyclopedia of Endangered Species. Grolier, 1992. Lowe, David W., John R. Matthews, Charles J. Moseley, World Wildlife Fund. The official World Wildlife Fund guide to endangered species of North America. Beacham, 1990–1994. ISBN 0933833172. Official World Wildlife Fund Guide to Endangered Species. Beacham, 1990–1992. Mammals Beacham, Walton, Kirk H. Beetz. Beacham's guide to international endangered species. Beacham, 1998–2001. ISBN 0933833342. Elbroch, Mark. Mammal tracks and sign: A guide to North American species. Stackpole Books, 2003. ISBN 0811726266. Folkens, Pieter A., Randall R. Reeves, National Audubon Society. Guide to marine mammals of the world. A. A. Knopf, 2002. ISBN 0375411410. Grzimek, Bernhard. Grzimek's Encyclopedia of Mammals. McGraw-Hill, 2nd ed., 1990. ISBN 0079095089. Macdonald, David. Encyclopedia of Mammals. Facts on File, 2006. ISBN 0816064946. Mammals: A Multimedia Encyclopedia. National Geographic Society/IBM, 1990. Nowak, Ronald. Walker's Mammals of the World. Johns Hopkins University Press, 1991. ISBN 0801857899. Reid, Fiona, National Audubon Society. A field guide to mammals of North America, north of Mexico. Houghton Mifflin, 2006. ISBN 0395935962. Whitaker, John O., National Audubon Society. National Audubon Society field guide to North American mammals. Knopf, 1996. ISBN 0679446311. Wilson, Don E., Sue Ruff, American Society of Mammalogists. The Smithsonian book of North American mammals. Smithsonian Institution Press, 1999. ISBN 1560988452. Canines Alderton, David (September 2008). Encyclopedia of Dogs (Hardcover). Bath: Parragon Inc. p. 384. ISBN 978-1-4075-2438-2. American Kennel Club. The complete dog book: Official publication of the American Kennel Club. Ballantine Books, 2006. ISBN 0345476263. Coile, D. Caroline (April 1, 2005). Encyclopedia of Dog Breeds: Profiles of More than 150 Breeds (2nd ed.).",
    "label": 0
  },
  {
    "text": "384. ISBN 978-1-4075-2438-2. American Kennel Club. The complete dog book: Official publication of the American Kennel Club. Ballantine Books, 2006. ISBN 0345476263. Coile, D. Caroline (April 1, 2005). Encyclopedia of Dog Breeds: Profiles of More than 150 Breeds (2nd ed.). Barron's Educational Series, Incorporated. p. 368. ISBN 978-0-7641-5700-4. De Prisco, Andrew and James B. Johnson. Canine Lexicon. T. F. H. Publications, 1993. De Vito, Dominique (September 1, 2005). World Atlas of Dog Breeds (Print) (6th ed.). Neptune City, NJ Lanham, MD: TFH Publications, Inc. Distributed in the U.S. to the Bookstore and library trade by National Book Network. p. 960. ISBN 978-0-7938-0656-0. DK Publishing (July 15, 2013). The Dog Encyclopedia (Hardcover) (1st ed.). DK Adult. p. 360. ISBN 978-1-4654-0844-0. Morris, Desmond. Dogs: The ultimate dictionary of over 1,000 dog breeds. Trafalgar Square, 2002. ISBN 1570762198. Wilcox, Bonnie; Walkowicz, Chris (March 1995). Atlas of Dog Breeds of the World (Print) (5th ed.). Neptune City, NJ Lanham, MD: TFH Publications, Inc. Distributed in the U.S. to the Bookstore and library trade by National Book Network. p. 912. ISBN 978-0-7938-1284-4. Cats Kelsey-Wood, Dennis. The Atlas of Cats of the World: Domesticated and Wild. T. F. H. Publications, 1989. Cattle Felius, Marleen. Cattle breeds: An encyclopedia. Misset, 1995. ISBN 9054390174. Horses Ensminger, M. F. The Complete Encyclopedia of Horses. A. S. Barnes, 1977. Griffin, James and Tom Gore. Horse Owner's Veterinary Handbook. Howell Book House, 1989. Hendricks, Bonnie L.; Dent, Anthony A. (30 August 2007). International Encyclopedia of Horse Breeds. University of Oklahoma Press. p. 42. ISBN 978-0-8061-3884-8. Kidd, Jane. International Encyclopedia of Horse Breeds. H. P. Books, 1986. Kidd, Jane. International Encyclopedia of Horse Breeds & Breeding. Crescent Books, 1989. Primates Jacobsen, Lawrence, Raymond Hamel, Cynthia Robinson, Wisconsin Primate Research Center. Primate info net. Wisconsin Primate Research Center, University of Wisconsin. . Human",
    "label": 0
  },
  {
    "text": "Breeds. H. P. Books, 1986. Kidd, Jane. International Encyclopedia of Horse Breeds & Breeding. Crescent Books, 1989. Primates Jacobsen, Lawrence, Raymond Hamel, Cynthia Robinson, Wisconsin Primate Research Center. Primate info net. Wisconsin Primate Research Center, University of Wisconsin. . Human Blakemore, Colin, Sheila Jennett, Alan Cuthbert. The Oxford companion to the body. Oxford University Press, 2001. ISBN 019852403X. Cooper, David N., Nature Publishing Group. Nature encyclopedia of the human genome. Nature Publishing Group, 2003. ISBN 0333803868. Dulbecco, Renato. Encyclopedia of Human Biology. Academic Press, 1991. Kristic, Radivoj V. Illustrated Encyclopedia of Human Histology. Springer-Verlag, 1984. Ramachandran, V. S. Encyclopedia of the human brain. Academic Press, 2002. ISBN 0122272102. Ulijaszek, Stanley J., Francis E. Johnston, M. A. Preece. The Cambridge encyclopedia of human growth and development. Cambridge University Press, 1998. ISBN 0521560462. Human evolution Jones, Stephen. Cambridge Encyclopedia of Human Evolution. Cambridge, 1993. Milner, Richard. Encyclopedia of Evolution: Humanity's Search for Its Origins. Facts on File, 1990. Tattersall, Ian. The Encyclopedia of Human Evolution and Prehistory. Garland. ISBN 0815316968. Reptiles and Amphibians Behler, John L., F. Wayne King, National Audubon Society. The Audubon Society field guide to North American reptiles and amphibians. Knopf, 1979. ISBN 0394508246. Bonin, Franck, Bernard Devaux, Alain Dupré. Turtles of the world. Johns Hopkins University Press, 2006. ISBN 0801884969. Campbell, Jonathan A., William W. Lamar, Edmund D. Brodie III. The venomous reptiles of the Western Hemisphere. Comstock, 2004. ISBN 0801441412. Center for North American Herpetology. Center for North American herpetology: Promoting the preservation and conservation of North American amphibians, crocodilians, reptiles, and turtles through scholarship and information. Center for North American Herpetology. . Conant, Roger, Joseph T. Collins, Isabelle Hunt Conant. A field guide to reptiles and amphibians: Eastern and central North America. Houghton Mifflin, 1998. ISBN 0395904528. Ernst, Carl H. Evelyn M. Ernst. Snakes of the",
    "label": 0
  },
  {
    "text": "Center for North American Herpetology. . Conant, Roger, Joseph T. Collins, Isabelle Hunt Conant. A field guide to reptiles and amphibians: Eastern and central North America. Houghton Mifflin, 1998. ISBN 0395904528. Ernst, Carl H. Evelyn M. Ernst. Snakes of the United States and Canada. Smithsonian Books, 2003. ISBN 1588340198. Halliday, Tim and Kraig Alder. The Encyclopedia of Reptiles and Amphibians. Facts on File, 1986. Rossi, John, Roxanne Rossi. Snakes of the United States and Canada: Natural history and care in captivity. Krieger, 2003. ISBN 1575240319. Smith, Hobart Muir. Handbook of lizards: Lizards of the United States and of Canada. Comstock, 1995. ISBN 0801482364. Stebbins, Robert C. A field guide to western reptiles and amphibians. Houghton Mifflin, 2003. ISBN 0395982723. Bacteria Fungi Hall, Ian R. Edible and poisonous mushrooms of the world. Timber Press, 2003. ISBN 0881925861. Kuo, Michael, John David Moore, Darvin DeShazer. 100 edible mushrooms. University of Michigan Press, 2007. ISBN 9780472031269. Kuo, Michael. MushroomExpert.com. Michael Kuo, 2000-. . 2000–. Lincoff, Gary, Carol Nehring, National Audubon Society. The Audubon Society field guide to North American mushrooms. Knopf, 1981. ISBN 0394519922. McKnight, Kent H., Vera B. McKnight, National Audubon Society. A field guide to mushrooms, North America. Houghton Mifflin, 1998. ISBN 0395421012. Phillips, Roger, Geoffrey Kibby, Nicky Foy. Mushrooms and other fungi of North America. Firefly Books, 2005. ISBN 1554071151. Turner, Nancy J., Adam F. Szczawinski. Common poisonous plants and mushrooms of North America. Timber Press, 1991. ISBN 0881921793. Plants Bailey, Liberty Hyde & Ethel Zoe Bailey. Hortus Third: A Concise Dictionary of Plants Cultivated in the United States and Canada. Macmillan, 3rd ed., 1976. Barker, Joan. The encyclopedia of North American wild flowers. Parragon, 2004. ISBN 1405430354. Beckett, Kenneth A. The RHS Encyclopedia of House Plants Including Greenhouse Plants. Salem House, 1987. Brickell, Christopher. The American Horticultural Society Encyclopedia",
    "label": 0
  },
  {
    "text": "Canada. Macmillan, 3rd ed., 1976. Barker, Joan. The encyclopedia of North American wild flowers. Parragon, 2004. ISBN 1405430354. Beckett, Kenneth A. The RHS Encyclopedia of House Plants Including Greenhouse Plants. Salem House, 1987. Brickell, Christopher. The American Horticultural Society Encyclopedia of Garden Plants. Macmillan, 1989. Brickell, Christopher, Trevor J. Cole, American Horticultural Society. American Horticultural Society encyclopedia of plants and flowers. DK Publishing, 2002. ISBN 0789489937. Burrows, George E. Toxic plants of North America. Wiley-Blackwell, 2012. ISBN 978-0-8138-2034-7, doi:10.1002/9781118413425. Dirr, Michael. Dirr's hardy trees and shrubs: An illustrated encyclopedia. Timber Press, 1997. ISBN 0881924040. Flora of North America Association. Flora of North America. Flora of North America Association. . Flora of North America Editorial Committee. Flora of North America North of Mexico. Oxford University Press, 1993–2006. ISBN 0195057139. Gerard, John, Thomas Johnson. The herbal; or, General history of plants. Dover Publications, 1975. ISBN 048623147X. Graf, Alfred Byrd. Exotica Series 4 International: Pictorial Cyclopedia of Exotic Plants from Tropical and Near-Tropical Regions. Scribner's, 12th ed., 1985. Graf, Alfred Byrd. Hortica: A Color Cyclopedia of Garden Flora in All Climates and Indoor Plants. Macmillan, 1992. Graf, Alfred Byrd. Tropica: Color Cyclopedia of Exotic Plants. Scribner's, 3rd ed., 1986. Herwig, Rob. The New Good Housekeeping Encyclopedia of House Plants. Hearst, rev. ed., 1990. Heywood, V. H., D. M. Moore, I. B. K. Richardson. Flowering plants of the world. Oxford University Press, 1993. ISBN 0195210379. Heywood, Vernon H. and Stuart R. Chat. Popular Encyclopedia of Plants. Cambridge, 1982. Hillier's Manual of Trees and Shrubs. Van Nostrand, 5th ed., 1983. Hogan, Sean. Flora: A gardener's encyclopedia. Timber Press, 2003. ISBN 0881925381. Hora, Bayard. The Oxford Encyclopedia of Trees of the World. Oxford, 1981. Kaufman, Sylvan Ramsey, Wallace Kaufman. Invasive plants: A guide to identification and the impacts and control of common North American species. Stackpole",
    "label": 0
  },
  {
    "text": "encyclopedia. Timber Press, 2003. ISBN 0881925381. Hora, Bayard. The Oxford Encyclopedia of Trees of the World. Oxford, 1981. Kaufman, Sylvan Ramsey, Wallace Kaufman. Invasive plants: A guide to identification and the impacts and control of common North American species. Stackpole Books, 2007. ISBN 0811733653. Lowe, David W., John R. Matthews, Charles J. Moseley, World Wildlife Fund. The official World Wildlife Fund guide to endangered species of North America. Beacham, 1990–1994. ISBN 0933833172. Mitchell, Alan. Trees of North America. Facts on File, 1987. Moore, David M. The Marshall Cavendish Illustrated Encyclopedia of Plants and Earth Sciences. Marshall Cavendish, 1988. Nelson, Lewis S., Richard Shih, Michael J. Balick, Lewis R. Goldfrank, Andrew Weil, New York Botanical Garden. Handbook of poisonous and injurious plants. New York Botanical Garden; Springer, 2007. ISBN 0387312684. Ness, Bryan D. Magill's encyclopedia of science: Plant life. Salem Press, 2003. ISBN 1587650843. Phillips, Ellen and Colston Burrell. Rodale's Illustrated Encyclopedia of Perennials. Rodale, 1993. Quattrocchi, Umberto. CRC world dictionary of grasses: Common names, scientific names, eponyms, synonyms, and etymology. CRC/Taylor & Francis, 2006. ISBN 0849313031. Rätsch, Christian, Albert Hofmann, John R. Baker. The encyclopedia of psychoactive plants: Ethnopharmacology and its applications. Park Street Press, 2005. ISBN 0892819782. Solomon, Jim, Missouri Botanical Garden. W3Tropicos: VAST (VAScular Tropicos) nomenclatural database. Missouri Botanical Garden. . Spellenberg, Richard. National Audubon Society field guide to North American wildflowers: Western region. Knopf, 2001. ISBN 0375402330. Stace, Clive A., Hilli Thompson. New flora of the British Isles. Cambridge University Press, 1997. ISBN 0521589355. Stickney, Robert R. Encyclopedia of aquaculture. Wiley, 2000. ISBN 0471291013. Thieret, John W, William A. Niering, Nancy C. Olmstead. National Audubon Society field guide to North American wildflowers: eastern region. Alfred A. Knopf, 2001. ISBN 0375402322. Turner, Nancy J., Adam F. Szczawinski. Common poisonous plants and mushrooms of North America. Timber Press, 1991.",
    "label": 0
  },
  {
    "text": "William A. Niering, Nancy C. Olmstead. National Audubon Society field guide to North American wildflowers: eastern region. Alfred A. Knopf, 2001. ISBN 0375402322. Turner, Nancy J., Adam F. Szczawinski. Common poisonous plants and mushrooms of North America. Timber Press, 1991. ISBN 0881921793. Webere, Ewald. Invasive plant species of the world: A reference guide to environmental weeds. CABI, 2003. ISBN 0851996957. Zomlefer, Wendy B. Guide to flowering plant families. University of North Carolina Press, 1994. ISBN 0807821608. Botany and horticulture Bradley, Fern Marshall & Barbara W. Ellis. Rodale's All-New Encyclopedia of Organic Gardening: The Indispensable Resource for Every Gardener. Rodale, 1992. Brickell, Christopher. American Horticultural Society Encyclopedia of Gardening. Dorling Kindersley. ISBN 0789496534. Everett, Thomas H. The New York Botanical Garden Illustrated Encyclopedia of Horticulture. Garland, 1981–1982. Fell, Derek. The Encyclopedia of Flowers. Smithmark, 1993. Huxley, Anthony. The New Royal Horticultural Society Dictionary of Gardening. Stockton Press, 1992. Moggi, Guido and Luciano Guignolini. Simon & Schuster's Guide to Garden Flowers. Simon & Schuster, 1983. Seymour, E. L. D. The Wise Garden Encyclopedia. HarperCollins, rev. ed., 1990. Shoemaker, Candice A., Chicago Botanic Garden. Encyclopedia of gardens: History and design. Fitzroy Dearborn, 2001. ISBN 1579581730. Taylor, Patrick. The Oxford companion to the garden. Oxford University Press, 2006. ISBN 9780198662556. Taylor's Encyclopedia of Gardening. Houghton Mifflin, 4th ed., 1961. Thomas, Brian, Denis J. Murphy, Brian G. Murray. Encyclopedia of applied plant sciences. Elsevier Academic, 2003. ISBN 0122270509. Westcott's Plant Disease Handbook. Van Nostrand, 5th ed., 1990. Woods, Christopher. Encyclopedia of Perennials: A Gardener's Guide. Facts on File, 1982. Wyman's Gardening Encyclopedia. Macmillan, 2nd ed., 1986. Yepsen, Roger B., Jr. The Encyclopedia of Natural Insect and Disease Control. Rodale, rev. ed., 1984. Trees Burns, Russell M., Barbara H. Honkala, U. S. Dept. of Agriculture, Forest Service. Silvics of North America. U. S. Dept. of Agriculture,",
    "label": 0
  },
  {
    "text": "1986. Yepsen, Roger B., Jr. The Encyclopedia of Natural Insect and Disease Control. Rodale, rev. ed., 1984. Trees Burns, Russell M., Barbara H. Honkala, U. S. Dept. of Agriculture, Forest Service. Silvics of North America. U. S. Dept. of Agriculture, Forest Service. 1990. . Cafferty, Steve. Firefly encyclopedia of trees. Firefly Books, 2005. ISBN 1554070511. Davis, Richard C. Encyclopedia of American forest and conservation history. Macmillan; Collier Macmillan, 1983. ISBN 0029073502. Encyclopedia of Wood: A Tree-by-Tree Guide to the World's Most Versatile Resources. Facts on File, 1989. Little, Elbert Luther, Sonja Bullaty, Angelo Lomeo, National Audubon Society. The Audubon Society field guide to North American trees. Knopf, 1980. ISBN 0394507614. Plotnik, Arthur, Mary Phelan Morton Arboretum. The urban tree book: An uncommon field guide for city and town. Three Rivers Press, 2000. ISBN 0812931033. Preston, Richard Joseph, Richard R. Braham. North American trees. Iowa State Press, 2002. ISBN 0813815266. Rushforth, Keith, Charles Hollis. National Geographic field guide to the trees of North America. National Geographic, 2006. ISBN 0792253108. Russell, Tony, Catherine Cutler. Trees: An illustrated identifier and encyclopedia. Hermes House, 2003. ISBN 1843099764. Protists Biochemistry Scott, Thomas A. & Mary Brewer. Concise Encyclopedia of Biochemistry. Walter de Gruyter, 2nd ed., 1988. Bioethics Post, Stephen Garrard. Encyclopedia of bioethics. Third edition. Macmillan Reference USA, 2003. ISBN 0028657748. ISSN 0950-4125; DOI:10.1108/09504120510573477. (5-Volume Set; 3062 pages). Reich, Warren Thomas Encyclopedia of Bioethics. First edition. New York: Free Press, 1978. ISBN 0029261805. ISBN 978-0029261804. (4-Volume Set; 1933 pages) Reich, Warren Thomas Encyclopedia of Bioethics. Second edition. New York: Free Press, 1982. (5-Volume Set; 2950 pages) Reich, Warren Thomas Encyclopedia of Bioethics. Third edition. New York: Simon & Schuster Macmillan, 1995; London: Simon and Schuster and Prentice Hall International, c1995. Rev. ed. (5-Volume Set; 2950 pages; 464 articles) ISBN 0028973550. ISBN 978-0028973555. Food, nutrition, and",
    "label": 0
  },
  {
    "text": "pages) Reich, Warren Thomas Encyclopedia of Bioethics. Third edition. New York: Simon & Schuster Macmillan, 1995; London: Simon and Schuster and Prentice Hall International, c1995. Rev. ed. (5-Volume Set; 2950 pages; 464 articles) ISBN 0028973550. ISBN 978-0028973555. Food, nutrition, and agriculture Agriculture American Veterinary Medical Association. AVMA: American Veterinary Medical Association. American Veterinary Medical Association, 2006-.. Arntzen, Charles J. and Ellen M. Ritter. Encyclopedia of agricultural science. Academic Press, 1994. ISBN 0122266706. Bailey, Liberty Hyde. Cyclopedia of American agriculture. Macmillan, 1909. Bailey, L. H., Ethel Zoe Bailey, Liberty Hyde Bailey Hortorium. Hortus third: A concise dictionary of plants cultivated in the United States and Canada. Macmillan, 1976. ISBN 0025054708. Bains, William. Biotechnology from A to Z. Oxford University Press, 2004. ISBN 0198524986. Brooklyn Botanic Garden, Janet Marinelli, and Stephen K-M. Tim. Brooklyn Botanic Garden gardener's desk reference. Henry Holt, 1998. ISBN 0805050957. Burns, Russell M., Barbara H. Honkala, U.S. Dept. of Agriculture, Forest Service. Silvics of North America. U.S. Dept. of Agriculture, Forest Service, 1990. . Burrows, George E. Toxic plants of North America. Wiley-Blackwell, 2012. ISBN 0813820340.. Cordell, Charles E., U.S. Forest Service. Forest nursery pests. U.S. Department of Agriculture, Forest Service, 1989. Davis, Richard C. Encyclopedia of American forest and conservation history. Macmillan; Collier Macmillan, 1983. ISBN 0029073502. Felius, Marleen. Cattle breeds: An encyclopedia. Misset, 1995. ISBN 9054390174. Flickinger, Michael C. and Steven W. Drew. Encyclopedia of bioprocess technology: Fermentation, biocatalysis, and bioseparation. John Wiley & Sons, 1999. Goodman, Robert M. Encyclopedia of plant and crop science. M. Dekker, 2004. ISBN 0824709446. . Goreham, Gary. Encyclopedia of rural America: The land and people. ABC-CLIO, 1997. ISBN 0874368421. Hanelt, Peter, R. Buttner, Rudolf Mansfield Institut für Pflanzengenetik und Kulturpflanzenforschung Gatersleben, Germany. Mansfeld's encyclopedia of agricultural and horticultural crops (except ornamentals). Springer, 2001. ISBN 3540410171. Heldman, Dennis R. Encyclopedia of",
    "label": 0
  },
  {
    "text": "America: The land and people. ABC-CLIO, 1997. ISBN 0874368421. Hanelt, Peter, R. Buttner, Rudolf Mansfield Institut für Pflanzengenetik und Kulturpflanzenforschung Gatersleben, Germany. Mansfeld's encyclopedia of agricultural and horticultural crops (except ornamentals). Springer, 2001. ISBN 3540410171. Heldman, Dennis R. Encyclopedia of agricultural, food, and biological engineering. Marcel Dekker, 2003. ISBN 0824709381. Hillel, Daniel and Jerry L. Hatfield. Encyclopedia of soils in the environment. Elsevier/Academic Press, 2005. ISBN 0123485304. Holton, James R., Judith A. Curry, J. A. Pyle. Encyclopedia of atmospheric sciences. Academic Press, 2003. ISBN 0122270908. Howard, Philip H. Hearth Taub Printup. Handbook of environmental degradation rates. Lewis, 1991. ISBN 0873713583. Johnson, Warren T., Howard H. Lyon, C. S. Koehler. Insects that feed on trees and shrubs. Comstock Publishing Associates, 1991. ISBN 0801426022. Knobil, Ernst and Jimmy D. Neill. Encyclopedia of reproduction. Academic Press, 1998. ISBN 0122270207. Lal, R. Encyclopedia of soil science. Taylor & Francis, 2006. ISBN 0849350530. Lederberg, Joshua. Encyclopedia of microbiology. Academic Press, 2000. ISBN 0122268008. Lord, Tony and Andrew Lawson. The encyclopedia of planting combinations: The ultimate visual guide to successful plant harmony. Firefly, 2002. ISBN 1552096238. Maloy, Otis C. and Timothy D. Murray. Encyclopedia of plant pathology. Wiley, 2001. ISBN 0471298174. Melhorn, Heinz. Encyclopedia of parasitology. Springer, 2008. ISBN 3540489975. Montgomery, John H. Agrochemicals desk reference. CRC Press, 1997. ISBN 1566701678. Nau, Jim. Ball culture guide: The encyclopedia of seed germination. Ball, 1999. ISBN 188305219X. Plimmer, Jack R. Encyclopedia of agrochemicals. Wiley-Interscience, 2003. ISBN 0471193631. Quattrocchi, Umberto. CRC world dictionary of grasses: Common names, scientific names, eponyms, synonyms, and etymology. CRC/Taylor & Francis, 2006. ISBN 0849313031. Roginski, Hubert, John W. Puquay, P. F. Fox. Encyclopedia of dairy sciences. Academic Press, 2003. ISBN 0122272358. . Schapsmeier, Edward L. and Frederick H. Schapsmeier. Encyclopedia of American Agricultural History. Greenwood, 1975. Scharpf, Robert, U. S. Forest Service. Diseases of",
    "label": 0
  },
  {
    "text": "Roginski, Hubert, John W. Puquay, P. F. Fox. Encyclopedia of dairy sciences. Academic Press, 2003. ISBN 0122272358. . Schapsmeier, Edward L. and Frederick H. Schapsmeier. Encyclopedia of American Agricultural History. Greenwood, 1975. Scharpf, Robert, U. S. Forest Service. Diseases of Pacific Coast conifers. Forest Service, U.S. Dept. of Agriculture, 1993. ISBN 0160417651. Solomon, J. D., U.S. Dept. of Agriculture, Forest Service. Guide to insect borers in North American broadleaf trees and shrubs. U.S. Dept. of Agriculture, Forest Service, 1995. Stickney, Robert R. Encyclopedia of aquaculture. Wiley, 2000. ISBN 0471291013. Talbot, Ross B. Historical dictionary of the international food agencies: FAO, WFP, WFC, IFAD. Scarecrow Press, 1994. ISBN 0810828472. Thomas, Brian, Denis J. Murphy, Brian G. Murray. Encyclopedia of applied plant sciences. Elsevier Academic, 2003. ISBN 0122270509. U. S. Food and Drug Administration. FDA directory. Food and Drug Law Institute, 1900s–. Wexler, Philip, Bruce D. Anderson, Ann de Peyster. Encyclopedia of toxicology. Elsevier Academic, 2005. ISBN 0127453512. Wrigley, Colin W., Harold Corke, Charles E. Walker. Encyclopedia of grain science. Elsevier Academic Press, 2004. ISBN 0127654909. Food Albala, Ken. Food cultures of the world encyclopedia. Greenwood, 2011. ISBN 9780313376269. Claiborne, Craig. The New York Times Food Encyclopedia. Times Books, 1985. Considine, Douglas M. & Glenn D. Considine. Foods and Food Production Encyclopedia. Van Nostrand, 1982. Coyle, L. Patrick, Jr.. The World Encyclopedia of Food. Facts on File, 1982. Encyclopedia of Food Engineering. 2nd ed., AVI Publishing, 1986. Encyclopedia of Food Science. AVI Publishing, 1978. Encyclopedia of Food Technology. AVI Publishing, 1974. Ensminger, Audrey H. Foods and Nutrition Encyclopedia. Pegus Press, 1983. Francis, F. J. Encyclopedia of food science and technology. Wiley, 2000. ISBN 0471192856. Horn, Jane and Janet Fletcher. Cooking A to Z. Ortho Books, 1988. Hui, Y. H. Encyclopedia of Food Science and Technology. Wiley, 1992. Hui, Y. H., and J.",
    "label": 0
  },
  {
    "text": "F. J. Encyclopedia of food science and technology. Wiley, 2000. ISBN 0471192856. Horn, Jane and Janet Fletcher. Cooking A to Z. Ortho Books, 1988. Hui, Y. H. Encyclopedia of Food Science and Technology. Wiley, 1992. Hui, Y. H., and J. D. Culbertson. Handbook of food science, technology, and engineering. Taylor & Francis, 2006. ISBN 9780849398476. Kraig, Bruce and Colleen Taylor Sen. Street food around the world: an encyclopedia of food and culture. ABC-CLIO, 2013. ISBN 9781598849547. Macrae, Robert. Encyclopedia of Food Science, Food Technology, and Nutrition. Academic Press, 1993. Mariani, John F. The Dictionary of American Food and Drink. 2nd ed., Hearst, 1994. Montagne, Prosper & Robert J. Courtine. Larousse Gastronomique: All-New American Edition of the World's Greatest Culinary Encyclopedia. Rev. ed., Crown, 1988. Stobart, Tom. Cook's Encyclopedia. Harper & Row, 1981. Nutrition Ensminger, Audrey H. Foods and Nutrition Encyclopedia. Pegus Press, 1983. Food for Health: A Nutrition Encyclopedia. Pegus Press, 1986. Margen, Sheldon. The Wellness Encyclopedia of Food and Nutrition: How to Buy, Store, and Prepare Every Variety of Fresh Food. Rebus, 1992. Mount Sinai School of Medicine Complete Book of Nutrition. St. Martin's Press, 1990. Tver, David F. and Percy Russell. Nutrition and Health Encyclopedia. 2nd ed., Van Nostrand, 1989. Winick, Myron. The Columbia Encyclopedia of Nutrition. Putnam, 1988. Yudin, John. Penguin Encyclopedia of Nutrition. Viking, 1985. Wines, beers, and spirits Alexis Lichine's New Encyclopedia of Wines and Spirits. 5th ed., Knopf, 1987. Grossman's Guide to Wines, Beers, & Spirits. 7th ed., Scribner's, 1983. Hugh Johnson's Modern Encyclopedia of Wine. 3rd ed., Simon & Schuster, 1991. Hugh Johnson's World Atlas of Wine. 3rd ed., Simon & Schuster, 1985. Robinson, Jancis. The Oxford companion to wine. Oxford University Press, 1999. ISBN 019866236X. Schoonmaker, Frank. The New Frank Schoonmaker Encyclopedia of Wine. Rev. ed., Morrow, 1988. Health, medicine, and drugs",
    "label": 0
  },
  {
    "text": "World Atlas of Wine. 3rd ed., Simon & Schuster, 1985. Robinson, Jancis. The Oxford companion to wine. Oxford University Press, 1999. ISBN 019866236X. Schoonmaker, Frank. The New Frank Schoonmaker Encyclopedia of Wine. Rev. ed., Morrow, 1988. Health, medicine, and drugs Adelman, George. Encyclopedia of Neuroscience. Birkhauser Boston, 1987. Fishbein, Morris. Fishbein's Illustrated Medical and Health Encyclopedia. H. S. Struttman, 1983. Who's who in medicine and healthcare. Marquis Who's Who, 1996–. ISSN 0000-1708. Drugs Plumb, Donald C. Plumb's veterinary drug handbook. PharmaVet; Distributed by Wiley, c2011. ISBN 9780470959640. Rätsch, Christian, Albert Hofmann, John R. Baker. The encyclopedia of psychoactive plants: Ethnopharmacology and its applications. Park Street Press, 2005. ISBN 0892819782. Stafford, Peter. Psychedelics Encyclopedia. 3rd ed., Ronin Publishing, 1992. Swarbrick, James and James C. Boylan. Encyclopedia of Pharmaceutical Technology. Marcel Dekker, 1988-. Drug and alcohol abuse Evans, Glen. The Encyclopedia of Drug Abuse. 2nd ed., Facts on File, 1991. Fay, John. Alcohol/Drug Abuse Dictionary and Encyclopedia. Charles C. Thomas, 1988. O'Brien, Robert & Morris Chaftetz. The Encyclopedia of Alcoholism. 2nd ed., Facts on File, 1991. Health Accomasso, V. (1995). Enciclopedia della medicina (in Italian). Vallardi A. ISBN 978-88-11-93806-4. Ammer, Christine (1997). Gesundheitslexikon der Frau: Anatomie, Diagnose, Therapie, Medikamente, Hormone, Sexualität, Schwangerschaft und Geburt, Seelische Gesundheit (in German). Dt. Taschenbuch-Verlag. ISBN 978-3-423-36014-2. Axt-Gadermann, Michaela (2005). Bertelsmann, Das grosse Gesundheitslexikon (in German). Wissen-Media-Verlag. ISBN 978-3-577-10373-2. Baert, Albert L. (2007). Encyclopedia of Diagnostic Imaging. Springer. ISBN 978-3-540-35278-5. Beers, Mark H.; Fletcher, Andrew J. (2008). Encyclopédie médicale: Le Manuel Merck (in French). Larousse. ISBN 978-2-03-582305-2. Breslow, Lester (2002). Encyclopedia of Public Health. MacMillan USA. ISBN 978-0-02-865350-1. Bullough, Vern L. (2001). Encyclopedia of Birth Control. ABC-CLIO. ISBN 978-1-57607-181-6. Fermie, Peter; Shepherd, Stephen; Kelly, Robert F. (2004). Family Health Encyclopedia: The Comprehensive Guide to the Whole Family's Health Needs. Lorenz Books. ISBN 978-0-7548-1162-6. Fink, George. Encyclopedia of",
    "label": 0
  },
  {
    "text": "978-0-02-865350-1. Bullough, Vern L. (2001). Encyclopedia of Birth Control. ABC-CLIO. ISBN 978-1-57607-181-6. Fermie, Peter; Shepherd, Stephen; Kelly, Robert F. (2004). Family Health Encyclopedia: The Comprehensive Guide to the Whole Family's Health Needs. Lorenz Books. ISBN 978-0-7548-1162-6. Fink, George. Encyclopedia of stress. Elsevier, 2007. ISBN 0120885034. Gullotta, Thomas P.; Bloom, Martin (2003). Encyclopedia of Primary Prevention and Health Promotion. Springer. ISBN 978-0-306-47296-1. Hamburger, Jean; Méry, Jean-Philippe; Leporrier, Michel (1981). Petite encyclopédie médicale: guide de pratique médicale (in French). Flammarion. 1,421 pages. ISBN 9782257123008. Jacoby, David B.; Youngson, R. M. (2004). Encyclopedia Of Family Health. Marshall Cavendish. ISBN 978-0-7614-7486-9. Kipman, Simon-Daniel (2005). Dictionnaire critique des termes de psychiatrie et de santé mentale (in French). Wolters Kluwer France. ISBN 978-2-7040-1163-6. Langer, Jerk W. (2009). Politikens bog om barnets sygdomme (in Danish). Gyldendals Bogklubber. ISBN 978-87-03-03314-3. Loue, Sana; Sajatovic, Martha (2008). Encyclopedia of Aging and Public Health. Springer. ISBN 978-0-387-33753-1. Loue, Sana; Sajatovic, Martha (2012). Encyclopedia of Immigrant Health. Springer. ISBN 978-1-4419-5659-0. Lovejoy, Frederick H. (1987). The New child health encyclopedia: the complete guide for parents. Delacorte Press. ISBN 978-0-385-29541-3. Macmillan Health Encyclopedia. Macmillan, 1993. The Marshall Cavendish Encyclopedia of Family Health. Marshall Cavendish, 1991. Navarra, Tova (2004). The Encyclopedia of Vitamins, Minerals, and Supplements. Infobase Publishing. ISBN 978-1-4381-2103-1. Ohm, Christian; Curic, Anton (1999). Gesundheitslexikon von A – Z.: 3000 verständlich formulierte Einträge zu Krankheiten und ihren Symptomen, Naturheilverfahren und Vorsorge (in German). Eco Verlag GmbH. ISBN 978-3-933468-52-9. QA International Collectif (2010). L'encyclopédie Familiale de la Santé: Comprendre, Prévenir, Soigner (in French). Québec Amerique. ISBN 978-2-7644-0865-0. Quevauvilliers, Jacques; Perlemuter, Léon; Perlemuter, Gabriel (2009). Dictionnaire médical de l'infirmière (in French). Elsevier Masson. ISBN 978-2-9941008-0-5. Reiche, Dagmar (2003). Roche-Lexikon Medizin (in German). Urban & Fischer. ISBN 978-3-437-15180-4. Reuter, Peter (2004). Springer Lexikon Medizin (in German). Springer DE. ISBN 978-3-540-20412-1. Rizzoli (2001). Grande Enciclopedia Rizzoli della Salute",
    "label": 0
  },
  {
    "text": "médical de l'infirmière (in French). Elsevier Masson. ISBN 978-2-9941008-0-5. Reiche, Dagmar (2003). Roche-Lexikon Medizin (in German). Urban & Fischer. ISBN 978-3-437-15180-4. Reuter, Peter (2004). Springer Lexikon Medizin (in German). Springer DE. ISBN 978-3-540-20412-1. Rizzoli (2001). Grande Enciclopedia Rizzoli della Salute e del Benessere. Con 6 CD-ROM (in Italian). Rizzoli. ISBN 978-88-17-93198-4. Ronzio, Robert A. (2003). The Encyclopedia of Nutrition and Good Health. Infobase Publishing. ISBN 978-0-8160-6630-8. Rothenberg, Robert E. (1984). Enciclopedia Medica Garzanti (in Italian). Garzanti. Turkington, Carol; Mitchell, Deborah R. (2010). The Encyclopedia of Alzheimer's Disease. Infobase Publishing. ISBN 978-1-4381-2858-0. U.S. National Library Medicine (2012): MedlinePlus including the freely accessible Medical Encyclopedia. Wagman, Richard J., ed. (1987). The medical and health encyclopedia. J.G. Ferguson Pub. Co. ISBN 978-0-89434-077-2. Wellness Encyclopedia. Houghton Mifflin, 1991. Physical disorders Wynbrandt, James & Mark D. Ludman. The Encyclopedia of Genetic Disorders and Birth Defects. Facts on File, 1991. Hearing disorders Turkington, Carol and Allen Sussman. Encyclopedia of Deafness and Hearing Disorders. Facts on File, 1992. Van Cleve, John V. Gallaudet Encyclopedia of Deaf People and Deafness. McGraw-Hill, 1987. Vision impairment Sardegna, Jill & T. Otis Paul. The Encyclopedia of Blindness and Vision Impairment. Facts on File, 1991. Women's health Ammer, Christine. The New A to Z of Women's Health: A Concise Encyclopedia. Facts on File, 1989. Foley, Denise and Eileen Nechas. Women's Encyclopedia of Health & Emotional Healing. Rodale, 1993. New Our Bodies, Ourselves. Rev. ed., Simon & Schuster, 1992. Medicine Bailey, Hamilton, W. J. Bishop, Harold Ellis. Bailey and Bishop's notable names in medicine and surgery. Lewis, 1983. ISBN 0718604660. Bendiner, Jessica, Elmer Bendiner. Biographical dictionary of medicine. Facts on File, 1990. ISBN 0816018642. Berkow, Robert. The Merck Manual of Diagnosis and Therapy. 16th ed., Merck, 1992. Bremer, Sydney, Jeffrey H. Miller, William Broughton. Encyclopedia of genetics. Academic Press, 2002. ISBN 0122270800. Bynum,",
    "label": 0
  },
  {
    "text": "Bendiner. Biographical dictionary of medicine. Facts on File, 1990. ISBN 0816018642. Berkow, Robert. The Merck Manual of Diagnosis and Therapy. 16th ed., Merck, 1992. Bremer, Sydney, Jeffrey H. Miller, William Broughton. Encyclopedia of genetics. Academic Press, 2002. ISBN 0122270800. Bynum, W. F., Helen Bynum. Dictionary of medical biography. Greenwood Press, 2006. ISBN 0313328773. Clayman, Charles B. The American Medical Association Encyclopedia of Medicine. Random House, 1989. Delves, Peter J., Ivan M. Roitt. Encyclopedia of immunology. Academic Press, 1998. ISBN 0122267656. Fischer, Isidor. Biographisches Lexikon der hervorragenden Ärzte der letzten 50 Jahre. 1932–33. Ganten, Detlev, Klaus Ruckpaul. Encyclopedic reference of genomics and proteomics in molecular medicine. Springer, 2005. ISBN 3540442448. Gomez, Joan. Dictionary of Symptoms. Rev. ed., Stein & Day, 1985. Griffith, H. Winter. Complete Guide to Symptoms, Illness & Surgery. 2nd ed., Price Stern Sloan, 1989. Hafner, Arthur Wayne, Fred W. Hunter, B. Michael Tarpey. Directory of deceased American physicians, 1804–1929: A genealogical guide to over 149,000 medical practitioners providing brief biographical sketches drawn from the American Medical Association's Deceased Physician Masterfile. American Medical Association, c1993. ISBN 0899705278. Hirsch, August, Ernst Julius Gurlt, A. Wernich, W. Haberling. Biographisches Lexikon der hervorragenden Ärzte aller Zeiten und Völker. Urban & Schwarzenberg, 1929–1934. (German) Jorde, Lynn B. Encyclopedia of genetics, genomics, proteomics, and bioinformatics. John Wiley and Sons, 2005. ISBN 9780470849743. Kahl, Günter. The dictionary of gene technology: Genomics, transcriptomics, proteomics. Wiley-VCH, 2004. ISBN 3527307656. Kaufman, Martin, Stuart Galishoff, Todd Lee Savitt. Dictionary of American medical biography. Greenwood Press, 1984. ISBN 031321378X. Kazazian, Haig H., John Wiley & Sons. Wiley encyclopedia of molecular medicine. John Wiley & Sons, 2002. ISBN 0471374946. Kaufman, Martin, Joellen Watson Hawkins, Loretta P. Higgins, Alice Howell Friedman. Dictionary of American nursing biography. Greenwood Press, 1988. ISBN 0313245207. Kelly, Howard A., Walter L. Burrage. Dictionary of American medical biography:",
    "label": 0
  },
  {
    "text": "medicine. John Wiley & Sons, 2002. ISBN 0471374946. Kaufman, Martin, Joellen Watson Hawkins, Loretta P. Higgins, Alice Howell Friedman. Dictionary of American nursing biography. Greenwood Press, 1988. ISBN 0313245207. Kelly, Howard A., Walter L. Burrage. Dictionary of American medical biography: Lives of eminent physicians of the United States and Canada, from the earliest times. London: D. Appleton and Co., 1928. Leroy, Francis. A century of Nobel prize recipients: Chemistry, physics, and medicine. Marcel Dekker, 2003. ISBN 0824708768. Levin, Beatrice. Women and medicine. Scarecrow Press, 2002. ISBN 0810842386. Magner, Lois N. Doctors, nurses, and medical practitioners: A bio-bibliographical sourcebook. Greenwood Press, 1997. ISBN 0313294526. Marquis Who's Who. Who's who in medicine and healthcare. Marquis Who's Who, 1996-. Martin, Edward A., Peter Froggatt. A biographical encyclopedia of medical travel authors. Edwin Mellen Press, 2010–. ISBN 9780773436817. Meyers, Robert A. Encyclopedia of molecular cell biology and molecular medicine. Wiley-VCH Verlag, 2004–2005. ISBN 3527305432. Miller, Sigmund. Symptoms: The Complete Home Medical Encyclopedia. Crowell, 1976. Mosby Medical Encyclopedia. Rev. ed., Penguin USA, 1992. Munk, William, G. H. Brown, Richard Robertson Trail. The roll of the Royal College of Physicians of London: Comprising biographical sketches of all the eminent physicians whose names are recorded in the Annals... Royal College of Physicians of London, 1878–. Pagel, Julius. Biographisches Lexikon hervorragender Ärzte des neunzehnten Jahrhunderts. Urban & Schwarzenberg, 1901. (German). Physician's Desk Reference. Medical Economics, 1947-. Pinzon, Soledad Mata, Carlos Zolla, Diego Méndez Granados Instituto Nacional Indigenista (Mexico). Diccionario enciclopédico de la medicina tradicional mexicana. Instituto Nacional Indigenista, 1994. ISBN 9682961335. Rédei, G. P. Encyclopedic dictionary of genetics, genomics, and proteomics. Wiley-Liss, 2003. ISBN 0471268216. Riott, Ivan M. & Peter J. Delves. Encyclopedia of Immunology. Academic Press, 1992. Rosenfeld, Isadore. Symptoms. Simon & Schuster, 1989. Rothenberg, Robert. New Illustrated Medical Encyclopedia for Home Use. Rev. ed., Galahad, 1986.",
    "label": 0
  },
  {
    "text": "genomics, and proteomics. Wiley-Liss, 2003. ISBN 0471268216. Riott, Ivan M. & Peter J. Delves. Encyclopedia of Immunology. Academic Press, 1992. Rosenfeld, Isadore. Symptoms. Simon & Schuster, 1989. Rothenberg, Robert. New Illustrated Medical Encyclopedia for Home Use. Rev. ed., Galahad, 1986. Scrivener, Laurice, J. Suzanne Barnes. A biographical dictionary of women healers: Midwives, nurses, and physicians. Oryx Press, 2002. ISBN 157356219X. Talbot, C. H., Eugene Ashby Hammond. The medical practitioners in medieval England: A biographical register. Wellcome Historical Medical Library, 1965. Tver, David F. & Howard F. Hunt. Encyclopedic Dictionary of Sports Medicine. Routledge, Chapman & Hall, 1986. Walton, John. The Oxford Companion to Medicine. Oxford, 1986. Webster, John G. Encyclopedia of Medical Devices and Instrumentation. Wiley, 1988. Windsor, Laura Lynn. Women in medicine: An encyclopedia. ABC-CLIO, 2002. ISBN 1576073920. The World Book/Rush-Presbyterian-St. Luke's Medical Center Medical Encyclopedia: Your Guide to Good Health. World Book, 1991. History of medicine Bynum, W. F. and Roy Porter. Companion Encyclopedia of the History of Medicine. Routledge, 1994. McGrew, Roderick E. and Margaret P. McGrew. Encyclopedia of Medical History. McGraw-Hill, 1985. Medical ethics Boyd, Kenneth M., Roger Higgs, A. J. Pinching. The new dictionary of medical ethics. BMJ Publ., 1997. ISBN 0727910019. Medical tests Griffith, H. Winter. Complete Guide to Medical Tests. Fisher Books, 1988. Pinckney, Cathey and Edward R. Pinckney. Do-It-Yourself Medical Testing. 3rd ed., Facts on File, 1989. Pinckney, Cathey and Edward R. Pinckney. The Patient's Guide to Medical Tests. 3rd ed., Facts on File, 1986. Shtasel, Philip. Medical Tests and Diagnostic Procedures. HarperCollins, 1991. Sobel, David and Tom Ferguson. People's Book of Medical Tests. Summit Books, 1985. Natural and alternative medicine Bricklin, Mark. Practical Encyclopedia of Natural Healing. Rev. ed., Rodale, 1983. Bricklin, Mark. Rodale's Encyclopedia of Natural Home Remedies. Rodale, 1982. Murray, Michael T. & Joseph E. Pizzorno. An Encyclopedia of",
    "label": 0
  },
  {
    "text": "Medical Tests. Summit Books, 1985. Natural and alternative medicine Bricklin, Mark. Practical Encyclopedia of Natural Healing. Rev. ed., Rodale, 1983. Bricklin, Mark. Rodale's Encyclopedia of Natural Home Remedies. Rodale, 1982. Murray, Michael T. & Joseph E. Pizzorno. An Encyclopedia of Natural Medicine. Prima Publishing, 1991. Olsen, Kristin. Encyclopedia of Alternative Health Care. Pocket Books, 1990. Visual Encyclopedia of Natural Healing. Rodale, 1991. Wade, Carlson. Home Encyclopedia of Symptoms, Ailments, and Their Natural Remedies. Parker, 1991. Nursing Marquis Who's Who. Who's who in American nursing. Marquis Who's Who, 1984-. Veterinary medicine American Veterinary Medical Association. AVMA: American Veterinary Medical Association. American Veterinary Medical Association, 2006-.. Concise Veterinary Dictionary. Oxford, 1988. Griffin, James and Tom Gore. Horse Owner's Veterinary Handbook. Howell Book House, 1989. Merck Veterinary Manual. Merck. Plumb, Donald C. Plumb's veterinary drug handbook. PharmaVet; Distributed by Wiley, c2011. ISBN 9780470959640. West, Geoffrey. Black's Veterinary Dictionary. Barnes & Noble Books, 1992. Microbiology Britton, Gabriel. Encyclopedia of environmental microbiology. Wiley, 2002. ISBN 0471354503. Lederberg, Joshua. Encyclopedia of microbiology. Academic Press, 2000. ISBN 0122268008. Neuroscience Houdé, Olivier Daniel Kayser, Vivian Waltz, Christian Cav. Dictionary of cognitive science: Neuroscience, psychology, artificial intelligence, linguistics, and philosophy. Psychology Press, 2004. ISBN 1579582516. Prehistoric life Dixon, Dougal. The Macmillan Illustrated Encyclopedia of Dinosaurs and Prehistoric Animals: A Visual Who's Who of Prehistoric Life. Macmillan, 1988. Encyclopedia of Prehistoric Life. McGraw-Hill, 1979. Shells Abbott, Tucker and Peter Dance. Compendium of Seashells. American Melacologists, 4th ed., 1990. Collector's Encyclopedia of Shells. McGraw-Hill, 1982. Rehder, Harald Alfred, James H. Carmichael, Jr., Carol Nehring, National Audubon Society. The Audubon Society field guide to North American seashells. Knopf, 1981. ISBN 0394519132. Wye, Kenneth F. The Encyclopedia of Shells. Facts on File, 1991. Murray, Michael T. Footnotes References Guide to Reference. American Library Association. Retrieved 5 December 2014. (subscription required). Kister, Kenneth F.",
    "label": 0
  },
  {
    "text": "Biological constraints are factors which make populations resistant to evolutionary change. One proposed definition of constraint is \"A property of a trait that, although possibly adaptive in the environment in which it originally evolved, acts to place limits on the production of new phenotypic variants.\" Constraint has played an important role in the development of such ideas as homology and body plans. Types of constraint Any aspect of an organism that has not changed over a certain period of time could be considered to provide evidence for \"constraint\" of some sort. To make the concept more useful, it is therefore necessary to divide it into smaller units. First, one can consider the pattern of constraint as evidenced by phylogenetic analysis and the use of phylogenetic comparative methods; this is often termed phylogenetic inertia, or phylogenetic constraint. It refers to the tendency of related taxa sharing traits based on phylogeny. Charles Darwin spoke of this concept in his 1859 book \"On the Origin of Species\", as being \"Unity of Type\" and went on to explain the phenomenon as existing because organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa due to these constraints. If one sees particular features of organisms that have not changed over rather long periods of time (many generations), then this could suggest some constraint on their ability to change (evolve). However, it is not clear that mere documentation of lack of change in a particular character is good evidence for constraint in the sense of the character being unable to change. For example, long-term stabilizing selection related to stable environments might cause stasis. It has often been considered more",
    "label": 0
  },
  {
    "text": "lack of change in a particular character is good evidence for constraint in the sense of the character being unable to change. For example, long-term stabilizing selection related to stable environments might cause stasis. It has often been considered more fruitful, to consider constraint in its causal sense: what are the causes of lack of change? Stabilizing selection The most common explanation for biological constraint is that stabilizing selection acts on an organism to prevent it changing, for example, so that it can continue to function in a tightly defined niche. This may be considered to be a form of external constraint, in the sense that the organism is constrained not by its makeup or genetics, but by its environment. The implication would be that if the population was in a new environment, its previously constrained features would potentially begin to evolve. Functional coupling and physico-chemical constraint Related to the idea of stabilizing selection is that of the requirement that organisms function adequately in their environment. Thus, where stabilizing selection acts because of the particular niche that is occupied, mechanical and physico-chemical constraints act in a more general manner. For example, the acceleration caused by gravity places constraints on the minimum bone density and strength for an animal of a particular size. Similarly, the properties of water mean that tissues must have certain osmotic properties in order to function properly. Functional coupling takes the idea that organisms are integrated networks of functional interactions (for example, the vertebral column of vertebrates is involved in the muscle, nerve, and vascular systems as well as providing support and flexibility) and therefore cannot be radically altered without causing severe functional disruption. This may be viewed as one type of trade-off. As Rupert Riedl pointed out, this degree of functional constraint — or burden —",
    "label": 0
  },
  {
    "text": "well as providing support and flexibility) and therefore cannot be radically altered without causing severe functional disruption. This may be viewed as one type of trade-off. As Rupert Riedl pointed out, this degree of functional constraint — or burden — generally varies according to position in the organism. Structures literally in the centre of the organism — such as the vertebral column — are often more burdened than those at the periphery, such as hair or toes. Lack of genetic variation and developmental integration This class of constraint depends on certain types of phenotype not being produced by the genotype (compare stabilizing selection, where there is no constraint on what is produced, but rather on what is naturally selected). For example, for a highly homozygous organism, the degree of observed phenotypic variability in its descendants would be lower than those of a heterozygous one. Similarly, developmental systems may be highly canalised, to prevent the generation of certain types of variation. Relationships of constraint classes Although they are separate, the types of constraints discussed are nevertheless relatable to each other. In particular, stabilizing selection, mechanical, and physical constraints might lead through time to developmental integration and canalisation. However, without any clear idea of any of these mechanisms, deducing them from mere patterns of stasis as deduced from phylogenetic patterns or the fossil record remains problematic. In addition, the terminology used to describe constraints has led to confusion. Examples “Variational inaccessibility. Despite mutations, certain character variants are never produced. These variants are therefore developmentally impossible to achieve and are never introduced into a population. This is implied by canalization and has been called both genetic and developmental constraint.” See also Allometry Convergent evolution Carrier's constraint Trade-off References Further reading Garland, Jr., T., C. J. Downs, and A. R. Ives. (2022). Trade-offs (and",
    "label": 0
  },
  {
    "text": "The biology of romantic love has been explored by such biological sciences as evolutionary psychology, evolutionary biology, anthropology and neuroscience. Neurochemicals and hormones such as dopamine and oxytocin are studied along with a variety of interrelated brain systems which produce the psychological experience and behaviors of romantic love. The study of romantic love is still in its infancy. As of 2021, there were a total of 42 biological studies on romantic love. Definition of romantic love The meaning of the term \"romantic love\" has changed considerably throughout history, making it difficult to simply define. Initially it was coined to refer to certain attitudes and behaviors described in a body of literature now referred to as courtly love. However, academic psychology and especially biology also consider romantic love in a different sense, which refers to a brain system (or systems) related to pair bonding or mating with associated psychological properties. Bode and Kushnick undertook a comprehensive review of romantic love from a biological perspective in 2021. They considered the psychology of romantic love, its mechanisms, development across the lifespan, functions, and evolutionary history. Based on the content of that review, they proposed a biological definition of romantic love: Romantic love is a motivational state typically associated with a desire for long-term mating with a particular individual. It occurs across the lifespan and is associated with distinctive cognitive, emotional, behavioral, social, genetic, neural, and endocrine activity in both sexes. Throughout much of the life course, it serves mate choice, courtship, sex, and pair-bonding functions. It is a suite of adaptations and by-products that arose sometime during the recent evolutionary history of humans. Romantic love in this sense is also not necessarily \"dyadic\", \"social\" or \"interpersonal\", despite being related to pair bonding. Romantic love can be experienced outside the context of a relationship,",
    "label": 0
  },
  {
    "text": "that arose sometime during the recent evolutionary history of humans. Romantic love in this sense is also not necessarily \"dyadic\", \"social\" or \"interpersonal\", despite being related to pair bonding. Romantic love can be experienced outside the context of a relationship, for example in the case of unrequited love where the feelings are not reciprocated. A person can develop romantic love feelings before any relationship has occurred, for only a potential partner. The potential partner can even be somebody they do not know well or aren't acquainted with at all, as in cases of love at first sight and parasocial attachments. The early stage of romantic love (which has obsessive and addictive features) might also be referred to as being \"in love\", passionate love, infatuation, limerence or obsessive love. Research has never settled on a unified terminology or set of methods. Distinctions are drawn between this early stage of romantic love and the \"attachment system\" theorized by the attachment theorists like John Bowlby. In the past, attachment theorists have argued that attachment theory and attachment styles can replace other theories of love, but academics on love have argued this is incorrect and that romantic love and attachment are not identical concepts. The early stage of romantic love is thought to involve additional brain systems for other purposes, with distinct evolutionary histories. Romantic love is also distinct from sexual attraction, although they most often occur together. Variation exists in the way romantic love is expressed in the population. A cross-cultural study of currently in-love people found four clusters, with varying degrees of intensity, obsessive thinking, commitment, frequency of sex and other differences. Other studies indicate romantic love can be experienced both with or without obsessional features. Typically, intense romantic love is limited to a duration of 12–18 months or as long as",
    "label": 0
  },
  {
    "text": "intensity, obsessive thinking, commitment, frequency of sex and other differences. Other studies indicate romantic love can be experienced both with or without obsessional features. Typically, intense romantic love is limited to a duration of 12–18 months or as long as 3 years, depending on the estimate; however, in a rare phenomenon called \"long-term intense romantic love\", some people experience intense attraction inside a relationship, even for 10 years or more. This is similar to early-stage intense romantic love, but at this later stage they exhibit less of the obsessional features. Independent emotion systems Helen Fisher and her colleagues proposed that the brain systems involved with mammalian reproduction can be separated into at least three parts: Neuroscientists currently believe that the basic emotions arise from distinct circuits (or systems) of neural activity; that humans share several of these primary emotion-motivation circuits with other mammals; and that these brain systems evolved to direct behavior [...]. It is hypothesized that among these primary neural systems are at least three discrete, interrelated emotion-motivation systems in the mammalian brain for mating, reproduction, and parenting: lust, attraction, and attachment [...]. Lust is the sex drive, or libido. Attraction (or early-stage romantic love, also called passionate love or infatuation) is associated with feelings of exhilaration, obsessive (or \"intrusive\") thoughts and the craving for emotional union. Attachment (the attachment system from attachment theory, and also called companionate love) is associated with feelings of calm, security and comfort, but separation anxiety when apart. In Fisher's theory, the systems tend to act in unison, but may become disassociated and act independently. For example, a person in a long-term partnership may feel deep attachment for their spouse, while experiencing intense romantic love (attraction) for some other individual, while being sexually attracted (lust) to still others, all at the same time. Lisa",
    "label": 0
  },
  {
    "text": "independently. For example, a person in a long-term partnership may feel deep attachment for their spouse, while experiencing intense romantic love (attraction) for some other individual, while being sexually attracted (lust) to still others, all at the same time. Lisa Diamond has also used independent emotions theory to explain why people can 'fall in love' sometimes without sexual desire, as in the case of \"platonic\" infatuation for a friend. Fisher associates each system with different neurotransmitters and/or hormones (lust: estrogen & androgens; attraction: dopamine, norepinephrine & serotonin; attachment: oxytocin & vasopressin), but modern research shows these associations are not as clearly defined as Fisher's theory proposes. Additionally, romantic love has been associated with endogenous opioids, cortisol and nerve growth factor, which are not included in Fisher's earlier model. With respect to the idea that the systems are independent, a more modern theory holds that the attachment system is active in early-stage romantic love, in addition to the infatuation component. Fisher's model is considered outdated, although the idea of interrelated systems is useful. Evolution of systems Evolutionary psychology Evolutionary psychology is seen as an organizing framework which offers explanations behind psychological functions (rather than merely describing them), as well as specifying theoretical constraints, like requiring that a given trait is adaptive in the form of providing reproductive benefit to an individual. Evolutionary psychology has proposed a variety of explanations for romantic love. Romantic love is a powerful commitment device. Romantic love suppresses the search for alternative mates (even irrationally so, when a more desirable one comes along), and signals this to the partner. Romantic love may also signal to alternative mates, disincentivizing them from pursuing oneself. The emergence of longer pair bonds in the evolution of humans coincided with the emergence of concealed ovulation, where it cannot (in general) be determined",
    "label": 0
  },
  {
    "text": "the partner. Romantic love may also signal to alternative mates, disincentivizing them from pursuing oneself. The emergence of longer pair bonds in the evolution of humans coincided with the emergence of concealed ovulation, where it cannot (in general) be determined when a woman is ovulating, requiring partners to stay together while having sex during the entire menstrual cycle. Commitment is seen as adaptive to facilitate this, and to facilitate child care. Love feelings might be the psychological reward produced by the brain when the problem of commitment is being solved. The intensity of romantic love feelings and why people become \"fools for love\" can be explained in terms of the handicap principle, which states that a contention arises between honest and fake signaling. When real emotions evolved, a niche would have been created for sham emotions (e.g. fake facial expressions) which are less risky to express. One explanation for why honest signals can evolve without becoming worthless (because of competing fakers) is that the honest signal can evolve if it's too expensive to fake. One example in nature is the peacock's tail, an example of conspicuous consumption, a cumbersome display which consumes nutrients. Only a healthy peacock can afford it, so in that case it may have evolved because it was a handicap, and used by females of the species as an indicator of health. Romantic love may have evolved to be as bewitching and besotting as it is, \"like handcuffing oneself to railroad tracks\", as a handicap meant to prove that one's commitment is truly real. Romantic love may have evolved to override rationality, so that one reproduces regardless of the considerable costs of raising a child, and regardless of any rational will to be single or child-free. Romantic love signals parental investment. Paternal investment in the form of",
    "label": 0
  },
  {
    "text": "have evolved to override rationality, so that one reproduces regardless of the considerable costs of raising a child, and regardless of any rational will to be single or child-free. Romantic love signals parental investment. Paternal investment in the form of pair bonds has been linked to better outcomes for children, both as infants and as they grow older. Children raised in pair bonds are more socially competitive and more likely to survive to reproductive age. Being in love makes people more creative, so romantic love may have evolved as a courtship display. It has been suggested that art, music and literature serve a function like a peacock's tail, but as a display of mental prowess, designed to impress and make a potential partner swoon. Creativity is believed by some authors to be especially a part of the male courtship display. Romantic love may conserve time and metabolic energy by focusing courtship efforts on a specific individual over others. Successful pair bonding predicts better health and survival. Happy, well-functioning romantic relationships contribute to mental and physical health, especially when stress is encountered. The end of a pair bond (e.g. divorce) is associated with vulnerability, such as to disease, depression, substance abuse, or negative outcomes for children. Victims of a heart attack, for example, are more likely to have another when they live alone. Monogamous pair bonding helps prevent sexually transmitted diseases (STDs) which compromise fertility, especially for women. Certain STDs (e.g. syphilis) increase the risk of miscarriage, and otherwise harm or can be passed to an unborn child. The strongest predictor of contracting an STD is the number of sexual partners, so limiting this is the best way to limit the risk of contracting a disease which would harm one's reproductive health. Romantic love promotes exclusivity via mate guarding. Romantic jealousy",
    "label": 0
  },
  {
    "text": "of contracting an STD is the number of sexual partners, so limiting this is the best way to limit the risk of contracting a disease which would harm one's reproductive health. Romantic love promotes exclusivity via mate guarding. Romantic jealousy is one of the most common correlates of being in love, which evolved as a protection from the threat of losing one's love to a romantic rival. Jealousy is seen as adaptive (when it motivates one to maintain their relationship) up to a point, but can also take the form of pathological jealousy where a sufferer has a delusional or paranoid belief in their partner's infidelity regardless of actual evidence. Time of evolution Although the exact moment during human evolution is unknown, modern romantic love is usually believed to have evolved either during or after the time of bipedalism. The earliest hominid found with extensive evidence of bipedalism (and some evidence of pair bonding) is Ardipithecus ramidus, from about 4.4 million years ago, although it may also be the case that bipedalism is older than this. It has been proposed that monogamous pair bonding (which is rare among mammals) evolved during this time, because walking bipedally requires mothers to carry infants in their arms or on their hip, instead of on their backs. With their hands occupied, mothers would be more vulnerable, requiring additional help for food and protection from males of the species (hence, husbands or fathers). A different selection pressure which has been proposed is the evolution of infant altriciality (immaturity and helplessness) and large brain size at birth, which occurred around 2 million years ago. At this time, brain size became so large that a fully-developed infant's head could not fit through the mother's pelvic birth canal (known as the obstetricial dilemma), requiring the infant to be",
    "label": 0
  },
  {
    "text": "birth, which occurred around 2 million years ago. At this time, brain size became so large that a fully-developed infant's head could not fit through the mother's pelvic birth canal (known as the obstetricial dilemma), requiring the infant to be born early and underdeveloped in comparison to other species. This would have also placed a greater burden on mothers, and made paternal support more valuable. Due to the general scarcity of evidence, it is still also possible that romantic love (or a precursor to it) predated bipedalism and altriciality, possibly originating in a common ancestor of humans and chimpanzees, 5-8 million years ago. While chimpanzees primarily mate opportunistically, some of their rarer reproductive strategies have features reminiscent of romantic love (involving mate guarding, and a more-than-fleeting association). One assumption behind hypotheses based on fossil evidence is that less sexual dimorphism in body mass (i.e. similarity) is indicative of monogamy, but the comparative similarity between the sexes in human body mass occurred as recently as 500,000 years ago. This suggests that there may have been multiple steps in the evolution of human pair bonding, and romantic love may have evolved during any of these periods. Courtship attraction Helen Fisher's theory is that romantic love (which she considers distinct from attachment) is a motivation system for choosing and focusing energy on a preferred mating partner. According to Fisher, this brain system evolved for mammalian mate choice, also called \"courtship attraction\". In this phenomenon, a preferred mating partner is chosen based on a display of physical traits (such as a peacock's tail feathers) or other behaviors. Fisher also includes the attraction to personality traits and other characteristics in her mate choice theory for humans. Courtship attraction shares similar behaviors with romantic love in humans, and both involve activation of dopaminergic reward circuits. In",
    "label": 0
  },
  {
    "text": "or other behaviors. Fisher also includes the attraction to personality traits and other characteristics in her mate choice theory for humans. Courtship attraction shares similar behaviors with romantic love in humans, and both involve activation of dopaminergic reward circuits. In most species, courtship attraction is as brief as lasting only minutes, hours, days or weeks, but intense romantic love can last much longer in humans. Fisher believes that during the timeline of human evolution, mammalian courtship attraction may have become prolonged and intensified as pair bonding evolved, eventually becoming the phenomenon of romantic love today. A critique of Fisher's theory published by Adam Bode holds that courtship attraction only encompasses love at first sight attraction or a crush, and the core components of romantic love (including the intense attraction and obsessive thoughts, in addition to attachment) evolved as a co-option of mother-infant bonding. A study on love at first sight found that even though people reporting the experience retrospectively will recall features resembling passionate love (\"constant thoughts about the person and the desire to be with him or her\"), people reporting love at first sight currently after just meeting the potential partner only report neutral scores (neither agreeing nor disagreeing) on a romantic love measure including a passion component. Some authors have speculated that the remembered account of falling in love at first sight (with high passion) is often actually a memory confabulation. Furthermore, the study found that the experience of love at first sight was related to the physical attractiveness of the potential partner. This led the researchers to conclude that love at first sight is actually a strong initial attraction, rather than resembling the state of being in love. Bode argues this more closely resembles the concept of courtship attraction, and can be considered a separate system from",
    "label": 0
  },
  {
    "text": "conclude that love at first sight is actually a strong initial attraction, rather than resembling the state of being in love. Bode argues this more closely resembles the concept of courtship attraction, and can be considered a separate system from core romantic love components. Courtship attraction may be characterized by dopamine, oxytocin and opioid activity, but little is known about it because existing studies were not designed to target it. Co-option of mother-infant bonding Co-option is an evolutionary process whereby a given trait is repurposed to take on a new function. One example is how a number of species of fish (e.g. catfish) have co-opted their gas bladder to produce sound. Co-opted traits can be morphological, but also behavioral. Co-option has been used as an explanation of how a species can develop an evolutionary adaptation very quickly sometimes, seemingly faster than Darwinism could explain. With this process, a seemingly \"new\" trait can develop quickly because its structure predated the time of adaptation, only needing to be modified to function in a new way. In some cases, co-option involves one gene whose function is altered, while in other cases the co-opted gene is a duplicate and the function of the original gene is retained. The terms \"co-option\" and \"exaptation\" are closely related, but have different connotations. Exaptation refers to structural continuity when a trait takes on a new function. Adam Bode has proposed that romantic love is \"a suite of adaptations and by-products\" consisting of a number of interrelated systems, several of which evolved by co-opting mother-infant bonding (attraction for bonding, obsessive thinking and attachment). The co-option theory says that the genes that regulate mother-infant bonding were recreated and took on a new function. Courtship attraction and sexual desire are \"causally linked adjuncts\" which were not co-opted, but were combined and",
    "label": 0
  },
  {
    "text": "obsessive thinking and attachment). The co-option theory says that the genes that regulate mother-infant bonding were recreated and took on a new function. Courtship attraction and sexual desire are \"causally linked adjuncts\" which were not co-opted, but were combined and modified in romantic love. The theory is based on the available human evidence, but also a literature arising from research on prairie voles that pair bonding uses the same mechanisms that mother-infant bonding uses. Academic literature has drawn a parallel between romantic love and the mother-infant dyad since the 1980s, with attachment theorists like Cindy Hazan and Phillip Shaver believing the two share a common biological process. In 1999, James Leckman & Linda Mayes compared features of romantic love and early parental love, finding substantial similarities. Both are altered mental states featuring preoccupations, exclusivity of focus, a longing for reciprocity and idealization of the other. The trajectories of both also share similarities, with preoccupation increasing during courtship (for romantic love) and around the time of birth (for parental love), then diminishing after a relationship is established (for romantic love) or shortly after the postpartum period (for parental love). (The use of \"baby talk\" by romantic lovers is another \"uncanny\" similarity.) In 2004, Andreas Bartels and Semir Zeki were the first to compare romantic love and maternal love with fMRI. This comparison looked at areas known to contain high densities of receptors for the attachment hormones oxytocin and vasopressin. Bartels & Zeki found precise overlap in some specific areas including the striatum (putamen, globus pallidus and caudate nucleus) and some overlap in the ventral tegmental area, areas with dopamine and oxytocin receptors. Each type of love was also associated with other unique activations. Notably, maternal love involved the periaqueductal gray matter, an area associated with endogenous pain suppression during intense emotional",
    "label": 0
  },
  {
    "text": "in the ventral tegmental area, areas with dopamine and oxytocin receptors. Each type of love was also associated with other unique activations. Notably, maternal love involved the periaqueductal gray matter, an area associated with endogenous pain suppression during intense emotional experiences such as childbirth. Two meta-analyses of fMRI experiments have also found similarities between maternal love and romantic love. A 2022 meta-analysis by Shih et al. found that both types of love were associated with the left ventral tegmental area (more associated with the pleasurable aspect of reward, or \"liking\"), while in addition romantic love also involved the right ventral tegmental area (more associated with reward \"wanting\"). In 2003, Lisa Diamond suggested that adult pair bonding is an exaptation of the affectional bond between infants and caregivers, using this to explain phenomena such as romantic friendships and \"platonic\" infatuations, or i.e. \"romantic\" passion without sexual desire. Some instances of this are reported by Dorothy Tennov in her study of \"limerence\" (i.e. love madness, commonly for an unreachable person), in which a younger woman who otherwise considered herself heterosexual would have this type of reaction towards an older woman. Among other examples are schoolgirls falling \"violently in love with each other, and suffering all the pangs of unrequited attachment, desperate jealousy etc.\" (historically called a \"smash\"), and Native American men who seemed to fall in love with each other and form intense, but non-sexual bonds. Helen Fisher's theory that sexual desire is a separate system from romantic love and attachment is also given as theoretical evidence. Diamond argues that romantic love without sexual desire can even happen in contradiction to one's sexual orientation: because it would not have been adaptive for a parent to only be able to bond with an opposite sex child, so the systems must have evolved independently",
    "label": 0
  },
  {
    "text": "without sexual desire can even happen in contradiction to one's sexual orientation: because it would not have been adaptive for a parent to only be able to bond with an opposite sex child, so the systems must have evolved independently from sexual orientation. People most often fall in love because of sexual desire, but Diamond suggests time spent together and physical touch can serve as a substitute. Diamond believes the connection between romantic love and sexual desire is \"bidirectional\" in that either one can cause the other to occur because of shared oxytocin pathways in the brain. New model Based on contentions over evolutionary theories and Fisher's outdated neurochemical model, Bode has suggested Fisher's model, while useful and the predominant one for a time, is oversimplified and proposes five systems: Sexual desire is associated with a drive to initiate and be receptive to sexual activity. Testosterone, dopamine, serotonin, norepinephrine, acetylcholine, histamine and opioids have been implicated in sexual behavior. Courtship attraction is for choosing and focusing energy on a preferred mating partner and promotes courtship behaviors. It can take the form of e.g. love at first sight attraction or a crush and also be intertwined with other forms of attraction, but might not precede a relationship in all cases. Courtship attraction may be associated with dopamine, oxytocin and opioids. Bonding attraction is the type of attraction for pair bond formation, characterized by a strong desire for proximity, separation anxiety when apart, exclusivity of focus and heightened awareness of the loved one. Bonding attraction is associated with dopamine and oxytocin activity, especially in the ventral tegmental area. According to Bode's arguments, this is the type of romantic attraction shown in fMRI experiments of early-stage romantic love. Obsessive thinking involves preoccupation or intrusive thinking about the loved one. Some authors have drawn",
    "label": 0
  },
  {
    "text": "especially in the ventral tegmental area. According to Bode's arguments, this is the type of romantic attraction shown in fMRI experiments of early-stage romantic love. Obsessive thinking involves preoccupation or intrusive thinking about the loved one. Some authors have drawn a comparison between this feature and obsessive-compulsive disorder, suggesting they share similar neurobiology, but the evidence for that is limited and ambiguous. Attachment is for pair bond maintenance, or maintaining very close personal relationships, with psychological features like a heightened sense of responsibility, longing for reciprocity and a powerful sense of empathy. Attachment is associated with oxytocin, dopamine and opioid activity, but there is also some evidence for the involvement of vasopressin. Bode suggests that the systems of bonding attraction, obsessive thinking and attachment (the three systems which were co-opted from mother-infant bonding) together form the core of romantic love (the necessary components). However, all five systems are merged into one single phenomenon of romantic love, with a variety of different outcomes depending on the circumstances. Mechanics Reward, motivation and addiction The early stage of romantic love is being compared to a behavioral addiction (i.e. addiction to a non-substance) but the \"substance\" involved is the loved person. Addiction involves a phenomenon known as incentive salience, also called \"wanting\" (in quotes). This is the property by which cues in the environment stand out to a person and become attention-grabbing and attractive, like a \"motivational magnet\" which pulls a person towards a particular reward. Incentive salience differs from craving in that craving is a conscious experience and incentive salience may or may not be. While incentive salience can give feelings of strong urgency to cravings, it can also motivate behavior unconsciously, as in an experiment where cocaine users were unaware of their own decisions to choose a low dose of cocaine (which",
    "label": 0
  },
  {
    "text": "not be. While incentive salience can give feelings of strong urgency to cravings, it can also motivate behavior unconsciously, as in an experiment where cocaine users were unaware of their own decisions to choose a low dose of cocaine (which they believed was placebo) more often than an actual placebo. In the incentive-sensitization theory of addiction, repeated drug use renders the brain hypersensitive to drugs and drug cues, resulting in pathological levels of \"wanting\" to use drugs. People in love are thought to experience incentive salience in response to their beloved. Lovers share other similarities with addicts as well, like tolerance, dependence, withdrawal, relapse, craving and mood modification. Incentive salience is mediated by dopamine projections in the mesocorticolimbic pathway of the brain, an area generally involved with reward, motivation and reinforcement learning. Dopamine signaling for incentive salience originates in the ventral tegmental area (VTA) and projects to areas such as the nucleus accumbens (NAc) in the ventral striatum. The VTA is one of two main areas of the brain with neurons which produce dopamine (the other being the substantia nigra pars compacta). Projections from the VTA innervate the NAc, where dopamine activity attaches motivational significance to stimuli associated with rewards. Brain scans of people in love using fMRI (commonly while looking at a photograph of their beloved) show activations in these areas like the VTA and NAc. Another dopamine-rich area of the reward system shown to be active in romantic love is the caudate nucleus, containing 80% of the brain's dopamine receptor sites, located in the dorsal striatum. The dorsal striatum is implicated in reinforcement learning, and the caudate nucleus has shown activity in response to a monetary reward and cocaine. This activity in reward and motivation areas suggests that early-stage intense romantic love is a motivation system or goal-oriented",
    "label": 0
  },
  {
    "text": "striatum is implicated in reinforcement learning, and the caudate nucleus has shown activity in response to a monetary reward and cocaine. This activity in reward and motivation areas suggests that early-stage intense romantic love is a motivation system or goal-oriented state (rather than a specific emotion), consistent with the description of romantic love as a desire or longing for union with another person. These activations are also consistent with the similarity between romantic love and addiction. In addiction research, a distinction is drawn between \"wanting\" a reward (i.e. incentive salience, tied to mesocorticolimbic dopamine) and \"liking\" a reward (i.e. pleasure, tied to hedonic hotspots), aspects which are dissociable. People can be addicted to drugs and compulsively seek them out, even when taking the drug no longer results in a high or the addiction is detrimental to one's life. They can also \"want\" (i.e. feel compelled towards, in the sense of incentive salience) something which they do not cognitively wish for. In a similar way, people who are in love may \"want\" a loved person even when interactions with them are not pleasurable. For example, they may want to contact an ex-partner after a rejection, even when the experience will only be painful. It is also possible for a person to be \"in love\" with somebody they do not like, or who treats them poorly. Academics have proposed a number of theories for how addictions begin and perpetuate. One prominent theory developed by Wolfram Schultz involves a dopamine signal which encodes a reward prediction error (RPE): the difference between the predicted value of a reward which would be received by performing a particular action and the actual value upon receiving it (i.e. whether the reward was better, equal to or worse than expected). In this theory, dopamine is also part of",
    "label": 0
  },
  {
    "text": "value of a reward which would be received by performing a particular action and the actual value upon receiving it (i.e. whether the reward was better, equal to or worse than expected). In this theory, dopamine is also part of a mechanism for reinforcement learning which associates rewards with the cues which predicted them. Drugs of abuse like cocaine hijack this mechanism by artificially overstimulating dopamine neurons, mimicking an RPE signal which is much stronger than could be produced naturally. An alternative model developed by Kent Berridge and Terry Robinson states that dopamine signaling causes the motivational output (incentive salience) which is proportional to RPE, but that the dopamine signal itself may be an effect of learning rather than causing it directly. There is, however, said to be overwhelming evidence that dopamine guides learning in addition to motivation. The computation of dopamine signaling is complicated, with inputs from a variety of areas in the brain, although its output (primarily from the VTA) is a relatively homogeneous signal encoding the level of RPE. One study has investigated whether people in long-term romantic relationships experienced RPE in response to having expectations about their partners' appraisal of them validated or violated, indicating they do. This study used fMRI to find that reward areas like the VTA and striatum responded in a way consistent with other research on RPE. Most fMRI studies of romantic love have had participants look at a photograph, and the resulting reward system activity has been interpreted in terms of salience. Research has not investigated whether romantic love shares all of the neurobiological aspects of addiction. Despite similarities, there are also differences between romantic love and addiction. One of the major differences is that the trajectories diverge, with the addictive aspects tending to disappear over time during a relationship in",
    "label": 0
  },
  {
    "text": "of the neurobiological aspects of addiction. Despite similarities, there are also differences between romantic love and addiction. One of the major differences is that the trajectories diverge, with the addictive aspects tending to disappear over time during a relationship in romantic love. By comparison, in a drug addiction, the detrimental aspects magnify over time with repeated drug use, turning into compulsions, a loss of control and a negative emotional state. It has been speculated that the difference between these could be related to oxytocin activity present in romantic love, but not in addiction. Oxytocin seems to ameliorate the effects of drug withdrawal, and it might inhibit the more long-term, excessive effects of addiction. Academics do not universally agree on whether or not love is always an addiction or when it needs to be treated. The term \"love addiction\" has had an amorphous definition over the years and does not yet denote a psychiatric condition, but recently one definition has been developed that \"Individuals addicted to love tend to experience negative moods and affects when away from their partners and have the strong urge and craving to see their partner as a way of coping with stressful situations.\" Other authors include rejected lovers as love addicts, or specify that love is an addiction when it involves abnormal processes which carry negative consequences. A broader view is that all love is addiction, or simply an appetite, similar to how humans are dependent on food. Research on behavioral addictions is more limited than research on drugs of abuse; however, there is a growing body of evidence that some people are susceptible to showing brain patterns in response to natural rewards (food, sex, etc.) similar to drug addicts, particularly in the case of gambling addiction. Romantic love may be a \"natural\" addiction, which differs",
    "label": 0
  },
  {
    "text": "body of evidence that some people are susceptible to showing brain patterns in response to natural rewards (food, sex, etc.) similar to drug addicts, particularly in the case of gambling addiction. Romantic love may be a \"natural\" addiction, which differs from the nature of drug addiction in that love may be prosocial and has been evolved for the purpose of pair bonding. Helen Fisher, Arthur Aron and colleagues have proposed that romantic love is a \"positive addiction\" (i.e. not harmful) when requited and a \"negative addiction\" when unrequited or inappropriate. Oxytocin, bonding and attraction Oxytocin is sometimes called the \"love hormone\", because of its involvement in the mechanisms of maternal behavior and adult pair bonding. Oxytocin is synthesized primarily in an area of the hypothalamus and released into the blood via the pituitary gland, where it's been found circulating in people in the early stages of romantic love. Additionally, the hypothalamus projects oxytocin to other areas of the brain, like the ventral tegmental area (VTA), nucleus accumbens (NAc), amygdala and hippocampus. The projections to reward areas (VTA and NAc) are thought to modulate social salience, or i.e. the level of dopamine activity in response to socially-relevant stimuli. This oxytocin signaling in reward pathways may also be the source of salience in response to a loved one. A placebo-controlled study found that administering intranasal oxytocin enhanced facial attractiveness of a romantic partner while viewing a photograph, as compared to an unfamiliar face. The effect was also measured using fMRI, which found enhanced brain activity in reward areas like the VTA and NAc. Another fMRI study found dopamine-rich genetic expression of an oxytocin receptor gene in the left VTA, and the left VTA has also been found active in response to facial attractiveness. In humans, circulating oxytocin levels have been associated with",
    "label": 0
  },
  {
    "text": "NAc. Another fMRI study found dopamine-rich genetic expression of an oxytocin receptor gene in the left VTA, and the left VTA has also been found active in response to facial attractiveness. In humans, circulating oxytocin levels have been associated with higher levels of interaction between partners, and also predicted which couples would still be together 6 months later. Anna Machin calls the combination of oxytocin and dopamine the \"glue\" which makes the early stages of a relationship possible. The role of oxytocin in human behavior is varied and complex. Oxytocin lowers inhibitions to forming new relationships by deactivating the amygdala, involved with processing fear and anxiety. Oxytocin can be released with physical touch, hence it's also sometimes called the \"cuddle hormone\". Oxytocin also plays a role in sexual behavior, being released during sexual arousal and orgasm. Aside from romantic and parental bonding, oxytocin activity has a role in the interactions with peers or strangers, for example facilitating facial recognition and eye contact. Oxytocin is believed to facilitate trust and altruistic behaviors towards in-groups (e.g. partners or children), but also aggression towards out-groups (e.g. strangers or conspecifics). Much of the research on oxytocin comes from experiments on monogamous prairie voles (notably by Larry Young), but this research is also used for making inferences about humans. In prairie voles, both oxytocin and dopamine signaling have been shown to influence pair bond development. For example, the number of oxytocin receptors in the NAc is positively related to how fast a partner preference is formed. A partner preference can also be prevented by injecting either a dopamine or oxytocin receptor antagonist (a drug which blocks transmission) into the NAc of a prairie vole directly. In a contemporary model of the brain systems involved with romantic love, this type of salience (or 'bonding attraction') is",
    "label": 0
  },
  {
    "text": "either a dopamine or oxytocin receptor antagonist (a drug which blocks transmission) into the NAc of a prairie vole directly. In a contemporary model of the brain systems involved with romantic love, this type of salience (or 'bonding attraction') is present throughout the entire time a person is experiencing romantic love, including during the early stages. This contrasts with some previous theories (e.g. proposed by Helen Fisher in 1998) which stated that oxytocin activity and dopamine activity were distinct (and independent) systems, and that oxytocin activity only became prominent at some later stage of a relationship. Levels of oxytocin would still vary from situation to situation because of differing types of stimuli, for example because of less regular interaction and physical touch in cases of unrequited love. This could be used to explain some of the maladaptive symptoms of infatuation (e.g. sleep difficulties, social anxiety, clammy hands, etc.), when dopaminergic activity is high without the calming effect of oxytocin from the attachment system. Brain opioid theory of social attachment Modern research is increasingly showing the importance of endogenous opioids in love and social attachment, particularly the β-endorphin (the most potent endogenous opioid) and the μ-opioid receptor system. While opioids have their origin being the body's natural painkiller, they're also implicated in a variety of other systems, essentially like neurotransmitters. Opioid receptors are located throughout the brain, including in the limbic system (affecting basic emotions) and neocortex (affecting more conscious decision-making). Opioids are linked to the consummatory part of reward, or i.e. \"liking\" or pleasure, and released in areas of the brain called hedonic hotspots (or pleasure centers). Hedonic hotspots are located in the nucleus accumbens, the ventral pallidum and other areas. This function includes social reward, or the pleasurable aspect of social interactions. The brain opioid theory of social attachment",
    "label": 0
  },
  {
    "text": "brain called hedonic hotspots (or pleasure centers). Hedonic hotspots are located in the nucleus accumbens, the ventral pallidum and other areas. This function includes social reward, or the pleasurable aspect of social interactions. The brain opioid theory of social attachment (BOTSA) is a long-running theory summarizing this connection, originally formulated in the 1980s and 1990s, based on a proposal by the psychiatrist Michael Liebowitz and research by the neuroscientist Jaak Panksepp. Starting in the 1990s, opioids were overshadowed by the interest in oxytocin and largely overlooked until more recently, possibly because of the difficulty studying them (requiring e.g. a PET scan, which is expensive). Opioids have been connected to a variety of social experiences, including the early stage of romantic love and attachment styles. While the addictive aspects of love have been compared to cocaine or amphetamine addiction, other aspects may also resemble an opioid addiction. BOTSA (as it was originally conceived of) predicts that in the absence of social relationships, individuals will have comparatively lower levels of endogenous opioids, motivating them to initiate contact with other people. Social contact then leads to feelings of euphoria and contentment, but individuals also need to continue contact to avoid withdrawal symptoms. Liebowitz originally argued that romantic relationships resemble narcotic addiction, and that individual neurochemical differences could also explain why some people are unable to commit, or stay in abusive relationships. Earlier experiments on BOTSA were animal studies, but in the 2000s this has been expanded to include human experiments. Among the animal studies which have been done, there's some evidence that separation distress is akin to opioid withdrawal. Studies on chicks, puppies, Guinea pigs, rats, sheep and monkeys have shown that administrating morphine reduces distress vocalizations when separated from the mother, and administrating naloxone (an opioid antagonist) increases them, even in the",
    "label": 0
  },
  {
    "text": "distress is akin to opioid withdrawal. Studies on chicks, puppies, Guinea pigs, rats, sheep and monkeys have shown that administrating morphine reduces distress vocalizations when separated from the mother, and administrating naloxone (an opioid antagonist) increases them, even in the presence of other members of the same species. In another study, mutant mouse pups with a μ-opioid receptor knockout (lacking the μ-receptor gene) vocalized less frequently in response to isolation than normal mice. Administration of morphine had no effect on distress vocalization frequency in the knockout mice, despite reducing it in normal mice. Furthermore, these knockout mice had a reduced preference for their mother's odor, which is normally the result of conditioning mediated by the endogenous opioid system. In nonhuman primates, studies have suggested that endogenous opioids provide the euphoria behind dyadic social grooming behaviors. Other animal studies have shown that endogenous opioids play a role in the desire for rough-and-tumble play (a physical, but also social behavior). In humans, physical activity with a social element (rowing, dancing, laughing) increased pain tolerance more when the activities were synchronized with other people. An fMRI experiment in 2010 investigated whether viewing a picture of a romantic partner could reduce pain sensitivity, and which areas of the brain became active. Participants were exposed to high temperatures (resulting in moderate or high pain levels) while viewing a picture of a romantic partner (whom they were intensely in love with), or a friend, or performing a word association task which has also been shown to reduce pain via distraction. Participants were then asked to rate how much pain they felt on a pain scale, and both viewing a romantic partner and performing the distraction task (but not viewing a friend) were found to reduce pain levels. The fMRI scans revealed that viewing a romantic partner",
    "label": 0
  },
  {
    "text": "how much pain they felt on a pain scale, and both viewing a romantic partner and performing the distraction task (but not viewing a friend) were found to reduce pain levels. The fMRI scans revealed that viewing a romantic partner activated reward circuits in the brain, while the distraction task did not. Brain areas were also correlated with pain relief to reveal that reward analgesia and distraction analgesia involved distinct areas. Some areas associated with sensory processing of pain also had decreased activity while viewing a romantic partner. An earlier experiment showed that viewing photos of a romantic partner reduced experimental pain, but did not pair it with a brain scan. A PET scan experiment in 2016 investigated whether non-sexual social touching between romantic partners was mediated by endogenous opioid activity. This study found that social touch did have an effect, but unexpectedly found that social touching decreased opioid activity in the brain rather than increasing it (despite being rated as pleasurable by participants). This is in contrast with prior PET research that pleasant affect is related to increased opioid activity. One possible explanation is that touching decreases stress, so this might also decrease the ongoing opioid activity in response to distress and pain. As this is also at odds (to some extent) with primate studies on grooming, there may be some variation between species in how opioids are involved with social reward. Other modern studies on humans include blood plasma levels, genetics and studies with drugs like morphine and naltrexone to see how they change social perception and behavior. Obsessive thinking Obsessive thinking about a loved one has been called a hallmark or a cardinal trait of romantic love, ensuring that the loved one is not forgotten. Some reports have been made that people can even spend as much",
    "label": 0
  },
  {
    "text": "Obsessive thinking Obsessive thinking about a loved one has been called a hallmark or a cardinal trait of romantic love, ensuring that the loved one is not forgotten. Some reports have been made that people can even spend as much as 85 to 100% of their days and nights thinking about a love object. One study found that on average people in love spent 65% of their waking hours thinking about their beloved. Another study used cluster analysis to find several different groups of lovers, with the least intense group spending 35% of their time on average and the most intense at 72%. Since the late 1990s, these obsessional features have been compared to obsessive–compulsive disorder (OCD). This is also sometimes paired with a theory that obsessive (or intrusive) thinking is related to serotonin levels being lowered while in love, although study results have been inconsistent or negative. Another theory relates obsessive thinking to addiction, because drug users exhibit obsessive thoughts about drug use, as well as compulsions. In 1999, James Leckman and Linda Mayes published a theoretical comparison between early-stage romantic love, early parental love and OCD. This paper was intended as an investigation into the origin of OCD, but it also relates to the evolutionary theory of romantic love. Both early-stage romantic love and OCD share features of preoccupation, intrusive thoughts, a heightened sense of responsibility, a need for things to be \"just right\" and some proximity-seeking behaviors. In some cases, obsessions experienced by OCD patients relate to what harms might happen to a family member, which resembles some behavioral patterns involved with romantic and parental love. The authors also speculate that psychasthenia (feelings of incompleteness, insufficiency or imperfection) resembles the \"longing for reciprocity\" and idealization which are features of romantic love. Two experiments have investigated whether there's",
    "label": 0
  },
  {
    "text": "some behavioral patterns involved with romantic and parental love. The authors also speculate that psychasthenia (feelings of incompleteness, insufficiency or imperfection) resembles the \"longing for reciprocity\" and idealization which are features of romantic love. Two experiments have investigated whether there's a relationship between romantic love and serotonin levels, by taking different measures using blood samples. Although serotonin levels in the central nervous system would actually be the measure of interest, it has been assumed that measures of peripheral serotonin can be used as a marker for this. A 1999 experiment led by Donatella Marazziti found that people in love had platelet serotonin transporter (SERT) density which was lower than controls, and similar to the density of a group of unmedicated OCD patients. Six of the 20 in-love participants were also retested after a period of 12 to 18 months, and SERT density had returned to normal. However, because Marazziti's experiment looked at SERT (rather than serotonin directly), this makes it ambiguous whether serotonin levels were actually higher or lower. SERT transports serotonin from blood plasma back into the platelets, so that a reduction in SERT could correspond to an increased plasma level. Another experiment in 2012 led by Sandra Langeslag which looked at blood serotonin levels found a contradictory result, with men and women being affected differently. Men had lower serotonin levels than controls, but women had higher serotonin levels. In women, obsessive thinking was also actually associated with increased serotonin. A 2025 study led by Adam Bode also found no association between SSRI use and obsessive thinking about a loved one, or the intensity of romantic love. Therefore, although the earlier experiments do suggest romantic love and serotonin are probably associated, the authors suggest that the idea of obsessive thinking being attributed to lowered serotonin levels seems inaccurate. Emotional",
    "label": 0
  },
  {
    "text": "loved one, or the intensity of romantic love. Therefore, although the earlier experiments do suggest romantic love and serotonin are probably associated, the authors suggest that the idea of obsessive thinking being attributed to lowered serotonin levels seems inaccurate. Emotional valence Rather than being a specific emotion itself, romantic love is believed to be a motivation or drive which elicits different emotions depending on the situation: positive feelings when things go well, and negative feelings when awry. Reciprocated love may elicit feelings of joy, ecstasy, or fulfillment, for example, but unrequited love may elicit feelings of sadness, anxiety, or despair. A 2014 study of Iranian young adults found that the early stage of romantic love was associated with the brighter side of hypomania (elation, mental and physical activity, and positive social interaction) and better sleep quality, but also stronger symptoms of depression and anxiety. Those authors conclude that romantic love is \"not entirely a joyful and happy period of life\". Romantic love may be either pleasant or unpleasant, regardless of the intensity level. One of Dorothy Tennov's interview participants recalls being in love this way: \"When I felt [Barry] loved me, I was intensely in love and deliriously happy; when he seemed rejecting, I was still intensely in love, only miserable beyond words.\" The intensity of love feelings is also distinct from whether an individual is satisfied with their relationship (although the measures have been shown to be related to some extent). One can be satisfied with their relationship because it fulfills some other need besides love for their partner (like money or child care), or conversely be in love with an abuser in an abusive relationship. Unrequited love is common among young adults. A study by Roy Baumeister and Sarah Stillwell found that 92.8% of participants reported at least",
    "label": 0
  },
  {
    "text": "(like money or child care), or conversely be in love with an abuser in an abusive relationship. Unrequited love is common among young adults. A study by Roy Baumeister and Sarah Stillwell found that 92.8% of participants reported at least one \"powerful or moderate\" experience of unrequited love in the past 5 years. A different study found 63% had a \"huge crush\" at least once in the past 2 years (but not letting the person know), and unrequited love was four times more frequent than equal love. Another found that 20% had experienced unrequited love more than 5 times, according to a definition that \"When one is experiencing this emotion, it has been described as having one’s emotions on a roller coaster, finding it difficult to concentrate, and thinking constantly about the person with whom you are in love. The person is said to have the power to produce extreme highs and lows of emotion in you, depending on how he or she acts towards you.\" In 2010, Helen Fisher, Arthur Aron and colleagues published their fMRI experiment investigating which areas of the brain might be active in recently rejected lovers. Participants had been in a relationship with their ex-partner for an average of 21 months, and then were post-rejection for an average of 63 days at the time of the experiment. These participants reported spending more than 85% of their waking hours thinking of their rejector, reported a lack of emotional control, and exhibited unhappiness, with sometimes more extreme emotions like depression, anger, and even paranoia in pre- and post-interviews. Similar to other fMRI experiments, the scan while looking at a photograph of the rejecting partner showed activations in dopaminergic reward system areas, like the ventral tegmental area and nucleus accumbens. These activations were also stronger than in a",
    "label": 0
  },
  {
    "text": "post-interviews. Similar to other fMRI experiments, the scan while looking at a photograph of the rejecting partner showed activations in dopaminergic reward system areas, like the ventral tegmental area and nucleus accumbens. These activations were also stronger than in a previous experiment of participants who were happily in love. The nucleus accumbens, prefrontal cortex and orbitofrontal cortex which were active have been associated with assessing one's gains and losses, and areas of the insular cortex and anterior cingulate cortex which were active have been involved with physical pain and pain regulation (respectively) in other studies. Stress and physiological arousal In the early stages of romantic love, individuals may start out hypervigilant (hyperaware and sensitive to a partner's cues) due to uncertainty and novelty, but become synchronized over time as a relationship progresses. Bonding is thought to be in part facilitated by coordinated behaviors which display reciprocity and events which evoke beneficial stress (eustress), like a passionate kiss. The stress response system involves two major systems: the autonomic nervous system and the hypothalamic–pituitary–adrenal axis. Some experiments have been done which support the idea that the stress response is involved during the early stage of romantic love, measuring cortisol levels; however, these experiments have been inconsistent with respect to cortisol being higher or lower. In drug addiction, corticotropin release factor (CRF) is involved with the aversive effects of withdrawal. Stress causes CRF to release into the ventral tegmental area and nucleus accumbens shell, motivating reinstatement of drug use. A similar effect is also hypothesized in pair bonds, where stress after separation or social loss motivates a person to return to the partner; however, experiments have not investigated this in humans, only rodents. Frustration attraction and uncertainty Dopamine neurons in the ventral tegmental area are theorized to encode a \"reward prediction error\" (RPE)",
    "label": 0
  },
  {
    "text": "social loss motivates a person to return to the partner; however, experiments have not investigated this in humans, only rodents. Frustration attraction and uncertainty Dopamine neurons in the ventral tegmental area are theorized to encode a \"reward prediction error\" (RPE) signal, rather than a reward per se. This RPE signaling indicates whether a given reward was either better, equal to, or worse than what was anticipated, and this is believed to be part of a reinforcement learning paradigm. Studies have shown that for learning about a stimulus to occur (so that behavior in response to it changes), the reward has to be surprising or unpredicted. Rewards which are better than predicted reinforce the behavior and cause it to become more frequent, while a reward which is worse than expected would be avoided. Dopamine neurons increase their firing rate when encountering an unexpected reward. After reinforcement learning occurs, dopamine neurons also fire in response to encountering cues in the environment which predicted the reward (e.g. in animal studies, a lever or a special sound). As predictions become updated and the rewards are the same as expected, dopamine activity comparatively diminishes. \"Frustration attraction\" (also called the \"Romeo and Juliet effect\") is the idea that adversity heightens romantic passion, for example, through social or physical barriers. The phenomenon has been remarked on by many authors, such as Socrates, Ovid, the Kama Sutra, and \"Dear Abby\". Bertrand Russell once opined that \"when a man has no difficulty in obtaining a woman, his feeling toward her does not take the form of romantic love\". Some common social barriers are parents who interfere with their children's romance (as in Romeo and Juliet), deceived spouses, or other social customs. Helen Fisher believes the phenomenon can be explained by the mechanics of dopamine, because animal studies have shown",
    "label": 0
  },
  {
    "text": "common social barriers are parents who interfere with their children's romance (as in Romeo and Juliet), deceived spouses, or other social customs. Helen Fisher believes the phenomenon can be explained by the mechanics of dopamine, because animal studies have shown that when a reward which is anticipated to be incoming is delayed, reward-expecting neurons prolong their firing (over comparatively short timescales—in these studies) until the reward is delivered. Passionate or infatuated love is also said to thrive in situations which involve the uncertainty of intermittent reinforcement, when consummation is withheld, when barriers prevent lovers from meeting regularly, or when one's perceptions of how likely their love is reciprocated are ambiguous and constantly changing. The type of situation resembles a slot machine, for example, where the rewards are designed to be always unpredictable so the gambler cannot understand the pattern. Unable to habituate to the experience, for some people the exhilarating high from the unexpected wins leads to gambling addiction and compulsions. If the machine paid out on a regular interval (so that the rewards were expected), it would not be as exciting. Uncertainty seems to magnify cue-triggered incentive salience \"wanting\", especially for gambling, but also for drug addictions. Uncertainty theory in the context of romantic love is associated with Dorothy Tennov's theory of limerence, an addictive, infatuated kind of love, commonly experienced for an unobtainable or unreachable person. In her study, Tennov observed reports of sometimes drastic emotional transitions caused by changes in one's perception over whether their love might be reciprocated, and these abrupt transitions could cause seeming emotional volatility even in otherwise stable individuals. The effect of uncertainty has also been interpreted as attachment anxiety. Intermittent maltreatment (also known as trauma bonding) has also been believed to intensify romantic \"passion\" (i.e. strong emotion, including suffering). This is, again,",
    "label": 0
  },
  {
    "text": "volatility even in otherwise stable individuals. The effect of uncertainty has also been interpreted as attachment anxiety. Intermittent maltreatment (also known as trauma bonding) has also been believed to intensify romantic \"passion\" (i.e. strong emotion, including suffering). This is, again, believed to be related to intermittent reinforcement and how one's expectations are confirmed or violated. According to Elaine Hatfield, \"Consistency generates little emotion; it is inconsistency that we respond to. [...] What would generate a spark of interest [...] is if our admiring friend suddenly started treating us with contempt—or if our arch enemy started inundating us with kindness.\" Positive illusions People in love tend to overemphasize the positive aspects of their loved one or relationship, while overlooking or devaluing negative aspects. This is regarded as a type of cognitive bias called positive illusions. The phenomenon has also been referred to as crystallization, idealization, \"love is blind\" bias, putting the loved one on a pedestal, or seeing through rose-colored glasses. In the past, some authors have depicted the phenomenon as a malady, arguing that people who idealize would have their partner fall short of their high expectations as a relationship progresses; however, despite this, significant modern scientific evidence has shown that positive illusions actually contribute to relationship satisfaction, long-term well-being and decreased risk for relationship discontinuation. The exact mechanism is not currently understood, but some brain areas are proposed to be related. The dopaminergic areas of the reward system which are active in romantic love may be involved with attributing salience to the positive characteristics of a loved one. The dorsal anterior cingulate cortex is involved with error detection and has been active during negative social evaluation and exclusion, so that reduced activation of this area would be an adaptive response to a partner's negative characteristics. Certain areas of the",
    "label": 0
  },
  {
    "text": "dorsal anterior cingulate cortex is involved with error detection and has been active during negative social evaluation and exclusion, so that reduced activation of this area would be an adaptive response to a partner's negative characteristics. Certain areas of the prefrontal cortex could also be exerting top-down control to suppress emotional responses to attractive alternatives. Information is then passed to the orbitofrontal cortex, where positive and negative information is weighed, resulting in a biased subjective value about the partner. Brain imaging Brain imaging techniques such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET) have been used to investigate which brain regions are involved in romantic love. Nearly all of these experiments have had participants look at a photograph of their beloved during an fMRI scan, with a few exceptions, although the specific procedures used have not always been identical. The differences in experimental design (e.g. length of time the participants had been in love, or the specific task given to participants during the scan) can be used to explain why the experiment results are sometimes different. In 2000, a study by Andreas Bartels and Semir Zeki of University College London was the first fMRI study of romantic love. The 17 participants were \"truly, deeply and madly in love\", had been together for a mean of 2.4 years, and were shown either one or two photographs of their loved one during the scan. Two main areas were active in this study: the middle insular cortex, associated with stomach churning or \"gut feelings\", which could have something to do with the feeling of \"butterflies in the stomach\", and part of the anterior cingulate cortex, associated with feelings of euphoria. Other activations were areas in the cerebrum, the caudate nucleus, putamen and the cerebellum. A later analysis in 2004 by",
    "label": 0
  },
  {
    "text": "with the feeling of \"butterflies in the stomach\", and part of the anterior cingulate cortex, associated with feelings of euphoria. Other activations were areas in the cerebrum, the caudate nucleus, putamen and the cerebellum. A later analysis in 2004 by the same authors also reports activity in the ventral tegmental area (VTA), which produces dopamine. The study also showed key deactivations, areas of the brain that were less active in romantic love compared to friendship love, in the amygdala and medial prefrontal cortex (mPFC). The amygdala is involved with fear and risk detection, and the mPFC is involved with understanding and predicting the intentions of other people, called mentalizing. These deactivations are taken as evidence that \"love is blind\", or i.e. that people in love discount the risks involved and misunderstand people's intentions, even leading to folly sometimes. In 2005, a study by Arthur Aron, Helen Fisher, Debra Mashek, Greg Strong, Haifang Li and Lucy Brown was the first fMRI study of early-stage intense romantic love. It has been praised as advancing the scientific understanding of infatuated love, even by a skeptic of fMRI literature. This study differed from Bartels & Zeki in that the 17 participants who had \"just fallen madly in love\" had been in love for a much shorter mean time of only 7.4 months. These participants were more intensely in love, and spent 85% or more of their waking hours thinking of their loved one. This study also had participants look at a photograph of their loved one during the scan. Reward and motivation areas were active, like the VTA and areas of the caudate. Activity was also found in the insular and cingulate cortex, involved with emotion. Some interesting areas were correlated with the length of the relationship, like the ventral pallidum, implicated in attachment",
    "label": 0
  },
  {
    "text": "active, like the VTA and areas of the caudate. Activity was also found in the insular and cingulate cortex, involved with emotion. Some interesting areas were correlated with the length of the relationship, like the ventral pallidum, implicated in attachment in prairie voles, and the anterior cingulate, implicated in obsessive thinking, cognition and emotion. This study also examined correlations with facial attractiveness to determine that the right VTA was active because of romantic passion rather than because the partner was aesthetically pleasing. Aesthetically pleasing faces elicited more activity in the left VTA, which is more associated with \"liking\" a reward (i.e. pleasure), whereas the right VTA is more associated with \"wanting\" a reward (i.e. incentive salience). In 2011, Xu et al. repeated the experiment by Aron et al., but using Chinese participants. Ortigue et al. used fMRI to investigate the subliminal influence of romantic love on motivation, interested in how these implicit neural representations might differ from previous experiments where subjects were consciously aware of the stimulus (viewing a photograph). In Ortique et al.'s study, participants were shown a subliminal prime word for 26ms (either their beloved's name, the name of a friend, or a word describing a personal passion like a hobby), followed by a series of symbols (#) for 150ms, followed by a target word for 26ms. This target was either an English word, non-word or blank, and participants were asked to identify whether it was a word or not. In trials with the love prime or passion prime, participants were faster to identify whether the target was a word or not, and this also correlated with scores on the Passionate Love Scale. The authors believe this shows that love priming activates motivation systems in the brain, rather than just evoking a particular emotion. The fMRI scanning showed",
    "label": 0
  },
  {
    "text": "a word or not, and this also correlated with scores on the Passionate Love Scale. The authors believe this shows that love priming activates motivation systems in the brain, rather than just evoking a particular emotion. The fMRI scanning showed brain regions active for love primes similar to previous experiments, including reward and motivation areas like the VTA and caudate, but with some additions. Subliminal love priming additionally activated the bilateral fusiform gyri and angular gyri, involved in integrating abstract representations. The authors relate this to the self-expansion model of interpersonal relationships, where self-expansion by integrating the characteristics of one's beloved into one's self (called inclusion of the other in the self) is a rewarding experience which may promote romantic love feelings. In brain scans of long-term intense romantic love (involving subjects who professed to be \"madly\" in love, but were together with their partner 10 years or more) led by Bianca Acevedo, attraction similar to early-stage romantic love was associated with dopamine reward center activity (\"wanting\"), but long-term attachment was associated with the globus palludus, a site for opiate receptors identified as a hedonic hotspot (\"liking\"). Long-term romantic lovers also showed lower levels of obsession compared to those in the early stage. An fMRI study led by Sandra Langeslag investigated the effect of attention on brain activity related to a loved one. In most other previous experiments, subjects only passively viewed a photograph, but this experiment used an oddball task to distinguish between instances where the loved one was either the intended target of the subject's attention or a distraction. Participants were given trials where they were presented with a random face for only 250ms (usually an unknown person) and instructed to watch for either a loved one or a friend, then press a button if the face was",
    "label": 0
  },
  {
    "text": "distraction. Participants were given trials where they were presented with a random face for only 250ms (usually an unknown person) and instructed to watch for either a loved one or a friend, then press a button if the face was the intended target for a given run. In some runs, the loved one would be the intended target for a button press, while the friend would be a distractor causing participants to press the button by mistake sometimes, while in other runs the friend would be the target and the loved one a distractor. This experiment found that activity in the dorsal striatum (an area of the reward system) was modulated by whether or not participants were instructed to pay attention to their loved one. That is, the dorsal striatum showed more response to the loved one than to the friend, but only when the loved one was the target. This led the authors to conclude that \"the dorsal striatum is not activated by beloved-related information per se, but only by beloved-related information that is attended\". This activity also tended to be smaller when participants had been in love or been in a relationship for longer. The dorsal striatum is implicated in reinforcement learning, so the authors interpret the increase in brain activity as reflecting prior reinforcement of social actions which leads the infatuated individuals to pay preferential attention to their loved one. Participants also tended to press the button by mistake more often when distracted by the loved one than the friend. Some brain scan experiments of early-stage romantic love have found activation of the posterior cingulate cortex, which is implicated in autobiographical memory of socially relevant stimuli (e.g. partner names) and attention. Most experiments (including long-term romantic love) have shown activity in the hippocampus and parahippocampal gyrus, areas",
    "label": 0
  },
  {
    "text": "Biospeleology, also known as cave biology, is a branch of biology dedicated to the study of organisms that live in caves and are collectively referred to as troglofauna. Biospeleology as a science History The first documented mention of a cave organism dates back to 1689, with the documentation of the olm, a cave salamander. Discovered in a cave in Slovenia, in the region of Carniola, it was mistaken for a baby dragon and was recorded by Johann Weikhard von Valvasor in his work The Glory of the Duchy of Carniola. The first formal study on cave organisms was conducted on the blind cave beetle. Found in 1831 by Luka Čeč, an assistant to the lamplighter, when exploring the newly discovered inner portions of the Postojna cave system in southwestern Slovenia. The specimen was turned over to Ferdinand J. Schmidt, who described it in the paper Illyrisches Blatt (1832). He named it Leptodirus Hochenwartii after the donor, and also gave it the Slovene name drobnovratnik and the German name Enghalskäfer, both meaning \"slender-necked (beetle)\". The article represents the first formal description of a cave animal (the olm, described in 1768, wasn't recognized as a cave animal at the time). Subsequent research by Schmidt revealed further previously unknown cave inhabitants, which aroused considerable interest among natural historians. For this reason, the discovery of L. hochenwartii (along with the olm) is considered as the starting point of biospeleology as a scientific discipline. Biospeleology was formalized as a science in 1907 by Emil Racoviță with his seminal work Essai sur les problèmes biospéologiques (\"Essay on biospeleological problems”). The first work on the systematisation of biospeleology however was the Catalogus Cavernarum Animalium (1934) by Hungarian biologist Endre Dudich and German-Jewish biologist Benno Wolf, which is also the first animal catalogue in history. Among other discoveries",
    "label": 0
  },
  {
    "text": "on biospeleological problems”). The first work on the systematisation of biospeleology however was the Catalogus Cavernarum Animalium (1934) by Hungarian biologist Endre Dudich and German-Jewish biologist Benno Wolf, which is also the first animal catalogue in history. Among other discoveries Dudich is also responsible for the discovery of around 220 cavedwelling species, realized that caves are not cohesive biotopes and he was the first to classify cave ecosystems into groups: Exotrophic caves Photo-endotrophic caves Chemo-endotrophic caves Photo-chemo-endotrophic caves In 1930, he and his Hungarian co-researchers also disproved the then prominent theory that cave-dwelling creatures are subject to constant starvation. Subdivisions Organisms Categories Cave organisms fall into three basic classes: Troglobite Troglobites are obligatory cavernicoles, specialized for cave life. Some can leave caves for short periods, and may complete parts of their life cycles above ground, but cannot live their entire lives outside of a cave environment. Examples include chemotrophic bacteria, some species of flatworms, springtails, and cavefish. Troglophile Troglophiles can live part or all of their lives in caves, but can also complete a life cycle in appropriate environments on the surface. Examples include cave crickets, bats, millipedes, pseudoscorpions and spiders. Trogloxene Trogloxenes frequent caves, and may require caves for a portion of its life cycle, but must return to the surface (or a parahypogean zone) for at least some portion of its life. Oilbirds and most Daddy longlegs are trogloxenes. Environmental Categories Cave environments fall into three general categories: Endogean Endogean environments are the parts of caves that are in communication with surface soils through cracks and rock seams, groundwater seepage, and root protrusion. Parahypogean Parahypogean environments are the threshold regions near cave mouths that extend to the last penetration of sunlight. Hypogean Hypogean or \"true\" cave environments. These can be in regular contact with the surface via wind",
    "label": 0
  },
  {
    "text": "groundwater seepage, and root protrusion. Parahypogean Parahypogean environments are the threshold regions near cave mouths that extend to the last penetration of sunlight. Hypogean Hypogean or \"true\" cave environments. These can be in regular contact with the surface via wind and underground rivers, or the migration of animals, or can be almost entirely isolated. Deep hypogean environments can host autonomous ecologies whose primary source of energy is not sunlight, but chemical energy liberated from limestone and other minerals by chemoautotrophic bacteria. Notable biospeleologists Emil Racoviță, co-founder of biospeleology, the first Romanian to go to the arctic, collected the type specimens of the flightless midge Belgica antarctica Carl Eigenmann, is credited with identifying and describing for the first time 195 genera containing nearly 600 species of fishes of North America and South America Louis Fage, was a founding member of the Commission de spéléologie René Jeannel, co-founder of biospeleology Curt Kosswig, is known as the Father of Turkish Zoology. Boris Sket, Approximately 35 animal species are named sketi, and three genera. Endre Dudich, founder and committee member of the Hungarian Speleological Society in 1926. Karel Absolon, though more famous for his archaeological and speleological discoveries, Absolon started his career out as a biospeleologist. Bibliography Bernard Collignon, Caving, scientific approaches., Edisud 1988 Fabien Steak, Approach biospéologie, File EFS Instruction No. 116, 1st Edition, 1997 C. Delamare-Debouteville, Life in caves, PUF, Que sais-je?, Paris 1971 Bernard Gèze, Scientific caving, Seuil, Paris, 1965, p. 137-167 R. and V. Decou Ginet, Introduction to biology and groundwater ecology, University Publishing Delarge 1977 René Jeannel, Animal cave in France, Lechevalier, Paris, 1926 René Jeannel, Living fossils caves, Gallimard, Paris, 1943 Edward Alfred Martel, Groundwater evolution, Flammarion, Paris, 1908, p. 242-289 Georges Émile Racovitza, Essay on biospéologiques problems, I Biospeologica 1907 Michel Siffre, Animals sinkholes and caves, Hachette,",
    "label": 0
  },
  {
    "text": "Chlororespiration is a respiratory process that is thought to occur in plant chloroplasts, involving the electron transport chain (ETC) in the thylakoid membrane. It is thought to involve NAD(P)H dehydrogenase (NDH) and plastid terminal oxidase (PTOX/IMMUTANS), forming an ETC utilizing molecular oxygen as the electron acceptor. This process also interacts with the ETC in the mitochondrion where respiration takes place, as well as with photosynthesis. If photosynthesis is inhibited by environmental stressors like water deficiency, increased heat, and/or increased/decreased light exposure, or even chilling stress, then chlororespiration is one of the crucial ways that plants use to compensate for chemical energy synthesis. Chlororespiration – the latest model Initially, the presence of chlororespiration as a legitimate respiratory process in plants was heavily doubted. However, experimentation on Chlamydomonas reinhardtii discovered plastoquinone (PQ) to be a redox carrier. The role of this redox carrier is to transport electrons from the NAD(P)H enzyme to an oxidase enzyme on the thylakoid membrane. Using this cyclic electron chain around photosystem I (PSI), chlororespiration compensates for the lack of light. This cyclic pathway also allows electrons to re-enter the PQ pool through NDH, which is then used to supply ATP to plant cells. In the year 2002, the discovery of the molecules: plastid terminal oxidase (PTOX) and NDH complexes, have revolutionised the concept of chlororespiration. Using evidence from experimentation on a Meillandina rose, this latest model observes the role of PTOX to be an enzyme that prevents the PQ pool from over-reducing, by stimulating its reoxidation. Whereas, the NDH complexes are responsible for providing a gateway for electrons to form an ETC. The presence of such molecules are apparent in the non-appressed thylakoid membranes of higher plants like Meillandina roses. The relation between chlororespiration, photosynthesis and respiration Experimentation with respiratory oxidase inhibitors (for instance, cyanide) on unicellular",
    "label": 0
  },
  {
    "text": "electrons to form an ETC. The presence of such molecules are apparent in the non-appressed thylakoid membranes of higher plants like Meillandina roses. The relation between chlororespiration, photosynthesis and respiration Experimentation with respiratory oxidase inhibitors (for instance, cyanide) on unicellular algae has revealed interactive pathways to be present between chloroplasts and mitochondria. Metabolic pathways responsible for photosynthesis are present in chloroplasts, whereas respiratory metabolic pathways are present in mitochondria. In these pathways, metabolic carriers (like phosphate) exchange NAD(P)H molecules between photosynthetic and respiratory ETCs. Evidence using mass spectrometry on algae and photosynthetic mutants of Chlamydomonas discovered that oxygen molecules were also being exchanged between photosynthetic and chlororespiratory ETCs. The mutant Chlamydomonas alga species lacks photosystems I and II (PSI and PSII), so when the alga underwent flash-induced PSI activity, it resulted in no effect on mitochondrial pathways of respiration. Instead, this flash-induced PSI activity caused an exchange between photosynthetic and chlororespiratory ETCs, which was observed using polarography. This flash of PSI activity is triggered by an over-reduction of the PQ pool and/or lack of the pyridine nucleotide in the thylakoid membrane. A reduction in such molecules then stimulates NAD(P)H dehydrogenase and PTOX to trigger chlororespiratory pathways. Furthermore, in the absence of light (and thus photosynthesis), chlororespiration plays an integral role in enabling metabolic pathways to compensate for chemical energy synthesis. This is achieved through the oxidation of stromal compounds, which increases the PQ pool and allows for the chlororespiratory ETC to take place. Stimulation of chlororespiration Heat and light as stimulants Quiles' experiment An experiment on oat plants by scientist Maria Quiles, revealed that extreme light intensity can inhibit photosynthesis and result in the lack of PSII activity. This reduction leads to an increase in NDH and PTOX levels which then causes the stimulation of chlororespiration. Oat leaves were incubated",
    "label": 0
  },
  {
    "text": "Maria Quiles, revealed that extreme light intensity can inhibit photosynthesis and result in the lack of PSII activity. This reduction leads to an increase in NDH and PTOX levels which then causes the stimulation of chlororespiration. Oat leaves were incubated and chlorophyll fluorescence emission was used to examine the effect of extreme light intensity. As the emission of the fluorescence increased, the PQ pool decreased. This stimulated the cyclic electron flow, causing NDH and PTOX levels to ultimately incline and initiate the process of chlororespiration within the thylakoid membrane. The effect of adding n-propyl gallate to the incubated leaves was also observed. N-propyl gallate is a molecule that helps distinguish between PQ reduction and oxidation activities by inhibiting PTOX. Quiles noted an increase in chlorophyll fluorescence inside the thylakoid membrane after the addition of n-propyl gallate. The result led to the stimulation of the NDH enzyme and its cyclic pathway, causing a continuous increase in chlorophyll fluorescence levelslevels. Quiles' conclusion After comparing the metabolic responses between oat plants under an average light intensity to that of oat plants under extreme light intensity, Quiles noted that the amount of PSII produced was of a lower amount in the leaves that underwent chlororespiration in extreme light. Whereas higher levels of PSII were yielded by those leaves that underwent average light intensity. A higher number of PSII is more efficient for chemical energy synthesis and thus for a plant's survival. Quiles indicates that, although the chlororespiratory pathway is less efficient, it still serves as a back-up response for energy production in plants. Ultimately, Quiles concluded that the intense light on oat plants had caused PSII levels to reduce and thus, initiate an influx of gateway (NDH) proteins to start the process of chlororespiration. Drought as a stimulant Paredes' and Quiles' experiment Scientists Miriam",
    "label": 0
  },
  {
    "text": "Quiles concluded that the intense light on oat plants had caused PSII levels to reduce and thus, initiate an influx of gateway (NDH) proteins to start the process of chlororespiration. Drought as a stimulant Paredes' and Quiles' experiment Scientists Miriam Paredes and Maria Quiles led an investigation on Meillandina roses, and their metabolic response to water deficiency. They noted how limited water irrigation can cause a reduction in PSII levels, which then results in the inhibition of photosynthesis. Paredes and Quiles also noticed the increase in chlororespiration activity as a protective mechanism for the lack of photosynthesis. In the experiment, the plants in drought were analysed with fluorescence imaging technique. This form of analysis detected increased levels of PTOX, and NDH activity within the plant. An increase in these two enzymes led to the initiation of chlororespiration. N-propyl gallate was also added to these water deficient plants. The effect resulted in increased chlorophyll fluorescence levels. Quiles recorded a similar outcome in the same species of plants that went under intense light. This increase in chlorophyll fluorescence is attributed to the influx of NDH in the thylakoid membrane, which then led to an increase in the by-product, hydrogen peroxide, inside the thylakoid membrane. Paredes' and Quiles' conclusion Paredes and Quiles concluded that chloroplasts under stress from drought rely on processes like the opening of stomata to disperse excess heat accumulated via metabolic processes within plant cells. These metabolic processes are responsible for chemical energy synthesis that can be achieved via chlororespiratory when decreased photosynthesis activity is evident. Darkness as a stimulant Gasulla's, Casano's and Guéra's experiment Scientists Francisco Gasulla, Leonardo Casano and Alfredo Guéra, observed lichen's metabolic response when placed in dark conditions. The light harvesting complex (LHC) inside the chloroplasts of Lichen is activated when subjected to darkness. Gasulla, Casano",
    "label": 0
  },
  {
    "text": "Gasulla's, Casano's and Guéra's experiment Scientists Francisco Gasulla, Leonardo Casano and Alfredo Guéra, observed lichen's metabolic response when placed in dark conditions. The light harvesting complex (LHC) inside the chloroplasts of Lichen is activated when subjected to darkness. Gasulla, Casano and Guéra, noticed that this increase in LHC activity caused PSII and the PQ pool within lichen to decrease, indicating the initiation of chlororespiration. Immunodetection analysis was used to determine the amount of LHC molecules inside lichen in a dark environment, and a luminous environment. By determining the amount of LHC within the chloroplast, scientists were able to notice decreased PSII activity. This reduction was caused by a loss in excitation energy in the PSII ETC, which then stimulated an incline in chlororespiratory pathways. Gasulla, Casano and Guéra, gathered this result, when both light-adapted and dark-adapted lichen were placed in darkness. They found that the level of LHC molecules in dark-adapted lichen had doubled compared to light adapted lichen. It was also noted that the chlororespiratory ETCs were triggered at a much earlier time in dark-adapted lichen compared to light-adapted lichen. This resulted in a faster metabolic rate and chemical synthesis response in dark-adapted lichen due to chlororespiration. Gasulla's, Casano's and Guéra's conclusion Gasulla, Casano and Guéra concluded that the longer the lichen are subjected to darkness, the quicker the chlororespiratory pathways can begin. This is due to the fast depletion of PTOX which reduce the PQ pool. These events then stimulate chlororespiratory ETCs into an ongoing loop until the lichen are placed in a luminous environment. They also derived LHC to be another indicator for chlororespiration. When PSII activity decreased, LHC concentrations inside the chloroplast increased due to loss in ETC activity. This then stimulated chlororespiratory activity to compensate for chemical energy synthesis. Chilling stress as a stimulant Segura's",
    "label": 0
  },
  {
    "text": "LHC to be another indicator for chlororespiration. When PSII activity decreased, LHC concentrations inside the chloroplast increased due to loss in ETC activity. This then stimulated chlororespiratory activity to compensate for chemical energy synthesis. Chilling stress as a stimulant Segura's and Quiles' experiment An experiment observing chilling stress on the tropical plant species, Spathiphyllum wallisii by scientists Maria Segura and Maria Quiles, showcased varying responses by chlororespiratory pathways when different parts of the plant were chilled at 10 degrees Celsius. Segura and Quiles noticed that when the roots of the plant were subjected to low temperatures (10 °C), the level of chlororespiratory molecules (NDH and PTOX) slightly varied when compared to the level of NDH and PTOX within the controlled plant. However, when the stem alone was cooled at 10 °C, the molecules NDH and PTOX increased in amount as a result of reduced PSI activity. Segura and Quiles then compared this result by subjecting only the leaves of the plant to 10 °C. They noticed that this had caused the PSII activity to stop, and thus inhibit the process of photosynthesis. The lack in photosynthetic activity, in combination with the incline in NDH and PTOX, triggered chlororespiratory pathways to begin chemical energy synthesis. Furthermore, Segura and Quiles also noted that the simultaneous chilling of the leaves and heating of the roots (whilst the plant is under illumination), can cause the slowing and eventual inhibition of PSII. This then led to an over-reduction in the PQ pool, which ultimately stimulated chlororespiration. Segura and Quiles utilised the imaging technique to determine the level of photosynthetic activity in the leaves of the plants. By discerning the percentage of photosynthesis efficiency, Segura and Quiles were able to determine the likelihood of triggering chlororespiratory pathways. They noticed that the percentage of photosynthesis efficiency remained",
    "label": 0
  },
  {
    "text": "the level of photosynthetic activity in the leaves of the plants. By discerning the percentage of photosynthesis efficiency, Segura and Quiles were able to determine the likelihood of triggering chlororespiratory pathways. They noticed that the percentage of photosynthesis efficiency remained high in test subjects where: only the leaves were chilled only the stem was chilled only the roots were chilled This high percentage of photosynthesis efficiency meant that the chances of chlororespiration taking place are slim. However, this was not true for the plant that underwent both stem chilling at 10 °C and root heating at 24 °C. The photosynthesis efficiency of this test subject was significantly lower when compared to the experimental control. This also indicated the inhibition of PSII activity which then caused chlororespiration to begin. Segura and Quiles also used an immunoblot analysis to deduce the effect of varying temperatures on different parts of the plant. Specifically, the immunoblot measures the amount of PTOX and NDH complex accumulated within the thylakoid membrane of the chloroplast organelle. An increase in NDH complex was evident in the plant where the stem was chilled at 10 degrees Celsius and the root heated at 24 degrees Celsius. Chlororespiration was stimulated in this plant. Dissimilarly, the immunoblot analysis detected no variation in the NDH and PTOX levels in test subjects where: only the leaves were chilled only the stem was chilled only the roots were chilled These test subjects had similar concentrations of NDH and PTOX when compared to the concentration of NDH and PTOX within the experimental control. Segura's and Quiles' conclusion Segura and Quiles concluded that chilling stress only induces chlororespiration when the stem is significantly cool and the roots are simultaneously warmer, compared to the average Spathiphyllum wallisii in controlled conditions. Segura and Quiles notice that PSII is present",
    "label": 0
  },
  {
    "text": "High throughput biology (or high throughput cell biology) is the use of automation equipment with classical cell biology techniques to address biological questions that are otherwise unattainable using conventional methods. It may incorporate techniques from optics, chemistry, biology or image analysis to permit rapid, highly parallel research into how cells function, interact with each other and how pathogens exploit them in disease. High throughput cell biology has many definitions, but is most commonly defined by the search for active compounds in natural materials like in medicinal plants. This is also known as high throughput screening (HTS) and is how most drug discoveries are made today, many cancer drugs, antibiotics, or viral antagonists have been discovered using HTS. The process of HTS also tests substances for potentially harmful chemicals that could be potential human health risks. HTS generally involves hundreds of samples of cells with the model disease and hundreds of different compounds being tested from a specific source. Most often a computer is used to determine when a compound of interest has a desired or interesting effect on the cell samples. Using this method has contributed to the discovery of the drug Sorafenib (Nexavar). Sorafenib is used as medication to treat multiple types of cancers, including renal cell carcinoma (RCC, cancer in the kidneys), hepatocellular carcinoma (liver cancer), and thyroid cancer. It helps stop cancer cells from reproducing by blocking the abnormal proteins present. In 1994, high throughput screening for this particular drug was completed. It was initially discovered by Bayer Pharmaceuticals in 2001. By using a RAF kinase biochemical assay, 200,000 compounds were screened from medicinal chemistry directed synthesis or combinatorial libraries to identify active molecules against activeRAF kinase. Following three trials of testing, it was found to have anti-angiogenic effects on the cancers, which stops the process of",
    "label": 0
  },
  {
    "text": "assay, 200,000 compounds were screened from medicinal chemistry directed synthesis or combinatorial libraries to identify active molecules against activeRAF kinase. Following three trials of testing, it was found to have anti-angiogenic effects on the cancers, which stops the process of creating new blood vessels in the body. Another discovery made using HTS is Maraviroc. It is an HIV entry inhibitor, and slows the process and prevents HIV from being able to enter human cells. It is used to treat a variety of cancers as well, reducing or blocking the metastasis of cancer cells, which is when cancer cells spread to a completely different part of the body from where it started. High throughput screening for Maraviroc was completed in 1997, and finalized in 2005 by Pfizer global research and development team. High-throughput biology serves as one facet of what has also been called \"omics research\" - the interface between large scale biology (genome, proteome, transcriptome), technology and researchers. High throughput cell biology has a definite focus on the cell, and methods accessing the cell such as imaging, gene expression microarrays, or genome wide screening. The basic idea is to take methods normally performed on their own and do a very large number of them without impacting their quality High throughput research can be defined as the automation of experiments such that large scale repetition becomes feasible. This is important because many of the questions faced by life science researchers now involve large numbers. For example, the Human Genome contains at least 21,000 genes, all of which can potentially contribute to cell function, or disease. To be able to capture an idea of how these genes interact with one another, which genes are involved in and where they are, methods that encompass from the cell to the genome are of interest.",
    "label": 0
  },
  {
    "text": "cell function, or disease. To be able to capture an idea of how these genes interact with one another, which genes are involved in and where they are, methods that encompass from the cell to the genome are of interest. Use of robotics Classical High throughput screening robotics are now being tied closer to cell biology, principally using technologies such as High-content screening. High throughput cell biology dictates methods that can take routine cell biology from low scale research to the speed and scale necessary to investigate complex systems, achieve high sample size, or efficiently screen through a collection. Use of microscopy and cytometry High-content screening technology is mainly based on automated digital microscopy and flow cytometry, in combination with IT-systems for the analysis and storage of the data. \"High-content\" or visual biology technology has two purposes, first to acquire spatially or temporally resolved information on an event and second to automatically quantify it. Spatially resolved instruments are typically automated microscopes, and temporal resolution still requires some form of fluorescence measurement in most cases. This means that a lot of HCS instruments are (fluorescence) microscopes that are connected to some form of image analysis package. These take care of all the steps in taking fluorescent images of cells and provide rapid, automated and unbiased assessment of experiments. Development of technology The technology can be defined as being at the same development point as the first automated DNA sequencers in the early 1990s. Automated DNA sequencing was a disruptive technology when it became practical and -even if early devices had shortcomings- it enabled genome scale sequencing projects and created the field of bioinformatics. The impact of a similarly disruptive and powerful technology on molecular cell biology and translational research is hard to predict but what is clear is that it will",
    "label": 0
  },
  {
    "text": "Poison exons (PEs); also called premature termination codon (PTC) exons or nonsense-mediated decay (NMD) exons] are a class of cassette exons that contain PTCs. Inclusion of a PE in a transcript targets the transcript for degradation via NMD. PEs are generally highly conserved elements of the genome and are thought to have important regulatory roles in biology. Targeting PE inclusion or exclusion in certain transcripts is being evaluated as a therapeutic strategy. Discovery In 2002, a model termed regulated unproductive splicing and translation (RUST) was proposed based on the finding that many (~one-third) alternatively spliced transcripts contain PEs. In this model, coupling alternative splicing to NMD (AS-NMD) is thought to tune transcript levels to regulate protein expression. Alternative splicing may also lead to NMD via other pathways besides PE inclusion, e.g., intron retention. PEs were initially characterized in RNA-binding proteins from the SR protein family. Genes for other RNA-binding proteins (RBPs) such as those for heterogenous nuclear ribonucleoprotein (hnRNP) also contain PEs. Numerous chromatin regulators also contain PEs, though these are less conserved than PEs within RBPs such as the SR proteins. Multiple spliceosomal components contain PEs. Certain PEs may occur only in specific tissues. PE-containing transcripts generally represent a minority of the overall transcript population, in part due to their active degradation via NMD, though this relative abundance can be elevated upon inhibition of NMD or certain biological states. Certain PE-containing transcripts are resistant to NMD and may be translated into truncated proteins. Regulation Cis-regulatory elements neighboring PEs have been found to affect PE inclusion. Many proteins whose corresponding genes contain PEs autoregulate PE inclusion in their respective transcripts and thereby control their own levels via a feedback loop. Cross-regulation of PE inclusion has also been observed. Differential splicing of PEs is implicated in biological processes such as differentiation,",
    "label": 0
  },
  {
    "text": "genes contain PEs autoregulate PE inclusion in their respective transcripts and thereby control their own levels via a feedback loop. Cross-regulation of PE inclusion has also been observed. Differential splicing of PEs is implicated in biological processes such as differentiation, neurodevelopment, dispersal of nuclear speckles during hypoxia, tumorigenesis, organism growth, and T cell expansion. Protein kinases that regulate phosphorylation of splicing factors can affect splicing processes, thus kinase inhibitors may affect inclusion of PEs. For example, CMGC kinase inhibitors and CDK9 inhibitors have been found to induce PE inclusion in RBM39. Small molecules that modulate chromatin accessibility can affect PE inclusion. Mutations in splicing factors can lead to inclusion of PEs in certain transcripts. PE inclusion can be regulated by external variables such as temperature and electrical activity. For example, PE inclusion in RBM3 transcript is lowered during hypothermia. This is mediated by temperature-dependent binding of the splicing factor HNRNPH1 to the RBM3 transcript. The neuronal RBPs NOVA1/2 are translocated from the nucleus to the cytoplasm during pilocarpine-induced seizure in mice, and it was found that NOVA1/2 regulates the expression of cryptic PEs. The glycosyltransferase O-GlcNAc transferase is responsible for installing the O-GlcNAc post-translational modification and contains a PE. It has been frequently observed that pharmacological or genetic perturbations that elevate cellular O-GlcNAc levels increase PE inclusion in the OGT transcript. Disease Proper regulation of PE inclusion and exclusion is important for health. Genetic mutations can affect inclusion of PEs and cause disease. For example, loss of CCAR1 leads to PE inclusion in the FANCA transcript, resulting in a Fanconi anemia phenotype. Dysregulation of components of the splicing machinery can also cause dysregulation of PE inclusion. Mutations in the splicing factor SF3B1 have been found to promote PE inclusion in BRD9, reducing BRD9 mRNA and protein levels and leading to",
    "label": 0
  },
  {
    "text": "anemia phenotype. Dysregulation of components of the splicing machinery can also cause dysregulation of PE inclusion. Mutations in the splicing factor SF3B1 have been found to promote PE inclusion in BRD9, reducing BRD9 mRNA and protein levels and leading to melanomagenesis. Mutations in U2AF1 promote PE inclusion in EIF4A2, leading to impaired global mRNA translation and acute myeloid leukemia (AML) chemoresistance through the integrated stress response pathway. The splicing factor SRSF6 contains a PE whose skipping is connected to T cell acute lymphoblastic leukemia (T-ALL), and PE inclusion in SRSF10 is linked to acute lymphoblastic leukemia (ALL). Intronic mutations can lead to PE inclusion, such as in the case of SCN1A, where mutations within intron 20 promote inclusion of the nearby PE 20N, leading to Dravet syndrome-like phenotypes in mouse models. An intronic mutation in FLNA has been found to impair binding of the splicing regulator PTBP1, leading to inclusion of a poison exon in FLNA transcripts that causes a brain-specific malformation. In RAD50, TGAGT deletion is associated with a cryptic poison exon that occurs 30 nucleotides downstream within intron 21 mediated by altered U2AF recognition. Differential inclusion of PEs in various splicing factor and hnRNP genes has been reported in type 1 diabetes. SRSF2 mutations have been found to promote PE inclusion in the epigenetic regulator EZH2, resulting in impaired hematopoietic differentiation. The TRA2B PE is essential for male fertility and meiotic cell division in mouse models. Deletion of this PE leads to an azoospermia phenotype. Clinical relevance Diagnostics With the advent of next-generation sequencing technologies, diagnostic genetic testing has emerged as a powerful tool to diagnose afflictions associated with specific genetic variants. Many diagnostic genetic testing efforts have focused on exome sequencing. PE annotations may improve the diagnostic yield of these tests for certain diseases. For example, variants",
    "label": 0
  },
  {
    "text": "has emerged as a powerful tool to diagnose afflictions associated with specific genetic variants. Many diagnostic genetic testing efforts have focused on exome sequencing. PE annotations may improve the diagnostic yield of these tests for certain diseases. For example, variants that affect PE inclusion in sodium channel genes (SCN1A, SCN2A, and SCN8A) have been found to be associated with epilepsies, and analogous variants in SNRPB have been found to be associated with cerebrocostomandibular syndrome. Therapeutic discovery As PE inclusion results in transcript degradation, targeted PE inclusion or exclusion is being evaluated as a therapeutic strategy. This strategy may prove especially applicable towards targets whose gene products are not easily ligandable such as \"undruggable\" proteins. Targeting PE inclusion/exlusion has been demonstrated with both small molecules and antisense oligonucleotides (ASOs). Small molecules may modulate splicing by stabilizing alternative splice sites. ASOs may block specific splice sites or target certain cis-regulatory elements to promote splicing at other sites. These ASOs may also be referred to as splice-switching oligonucleotides (SSOs). ASO walks tiling different ASOs across a gene sequence may be necessary to identify ASOs that have the desired effect on PE inclusion. Stoke Therapeutics is evaluating a strategy termed Targeted Augmentation of Nuclear Gene Output (TANGO). Targeting exon 20N in SCN1A mRNA with the antisense oligonucleotide zorevunersen (STK-001) blocks inclusion of this PE, leading to elevated levels of the productive SCN1A transcript and the gene product sodium channel protein 1 subunit alpha (NaV1.1). In mouse models of Dravet syndrome, which is driven by mutations in SCN1A, zorevunersen was able to reduce incidence of electrographic seizures and sudden unexpected death in epilepsy and prolong survival. As of October 2024, zorevunersen is being evaluated in phase 2 clinical trials (NCT04740476). Zorevunersen received FDA Breakthrough Therapy Designation in December 2024. Also in December 2024, Stoke Therapeutics",
    "label": 0
  },
  {
    "text": "electrographic seizures and sudden unexpected death in epilepsy and prolong survival. As of October 2024, zorevunersen is being evaluated in phase 2 clinical trials (NCT04740476). Zorevunersen received FDA Breakthrough Therapy Designation in December 2024. Also in December 2024, Stoke Therapeutics disclosed that zorevunersen is generally well tolerated and shows substantial and sustained reductions in convulsive seizure frequency. Stoke Therapeutics expects to launch a phase 3 clinical trial in 2025 evaluating zorevunersen for reduction in seizure frequency as the primary endpoint and cognition and behavioral changes as secondary endpoints. Stoke Therapeutics is also evaluating the ASO STK-002 for treatment of autosomal dominant optic atrophy (ADOA). STK-002 promotes removal of a PE in the transcript of OPA1, leading to elevated OPA1 protein levels. Remix Therapeutics developed REM-422, which is an oral small molecule that promotes PE inclusion in the oncogene MYB. REM-422 was discovered through a screening campaign for molecules that promote PE inclusion in MYB. Subsequent in vitro experiments showed that REM-422 selectively facilitates binding of the U1 snRNP complex to oligonucleotides containing the MYB 5' splice site sequence. In various AML cell lines, REM-422 leads to degradation of MYB mRNA and lower MYB protein levels. REM-422 demonstrated antitumor activity in mouse xenograft models of acute myeloid leukemia. As of October 2024, REM-422 is being evaluated in phase 1 clinical trials (NCT06118086, NCT06297941). The splicing modulator small molecule risdiplam, originally developed to promote exon 7 inclusion in the SMN2 transcript for treatment of spinal muscular atrophy, dose-dependently promotes PE inclusion in the MYB transcript as well. Rgenta Therapeutics has also developed RGT-61159, an oral small molecule that promotes PE inclusion in MYB, as a potential treatment for adenoid cystic carcinoma (ACC). RGT-61159 is being evaluated in phase 1 clinical trials (NCT06462183). PTC Therapeutics is evaluating the oral small molecule PTC518 as",
    "label": 0
  },
  {
    "text": "Physics is the scientific study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. It is one of the most fundamental scientific disciplines. A scientist who specializes in the field of physics is called a physicist. Physics is one of the oldest academic disciplines. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century, these natural sciences branched into separate research endeavors. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy. Advances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of technologies that have transformed modern society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus. History The word physics comes from the Latin physica ('study of nature'), which itself is a borrowing of the Greek φυσική (phusikḗ 'natural science'), a term derived from φύσις (phúsis 'origin, nature, property'). Ancient astronomy Astronomy is one of the oldest natural sciences. Early civilizations dating before 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, had a predictive knowledge and a basic awareness of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed",
    "label": 0
  },
  {
    "text": "Egyptians, and the Indus Valley Civilization, had a predictive knowledge and a basic awareness of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which could not explain the positions of the planets. According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the Northern Hemisphere. Natural philosophy Natural philosophy has its origins in Greece during the Archaic period (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus. Aristotle and Hellenistic physics During the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy developed along many lines of inquiry. Aristotle (Greek: Ἀριστοτέλης, Aristotélēs) (384–322 BCE), a student of Plato, wrote on many subjects, including a substantial treatise on \"Physics\" – in the 4th century BC. Aristotelian physics was influential for about two millennia. His approach mixed some limited",
    "label": 0
  },
  {
    "text": "inquiry. Aristotle (Greek: Ἀριστοτέλης, Aristotélēs) (384–322 BCE), a student of Plato, wrote on many subjects, including a substantial treatise on \"Physics\" – in the 4th century BC. Aristotelian physics was influential for about two millennia. His approach mixed some limited observation with logical deductive arguments, but did not rely on experimental verification of deduced statements. Aristotle's foundational work in Physics, though very imperfect, formed a framework against which later thinkers further developed the field. His approach is entirely superseded today. He explained ideas such as motion (and gravity) with the theory of four elements. Aristotle believed that each of the four classical elements (air, fire, water, earth) had its own natural place. Because of their differing densities, each element will revert to its own specific place in the atmosphere. So, because of their weights, fire would be at the top, air underneath fire, then water, then lastly earth. He also stated that when a small amount of one element enters the natural place of another, the less abundant element will automatically go towards its own natural place. For example, if there is a fire on the ground, the flames go up into the air in an attempt to go back into its natural place where it belongs. His laws of motion included: that heavier objects will fall faster, the speed being proportional to the weight and the speed of the object that is falling depends inversely on the density object it is falling through (e.g. density of air). He also stated that, when it comes to violent motion (motion of an object when a force is applied to it by a second object) that the speed that object moves, will only be as fast or strong as the measure of force applied to it. The problem of motion and its",
    "label": 0
  },
  {
    "text": "an object when a force is applied to it by a second object) that the speed that object moves, will only be as fast or strong as the measure of force applied to it. The problem of motion and its causes was studied carefully, leading to the philosophical notion of a \"prime mover\" as the ultimate source of all motion in the world (Book 8 of his treatise Physics). Medieval European and Islamic The Western Roman Empire fell to invaders and internal decay in the fifth century, resulting in a decline in intellectual pursuits in western Europe. By contrast, the Eastern Roman Empire (usually known as the Byzantine Empire) resisted the attacks from invaders and continued to advance various fields of learning, including physics. In the sixth century, John Philoponus challenged the dominant Aristotelian approach to science although much of his work was focused on Christian theology. In the sixth century, Isidore of Miletus created an important compilation of Archimedes' works that are copied in the Archimedes Palimpsest. Islamic scholarship inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further. The most notable innovations under Islamic scholarship were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. In his Book of Optics (also known as Kitāb al-Manāẓir) Ibn al-Haytham presented the idea of light rays as an alternative to the ancient Greek idea about visual rays. Like Ptolemy, Ibn al-Haytham applied controlled experiments, verifying the laws of refraction and reflection with the new concept of light rays, but still lacking the concept of image formation. Scientific Revolution Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the",
    "label": 0
  },
  {
    "text": "with the new concept of light rays, but still lacking the concept of image formation. Scientific Revolution Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics. Major developments in this period include the replacement of the geocentric model of the Solar System with the heliocentric Copernican model, the laws governing the motion of planetary bodies (determined by Johannes Kepler between 1609 and 1619), Galileo's pioneering work on telescopes and observational astronomy in the 16th and 17th centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation (that would come to bear his name). Newton, and separately Gottfried Wilhelm Leibniz, developed calculus, the mathematical study of continuous change, and Newton applied it to solve physical problems. 19th century The discovery of laws in thermodynamics, chemistry, and electromagnetics resulted from research efforts during the Industrial Revolution as energy needs increased. By the end of the 19th century, theories of thermodynamics, mechanics, and electromagnetics matched a wide variety of observations. Taken together these theories became the basis for what would later be called classical physics. A few experimental results remained inexplicable. Classical electromagnetism presumed a medium, an luminiferous aether to support the propagation of waves, but this medium could not be detected. The intensity of light from hot glowing blackbody objects did not match the predictions of thermodynamics and electromagnetism. The character of electron emission of illuminated metals differed from predictions. These failures, seemingly insignificant in the big picture would upset the physics world in first two decades of the 20th century. 20th century Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories",
    "label": 0
  },
  {
    "text": "the physics world in first two decades of the 20th century. 20th century Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted that the speed of light depends on the motion of the observer, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism. This discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black-body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency. This, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics improving on classical physics at very small scales. Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrödinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups. Core theories Physics deals with a wide variety of systems, although certain theories are used by all physicists. Each of these theories was experimentally tested numerous times and found to be an adequate approximation",
    "label": 0
  },
  {
    "text": "the study of probabilities and groups. Core theories Physics deals with a wide variety of systems, although certain theories are used by all physicists. Each of these theories was experimentally tested numerous times and found to be an adequate approximation of nature. These central theories are important tools for research into more specialized topics, and any physicist, regardless of their specialization, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity. Distinction between classical and modern physics In the first decades of the 20th century physics was revolutionized by the discoveries of quantum mechanics and relativity. The changes were so fundamental that these new concepts became the foundation of \"modern physics\", with other topics becoming \"classical physics\". The majority of applications of physics are essentially classical. The laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Classical theory Classical physics includes the traditional branches and topics that were recognized and well-developed before the beginning of the 20th century—classical mechanics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern",
    "label": 0
  },
  {
    "text": "may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics. Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest. Modern theory The discovery of relativity and of quantum mechanics in the first decades of the 20th century transformed the conceptual basis of physics without reducing the practical value of most of the physical theories developed up to that time. Consequently the topics of physics have come to be divided into \"classical physics\" and \"modern physics\", with the latter category including effects related to quantum mechanics and relativity. Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned",
    "label": 0
  },
  {
    "text": "into \"classical physics\" and \"modern physics\", with the latter category including effects related to quantum mechanics and relativity. Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics study matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsensical notions of space, time, matter, and energy are no longer valid. The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with motion in the absence of gravitational fields and the general theory of relativity with motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in many areas of modern physics. Fundamental concepts in modern physics include: Action Causality Covariance Particle Physical field Physical interaction Quantum Statistical ensemble Symmetry Wave Research Scientific method",
    "label": 0
  },
  {
    "text": "with gravitation. Both quantum theory and the theory of relativity find applications in many areas of modern physics. Fundamental concepts in modern physics include: Action Causality Covariance Particle Physical field Physical interaction Quantum Statistical ensemble Symmetry Wave Research Scientific method Physicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of a theory. A scientific law is a concise verbal or mathematical statement of a relation that expresses a fundamental principle of some theory, such as Newton's law of universal gravitation. Theory and experiment Theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they strongly affect and depend upon each other. Progress in physics frequently comes about when experimental results defy explanation by existing theories, prompting intense focus on applicable modeling, and when new theories generate experimentally testable predictions, which inspire the development of new experiments (and often related equipment). Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory. Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas",
    "label": 0
  },
  {
    "text": "Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories; they then explore the consequences of these ideas and work toward making testable predictions. Experimental physics expands, and is expanded by, engineering and technology. Experimental physicists who are involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry, developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas that have not been explored well by theorists. Scope and aims Physics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the \"fundamental science\". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together. For example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand",
    "label": 0
  },
  {
    "text": "objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of \"unifying\" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (theory of everything) for why nature is as it is (see section Current research below for more information). Current research Research in physics is continually progressing on a large number of fronts. In condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers. In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. The Large Hadron Collider has already found the Higgs boson, but future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. Research on the nature of the major mysteries of dark matter and dark energy is also currently ongoing. Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by",
    "label": 0
  },
  {
    "text": "dark energy is also currently ongoing. Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections. These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 Annual Review of Fluid Mechanics, Horace Lamb said: I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic. Branches and fields Fields The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table. Since the 20th century, the individual fields of physics have become increasingly specialized, and today most physicists work in a single field for their entire careers. \"Universalists\" such as Einstein (1879–1955) and Lev Landau (1908–1968), who worked in multiple fields of physics, are now very rare. Contemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach. Nuclear and",
    "label": 0
  },
  {
    "text": "very rare. Contemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach. Nuclear and particle Particle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high-energy accelerators, detectors, and computer programs necessary for this research. The field is also called \"high-energy physics\" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles. Currently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of the Higgs mechanism. Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Atomic, molecular, and optical Atomic, molecular, and optical physics (AMO) is the study of matter—matter and light—matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods",
    "label": 0
  },
  {
    "text": "Atomic, molecular, and optical Atomic, molecular, and optical physics (AMO) is the study of matter—matter and light—matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view). Atomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of nuclear physics. Molecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm. Condensed matter Condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the \"condensed\" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong. The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins",
    "label": 0
  },
  {
    "text": "between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices. Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term condensed matter physics was apparently coined by Philip Anderson when he renamed his research group—previously solid-state theory—in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering. Astrophysics Astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics. The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the Earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy. Physical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe",
    "label": 0
  },
  {
    "text": "Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang. The Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the ΛCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter. Other aspects Education Careers Philosophy Physics, as with the rest of science, relies on the philosophy of science and its \"scientific method\" to advance knowledge of the physical world. The scientific method employs a priori and a posteriori reasoning as well as the use of Bayesian inference to measure the validity of a given theory. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism, and realism. Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schrödinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose has been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, The Road to Reality. Hawking referred to himself as an \"unashamed reductionist\" and took issue with Penrose's views. Mathematics provides a compact and exact language used to describe the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton. Some theorists, like Hilary Putnam and Penelope Maddy, hold that logical truths, and therefore mathematical reasoning, depend on the empirical",
    "label": 0
  },
  {
    "text": "exact language used to describe the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton. Some theorists, like Hilary Putnam and Penelope Maddy, hold that logical truths, and therefore mathematical reasoning, depend on the empirical world. This is usually combined with the claim that the laws of logic express universal regularities found in the structural features of the world, which may explain the peculiar relation between these fields. Physics uses mathematics to organize and formulate experimental results. From those results, precise or estimated solutions are obtained, or quantitative results, from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical data, with their units of measure and estimates of the errors in the measurements. Technologies based on mathematics, like computation have made computational physics an active area of research. Ontology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data. The distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a \"mathematical model of a physical situation\" (system) and a \"mathematical description of a physical law\" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for. Fundamental vs.",
    "label": 0
  },
  {
    "text": "a physical law\" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for. Fundamental vs. applied physics Physics is a branch of fundamental science (also called basic science). Physics is also called \"the fundamental science\" because all branches of natural science including chemistry, astronomy, geology, and biology are constrained by laws of physics. Similarly, chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the molecular and atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge. Fundamental physics seeks to better explain and understand phenomena in all spheres, without a specific practical application as a goal, other than the deeper insight into the phenomema themselves. Applied physics is a general term for physics research and development that is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem. The approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics. Physics is used heavily in engineering. For example, statics, a subfield of mechanics,",
    "label": 0
  },
  {
    "text": "mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics. Physics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations. With the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the Earth, a physicist can reasonably model Earth's mass, temperature, and rate of rotation, as a function of time allowing the extrapolation forward or backward in time and so predict future or prior events. It also allows for simulations in engineering that speed up the development of a new technology. There is also considerable interdisciplinarity, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics). See also Earth science – Fields of natural science related to Earth Neurophysics – Study of the nervous system with physics Psychophysics – Branch of knowledge relating physical stimuli and psychological perception Relationship between mathematics and physics Science tourism – Travel to notable science locations Lists List of important publications in physics List of physicists Lists of physics equations Notes References Sources External links Physics at Quanta Magazine Usenet Physics FAQ – FAQ compiled by sci.physics and other physics newsgroups Website of the Nobel Prize in physics Archived 7 December 2021 at the Wayback",
    "label": 0
  },
  {
    "text": "In space physics, an electrostatic solitary wave (ESW) is a type of electromagnetic soliton occurring during short time scales (when compared to the general time scales of variations in the average electric field) in plasma. When a rapid change occurs in the electric field in a direction parallel to the orientation of the magnetic field, and this perturbation is caused by a unipolar or dipolar electric potential, it is classified as an ESW. Since the creation of ESWs is largely associated with turbulent fluid interactions, some experiments use them to compare how chaotic a measured plasma's mixing is. As such, many studies which involve ESWs are centered around turbulence, chaos, instabilities, and magnetic reconnection. History The discovery of solitary waves in general is attributed to John Scott Russell in 1834, with their first mathematical conceptualization being finalized in 1871 by Joseph Boussinesq (and later refined and popularized by Lord Rayleigh in 1876). However, these observations and solutions were for oscillations of a physical medium (usually water), and not describing the behavior of non-particle waves (including electromagnetic waves). For solitary waves outside of media, which ESWs are classified asa, the first major framework was likely developed by Louis de Broglie in 1927, though his work on the subject was temporarily abandoned and was not completed until the 1950s. Electrostatic structures were first observed near Earth's polar cusp by Donald Gurnett and Louis A. Frank using data from the Hawkeye 1 satellite in 1978. However, it is Michael Temerin, William Lotko, Forrest Mozer, and Keith Cernyb who are credited with the first observation of electrostatic solitary waves in Earth's magnetosphere in 1982. Since then, a wide variety of magnetospheric satellites have observed and documented ESWs, allowing for analysis of them and the surrounding plasma conditions. Detection Electrostatic solitary waves, by their nature,",
    "label": 0
  },
  {
    "text": "observation of electrostatic solitary waves in Earth's magnetosphere in 1982. Since then, a wide variety of magnetospheric satellites have observed and documented ESWs, allowing for analysis of them and the surrounding plasma conditions. Detection Electrostatic solitary waves, by their nature, are a phenomenon occurring in the electric field of a plasma. As such, ESWs are technically detectable by any instrument that can measure changes to the electric field during a sufficiently short time window. However, since a given plasma's electric field can vary widely depending on the properties of the plasma and since ESWs occur in short time windows, detection of ESWs can require additional screening of the data in addition to the measurement of the electric field itself. One solution to this obstacle for detecting ESWs, implemented by NASA's Magnetospheric Multiscale Mission (MMS), is to use a digital signal processor to analyze the electric field data and isolate short-duration spikes as a candidate for an ESW. Though the following detection algorithm is specific to MMS, other ESW-detecting algorithms function on similar principles. To detect an ESW, the data from a device measuring the electric field is sent to the digital signal processor. This data is analyzed across a short time window (in the case of MMS, 1 millisecond), taking both the average electric field magnitude and the largest electric field magnitude during that time window. If the peak field strength exceeds some multiple of the average field strength (4 times the field strength in MMS), then the time window is considered to contain an ESW. After this occurs, the ESW can be associated with the peak electric field strength and categorized accordingly. These algorithms vary in success at detection, since both the time window and detection multiplier are chosen by scientists based on the parameters they wish to detect.",
    "label": 0
  },
  {
    "text": "ESW can be associated with the peak electric field strength and categorized accordingly. These algorithms vary in success at detection, since both the time window and detection multiplier are chosen by scientists based on the parameters they wish to detect. As such, these algorithms often have false positives and false negatives. Interactions One of the primary physical consequences of ESWs is their creation of electron phase-space holes, a type of structure which prevents low velocity electrons from remaining close to the source of the ESW. These phase-space holes, like the ESWs themselves, can travel stably through the surrounding plasma. Since most plasmas are overall electrically neutral, these phase-space holes often end up behaving as a positive pseudoparticle. In general, in order to form an electron phase-space hole, the electric potential energy associated with the ESW's potential needs to exceed the kinetic energy of electrons in the plasma (behavior analogous to potential hills). Research has shown that one possible set of situations where this occurs naturally are kinetic instabilities. One observed example of this is the increased occurrence of these holes near Earth's bow shock and magnetopause, where the incoming solar wind collides with Earth's magnetosphere to produce large amounts of turbulence in the plasma. Forms The definition of an ESW is broad enough that, on occasion, research distinguishes between different types: Ion-acoustic solitary waves: A type of ESW that occurs when the electric potential that causes the ESW produces an ion acoustic wave. Electron-acoustic solitary waves: A type of ESW that produces an acoustic wave associated with electrons. These tend to be substantially faster and higher frequency than ion-acoustic solitary waves. Supersolitary waves: A type of ESW whose electric potential include pulses on even smaller time scales than the ESW itself. See also Soliton Interplanetary magnetic field Solar wind Electric",
    "label": 0
  },
  {
    "text": "Naïve physics or folk physics is the untrained human perception of basic physical phenomena. In the field of artificial intelligence the study of naïve physics is a part of the effort to formalize the common knowledge of human beings. Many ideas of folk physics are simplifications, misunderstandings, or misperceptions of well-understood phenomena, incapable of giving useful predictions of detailed experiments, or simply are contradicted by more thorough observations. They may sometimes be true, be true in certain limited cases, be true as a good first approximation to a more complex effect, or predict the same effect but misunderstand the underlying mechanism. Naïve physics is characterized by a mostly intuitive understanding humans have about objects in the physical world. Certain notions of the physical world may be innate. Examples Some examples of naïve physics include commonly understood, intuitive, or everyday-observed rules of nature: What goes up must come down A dropped object falls straight down A solid object cannot pass through another solid object A vacuum sucks things towards it An object is either at rest or moving, in an absolute sense Two events are either simultaneous or they are not Many of these and similar ideas formed the basis for the first works in formulating and systematizing physics by Aristotle and the medieval scholastics in Western civilization. In the modern science of physics, they were gradually contradicted by the work of Galileo, Newton, and others. The idea of absolute simultaneity survived until 1905, when the special theory of relativity and its supporting experiments discredited it. Psychological research The increasing sophistication of technology makes possible more research on knowledge acquisition. Researchers measure physiological responses such as heart rate and eye movement in order to quantify the reaction to a particular stimulus. Concrete physiological data is helpful when observing infant behavior, because",
    "label": 0
  },
  {
    "text": "of technology makes possible more research on knowledge acquisition. Researchers measure physiological responses such as heart rate and eye movement in order to quantify the reaction to a particular stimulus. Concrete physiological data is helpful when observing infant behavior, because infants cannot use words to explain things (such as their reactions) the way most adults or older children can. Research in naïve physics relies on technology to measure eye gaze and reaction time in particular. Through observation, researchers know that infants get bored looking at the same stimulus after a certain amount of time. That boredom is called habituation. When an infant is sufficiently habituated to a stimulus, he or she will typically look away, alerting the experimenter to his or her boredom. At this point, the experimenter will introduce another stimulus. The infant will then dishabituate by attending to the new stimulus. In each case, the experimenter measures the time it takes for the infant to habituate to each stimulus. As an example of the use of this method, research by Susan Hespos and colleagues studied five-month-old infants' responses to the physics of liquids and solids. Infants in this research were shown liquid being poured from one glass to another until they were habituated to the event. That is, they spent less time looking at this event. Then, the infants were shown an event in which the liquid turned to a solid, which tumbled from the glass rather flowed. The infants looked longer at the new event; that is, they dishabituated. Researchers infer that the longer the infant takes to habituate to a new stimulus, the more it violates his or her expectations of physical phenomena. When an adult observes an optical illusion that seems physically impossible, they will attend to it until it makes sense. It is commonly",
    "label": 0
  },
  {
    "text": "to habituate to a new stimulus, the more it violates his or her expectations of physical phenomena. When an adult observes an optical illusion that seems physically impossible, they will attend to it until it makes sense. It is commonly believed that our understanding of physical laws emerges strictly from experience. But research shows that infants, who do not yet have such expansive knowledge of the world, have the same extended reaction to events that appear physically impossible. Such studies hypothesize that all people are born with an innate ability to understand the physical world. Smith and Casati (1994) have reviewed the early history of naïve physics, and especially the role of the Italian psychologist Paolo Bozzi. Types of experiments The basic experimental procedure of a study on naïve physics involves three steps: prediction of the infant's expectation, violation of that expectation, and measurement of the results. As mentioned above, the physically impossible event holds the infant's attention longer, indicating surprise when expectations are violated. Solidity An experiment that tests an infant's knowledge of solidity involves the impossible event of one solid object passing through another. First, the infant is shown a flat, solid square moving from 0° to 180° in an arch formation. Next, a solid block is placed in the path of the screen, preventing it from completing its full range of motion. The infant habituates to this event, as it is what anyone would expect. Then, the experimenter creates the impossible event, and the solid screen passes through the solid block. The infant is confused by the event and attends longer than in probable event trial. Occlusion An occlusion event tests the knowledge that an object exists even if it is not immediately visible. Jean Piaget originally called this concept object permanence. When Piaget formed his developmental",
    "label": 0
  },
  {
    "text": "event and attends longer than in probable event trial. Occlusion An occlusion event tests the knowledge that an object exists even if it is not immediately visible. Jean Piaget originally called this concept object permanence. When Piaget formed his developmental theory in the 1950s, he claimed that object permanence is learned, not innate. The children's game peek-a-boo is a classic example of this phenomenon, and one which obscures the true grasp infants have on permanence. To disprove this notion, an experimenter designs an impossible occlusion event. The infant is shown a block and a transparent screen. The infant habituates, then a solid panel is placed in front of the objects to block them from view. When the panel is removed, the block is gone, but the screen remains. The infant is confused because the block has disappeared indicating that they understand that objects maintain location in space and do not simply disappear. Containment A containment event tests the infant's recognition that an object that is bigger than a container cannot fit completely into that container. Elizabeth Spelke, one of the psychologists who founded the naïve physics movement, identified the continuity principle, which conveys an understanding that objects exist continuously in time and space. Both occlusion and containment experiments hinge on the continuity principle. In the experiment, the infant is shown a tall cylinder and a tall cylindrical container. The experimenter demonstrates that the tall cylinder fits into the tall container, and the infant is bored by the expected physical outcome. The experimenter then places the tall cylinder completely into a much shorter cylindrical container, and the impossible event confuses the infant. Extended attention demonstrates the infant's understanding that containers cannot hold objects that exceed them in height. Baillargeon's research The published findings of Renee Baillargeon brought innate knowledge to the",
    "label": 0
  },
  {
    "text": "Negative air ions (NAI) are negatively charged single gas molecules or ion clusters in the air. They play a role in maintaining the charge balance of the atmosphere. The main components of air are molecular nitrogen and oxygen. Due to the strong electronegativity of oxygen and oxygen-containing molecules, they can easily capture electrons to form negatively charged air ions, most of which are superoxide radicals ·O2−, so NAI are mainly composed of negative oxygen ions, also called air negative oxygen ions. The ions can be produced by natural means such as lightning, or artificially by methods such as a corona discharge. They can play a role in electrostatic removal of air particulates both for industrial applications and with indoor air, and there are claims that they have a beneficial health effect although the evidence for this is weak. Generation mechanism Various negative air ions are formed by combining active neutral molecules and electrons in the gas through a series of ion-molecule reactions. In the air, due to the presence of many water molecules, the negative air ions typically combine with water to form hydrated negative air ions, such as O−·(H2O)n, O2−·(H2O)n, O3−·(H2O)n, OH−·(H2O)n, CO3−·(H2O)n, HCO3−·(H2O)n, CO4−·(H2O)n, NO2−·(H2O)n, NO3−·(H2O)n, etc. The ion clusters formed by the combination of small ions and water molecules have a longer survival period due to their large volume and the fact that the charge is protected by water molecules and is not easy to transfer. This is because in the molecular collision, the larger the molecular volume, the less energy is lost when encountering collisions with other molecules, thereby extending the survival time of negative air ions. Generation methods Negative air ions can be produced by two methods: natural or artificial. The methods of producing negative air ions in nature include the waterfall effect, lightning ionization",
    "label": 0
  },
  {
    "text": "other molecules, thereby extending the survival time of negative air ions. Generation methods Negative air ions can be produced by two methods: natural or artificial. The methods of producing negative air ions in nature include the waterfall effect, lightning ionization and from plants. Natural methods can produce a large number of negative air ions. The artificial means of producing negative air ions include corona discharge and water vapour. Compared with the negative air ions produced in nature, although artificial methods can produce high levels of negative air ions, there are differences in the types and concentrations of negative air ions. Natural environments Waterfall method The mechanism of producing negative air ions by the waterfall method connects to the generation of static electricity due to water droplets hitting surfaces, first discovered by Philipp Lenard in 1892 and now called the spray electrification or waterfall effect. Falling water generates static electricity either by collisions between water drops or with the ground, leading to the finer mist in updrafts being mainly negatively charged, with positive near the lower surface. It can also occur for sliding drops. The experimental fact that charging occurs is very well documented, although details of the exact process remain a topic of debate. High speed video observations show how water spray can breakup into different droplets. Larger droplets tend to be positively charged, while smaller ones negative; this charge separation is also well documented for solids. For instance it occurs in commercial powder processing and in natural processes such as dust storms, There can be electric fields of up to 160kV/m with moderate wind conditions, which leads to Coulomb forces of about the same magnitude as gravity. There can also be contributions from contact potential differences between liquids or gases and other materials, similar to the work function differences",
    "label": 0
  },
  {
    "text": "to 160kV/m with moderate wind conditions, which leads to Coulomb forces of about the same magnitude as gravity. There can also be contributions from contact potential differences between liquids or gases and other materials, similar to the work function differences for solids. It has been suggested that a triboelectric series for liquids is useful. One difference from solids is that often liquids have charged double layers, and most of the work to date supports that ion transfer (rather than electron) dominates for liquids as first suggested by Irving Langmuir in 1938. All these processes will lead to charges, relatively high electric fields and these in turn can lead to the formation of gaseous ions. Lightning strike In the atmosphere positive and negative charges will accumulate above and below the clouds. One of the most commonly cited ways that charge separation is generated is via the collisions of different forms of solid ice, for instance graupel and ice particles. Similar to the collisions between other types of particles mentioned above, the ice particles become positively charged and the graupel negative. The two have different densities, with the ice particles tending to rise and the graupel fall or move towards the center of the storm. This charge separation in different regions of the clouds leads to large electric fields. When the electric field strength between the cloud and the ground or different parts of a storm exceeds the dielectric strength of the air, discharge will occur and break through the air. During the lightning discharge process, charged particles bombard the surrounding air molecules, ionizing the molecules to generate negative air ions. At the moment of the lightning strike, hundreds of millions of negative air ions will be generated. Plant discharge The leaves of vegetation, particularly indoor plants can lead to ions. The",
    "label": 0
  },
  {
    "text": "molecules, ionizing the molecules to generate negative air ions. At the moment of the lightning strike, hundreds of millions of negative air ions will be generated. Plant discharge The leaves of vegetation, particularly indoor plants can lead to ions. The rate is low, can be increased by applying electric fields and there are indications that it can be enhanced at the tips of leaves although there are also physiological factors. The light exposure conditions also play a role, with an exponential dependence on the illumination intensity. Bioreactor have been explored which can produce levels of ions that could be useful for reducing particular contaminants. Artificial ionization Corona discharge method Currently, the most common artificial method is to use corona discharge to produce negative ions. The specific process of using corona discharge to produce gaseous negative ions is to connect the high-voltage negative electrode to a thin needle-shaped wire or a conductor with a very small radius of curvature, so that a strong electric field is generated near the electrode, releasing electrons. The electrons to collide with gas molecules, and produce new free electrons and positive ions. The newly generated free electrons will repeat the previous process, and this process will be repeated. The water vapour method The water vapour method refers to the use of artificial technology and modern instruments to simulate the process of charging via the waterfall effect, using high-speed airflow to collide with water droplets, dispersing larger water droplets into a large number of microdroplets. As the water droplet dispersion occurs, the Leonard effect is active, generating negative ions. The air moving past the water droplets may also lead to triboelectric charging, similar to aircraft \"precipitation static\" or \"P-static\"; aircraft typically have one or more static wicks to remove this. Detection Detection of negative air ions is",
    "label": 0
  },
  {
    "text": "negative ions. The air moving past the water droplets may also lead to triboelectric charging, similar to aircraft \"precipitation static\" or \"P-static\"; aircraft typically have one or more static wicks to remove this. Detection Detection of negative air ions is divided into measurement and identification. NAI measurement can be achieved by measuring the change in atmospheric conductivity when NAI pass through a conductive tube. NAI identification is typically performed by mass spectrometry, which can effectively identify a variety of negative ions, including O−,O2−,O3−,CO3−,HCO3−,NO3−, etc. Application of negative air ions Environmental improvement Negative air ions are a component of electrostatic filters which are used to remove dust and particulate pollutants (PM) both in industry and for indoor pollutants. One area that has been studied is the use of a corona-negative ions generator in experiments on particles sedimentation through three steps: charging, migration, and sedimentation. It has been found that charged PM will settle faster or sink faster under the action of gravity so that PM will settle/precipitate faster than uncharged PM. In addition, experimental studies have shown that negative air ions have a specific degradation effect on chloroform, toluene, and 1,5-Hexadiene and produce carbon dioxide and water as final products through chemical reactions. Negative ion therapy Negative air ionization therapy uses air ionisers as a non-pharmaceutical treatment for respiratory disease, allergy, or stress-related health conditions. As a preventative approach to help reduce pollutants as described above they have a role. In addition to this there are claims that they have a very specific role by themselves, with many articles on this in the popular science literature. However, detailed reviews ot the evidence indicates that many of these treatments using NAIs have minimal demonstrated heath benefits. For instance, in 2018 Jiang et.al. wrote \"The presence of NAIs is credited for increasing psychological",
    "label": 0
  },
  {
    "text": "In thermodynamics, nucleation is the first step in the formation of either a new thermodynamic phase or structure via self-assembly or self-organization within a substance or mixture. Nucleation is typically defined to be the process that determines how long an observer has to wait before the new phase or self-organized structure appears. For example, if a volume of water is cooled (at atmospheric pressure) significantly below 0 °C, it will tend to freeze into ice, but volumes of water cooled only a few degrees below 0 °C often stay completely free of ice for long periods (supercooling). At these conditions, nucleation of ice is either slow or does not occur at all. However, at lower temperatures nucleation is fast, and ice crystals appear after little or no delay. Nucleation is a common mechanism which generates first-order phase transitions, and it is the start of the process of forming a new thermodynamic phase. In contrast, new phases at continuous phase transitions start to form immediately. Nucleation is often very sensitive to impurities in the system. These impurities may be too small to be seen by the naked eye, but still can control the rate of nucleation. Because of this, it is often important to distinguish between heterogeneous nucleation and homogeneous nucleation. Heterogeneous nucleation occurs at nucleation sites on surfaces in the system. Homogeneous nucleation occurs away from a surface. Characteristics Nucleation is usually a stochastic (random) process, so even in two identical systems nucleation will occur at different times. A common mechanism is illustrated in the animation to the right. This shows nucleation of a new phase (shown in red) in an existing phase (white). In the existing phase microscopic fluctuations of the red phase appear and decay continuously, until an unusually large fluctuation of the new red phase is so",
    "label": 0
  },
  {
    "text": "shows nucleation of a new phase (shown in red) in an existing phase (white). In the existing phase microscopic fluctuations of the red phase appear and decay continuously, until an unusually large fluctuation of the new red phase is so large it is more favourable for it to grow than to shrink back to nothing. This nucleus of the red phase then grows and converts the system to this phase. The standard theory that describes this behaviour for the nucleation of a new thermodynamic phase is called classical nucleation theory. However, the CNT fails in describing experimental results of vapour to liquid nucleation even for model substances like argon by several orders of magnitude. For nucleation of a new thermodynamic phase, such as the formation of ice in water below 0 °C, if the system is not evolving with time and nucleation occurs in one step, then the probability that nucleation has not occurred should undergo exponential decay. This is seen for example in the nucleation of ice in supercooled small water droplets. The decay rate of the exponential gives the nucleation rate. Classical nucleation theory is a widely used approximate theory for estimating these rates, and how they vary with variables such as temperature. It correctly predicts that the time you have to wait for nucleation decreases extremely rapidly when supersaturated. It is not just new phases such as liquids and crystals that form via nucleation followed by growth. The self-assembly process that forms objects like the amyloid aggregates associated with Alzheimer's disease also starts with nucleation. Energy consuming self-organising systems such as the microtubules in cells also show nucleation and growth. Heterogeneous nucleation often dominates homogeneous nucleation Heterogeneous nucleation, nucleation with the nucleus at a surface, is much more common than homogeneous nucleation. For example, in the nucleation",
    "label": 0
  },
  {
    "text": "self-organising systems such as the microtubules in cells also show nucleation and growth. Heterogeneous nucleation often dominates homogeneous nucleation Heterogeneous nucleation, nucleation with the nucleus at a surface, is much more common than homogeneous nucleation. For example, in the nucleation of ice from supercooled water droplets, purifying the water to remove all or almost all impurities results in water droplets that freeze below around −35 °C, whereas water that contains impurities may freeze at −5 °C or warmer. This observation that heterogeneous nucleation can occur when the rate of homogeneous nucleation is essentially zero, is often understood using classical nucleation theory. This predicts that the nucleation slows exponentially with the height of a free energy barrier ΔG*. This barrier comes from the free energy penalty of forming the surface of the growing nucleus. For homogeneous nucleation the nucleus is approximated by a sphere, but as we can see in the schematic of macroscopic droplets to the right, droplets on surfaces are not complete spheres and so the area of the interface between the droplet and the surrounding fluid is less than a sphere's 4 π r 2 {\\displaystyle 4\\pi r^{2}} . This reduction in surface area of the nucleus reduces the height of the barrier to nucleation and so speeds nucleation up exponentially. Nucleation can also start at the surface of a liquid. For example, computer simulations of gold nanoparticles show that the crystal phase sometimes nucleates at the liquid-gold surface. Computer simulation studies of simple models Classical nucleation theory makes a number of assumptions, for example it treats a microscopic nucleus as if it is a macroscopic droplet with a well-defined surface whose free energy is estimated using an equilibrium property: the interfacial tension σ. For a nucleus that may be only of order ten molecules across it is",
    "label": 0
  },
  {
    "text": "microscopic nucleus as if it is a macroscopic droplet with a well-defined surface whose free energy is estimated using an equilibrium property: the interfacial tension σ. For a nucleus that may be only of order ten molecules across it is not always clear that we can treat something so small as a volume plus a surface. Also nucleation is an inherently out of thermodynamic equilibrium phenomenon so it is not always obvious that its rate can be estimated using equilibrium properties. However, modern computers are powerful enough to calculate essentially exact nucleation rates for simple models. These have been compared with the classical theory, for example for the case of nucleation of the crystal phase in the model of hard spheres. This is a model of perfectly hard spheres in thermal motion, and is a simple model of some colloids. For the crystallization of hard spheres the classical theory is a very reasonable approximate theory. So for the simple models we can study, classical nucleation theory works quite well, but we do not know if it works equally well for (say) complex molecules crystallising out of solution. The spinodal region Phase-transition processes can also be explained in terms of spinodal decomposition, where phase separation is delayed until the system enters the unstable region where a small perturbation in composition leads to a decrease in energy and, thus, spontaneous growth of the perturbation. This region of a phase diagram is known as the spinodal region and the phase separation process is known as spinodal decomposition and may be governed by the Cahn–Hilliard equation. The nucleation of crystals In many cases, liquids and solutions can be cooled down or concentrated up to conditions where the liquid or solution is significantly less thermodynamically stable than the crystal, but where no crystals will form",
    "label": 0
  },
  {
    "text": "Cahn–Hilliard equation. The nucleation of crystals In many cases, liquids and solutions can be cooled down or concentrated up to conditions where the liquid or solution is significantly less thermodynamically stable than the crystal, but where no crystals will form for minutes, hours, weeks or longer; this process is called supercooling. Nucleation of the crystal is then being prevented by a substantial barrier. This has consequences, for example cold high altitude clouds may contain large numbers of small liquid water droplets that are far below 0 °C. In small volumes, such as in small droplets, only one nucleation event may be needed for crystallisation. In these small volumes, the time until the first crystal appears is usually defined to be the nucleation time. Calcium carbonate crystal nucleation depends not only on degree of supersaturation but also the ratio of calcium to carbonate ions in aqueous solutions. In larger volumes many nucleation events will occur. A simple model for crystallisation in that case, that combines nucleation and growth is the KJMA or Avrami model. Although the existing theories including the classical nucleation theory explain well the steady nucleation state when the crystal nucleation rate is not time dependent, the initial non-steady state transient nucleation, and even more mysterious incubation period, require more attention of the scientific community. Chemical ordering of the undercooling liquid prior to crystal nucleation was suggested to be responsible for that feature by reducing the energy barrier for nucleation. Primary and secondary nucleation The time until the appearance of the first crystal is also called primary nucleation time, to distinguish it from secondary nucleation times. Primary here refers to the first nucleus to form, while secondary nuclei are crystal nuclei produced from a preexisting crystal. Primary nucleation describes the transition to a new phase that does not rely",
    "label": 0
  },
  {
    "text": "to distinguish it from secondary nucleation times. Primary here refers to the first nucleus to form, while secondary nuclei are crystal nuclei produced from a preexisting crystal. Primary nucleation describes the transition to a new phase that does not rely on the new phase already being present, either because it is the very first nucleus of that phase to form, or because the nucleus forms far from any pre-existing piece of the new phase. Particularly in the study of crystallisation, secondary nucleation can be important. This is the formation of nuclei of a new crystal directly caused by pre-existing crystals. For example, if the crystals are in a solution and the system is subject to shearing forces, small crystal nuclei could be sheared off a growing crystal, thus increasing the number of crystals in the system. So both primary and secondary nucleation increase the number of crystals in the system but their mechanisms are very different, and secondary nucleation relies on crystals already being present. Experimental observations on the nucleation times for the crystallisation of small volumes It is typically difficult to experimentally study the nucleation of crystals. The nucleus is microscopic, and thus too small to be directly observed. In large liquid volumes there are typically multiple nucleation events, and it is difficult to disentangle the effects of nucleation from those of growth of the nucleated phase. These problems can be overcome by working with small droplets. As nucleation is stochastic, many droplets are needed so that statistics for the nucleation events can be obtained. To the right is shown an example set of nucleation data. It is for the nucleation at constant temperature and hence supersaturation of the crystal phase in small droplets of supercooled liquid tin; this is the work of Pound and La Mer. Nucleation occurs",
    "label": 0
  },
  {
    "text": "shown an example set of nucleation data. It is for the nucleation at constant temperature and hence supersaturation of the crystal phase in small droplets of supercooled liquid tin; this is the work of Pound and La Mer. Nucleation occurs in different droplets at different times, hence the fraction is not a simple step function that drops sharply from one to zero at one particular time. The red curve is a fit of a Gompertz function to the data. This is a simplified version of the model Pound and La Mer used to model their data. The model assumes that nucleation occurs due to impurity particles in the liquid tin droplets, and it makes the simplifying assumption that all impurity particles produce nucleation at the same rate. It also assumes that these particles are Poisson distributed among the liquid tin droplets. The fit values are that the nucleation rate due to a single impurity particle is 0.02/s, and the average number of impurity particles per droplet is 1.2. Note that about 30% of the tin droplets never freeze; the data plateaus at a fraction of about 0.3. Within the model this is assumed to be because, by chance, these droplets do not have even one impurity particle and so there is no heterogeneous nucleation. Homogeneous nucleation is assumed to be negligible on the timescale of this experiment. The remaining droplets freeze in a stochastic way, at rates 0.02/s if they have one impurity particle, 0.04/s if they have two, and so on. These data are just one example, but they illustrate common features of the nucleation of crystals in that there is clear evidence for heterogeneous nucleation, and that nucleation is clearly stochastic. Ice The freezing of small water droplets to ice is an important process, particularly in the formation",
    "label": 0
  },
  {
    "text": "illustrate common features of the nucleation of crystals in that there is clear evidence for heterogeneous nucleation, and that nucleation is clearly stochastic. Ice The freezing of small water droplets to ice is an important process, particularly in the formation and dynamics of clouds. Water (at atmospheric pressure) does not freeze at 0 °C, but rather at temperatures that tend to decrease as the volume of the water decreases and as the concentration of dissolved chemicals in the water increases. Thus small droplets of water, as found in clouds, may remain liquid far below 0 °C. An example of experimental data on the freezing of small water droplets is shown at the right. The plot shows the fraction of a large set of water droplets, that are still liquid water, i.e., have not yet frozen, as a function of temperature. Note that the highest temperature at which any of the droplets freezes is close to -19 °C, while the last droplet to freeze does so at almost -35 °C. Examples Nucleation of fluids (gases and liquids) Clouds form when wet air cools (often because the air rises) and many small water droplets nucleate from the supersaturated air. The amount of water vapour that air can carry decreases with lower temperatures. The excess vapor begins to nucleate and to form small water droplets which form a cloud. Nucleation of the droplets of liquid water is heterogeneous, occurring on particles referred to as cloud condensation nuclei. Cloud seeding is the process of adding artificial condensation nuclei to quicken the formation of clouds. Bubbles of carbon dioxide nucleate shortly after the pressure is released from a container of carbonated liquid. Nucleation in boiling can occur in the bulk liquid if the pressure is reduced so that the liquid becomes superheated with respect to",
    "label": 0
  },
  {
    "text": "Bubbles of carbon dioxide nucleate shortly after the pressure is released from a container of carbonated liquid. Nucleation in boiling can occur in the bulk liquid if the pressure is reduced so that the liquid becomes superheated with respect to the pressure-dependent boiling point. More often, nucleation occurs on the heating surface, at nucleation sites. Typically, nucleation sites are tiny crevices where free gas-liquid surface is maintained or spots on the heating surface with lower wetting properties. Substantial superheating of a liquid can be achieved after the liquid is de-gassed and if the heating surfaces are clean, smooth and made of materials well wetted by the liquid. Some champagne stirrers operate by providing many nucleation sites via high surface-area and sharp corners, speeding the release of bubbles and removing carbonation from the wine. The Diet Coke and Mentos eruption offers another example. The surface of Mentos candy provides nucleation sites for the formation of carbon-dioxide bubbles from carbonated soda. Both the bubble chamber and the cloud chamber rely on nucleation, of bubbles and droplets, respectively. Nucleation of crystals The most common crystallisation process on Earth is the formation of ice. Liquid water does not freeze at 0 °C unless there is ice already present; cooling significantly below 0 °C is required to nucleate ice and for the water to freeze. For example, small droplets of very pure water can remain liquid down to below -30 °C although ice is the stable state below 0 °C. Many of the materials we make and use are crystalline, but are made from liquids, e.g. crystalline iron made from liquid iron cast into a mold, so the nucleation of crystalline materials is widely studied in industry. It is used heavily in the chemical industry for cases such as in the preparation of metallic ultradispersed",
    "label": 0
  },
  {
    "text": "Surface stress was first defined by Josiah Willard Gibbs (1839–1903) as the amount of the reversible work per unit area needed to elastically stretch a pre-existing surface. Depending upon the convention used, the area is either the original, unstretched one which represents a constant number of atoms, or sometimes is the final area; these are atomistic versus continuum definitions. Some care is needed to ensure that the definition used is also consistent with the elastic strain energy, and misinterpretations and disagreements have occurred in the literature. A similar term called \"surface free energy\", the excess free energy per unit area needed to create a new surface, is sometimes confused with \"surface stress\". Although surface stress and surface free energy of liquid–gas or liquid–liquid interface are the same, they are very different in solid–gas or solid–solid interface. Both terms represent an energy per unit area, equivalent to a force per unit length, so are sometimes referred to as \"surface tension\", which contributes further to the confusion in the literature. Thermodynamics of surface stress The continuum definition of surface free energy is the amount of reversible work d w {\\displaystyle dw} performed to create new area d A {\\displaystyle dA} of surface, expressed as: d w = γ d A {\\displaystyle dw=\\gamma dA} In this definition the number of atoms at the surface is proportional to the area. Gibbs was the first to define another surface quantity, different from the surface free energy γ {\\displaystyle \\gamma } , that is associated with the reversible work per unit area needed to elastically stretch a pre-existing surface. In a continuum approach one can define a surface stress tensor f i j {\\displaystyle f_{ij}} that relates the work associated with the variation in γ A {\\displaystyle \\gamma A} , the total excess free energy of",
    "label": 0
  },
  {
    "text": "a pre-existing surface. In a continuum approach one can define a surface stress tensor f i j {\\displaystyle f_{ij}} that relates the work associated with the variation in γ A {\\displaystyle \\gamma A} , the total excess free energy of the surface due to a strain tensor e i j {\\displaystyle e_{ij}} A f i j = d ( γ A ) / d e i j = A d γ / d e i j + γ d A / d e i j {\\displaystyle Af_{ij}=d(\\gamma A)/de_{ij}=Ad\\gamma /de_{ij}+\\gamma dA/de_{ij}} In general there is no change in area for shear, which means that for the second term on the right i = j {\\displaystyle i=j} and d A / d e i j = A δ i j {\\displaystyle dA/de_{ij}=A\\delta _{ij}} , using the Kronecker delta. Cancelling the area then gives f i j = d γ / d e i j + δ i j γ {\\displaystyle f_{ij}=d\\gamma /de_{ij}+\\delta _{ij}\\gamma } called the Shuttleworth equation. An alternative approach is an atomistic one, which defines all quantities in terms of the number of atoms, not continuum measures such as areas. This is related to the ideal of using Gibb's equimolar quantities rather than continuum numbers such as area, that is keeping the number of surface atoms constant. In this case the surface stress is defined as the derivative of the surface energy with strain, that is (deliberately using a different symbol) g i j = d γ / d e i j {\\displaystyle g_{ij}=d\\gamma /de_{ij}} This second definition is more convenient in many cases. A conventional liquid cannot sustain strains, so in the continuum definition the surface stress and surface energies are the same, whereas in the atomistic approach the surface stress is zero for a liquid. So long as",
    "label": 0
  },
  {
    "text": "convenient in many cases. A conventional liquid cannot sustain strains, so in the continuum definition the surface stress and surface energies are the same, whereas in the atomistic approach the surface stress is zero for a liquid. So long as care is taken the choice of the two does not matter, although this has been a little contentious in the literature. Physical origins of surface stress The origin of surface stress is the difference between bonding in the bulk and at a surface. The bulk spacings set the values of the in-plane surface spacings, and consequently the in-plane distance between atoms. However, the atoms at the surface have a different bonding, so would prefer to be at a different spacing, often (but not always) closer together. If they want to be closer, then d γ / d e i j {\\displaystyle d\\gamma /de_{ij}} will be positive—a tensile or expansive strain will increase the surface energy. For many metals the derivative is positive, but in other cases it is negative, for instance solid argon and some semiconductors. The sign can also strongly depend upon molecules adsorbed on the surface. If these want to be further apart that will introduce a negative component. Surface stress values Theoretical calculations The most common method to calculate the surface stresses is by calculating the surface free energy and its derivative with respect to elastic strain. Different methods have been used such as first principles, atomistic potential calculations and molecular dynamics simulations, with density functional theory most common. A large tabulation of calculated values for metals has been given by Lee et al. Typical values of the surface energies are 1-2 Joule per metre squared ( J m − 2 {\\displaystyle Jm^{-2}} ), with the trace of the surface stress tensor g i j {\\displaystyle g_{ij}}",
    "label": 0
  },
  {
    "text": "has been given by Lee et al. Typical values of the surface energies are 1-2 Joule per metre squared ( J m − 2 {\\displaystyle Jm^{-2}} ), with the trace of the surface stress tensor g i j {\\displaystyle g_{ij}} in the range of -1 to 1 J m − 2 {\\displaystyle Jm^{-2}} . Some metals such as aluminum are calculated to have fairly high, positive values (e.g. 0.82) indicating a strong propensity to contract, whereas others such as calcium are quite negative at -1.25, and others are close to zero such as cesium (-0.02). Surface stress effects Whenever there is a balance between a bulk elastic energy contribution and a surface energy term, surface stresses can be important. Surface contributions are more important at small sizes, so surface stress effects are often important at the nanoscale. Surface structural reconstruction As mentioned above, often the atoms at a surface would like to be either closer together or further apart. Countering this, the atoms below (substrate) have a fixed in-plane spacing onto which the surface has to register. One way to reduce the total energy is to have extra atoms in the surface, or remove some. This occurs for the gold (111) surface where there is approximately a 5% higher surface density when it has reconstructed. The misregistry with the underlying bulk is accommodated by having partial partial dislocations between the first two layers. The silicon (111) is similar, with a 7x7 reconstruction with both more atoms in the plane and some added atoms (called adatoms) on top. Different is the case for anatase (001) surfaces. Here the atoms want to be further apart, so one row \"pops out\" and sits further from the bulk. Adsorbate-induced changes in the surface stress When atoms or molecules are adsorbed on a surface, two",
    "label": 0
  },
  {
    "text": "case for anatase (001) surfaces. Here the atoms want to be further apart, so one row \"pops out\" and sits further from the bulk. Adsorbate-induced changes in the surface stress When atoms or molecules are adsorbed on a surface, two phenomena can lead to a change in the surface stress. One is a change in the electron density of the atoms in the surface, which changes the in-plane bonding and thus the surface stress. A second is due to interactions between the adsorbed atoms or molecules themselves, which may want to be further apart (or closer) than is possible with the atomic spacings in the surface. Note that since adsorption often depends strongly upon the environment, for instance gas pressure and temperature, the surface stress tensor will show a similar dependence. Lattice parameter changes in nanoparticles For a spherical particle the surface area will scale as the square of the size, while the volume scales as the cube. Therefore surface contributions to the energy can become important at small sizes in nanoparticles. If the energy of the surface atoms is lower when they are closer, this can be accomplished by shrinking the whole particle. The gain in energy from the surface stress will scale as the area, balanced by an energy cost for the shrinking (deformation) that scales as the volume. Combined these lead to a change in the lattice parameter that scales inversely with size. This has been measured for many materials using either electron diffraction or x-ray diffraction. This phenomenon has sometimes been written as equivalent to the Laplace pressure, also called the capillary pressure, in both cases with a surface tension. This is not correct since these are terms that apply to liquids. One complication is that the changes in lattice parameter lead to more involved forms",
    "label": 0
  },
  {
    "text": "Laplace pressure, also called the capillary pressure, in both cases with a surface tension. This is not correct since these are terms that apply to liquids. One complication is that the changes in lattice parameter lead to more involved forms for nanoparticles with more complex shapes or when surface segregation can occur. Stabilization of decahedral and icosahedral nanoparticles Also in the area of nanoparticles, surface stress can play a significant role in the stabilization of decahedral nanoparticle and icosahedral twins. In both cases an arrangement of internal twin boundaries leads to lower energy surface energy facets. Balancing this there are nominal angular gaps (disclinations) which are removed by an elastic deformation. While the main energy contributions are the external surface energy and the strain energy, the surface stress couples the two and can have an important role in the overall stability. Deformation and instabilities at surfaces During thin film growth, there can be a balance between surface energy and internal strain, with surface stress a coupling term combining the two. Instead of growing as a continuous thin film, a morphological instability can occur and the film can start to become very uneven, in many cases due to a breakdown of a balance between elastic and surface energies. The surface stress can lead to comparable wrinkling in nanowires, and also a morphological instability in a thin film. See also Gibbs free energy – Type of thermodynamic potential Nanowire – Wire with a diameter in the nanometres Nanoparticles – Particle with size less than 100 nmPages displaying short descriptions of redirect targets Surface energy – Excess energy at the surface of a material relative to its interior Surface science – Study of physical and chemical phenomena that occur at the interface of two phases Surface tension – Tendency of a liquid surface",
    "label": 0
  },
  {
    "text": "The toroidal solenoid was an early 1946 design for a fusion power device designed by George Paget Thomson and Moses Blackman of Imperial College London. It proposed to confine a deuterium fuel plasma to a toroidal (donut-shaped) chamber using magnets, and then heating it to fusion temperatures using radio frequency energy in the fashion of a microwave oven. It is notable for being the first such design to be patented, filing a secret patent on 8 May 1946 and receiving it in 1948. A critique by Rudolf Peierls noted several problems with the concept. Over the next few years, Thomson continued to suggest starting an experimental effort to study these issues, but was repeatedly denied as the underlying theory of plasma diffusion was not well developed. When similar concepts were suggested by Peter Thonemann that included a more practical heating arrangement, John Cockcroft began to take the concept more seriously, establishing small study groups at Harwell. Thomson adopted Thonemann's concept, abandoning the radio frequency system. When the patent had still not been granted in early 1948, the Ministry of Supply inquired about Thomson's intentions. Thomson explained the problems he had getting a program started and that he did not want to hand off the rights until that was clarified. As the directors of the UK nuclear program, the Ministry quickly forced Harwell's hand to provide funding for Thomson's program. Thomson then released his rights the patent, which was granted late that year. Cockcroft also funded Thonemann's work, and with that, the UK fusion program began in earnest. After the news furor over the Huemul Project in February 1951, significant funding was released and led to rapid growth of the program in the early 1950s, and ultimately to the ZETA reactor of 1958. Conceptual development The basic understanding of nuclear fusion",
    "label": 0
  },
  {
    "text": "furor over the Huemul Project in February 1951, significant funding was released and led to rapid growth of the program in the early 1950s, and ultimately to the ZETA reactor of 1958. Conceptual development The basic understanding of nuclear fusion was developed during the 1920s as physicists explored the new science of quantum mechanics. George Gamow's 1928 work on quantum tunnelling demonstrated that nuclear reactions could take place at lower energies than classical theory predicted. Using this theory, in 1929 Fritz Houtermans and Robert Atkinson demonstrated that expected reaction rates in the core of the Sun supported Arthur Eddington's 1920 suggestion that the Sun is powered by fusion. In 1934, Mark Oliphant, Paul Harteck and Ernest Rutherford were the first to achieve fusion on Earth, using a particle accelerator to shoot deuterium nuclei into a metal foil containing deuterium, lithium or other elements. This allowed them to measure the nuclear cross section of various fusion reactions, and determined that the deuterium-deuterium reaction occurred at a lower energy than other reactions, peaking at about 100,000 electronvolts (100 keV). This energy corresponds to the average energy of particles in a gas heated to a billion Kelvin. Materials heated beyond a few tens of thousand Kelvin dissociate into their electrons and nuclei, producing a gas-like state of matter known as plasma. In any gas the particles have a wide range of energies, normally following the Maxwell–Boltzmann statistics. In such a mixture, a small number of particles will have much higher energy than the bulk. This leads to an interesting possibility; even at temperatures well below 100,000 eV, some particles will randomly have enough energy to undergo fusion. Those reactions release huge amounts of energy. If that energy can be captured back into the plasma, it can heat other particles to that energy as",
    "label": 0
  },
  {
    "text": "well below 100,000 eV, some particles will randomly have enough energy to undergo fusion. Those reactions release huge amounts of energy. If that energy can be captured back into the plasma, it can heat other particles to that energy as well, making the reaction self-sustaining. In 1944, Enrico Fermi calculated this would occur at about 50,000,000 K. Confinement Taking advantage of this possibility requires the fuel plasma to be held together long enough that these random reactions have time to occur. Like any hot gas, the plasma has an internal pressure and thus tends to expand according to the ideal gas law. For a fusion reactor, the problem is keeping the plasma contained against this pressure; any known physical container would melt at temperatures in the thousands of Kelvin, far below the millions needed for fusion. A plasma is electrically conductive, and is subject to electric and magnetic fields. In a magnetic field, the electrons and nuclei orbit the magnetic field lines. A simple confinement system is a plasma-filled tube placed inside the open core of a solenoid. The plasma naturally wants to expand outwards to the walls of the tube, as well as move along it, towards the ends. The solenoid creates a magnetic field running down the centre of the tube, which the particles will orbit, preventing their motion towards the sides. Unfortunately, this arrangement does not confine the plasma along the length of the tube, and the plasma is free to flow out the ends. Initial design The obvious solution to this problem is to bend the tube, and solenoid, around to form a torus (a ring or doughnut shape). Motion towards the sides remains constrained as before, and while the particles remain free to move along the lines, in this case, they will simply circulate around",
    "label": 0
  },
  {
    "text": "tube, and solenoid, around to form a torus (a ring or doughnut shape). Motion towards the sides remains constrained as before, and while the particles remain free to move along the lines, in this case, they will simply circulate around the long axis of the tube. But, as Fermi pointed out, when the solenoid is bent into a ring, the electrical windings of the solenoid would be closer together on the inside than the outside. This would lead to an uneven field across the tube, and the fuel will slowly drift out of the centre. Some additional force needs to counteract this drift, providing long-term confinement. Thomson began development of his concept in February 1946. He noted that this arrangement caused the positively charged fuel ions to drift outward more rapidly than the negatively charged electrons. This would result in a negative area in the center of the chamber that would develop over a short period. This net negative charge would then produce an attractive force on the ions, keeping them from drifting too far from the center, and thus preventing them from drifting to the walls. It appeared this could provide long-term confinement. This leaves the issue of how to heat the fuel to the required temperatures. Thomson proposed injecting a cool plasma into the torus and then heating it with radio frequency signals beamed into the chamber. The electrons in the plasma would be \"pumped\" by this energy, transferring it to the ions though collisions. If the chamber held a plasma with densities on the order of 1014 to 1015 nuclei/cm3, it would take several minutes to reach the required temperatures. Filing a patent In early March, Thomson sent a copy of his proposal to Rudolf Peierls, then at the University of Birmingham. Peierls immediately pointed out a",
    "label": 0
  },
  {
    "text": "to 1015 nuclei/cm3, it would take several minutes to reach the required temperatures. Filing a patent In early March, Thomson sent a copy of his proposal to Rudolf Peierls, then at the University of Birmingham. Peierls immediately pointed out a concern; both Peierls and Thomson had been to meetings at the Los Alamos in 1944 where Edward Teller held several informal talks, including the one in which Fermi outlined the basic conditions needed for fusion. This was in the context of an H-bomb, or \"the super\" as it was then known. Peierls noted that the US might claim priority on such information and consider it highly secret, which meant that while Thomson was privy to the information, it was unlikely others at Imperial were. Considering the problem, Thomson decided to attempt to file a patent on the concept. This would ensure the origins of the concepts would be recorded, and prove that the ideas were due to efforts in the UK and not his previous work on the atom bomb. At the time, Thomson was not concerned with establishing personal priority for the concept nor generating income from it. At his suggestion, on 26 March 1946 they met with Arthur Block of the Ministry of Supply (MoS), which led to B.L. Russel, the MoS' patent agent, beginning to write a patent application that would be owned entirely by the government. Peierls' concerns Peierls then followed up with a lengthy critique of the concept, noting three significant issues. The major concern was that the system as a whole used a toroidal field to confine the electrons, and the electric field resulting to confine the ions. Peierls pointed out that this \"cross field\" would cause the particles to be forced across the magnetic lines due to the right hand rule, causing the",
    "label": 0
  },
  {
    "text": "field to confine the electrons, and the electric field resulting to confine the ions. Peierls pointed out that this \"cross field\" would cause the particles to be forced across the magnetic lines due to the right hand rule, causing the electrons to orbit around the chamber in the poloidal direction, eliminating the area of increased electrons in the center, and thereby allowing the ions to drift to the walls. Using Thomson's own figures for the conditions in an operating reactor, Peierls demonstrated that the resulting neutralized region would extend all the way to the walls, by less than the radius of the electrons in the field. There would be no confinement of the ions. He also included two additional concerns. One involved the issue of the deuterium fuel ions impacting with the walls of the chamber and the effects that would have, and the other that having electrons leave the plasma would cause an ion to be forced out to maintain charge balance, which would quickly \"clean up\" all of the gas in the chamber. Pinch emerges Thomson was not terribly concerned about the two minor problems but accepted that the primary one about the crossed fields was a serious issue. Considering the issue, a week later he wrote back with a modified concept. In this version, the external magnets producing the toroidal field were removed, and confinement was instead provided by running a current through the plasma. He proposed inducing this current using radio signals injected through slots cut into the torus at spaces that would create a wave moving around the torus similar to the system used in linear accelerators used to accelerate electrons. A provisional patent was filed on 8 May 1946, updated to use the new confinement system. In the patent, Thomson noted that the primary",
    "label": 0
  },
  {
    "text": "moving around the torus similar to the system used in linear accelerators used to accelerate electrons. A provisional patent was filed on 8 May 1946, updated to use the new confinement system. In the patent, Thomson noted that the primary problem would be overcoming energy losses through bremsstrahlung. He calculated that a plasma density of 1015 would remain stable long enough for the energy of the pumped electrons to heat the D fuel to the required 100 keV over the time of several minutes. Although the term \"pinch effect\" is not mentioned, except for the current generation concept, the description was similar to the pinch machines that would become widespread in the 1950s. Further criticism Thomson was then sent to New York City as part of the British delegation to the United Nations Atomic Energy Commission and did not return until late in the year. After he returned, in January 1947, John Cockcroft called a meeting at Harwell to discuss his ideas with a group including Peierls, Moon and Sayers from Birmingham University, Tuck from the Clarendon Laboratory at Oxford University, and Skinner, Frisch, Fuchs, French and Bretscher from Harwell. Thomson described his concept, including several possible ways to drive the current. Peierls reiterated his earlier concerns, mentioning the observations by Mark Oliphant and Harrie Massey who had worked with David Bohm on isotopic separation at Berkeley. Bohm had observed greatly increased rates of diffusion well beyond what classical diffusion would suggest, today known as Bohm diffusion. If this was inherent to such designs, Peierls suggested there was no way the device would work. He then added a highly prescient statement that there may be further unknown instabilities that would ruin confinement. Peierls concluded by suggesting initial studies on the pinch effect be carried out by Moon in Birmingham, where",
    "label": 0
  },
  {
    "text": "the device would work. He then added a highly prescient statement that there may be further unknown instabilities that would ruin confinement. Peierls concluded by suggesting initial studies on the pinch effect be carried out by Moon in Birmingham, where Moon had some experience in these sorts of devices and especially because Sayers was already planning experiments with powerful spark discharges in deuterium. There is no record that this work was carried out, although theoretical studies on the behaviour of plasma in a pinch was worked on. Early experiments The main outcome of the meeting was to introduce Thomson to the wirbelrohr, a new type of particle accelerator built in 1944 in Germany. The wirbelrohr used a cyclotron-like arrangement to accelerate the electrons in a plasma, which its designer, Max Steenbeck, believed would cause them to \"break away\" from the ions and accelerate to very high speeds. The parallels between this device and Thomson's concept were obvious, but Steenbeck's acceleration mechanism was novel and presented a potentially more efficient heating system. When he returned to London after the meeting, Thomson had two PhD students put on the project, with Alan Ware tasked with building a wirbelrohr and Stanley Cousins starting a mathematical study on diffusion of plasma in a magnetic field. Ware build a device using 3 cm tube bent around into a 25 cm wide torus. Using a wide variety of gas pressures and currents up to 13,000 Amps, Ware was able to show some evidence of the pinching of the plasma, but failed, as had the Germans, to find any evidence of the break away electrons. With this limited success, Ware and Cousins built a second device at 40 cm and up to 27,000 Amps. Once again, no evidence of electron break away was seen, but this time",
    "label": 0
  },
  {
    "text": "find any evidence of the break away electrons. With this limited success, Ware and Cousins built a second device at 40 cm and up to 27,000 Amps. Once again, no evidence of electron break away was seen, but this time a new high-speed rotating-mirror camera was able to directly image the plasma during the discharge and was able to conclusively show the plasma was indeed being pinched. Classification concerns While Cousins and Ware began their work, in April 1947 Thomson filed a more complete patent application. This described a larger 4 metres (13 ft) wide torus with many ports for injecting and removing gas and to inject the radio frequency energy to drive the current. The entire system was then placed within a large magnet that produced a moderate 0.15 T vertical magnetic field across the entire torus, which kept the electrons confined. He predicted that a power input of 1.9 MW would be needed and calculated that the D-D and D-T reactions would generate 9 MW of fusion energy, of which 1.9 MW was in the form of neutrons. He suggested that the neutrons could be used as a power source, but also if the system was surrounded by natural uranium, mostly 238U, the neutrons would transmute it into plutonium-239, a major component of atomic bombs. It was this last part that raised new concerns. If, as Thomson described, one could make a relatively simple device that could produce plutonium there was an obvious nuclear security concern and such work would need to be secret. Neither Thomson or Harwell were happy performing secret work at the university. Considering the problem, Thomson suggested moving this work to RAF Aldermaston. Associated Electrical Industries (AEI) was outgrowing their existing labs in Rugby and Trafford Park, and had already suggested building a new",
    "label": 0
  },
  {
    "text": "were happy performing secret work at the university. Considering the problem, Thomson suggested moving this work to RAF Aldermaston. Associated Electrical Industries (AEI) was outgrowing their existing labs in Rugby and Trafford Park, and had already suggested building a new secure lab at Aldermaston. AEI was looking to break into the emerging nuclear power field, and its director of research, Thomas Allibone, was a friend of Thomson's. Allibone strongly supported Thomson's suggestion, and further backing was received from Nobel winner James Chadwick. Cockcroft, on the other hand, believed it was too early to start the large program Thomson was suggesting, and continued to delay. Thonemann's concept Around the same time, Cockcroft learned of similar work carried out independently by Peter Thonemann at Clarendon, triggering a small theoretical program at Harwell to consider it. But all suggestions of a larger development program continued to be rejected. Thonemann's concept was to replace the radio frequency injection used by Thonemann and arrange the reactor like a betatron, that is, wrapping the torus in a large magnet and using its field to induce a current in the torus in a fashion similar to an electrical transformer. Betatrons had a natural limitation that the number of electrons in them was limited due to their self-repulsion, known as the space charge limit. Some had suggested introducing a gas to the chamber; when ionized by the accelerated electrons, the leftover ions would produce a positive charge that would help neutralize the chamber as a whole. Experiments to this end instead showed that collisions between the electrons and ions would scatter so rapidly that the number of electrons remaining was actually lower than before. This effect, however, was precisely what was desired in a fusion reactor, where the collisions would heat the deuterium ions. At an accidental meeting",
    "label": 0
  },
  {
    "text": "ions would scatter so rapidly that the number of electrons remaining was actually lower than before. This effect, however, was precisely what was desired in a fusion reactor, where the collisions would heat the deuterium ions. At an accidental meeting at Clarendon, Thonemann ended up describing his idea to Thomson. Thonemann was not aware he was talking to Thomson, nor of Thomson's work on similar ideas. Thomson followed up with Skinner, who strongly supported Thonemann's concept over Thomson's. Skinner then wrote a paper on the topic, \"Thermonuclear Reactions by Electrical Means\", and presented it to the Atomic Energy Commission on 8 April 1948. He clearly pointed out where the unknowns were in the concepts, and especially the possibility of destructive instabilities that would ruin confinement. He concluded that it would be \"useless to do much further planning\" before further study on the instability issues. It was at this point that a curious bit of legality comes into the events. In February 1948, Thompson's original patent filing had not been granted as the Ministry of Supply was not sure about his intentions on assigning the rights. Blackman was ill with malaria in South Africa, and the issue was put off for a time. It was raised again in May when he returned, resulting in a mid-July meeting. Thompson complained that Harwell was not supporting their efforts, and that as none of this was classified, he wanted to remain open to turning to private funding. In that case, he was hesitant to assign the rights to the Ministry. The Ministry, who was in charge of the nuclear labs including Harwell, quickly arranged for Cockroft to fund Thompson's development program. The program was approved in November, and the patent was assigned to the Ministry by the end of the year. Move to AEI",
    "label": 0
  },
  {
    "text": "in charge of the nuclear labs including Harwell, quickly arranged for Cockroft to fund Thompson's development program. The program was approved in November, and the patent was assigned to the Ministry by the end of the year. Move to AEI The work on fusion at Harwell and Imperial remained relatively low-level until 1951, when two events occurred that changed the nature of the program significantly. The first was the January 1950 confession by Klaus Fuchs that he had been passing atomic information to the Soviets. His confession led to immediate and sweeping classification of almost anything nuclear related. This included all fusion related work, as the previous fears about the possibility of using fusion as a neutron source to produce plutonium now seemed like a serious issue. The earlier plans to move the team from Imperial were put into effect immediately, with the AEI labs being set up at the former Aldermaston and opening in April. This lab soon became the Atomic Weapons Research Establishment. The second was the February 1951 announcement that Argentina had successfully produced fusion in its Huemul Project. Physicists around the world quickly dismissed it as impossible, which was revealed to be the case by 1952. However, it also had the effect of making politicians learn of the concept of fusion, and its potential as an energy source. Physicists working on the concept suddenly found themselves able to talk to high-ranking politicians, who proved rather receptive to increasing their budgets. Within weeks, programs in the US, UK and USSR were seeing dramatic expansion. By the summer of 1952, the UK fusion program was developing several machines based on Thonemann's overall design, and Thomson's original RF-concept was put aside. Notes References Citations Bibliography Hendry, John; Lawson, John (January 1993). Fusion Research in the UK 1945 – 1960",
    "label": 0
  },
  {
    "text": "of 1952, the UK fusion program was developing several machines based on Thonemann's overall design, and Thomson's original RF-concept was put aside. Notes References Citations Bibliography Hendry, John; Lawson, John (January 1993). Fusion Research in the UK 1945 – 1960 (PDF). AEA Technology. Clery, Daniel (2014). A Piece of the Sun: The Quest for Fusion Energy. MIT Press. ISBN 978-1-4683-1041-2. Bethe, Hans (1939). \"Energy Production in Stars\". Physical Review. 55 (5): 434–456. Bibcode:1939PhRv...55..434B. doi:10.1103/PhysRev.55.434. PMID 17835673. Oliphant, Mark; Harteck, Paul; Rutherford, Ernest (1934). \"Transmutation Effects Observed with Heavy Hydrogen\". Proceedings of the Royal Society. 144 (853): 692–703. Bibcode:1934RSPSA.144..692O. doi:10.1098/rspa.1934.0077. Hill, Charles (2013). An Atomic Empire: A Technical History of the Rise and Fall of the British Atomic Energy Programme. World Scientific. ISBN 978-1-908977-43-4. Furth, Harold (30 April 1981). \"Father of the tokamak\". New Scientist. Vol. 90, no. 1251. pp. 274–276. Braams, C. M.; Stott, P. E. (2002). Nuclear Fusion: Half a Century of Magnetic Confinement Fusion Research. CRC Press. Bibcode:2002nfhc.book.....B. ISBN 978-1-4200-3378-6. Asimov, Isaac (1972). Worlds Within Worlds: The Story of Nuclear Energy (PDF). Vol. 3. U.S. Atomic Energy Commission. Bishop, Amasa (1958). Project Sherwood; the U.S. program in controlled fusion. Addison-Wesley books in nuclear science and metallurgy. Addison-Wesley. Braams, C. M.; Stott, P. E. (2002). Nuclear Fusion: Half a Century of Magnetic Confinement Fusion Research. CRC Press. Bibcode:2002nfhc.book.....B. ISBN 978-1-4200-3378-6. Bromberg, Joan Lisa (1982). Fusion: Science, Politics, and the Invention of a New Energy Source. MIT Press. ISBN 978-0-262-02180-7. Hazeltine, R. D.; Meiss, J. D. (2013). Plasma Confinement. Courier. ISBN 978-0-486-15103-8. Phillips, James (Winter–Spring 1983). \"Magnetic Fusion\" (PDF). Los Alamos Science. McCracken, Garry; Stott, Peter (2012). Fusion: The Energy of the Universe. Academic Press. ISBN 978-0-12-384657-0. Thomson, George (30 January 1958). \"Thermonuclear Fusion: The Task and the Triumph\". New Scientist. Vol. 3, no. 63. pp. 11–13. Hansen, James (Spring",
    "label": 0
  },
  {
    "text": "Geography (from Ancient Greek γεωγραφία geōgraphía; combining gê 'Earth' and gráphō 'write', literally 'Earth writing') is the study of the lands, features, inhabitants, and phenomena of Earth. Geography is an all-encompassing discipline that seeks an understanding of Earth and its human and natural complexities—not merely where objects are, but also how they have changed and come to be. While geography is specific to Earth, many concepts can be applied more broadly to other celestial bodies in the field of planetary science. Geography has been called \"a bridge between natural science and social science disciplines.\" The history of geography as a discipline spans cultures and millennia, being independently developed by multiple groups, and cross-pollinated by trade between these groups. Geography as a discipline dates back to the earliest attempts to understand the world spatially, with the earliest example of an attempted world map dating to the 9th century BCE in ancient Babylon. Origins of many of the concepts in geography can be traced to Greek Eratosthenes of Cyrene, who may have coined the term \"geographia\" (c. 276 BC – c. 195/194 BC). The first recorded use of the word γεωγραφία was as the title of a book by Greek scholar Claudius Ptolemy (100 – 170 AD). During the Middle Ages, geography was influenced by Islamic scholars, like Muhammad al-Idrisi, producing detailed maps of the world. The Age of Discovery was influential in the development of geography, as European explorers mapped the New World. Modern developments include the development of geomatics and geographic information science. The core concepts of geography consistent between all approaches are a focus on space, place, time, and scale. Today, geography is an extremely broad discipline with multiple approaches and modalities. The main branches of geography are physical geography, human geography, and technical geography. Physical geography focuses on",
    "label": 0
  },
  {
    "text": "all approaches are a focus on space, place, time, and scale. Today, geography is an extremely broad discipline with multiple approaches and modalities. The main branches of geography are physical geography, human geography, and technical geography. Physical geography focuses on the natural environment, human geography focuses on how humans interact with the Earth, and technical geography focuses on the development of tools for understanding geography. Techniques employed can generally be broken down into quantitative and qualitative approaches, with many studies taking mixed-methods approaches. Common techniques include cartography, remote sensing, interviews, and surveying. Fundamentals Geography is a systematic study of the Earth (other celestial bodies are specified, such as \"geography of Mars\", or given another name, such as areography in the case of Mars, or selenography in the case of the Moon, or planetography for the general case), its features, and phenomena that take place on it. For something to fall into the domain of geography, it generally needs some sort of spatial component that can be placed on a map, such as coordinates, place names, or addresses. This has led to geography being associated with cartography and place names. Although many geographers are trained in toponymy and cartology, this is not their main preoccupation. Geographers study the Earth's spatial and temporal distribution of phenomena, processes, and features as well as the interaction of humans and their environment. Because space and place affect a variety of topics, such as economics, health, climate, plants, and animals, geography is highly interdisciplinary. The interdisciplinary nature of the geographical approach depends on an attentiveness to the relationship between physical and human phenomena and their spatial patterns. While narrowing down geography to a few key concepts is extremely challenging, and subject to tremendous debate within the discipline, several sources have approached the topic. The 1st edition",
    "label": 0
  },
  {
    "text": "the relationship between physical and human phenomena and their spatial patterns. While narrowing down geography to a few key concepts is extremely challenging, and subject to tremendous debate within the discipline, several sources have approached the topic. The 1st edition of the book \"Key Concepts in Geography\" broke down this into chapters focusing on \"Space,\" \"Place,\" \"Time,\" \"Scale,\" and \"Landscape.\" The 2nd edition of the book expanded on these key concepts by adding \"Environmental systems,\" \"Social Systems,\" \"Nature,\" \"Globalization,\" \"Development,\" and \"Risk,\" demonstrating how challenging narrowing the field can be. Another approach used extensively in teaching geography are the Five themes of geography established by \"Guidelines for Geographic Education: Elementary and Secondary Schools,\" published jointly by the National Council for Geographic Education and the Association of American Geographers in 1984. These themes are Location, place, relationships within places (often summarized as Human-Environment Interaction), movement, and regions. The five themes of geography have shaped how American education approaches the topic in the years since. Space Just as all phenomena exist in time and thus have a history, they also exist in space and have a geography. For something to exist in the realm of geography, it must be able to be described spatially. Thus, space is the most fundamental concept at the foundation of geography. The concept is so basic, that geographers often have difficulty defining exactly what it is. Absolute space is the exact site, or spatial coordinates, of objects, persons, places, or phenomena under investigation. We exist in space. Absolute space leads to the view of the world as a photograph, with everything frozen in place when the coordinates were recorded. Today, geographers are trained to recognize the world as a dynamic space where all processes interact and take place, rather than a static image on a map. Place",
    "label": 0
  },
  {
    "text": "a photograph, with everything frozen in place when the coordinates were recorded. Today, geographers are trained to recognize the world as a dynamic space where all processes interact and take place, rather than a static image on a map. Place Place is one of the most complex and important terms in geography. In human geography, place is the synthesis of the coordinates on the Earth's surface, the activity and use that occurs, has occurred, and will occur at the coordinates, and the meaning ascribed to the space by human individuals and groups. This can be extraordinarily complex, as different spaces may have different uses at different times and mean different things to different people. In physical geography, a place includes all of the physical phenomena that occur in space, including the lithosphere, atmosphere, hydrosphere, and biosphere. Places do not exist in a vacuum and instead have complex spatial relationships with each other, and place is concerned how a location is situated in relation to all other locations. As a discipline then, the term place in geography includes all spatial phenomena occurring at a location, the diverse uses and meanings humans ascribe to that location, and how that location impacts and is impacted by all other locations on Earth. In one of Yi-Fu Tuan's papers, he explains that in his view, geography is the study of Earth as a home for humanity, and thus place and the complex meaning behind the term is central to the discipline of geography. Time Time is usually thought to be within the domain of history, however, it is of significant concern in the discipline of geography. In physics, space and time are not separated, and are combined into the concept of spacetime. Geography is subject to the laws of physics, and in studying things that",
    "label": 0
  },
  {
    "text": "however, it is of significant concern in the discipline of geography. In physics, space and time are not separated, and are combined into the concept of spacetime. Geography is subject to the laws of physics, and in studying things that occur in space, time must be considered. Time in geography is more than just the historical record of events that occurred at various discrete coordinates; but also includes modeling the dynamic movement of people, organisms, and things through space. Time facilitates movement through space, ultimately allowing things to flow through a system. The amount of time an individual, or group of people, spends in a place will often shape their attachment and perspective to that place. Time constrains the possible paths that can be taken through space, given a starting point, possible routes, and rate of travel. Visualizing time over space is challenging in terms of cartography, and includes Space-Prism, advanced 3D geovisualizations, and animated maps. Scale Scale in the context of a map is the ratio between a distance measured on the map and the corresponding distance as measured on the ground. This concept is fundamental to the discipline of geography, not just cartography, in that phenomena being investigated appear different depending on the scale used. Scale is the frame that geographers use to measure space, and ultimately to understand a place. Laws of geography During the quantitative revolution, geography shifted to an empirical law-making (nomothetic) approach. Several laws of geography have been proposed since then, most notably by Waldo Tobler and can be viewed as a product of the quantitative revolution. In general, some dispute the entire concept of laws in geography and the social sciences. These criticisms have been addressed by Tobler and others, such as Michael Frank Goodchild. However, this is an ongoing source of debate",
    "label": 0
  },
  {
    "text": "the quantitative revolution. In general, some dispute the entire concept of laws in geography and the social sciences. These criticisms have been addressed by Tobler and others, such as Michael Frank Goodchild. However, this is an ongoing source of debate in geography and is unlikely to be resolved anytime soon. Several laws have been proposed, and Tobler's first law of geography is the most generally accepted in geography. Some have argued that geographic laws do not need to be numbered. The existence of a first invites a second, and many have proposed themselves as that. It has also been proposed that Tobler's first law of geography should be moved to the second and replaced with another. A few of the proposed laws of geography are below: Tobler's first law of geography: \"Everything is related to everything else, but near things are more related than distant.\" Tobler's second law of geography: \"The phenomenon external to a geographic area of interest affects what goes on inside.\" Arbia's law of geography: \"Everything is related to everything else, but things observed at a coarse spatial resolution are more related than things observed at a finer resolution.\" Spatial heterogeneity: Geographic variables exhibit uncontrolled variance. The uncertainty principle: \"That the geographic world is infinitely complex and that any representation must therefore contain elements of uncertainty, that many definitions used in acquiring geographic data contain elements of vagueness, and that it is impossible to measure location on the Earth's surface exactly.\" Additionally, several variations or amendments to these laws exist within the literature, although not as well supported. For example, one paper proposed an amended version of Tobler's first law of geography, referred to in the text as the Tobler–von Thünen law, which states: \"Everything is related to everything else, but near things are more related than",
    "label": 0
  },
  {
    "text": "supported. For example, one paper proposed an amended version of Tobler's first law of geography, referred to in the text as the Tobler–von Thünen law, which states: \"Everything is related to everything else, but near things are more related than distant things, as a consequence of accessibility.\" Sub-disciplines Geography is a branch of inquiry that focuses on spatial information on Earth. It is an extremely broad topic and can be broken down multiple ways. There have been several approaches to doing this spanning at least several centuries, including \"four traditions of geography\" and into distinct branches. The Four traditions of geography are often used to divide the different historical approach theories geographers have taken to the discipline. In contrast, geography's branches describe contemporary applied geographical approaches. Four traditions Geography is an extremely broad field. Because of this, many view the various definitions of geography proposed over the decades as inadequate. To address this, William D. Pattison proposed the concept of the \"Four traditions of Geography\" in 1964. These traditions are the Spatial or Locational Tradition, the Man-Land or Human-Environment Interaction Tradition (sometimes referred to as Integrated geography), the Area Studies or Regional Tradition, and the Earth Science Tradition. These concepts are broad sets of geography philosophies bound together within the discipline. They are one of many ways geographers organize the major sets of thoughts and philosophies within the discipline. Branches In another approach to the abovementioned four traditions, geography is organized into applied branches. The UNESCO Encyclopedia of Life Support Systems organizes geography into the three categories of human geography, physical geography, and technical geography. Some publications limit the number of branches to physical and human, describing them as the principal branches. Human geography largely focuses on the built environment and how humans create, view, manage, and influence space. Physical",
    "label": 0
  },
  {
    "text": "physical geography, and technical geography. Some publications limit the number of branches to physical and human, describing them as the principal branches. Human geography largely focuses on the built environment and how humans create, view, manage, and influence space. Physical geography examines the natural environment and how organisms, climate, soil, water, and landforms produce and interact, studying spatial patterns in the natural environment, atmosphere, hydrosphere, biosphere, and geosphere. The difference between these approaches led to the development of integrated geography, which combines physical and human geography and concerns the interactions between the environment and humans. Technical geography involves studying and developing the tools and techniques used by geographers, such as remote sensing, cartography, and geographic information system. It is the newest of the branches, and often other terms are used in the literature to describe the emerging category. While human and physical geographers use the techniques employed by technical geographers, technical geography is more concerned with the fundamental spatial concepts and technologies than the nature of the data. It is therefore closely associated with the spatial tradition of geography while being applied to the other two major branches. These branches use similar geographic philosophies, concepts, and tools and often overlap significantly, so geographers rarely focus on just one of these topics, often using one as their primary focus and then incorporating data and methods from the other branches. Often, geographers are asked to describe what they do by individuals outside the discipline and are likely to identify closely with a specific branch, or sub-branch when describing themselves to lay people. Physical Physical geography (or physiography) focuses on geography as an Earth science. It aims to understand the physical problems and the issues of lithosphere, hydrosphere, atmosphere, pedosphere, and global flora and fauna patterns (biosphere). Physical geography is the study of",
    "label": 0
  },
  {
    "text": "Physical Physical geography (or physiography) focuses on geography as an Earth science. It aims to understand the physical problems and the issues of lithosphere, hydrosphere, atmosphere, pedosphere, and global flora and fauna patterns (biosphere). Physical geography is the study of earth's seasons, climate, atmosphere, soil, streams, landforms, and oceans. Physical geographers will often work in identifying and monitoring the use of natural resources. Human Human geography (or anthropogeography) is a branch of geography that focuses on studying patterns and processes that shape human society. It encompasses the human, political, cultural, social, and economic aspects. In industry, human geographers often work in city planning, public health, or business analysis. Various approaches to the study of human geography have also arisen through time and include behavioral geography, culture theory, feminist geography, and geosophy. Human geographers study people and their communities, cultures, economies, and environmental interactions by studying their relations with and across space and place. Technical Technical geography concerns studying and developing tools, techniques, and statistical methods employed to collect, analyze, use, and understand spatial data. Technical geography is the most recently recognized, and controversial, of the branches. Its use dates back to 1749, when a book published by Edward Cave organized the discipline into a section containing content such as cartographic techniques and globes. There are several other terms, often used interchangeably with technical geography to subdivide the discipline, including \"techniques of geographic analysis,\" \"Geographic Information Technology,\" \"Geography method's and techniques,\" \"Geographic Information Science,\" \"geoinformatics,\" \"geomatics,\" and \"information geography\". There are subtle differences to each concept and term; however, technical geography is one of the broadest, is consistent with the naming convention of the other two branches, has been in use since the 1700s, and has been used by the UNESCO Encyclopedia of Life Support Systems to divide geography into themes.",
    "label": 0
  },
  {
    "text": "is one of the broadest, is consistent with the naming convention of the other two branches, has been in use since the 1700s, and has been used by the UNESCO Encyclopedia of Life Support Systems to divide geography into themes. As academic fields increasingly specialize in their nature, technical geography has emerged as a branch of geography specializing in geographic methods and thought. The emergence of technical geography has brought new relevance to the broad discipline of geography by serving as a set of unique methods for managing the interdisciplinary nature of the phenomena under investigation. A technical geographer might work as a GIS analyst, a GIS developer working to make new software tools, or create general reference maps incorporating human and natural features. Methods All geographic research and analysis start with asking the question \"where,\" followed by \"why there.\" Geographers start with the fundamental assumption set forth in Tobler's first law of geography, that \"everything is related to everything else, but near things are more related than distant things.\" As spatial interrelationships are key to this synoptic science, maps are a key tool. Classical cartography has been joined by a more modern approach to geographical analysis, computer-based geographic information systems (GIS). In their study, geographers use four interrelated approaches: Analytical – Asks why we find features and populations in a specific geographic area. Descriptive – Simply specifies the locations of features and populations. Regional – Examines systematic relationships between categories for a specific region or location on the planet. Systematic – Groups geographical knowledge into categories that can be explored globally. Quantitative methods Quantitative methods in geography became particularly influential in the discipline during the quantitative revolution of the 1950s and 60s. These methods revitalized the discipline in many ways, allowing scientific testing of hypotheses and proposing scientific geographic",
    "label": 0
  },
  {
    "text": "explored globally. Quantitative methods Quantitative methods in geography became particularly influential in the discipline during the quantitative revolution of the 1950s and 60s. These methods revitalized the discipline in many ways, allowing scientific testing of hypotheses and proposing scientific geographic theories and laws. The quantitative revolution heavily influenced and revitalized technical geography, and lead to the development of the subfield of quantitative geography. Quantitative cartography Cartography is the art, science, and technology of making maps. Cartographers study the Earth's surface representation with abstract symbols (map making). Although other subdisciplines of geography rely on maps for presenting their analyses, the actual making of maps is abstract enough to be regarded separately. Cartography has grown from a collection of drafting techniques into an actual science. Cartographers must learn cognitive psychology and ergonomics to understand which symbols convey information about the Earth most effectively and behavioural psychology to induce the readers of their maps to act on the information. They must learn geodesy and fairly advanced mathematics to understand how the shape of the Earth affects the distortion of map symbols projected onto a flat surface for viewing. It can be said, without much controversy, that cartography is the seed from which the larger field of geography grew. Geographic information systems Geographic information systems (GIS) deal with storing information about the Earth for automatic retrieval by a computer in an accurate manner appropriate to the information's purpose. In addition to all of the other subdisciplines of geography, GIS specialists must understand computer science and database systems. GIS has revolutionized the field of cartography: nearly all mapmaking is now done with the assistance of some form of GIS software. The science of using GIS software and GIS techniques to represent, analyse, and predict the spatial relationships is called geographic information science (GISc). Remote sensing",
    "label": 0
  },
  {
    "text": "nearly all mapmaking is now done with the assistance of some form of GIS software. The science of using GIS software and GIS techniques to represent, analyse, and predict the spatial relationships is called geographic information science (GISc). Remote sensing Remote sensing is the art, science, and technology of obtaining information about Earth's features from measurements made at a distance. Remotely sensed data can be either passive, such as traditional photography, or active, such as LiDAR. A variety of platforms can be used for remote sensing, including satellite imagery, aerial photography (including consumer drones), and data obtained from hand-held sensors. Products from remote sensing include Digital elevation model and cartographic base maps. Geographers increasingly use remotely sensed data to obtain information about the Earth's land surface, ocean, and atmosphere, because it: (a) supplies objective information at a variety of spatial scales (local to global), (b) provides a synoptic view of the area of interest, (c) allows access to distant and inaccessible sites, (d) provides spectral information outside the visible portion of the electromagnetic spectrum, and (e) facilitates studies of how features/areas change over time. Remotely sensed data may be analyzed independently or in conjunction with other digital data layers (e.g., in a geographic information system). Remote sensing aids in land use, land cover (LULC) mapping, by helping to determine both what is naturally occurring on a piece of land and what human activities are taking place on it. Geostatistics Geostatistics deal with quantitative data analysis, specifically the application of a statistical methodology to the exploration of geographic phenomena. Geostatistics is used extensively in a variety of fields, including hydrology, geology, petroleum exploration, weather analysis, urban planning, logistics, and epidemiology. The mathematical basis for geostatistics derives from cluster analysis, linear discriminant analysis and non-parametric statistical tests, and a variety of other",
    "label": 0
  },
  {
    "text": "used extensively in a variety of fields, including hydrology, geology, petroleum exploration, weather analysis, urban planning, logistics, and epidemiology. The mathematical basis for geostatistics derives from cluster analysis, linear discriminant analysis and non-parametric statistical tests, and a variety of other subjects. Applications of geostatistics rely heavily on geographic information systems, particularly for the interpolation (estimate) of unmeasured points. Geographers are making notable contributions to the method of quantitative techniques. Qualitative methods Qualitative methods in geography are descriptive rather than numerical or statistical in nature. They add context to concepts, and explore human concepts like beliefs and perspective that are difficult or impossible to quantify. Human geography is much more likely to employ qualitative methods than physical geography. Increasingly, technical geographers are attempting to employ GIS methods to qualitative datasets. Qualitative cartography Qualitative cartography employs many of the same software and techniques as quantitative cartography. It may be employed to inform on map practices, or to visualize perspectives and ideas that are not strictly quantitative in nature. An example of a form of qualitative cartography is a Chorochromatic map of nominal data, such as land cover or dominant language group in an area. Another example is a deep map, or maps that combine geography and storytelling to produce a product with greater information than a two-dimensional image of places, names, and topography. This approach offers more inclusive strategies than more traditional cartographic approaches for connecting the complex layers that makeup places. Ethnography Ethnographical research techniques are used by human geographers. In cultural geography, there is a tradition of employing qualitative research techniques, also used in anthropology and sociology. Participant observation and in-depth interviews provide human geographers with qualitative data. Geopoetics Geopoetics is an interdisciplinary approach that combines geography and poetry to explore the interconnectedness between humans, space, place, and the environment.",
    "label": 0
  },
  {
    "text": "techniques, also used in anthropology and sociology. Participant observation and in-depth interviews provide human geographers with qualitative data. Geopoetics Geopoetics is an interdisciplinary approach that combines geography and poetry to explore the interconnectedness between humans, space, place, and the environment. Geopoetics is employed as a mixed methods tool to explain the implications of geographic research. It is often employed to address and communicate the implications of complex topics, such as the anthropocene. Interviews Geographers employ interviews to gather data and acquire valuable understandings from individuals or groups regarding their encounters, outlooks, and opinions concerning spatial phenomena. Interviews can be carried out through various mediums, including face-to-face interactions, phone conversations, online platforms, or written exchanges. Geographers typically adopt a structured or semi-structured approach during interviews involving specific questions or discussion points when utilized for research purposes. These questions are designed to extract focused information about the research topic while being flexible enough to allow participants to express their experiences and viewpoints, such as through open-ended questions. Origin and history The concept of geography is present in all cultures, and therefore the history of the discipline is a series of competing narratives, with concepts emerging at various points across space and time. The oldest known world maps date back to ancient Babylon from the 9th century BC. The best known Babylonian world map, however, is the Imago Mundi of 600 BC. The map as reconstructed by Eckhard Unger shows Babylon on the Euphrates, surrounded by a circular landmass showing Assyria, Urartu, and several cities, in turn surrounded by a \"bitter river\" (Oceanus), with seven islands arranged around it so as to form a seven-pointed star. The accompanying text mentions seven outer regions beyond the encircling ocean. The descriptions of five of them have survived. In contrast to the Imago Mundi, an earlier",
    "label": 0
  },
  {
    "text": "with seven islands arranged around it so as to form a seven-pointed star. The accompanying text mentions seven outer regions beyond the encircling ocean. The descriptions of five of them have survived. In contrast to the Imago Mundi, an earlier Babylonian world map dating back to the 9th century BC depicted Babylon as being further north from the center of the world, though it is not certain what that center was supposed to represent. The ideas of Anaximander (c. 610–545 BC): considered by later Greek writers to be the true founder of geography, come to us through fragments quoted by his successors. Anaximander is credited with the invention of the gnomon, the simple, yet efficient Greek instrument that allowed the early measurement of latitude. Thales is also credited with the prediction of eclipses. The foundations of geography can be traced to ancient cultures, such as the ancient, medieval, and early modern Chinese. The Greeks, who were the first to explore geography as both art and science, achieved this through Cartography, Philosophy, and Literature, or through Mathematics. There is some debate about who was the first person to assert that the Earth is spherical in shape, with the credit going either to Parmenides or Pythagoras. Anaxagoras was able to demonstrate that the profile of the Earth was circular by explaining eclipses. However, he still believed that the Earth was a flat disk, as did many of his contemporaries. One of the first estimates of the radius of the Earth was made by Eratosthenes. The first rigorous system of latitude and longitude lines is credited to Hipparchus. He employed a sexagesimal system that was derived from Babylonian mathematics. The meridians were subdivided into 360°, with each degree further subdivided into 60 (minutes). To measure the longitude at different locations on Earth, he",
    "label": 0
  },
  {
    "text": "lines is credited to Hipparchus. He employed a sexagesimal system that was derived from Babylonian mathematics. The meridians were subdivided into 360°, with each degree further subdivided into 60 (minutes). To measure the longitude at different locations on Earth, he suggested using eclipses to determine the relative difference in time. The extensive mapping by the Romans as they explored new lands would later provide a high level of information for Ptolemy to construct detailed atlases. He extended the work of Hipparchus, using a grid system on his maps and adopting a length of 56.5 miles for a degree. From the 3rd century onwards, Chinese methods of geographical study and writing of geographical literature became much more comprehensive than what was found in Europe at the time (until the 13th century). Chinese geographers such as Liu An, Pei Xiu, Jia Dan, Shen Kuo, Fan Chengda, Zhou Daguan, and Xu Xiake wrote important treatises, yet by the 17th century advanced ideas and methods of Western-style geography were adopted in China. During the Middle Ages, the fall of the Roman empire led to a shift in the evolution of geography from Europe to the Islamic world. Muslim geographers such as Muhammad al-Idrisi produced detailed world maps (such as Tabula Rogeriana), while other geographers such as Yaqut al-Hamawi, Abu Rayhan Biruni, Ibn Battuta, and Ibn Khaldun provided detailed accounts of their journeys and the geography of the regions they visited. Turkish geographer Mahmud al-Kashgari drew a world map on a linguistic basis, and later so did Piri Reis (Piri Reis map). Further, Islamic scholars translated and interpreted the earlier works of the Romans and the Greeks and established the House of Wisdom in Baghdad for this purpose. Abū Zayd al-Balkhī, originally from Balkh, founded the \"Balkhī school\" of terrestrial mapping in Baghdad. Suhrāb, a",
    "label": 0
  },
  {
    "text": "translated and interpreted the earlier works of the Romans and the Greeks and established the House of Wisdom in Baghdad for this purpose. Abū Zayd al-Balkhī, originally from Balkh, founded the \"Balkhī school\" of terrestrial mapping in Baghdad. Suhrāb, a late tenth century Muslim geographer accompanied a book of geographical coordinates, with instructions for making a rectangular world map with equirectangular projection or cylindrical equidistant projection. Abu Rayhan Biruni (976–1048) first described a polar equi-azimuthal equidistant projection of the celestial sphere. He was regarded as the most skilled when it came to mapping cities and measuring the distances between them, which he did for many cities in the Middle East and the Indian subcontinent. He often combined astronomical readings and mathematical equations to develop methods of pin-pointing locations by recording degrees of latitude and longitude. He also developed similar techniques when it came to measuring the heights of mountains, depths of the valleys, and expanse of the horizon. He also discussed human geography and the planetary habitability of the Earth. He also calculated the latitude of Kath, Khwarezm, using the maximum altitude of the Sun, and solved a complex geodesic equation to accurately compute the Earth's circumference, which was close to modern values of the Earth's circumference. His estimate of 6,339.9 km for the Earth radius was only 16.8 km less than the modern value of 6,356.7 km. In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, al-Biruni developed a new method of using trigonometric calculations based on the angle between a plain and mountain top, which yielded more accurate measurements of the Earth's circumference, and made it possible for it to be measured by a single person from a single location. The European Age of Discovery during the 16th",
    "label": 0
  },
  {
    "text": "between a plain and mountain top, which yielded more accurate measurements of the Earth's circumference, and made it possible for it to be measured by a single person from a single location. The European Age of Discovery during the 16th and the 17th centuries, where many new lands were discovered and accounts by European explorers such as Christopher Columbus, Marco Polo, and James Cook revived a desire for both accurate geographic detail and more solid theoretical foundations in Europe. In 1650, the first edition of the Geographia Generalis was published by Bernhardus Varenius, which was later edited and republished by others including Isaac Newton. This textbook sought to integrate new scientific discoveries and principles into classical geography and approach the discipline like the other sciences emerging, and is seen by some as the division between ancient and modern geography in the West. The Geographia Generalis contained both theoretical background and practical applications related to ship navigation. The remaining problem facing both explorers and geographers was finding the latitude and longitude of a geographic location. While the problem of latitude was solved long ago, but that of longitude remained; agreeing on what zero meridians should be was only part of the problem. It was left to John Harrison to solve it by inventing the chronometer H-4 in 1760, and later in 1884 for the International Meridian Conference to adopt by convention the Greenwich meridian as zero meridians. The 18th and 19th centuries were the times when geography became recognized as a discrete academic discipline, and became part of a typical university curriculum in Europe (especially Paris and Berlin). The development of many geographic societies also occurred during the 19th century, with the foundations of the Société de Géographie in 1821, the Royal Geographical Society in 1830, Russian Geographical Society in 1845,",
    "label": 0
  },
  {
    "text": "curriculum in Europe (especially Paris and Berlin). The development of many geographic societies also occurred during the 19th century, with the foundations of the Société de Géographie in 1821, the Royal Geographical Society in 1830, Russian Geographical Society in 1845, American Geographical Society in 1851, the Royal Danish Geographical Society in 1876 and the National Geographic Society in 1888. The influence of Immanuel Kant, Alexander von Humboldt, Carl Ritter, and Paul Vidal de la Blache can be seen as a major turning point in geography from philosophy to an academic subject. Geographers such as Richard Hartshorne and Joseph Kerski have regarded both Humboldt and Ritter as the founders of modern geography, as Humboldt and Ritter were the first to establish geography as an independent scientific discipline. Over the past two centuries, the advancements in technology with computers have led to the development of geomatics and new practices such as participant observation and geostatistics being incorporated into geography's portfolio of tools. In the West during the 20th century, the discipline of geography went through four major phases: environmental determinism, regional geography, the quantitative revolution, and critical geography. The strong interdisciplinary links between geography and the sciences of geology and botany, as well as economics, sociology, and demographics, have also grown greatly, especially as a result of earth system science that seeks to understand the world in a holistic view. New concepts and philosophies have emerged from the rapid advancement of computers, quantitative methods, and interdisciplinary approaches. The 1962 book Theoretical Geography by William Bunge, which argued for a nomothetic approach to geography and that from a purely spatial perspective there was no real difference between human and physical geography, has been described by Kevin R. Cox as \"perhaps the seminal text of the spatial-quantitative revolution.\" In 1970, Waldo Tobler proposed the",
    "label": 0
  },
  {
    "text": "geography and that from a purely spatial perspective there was no real difference between human and physical geography, has been described by Kevin R. Cox as \"perhaps the seminal text of the spatial-quantitative revolution.\" In 1970, Waldo Tobler proposed the first law of geography, \"everything is related to everything else, but near things are more related than distant things.\" This law summarizes the first assumption geographers make about the world. Related fields Geology The discipline of geography, especially physical geography, and geology have significant overlap. In the past, the two have often shared academic departments at universities, a point that has led to conflict over resources. Both disciplines do seek to understand the rocks on the Earth's surface and the processes that change them over time. Geology employs many of the tools and techniques of technical geographers, such as GIS and remote sensing to aid in geological mapping. However, geology includes research that goes beyond the spatial component, such as the chemical analysis of rocks and biogeochemistry. History The discipline of History has significant overlap with geography, especially human geography. Like geology, history and geography have shared university departments. Geography provides the spatial context within which historical events unfold. The physical geographic features of a region, such as its landforms, climate, and resources, shape human settlements, trade routes, and economic activities, which in turn influence the course of historical events. Thus, a historian must have a strong foundation in geography. Historians employ the techniques of technical geographers to create historical atlases and maps. Planetary science While the discipline of geography is normally concerned with the Earth, the term can also be informally used to describe the study of other worlds, such as the planets of the Solar System and even beyond. The study of systems larger than the Earth itself",
    "label": 0
  },
  {
    "text": "The following outline is provided as an overview of and topical guide to geography: Geography – study of Earth and its people. Nature of geography Geography as an academic discipline – a body of knowledge given to − or received by − a disciple (student); a branch or sphere of knowledge, or field of study, that an individual has chosen to specialize in. Modern geography is an all-encompassing discipline that seeks to understand the Earth and its human and natural complexities − not merely where objects are, but how they have changed and come to be. Geography has been called 'the world discipline'. a field of science – widely recognized category of specialized expertise within science, and typically embodies its own terminology and nomenclature. This field will usually be represented by one or more scientific journals, where peer-reviewed research is published. There are many geography-related scientific journals. a natural science – field of academic scholarship that explores aspects of the natural environment (physical geography). a social science – field of academic scholarship that explores aspects of human society (human geography). an interdisciplinary field – a field that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions have emerged. Many of the branches of physical geography are also branches of Earth science Branches of geography As \"the bridge between the human and physical sciences,\" geography is divided into two main branches: human geography physical geography Other branches include: integrated geography technical geography regional geography Physical geography Physical geography – examines the natural environment and how the climate, vegetation and life, soil, water, and landforms are produced and interact. Fields of physical geography Geomorphology – study of landforms and the processes that them, and more broadly, of the processes controlling the topography of any planet. It",
    "label": 0
  },
  {
    "text": "the climate, vegetation and life, soil, water, and landforms are produced and interact. Fields of physical geography Geomorphology – study of landforms and the processes that them, and more broadly, of the processes controlling the topography of any planet. It seeks to understand why landscapes look the way they do, to understand landform history and dynamics, and to predict future changes through field observation, physical experiments, and numerical modeling. Hydrology – study water movement, distribution, and quality throughout the Earth, including the hydrologic cycle, water resources, and environmental watershed sustainability. Glaciology – study of glaciers, or more generally ice and natural phenomena that involve ice. Oceanography – studies a wide range of topics about oceans, including marine organisms and ecosystem dynamics; ocean currents, waves, and geophysical fluid dynamics; plate tectonics and the geology of the sea floor; and fluxes of various chemical substances and physical properties within the ocean and across its boundaries. Biogeography – study of species distribution spatially and temporally. Over areal ecological changes, it is also tied to the concepts of species and their past, or present living 'refugium', their survival locales, or their interim living sites. It aims to reveal where organisms live and at what abundance. Climatology – study of climate, scientifically defined as weather conditions averaged over a period of time. Meteorology is the interdisciplinary scientific study of the atmosphere that focuses on weather processes and short-term forecasting (in contrast with climatology). Pedology – study of soils in their natural environment that deals with pedogenesis, soil morphology, and soil classification. Palaeogeography – study of what geography was in times past, most often concerning the physical landscape and the human or cultural environment. Coastal geography – study of the dynamic interface between the ocean and the land, incorporating both the physical geography (i.e., coastal geomorphology,",
    "label": 0
  },
  {
    "text": "what geography was in times past, most often concerning the physical landscape and the human or cultural environment. Coastal geography – study of the dynamic interface between the ocean and the land, incorporating both the physical geography (i.e., coastal geomorphology, geology, and oceanography) and the human geography (sociology and history) of the coast. It involves understanding coastal weathering processes, particularly wave action, sediment movement, and weather, as well as how humans interact with the coast. Quaternary science – focuses on the Quaternary period, which encompasses the last 2.6 million years, including the last ice age and the Holocene period. Landscape ecology – the relationship between spatial patterns of urban development and ecological processes on many landscape scales and organizational levels. Approaches of physical geography Quantitative geography – Quantitative research tools and methods applied to geography. See also the quantitative revolution. Systems approach – Human geography Human geography – one of the two main subfields of geography is the study of human use and understanding of the world and the processes that have affected it. Human geography broadly differs from physical geography in that it focuses on the built environment and how space is created, viewed, and managed by humans, as well as the influence humans have on the space they occupy. Fields of human geography Cultural geography – study of cultural products and norms and their variations across and relations to spaces and places. It focuses on describing and analyzing the ways language, religion, economy, government, and other cultural phenomena vary or remain constant from one place to another and on explaining how humans function spatially. Children's geographies – study of places and spaces of children's lives, characterized experientially, politically and ethically. Children's geographies rest on the idea that children as a social group share certain characteristics that are experientially,",
    "label": 0
  },
  {
    "text": "on explaining how humans function spatially. Children's geographies – study of places and spaces of children's lives, characterized experientially, politically and ethically. Children's geographies rest on the idea that children as a social group share certain characteristics that are experientially, politically, and ethically significant and worthy of study. The pluralization in the title is intended to imply that children's lives will be markedly different in differing times and places and in differing circumstances such as gender, family, and class. The range of foci within children's geographies includes: Children and the city Children and the countryside Children and technology Children and nature, Children and globalization Methodologies of researching children's worlds Ethics of researching children's worlds Otherness of childhood Animal geographies – studies the spaces and places occupied by animals in human culture because social life and space are heavily populated by animals of many different kinds and in many differing ways (e.g., farm animals, pets, wild animals in the city). Another impetus that has influenced the development of the field is ecofeminist and other environmentalist viewpoints on nature-society relations (including questions of animal welfare and rights). Language geography – studies the geographic distribution of language or its constituent elements. There are two principal fields of study within the geography of language: Geography of languages – deals with the distribution through history and space of languages, Linguistic geography – deals with regional linguistic variations within languages. Sexuality and space – encompasses all relationships and interactions between human sexuality, space, and place, including the geographies of LGBT residence, public sex environments, sites of queer resistance, global sexualities, sex tourism, the geographies of prostitution and adult entertainment, use of sexualised locations in the arts, and sexual citizenship. Religion geography – study of the influence of geography, i.e., place and space, on religious belief. Development",
    "label": 0
  },
  {
    "text": "queer resistance, global sexualities, sex tourism, the geographies of prostitution and adult entertainment, use of sexualised locations in the arts, and sexual citizenship. Religion geography – study of the influence of geography, i.e., place and space, on religious belief. Development geography – study of the Earth's geography concerning its inhabitants' standard of living and quality of life. Measures development by looking at economic, political, and social factors and seeks to understand both the geographical causes and consequences of varying development, in part by comparing More Economically Developed Countries (MEDCs) with Less Economically Developed Countries (LEDCs). Economic geography – study of the location, distribution, and spatial organization of economic activities worldwide. Subjects of interest include but are not limited to the location of industries, economies of agglomeration (also known as \"linkages\"), transportation, international trade and development, real estate, gentrification, ethnic economies, gendered economies, core-periphery theory, the economics of urban form, the relationship between the environment and the economy (tying into a long history of geographers studying culture-environment interaction), and globalization. Marketing geography – a discipline within marketing analysis that uses geolocation (geographic information) in the process of planning and implementation of marketing activities. It can be used in any aspect of the marketing mix – the product, price, promotion, or place (geo-targeting). Transportation geography – branch of economic geography that investigates spatial interactions between people, freight, and information. It studies humans and their use of vehicles or other modes of traveling and how flows of finished goods and raw materials service markets. Health geography – application of geographical information, perspectives, and methods to the study of health, disease, and health care, to provide a spatial understanding of a population's health, the distribution of disease in an area, and the environment's effect on health and disease. It also deals with accessibility to",
    "label": 0
  },
  {
    "text": "methods to the study of health, disease, and health care, to provide a spatial understanding of a population's health, the distribution of disease in an area, and the environment's effect on health and disease. It also deals with accessibility to health care and spatial distribution of health care providers. Time geography – study of the temporal factor on spatial human activities within the following constraints: Authority - limits of accessibility to certain places or domains placed on individuals by owners or authorities Capability - limitations on the movement of individuals based on their nature. For example, movement is restricted by biological factors, such as the need for food, drink, and sleep Coupling - restraint of an individual, anchoring him or her to a location while interacting with other individuals to complete a task Historical geography – the study of the human, physical, fictional, theoretical, and \"real\" geographies of the past. It seeks to determine how cultural features of various societies across the planet emerged and evolved by understanding how a place or region changes through time, including how people have interacted with their environment and created the cultural landscape. Political geography – study of the spatially uneven outcomes of political processes and how political processes are themselves affected by spatial structures. The inter-relationships between people, state, and territory. Electoral geography – study of the relationship between election results and the regions they affect (such as the environmental impact of voting decisions), and of the effects of regional factors upon voting behavior. Geopolitics – analysis of geography, history, and social science concerning spatial politics and patterns at various scales, ranging from the level of the state to international. Strategic geography – concerned with the control of, or access to, spatial areas that affect the security and prosperity of nations. Military geography",
    "label": 0
  },
  {
    "text": "concerning spatial politics and patterns at various scales, ranging from the level of the state to international. Strategic geography – concerned with the control of, or access to, spatial areas that affect the security and prosperity of nations. Military geography – applying geographic tools, information, and techniques to solve military problems in peacetime or war. Population geography – study of how spatial variations in the distribution, composition, migration, and growth of populations are related to the nature of places. Tourism geography – study of travel and tourism, as an industry and as a social and cultural activity, and their effect on places, including the environmental impact of tourism, the geographies of tourism and leisure economies, answering tourism industry and management concerns and the sociology of tourism and locations of tourism. Urban geography – the study of urban areas, in terms of concentration, infrastructure, economy, and environmental impacts. Approaches of human geography Behavioral geography – Approach to human geography that examines human behavior using a disaggregate approach Cognitive geography – Interdisciplinary study of cognitive science and geography Critical geography – Variant of social science that seeks to interpret and change the world Feminist geography – Approach in human geography which applies the theories, methods and critiques of feminism Marxist geography – Strand of critical geography Non-representational theory – Post-structuralist human geography theory Postcolonialism – Study of the cultural legacy of colonialism and imperialism Post-structuralism – Philosophical school and tradition – Qualitative geography – qualitative research tools and methods applied to geography. Integrated geography Integrated geography – branch of geography that describes the spatial aspects of interactions between humans and the natural world. It requires an understanding of the dynamics of geology, meteorology, hydrology, biogeography, ecology, and geomorphology, as well as the ways in which human societies conceptualize the environment. Technical geography",
    "label": 0
  },
  {
    "text": "the spatial aspects of interactions between humans and the natural world. It requires an understanding of the dynamics of geology, meteorology, hydrology, biogeography, ecology, and geomorphology, as well as the ways in which human societies conceptualize the environment. Technical geography Technical geography – branch of geography and the discipline of studying, developing, and applying methods to gather, store, process, and deliver geographic or spatially referenced information. It is a widespread interdisciplinary field that includes the tools and techniques used in land surveying, remote sensing, cartography, Geographic Information Systems (GIS), Global Navigation Satellite Systems, photogrammetry, and related forms of earth mapping. Cyber geography – study of the physical network of broadband cables Fields contributing to technical geography Geomatics – Geographic data discipline Photogrammetry – Taking measurements using photography Cartography – Study and practice of making maps Digital terrain modelling – 3D computer-generated imagery and measurements of terrainPages displaying short descriptions of redirect targets Geodesy – Science of measuring the shape, orientation, and gravity of Earth Geographic information system – System to capture, manage, and present geographic data Geospatial – Data and information having an implicit or explicit association with a locationPages displaying short descriptions of redirect targets Global navigation satellite systems represented by Satellite navigation – Use of satellite signals for navigation or geo-spatial positioning – Any system that uses satellite radio signals to provide autonomous geo-spatial positioning Global Positioning System – American satellite-based radio navigation service Hydrography – Measurement of bodies of water Mathematics – Area of knowledge Navigation – Process of monitoring and controlling the movement of a craft or vehicle Remote sensing – Obtaining information through non-contact sensors Surveying – Science of determining the positions of points and the distances and angles between them Regional geography Regional geography – study of world regions. Attention is paid to unique",
    "label": 0
  },
  {
    "text": "or vehicle Remote sensing – Obtaining information through non-contact sensors Surveying – Science of determining the positions of points and the distances and angles between them Regional geography Regional geography – study of world regions. Attention is paid to unique characteristics of a particular region such as its natural elements, human elements, and regionalization which covers the techniques of delineating space into regions. Regional geography breaks down into the study of specific regions. Region – an area, defined by physical characteristics, human characteristics, or functional characteristics. The term is used in various ways among the different branches of geography. A region can be seen as a collection of smaller units, such as a country and its political divisions, or as one part of a larger whole, as in a country on a continent. Continents Continent – one of several large landmasses on Earth. They are generally identified by convention rather than any specific criteria, but seven areas are commonly regarded as continents. They are: 1. Africa (outline) – 2. Antarctica – 3. Australia (outline) – The Americas: 4. North America (outline) – 5. South America (outline) – Eurasia: 6. Europe (outline) – 7. Asia (outline) – Subregions Subregion (list) Biogeographic regions Biogeographic realm The World Wildlife Fund (WWF) developed a system of eight biogeographic realms (ecozones): Nearctic 22.9 mil. km2 (including most of North America) Palearctic 54.1 mil. km2 (including the bulk of Eurasia and North Africa) Afrotropic 22.1 mil. km2 (including Sub-Saharan Africa) Indomalaya 7.5 mil. km2 (including the South Asian subcontinent and Southeast Asia) Australasia 7.7 mil. km2 (including Australia, New Guinea, and neighboring islands). The northern boundary of this zone is known as the Wallace line. Neotropic 19.0 mil. km2 (including South America and the Caribbean) Oceania 1.0 mil. km2 (including Polynesia, Fiji and Micronesia) Antarctic 0.3",
    "label": 0
  },
  {
    "text": "(including Australia, New Guinea, and neighboring islands). The northern boundary of this zone is known as the Wallace line. Neotropic 19.0 mil. km2 (including South America and the Caribbean) Oceania 1.0 mil. km2 (including Polynesia, Fiji and Micronesia) Antarctic 0.3 mil. km2 (including Antarctica). Ecoregions Ecoregion Biogeographic realms are further divided into ecoregions. The World has over 800 terrestrial ecoregions. See Lists of ecoregions by country. Geography of the political divisions of the World Other regions Atlantic World Bermuda Triangle Pacific Rim Pacific Ring of Fire History of geography Topics pertaining to the geographical study of the World throughout history: By period Ancient roads Ancient Greek geography Age of Discovery Major explorations after the Age of Discovery Critical geography Environmental determinism By region Chinese geography History of human geography in China By subject Historical population of the world By field History of human geography History of cartography – Evolution of the art and science of mapmaking History of longitude – Record of humanity's attempts to find east-west position on Earth Longitude Prize – 2014 British inducement prize contest History of cultural geography History of economic geography History of health geography History of political geography History of demography History of physical geography History of biogeography History of climatology History of meteorology History of geodesy History of geomorphology History of hydrology History of oceanography History of landscape ecology History of regional geography Elements of geography Topics common to the various branches of geography include: Tasks and tools of geography Exploration – Process of investigating unfamiliar things Geocode, also known as Geospatial Entity Object Code – Process of turning a place name/address to coordinatesPages displaying short descriptions of redirect targets Geographic information system (GIS) – System to capture, manage, and present geographic data Globe – Scale model of a celestial body Map –",
    "label": 0
  },
  {
    "text": "Object Code – Process of turning a place name/address to coordinatesPages displaying short descriptions of redirect targets Geographic information system (GIS) – System to capture, manage, and present geographic data Globe – Scale model of a celestial body Map – Symbolic depiction of spatial relationships Atlas – Collection of maps Cartography – Study and practice of making maps Outline of cartography – Overview of and topical guide to cartography Map projection – Systematic representation of the surface of a sphere or ellipsoid onto a plane Demographics – Science that deals with populations and their structures, statistically and theoreticallyPages displaying short descriptions of redirect targets Spatial analysis – Techniques to study geometric data Surveying – Science of determining the positions of points and the distances and angles between them Types of geographic features Geographic feature – component of a planet that can be referred to as a location, place, site, area, or region, and therefore may show up on a map. A geographic feature may be natural or artificial. Location and place Location – Absolute location – Point or an area on Earth's surface or elsewherePages displaying short descriptions of redirect targets Latitude – Geographic coordinate specifying north-south position Prime meridian – Line of longitude, at which longitude is defined to be 0° Longitude – East-West geographic coordinate Equator – Imaginary line halfway between Earth's North and South poles Tropic of Cancer – Line of northernmost latitude at which the Sun can be directly overhead Tropic of Capricorn – Line of southernmost latitude at which the Sun can be directly overhead Arctic Circle – Boundary of the Arctic Antarctic Circle – Boundary of the Antarctic North Pole – Northernmost point on Earth South Pole – Southernmost point on Earth Altitude – Height in relation to a specified reference point Elevation –",
    "label": 0
  },
  {
    "text": "Arctic Circle – Boundary of the Arctic Antarctic Circle – Boundary of the Antarctic North Pole – Northernmost point on Earth South Pole – Southernmost point on Earth Altitude – Height in relation to a specified reference point Elevation – Height of a geographic location above a fixed reference point Place Aspects of a place or region Climate – Long-term weather pattern of a region Population – Group of individuals of a species, separated from other groups by in some manner Demographics – Science that deals with populations and their structures, statistically and theoreticallyPages displaying short descriptions of redirect targets Population density – Measurement of population per unit area or unit volume Human overpopulation – Proposed condition wherein human numbers exceed the carrying capacity of the environment World population – Total number of living humans on Earth Sense of place – Term used in behavioral sciences and urban planning Terrain – Dimension and shape of land surfaces Topography – Study of the forms of land surfaces in earth Tourist attraction – Place of interest where tourists visit Lists of places Geography is a worldwide study Natural geographic features Natural geographic feature – an ecosystem or natural landform. Ecosystems Ecosystem – community of living organisms in conjunction with the nonliving components of their environment (things like air, water and mineral soil), interacting as a system. These biotic and abiotic components are regarded as linked together through nutrient cycles and energy flows. Biodiversity hotspot – Biodiverse region under threat Realm – broadest biogeographic division of the Earth's land surface, based on distributional patterns of terrestrial organisms. Ecoprovince – biogeographic unit smaller than a realm that contains one or more ecoregions. Ecoregion – Ecological and geographical area Ecodistrict – Environmental planning Ecosection Ecosite Ecotope – Ecologically distinct landscape feature Ecoelement Biome – Biogeographical",
    "label": 0
  },
  {
    "text": "patterns of terrestrial organisms. Ecoprovince – biogeographic unit smaller than a realm that contains one or more ecoregions. Ecoregion – Ecological and geographical area Ecodistrict – Environmental planning Ecosection Ecosite Ecotope – Ecologically distinct landscape feature Ecoelement Biome – Biogeographical unit with a particular biological community Bioregion – Ecology terminology Biotope – Habitat for communities made up of populations of multiple species Bioelement Natural landforms Natural landform – terrain or body of water. Landforms are topographical elements, and are defined by their surface form and location in the landscape. Landforms are categorized by traits such as elevation, slope, orientation, stratification, rock exposure, and soil type. Some landforms are artificial, such as certain islands, but most landforms are natural. Natural terrain feature types Continent – Large geographical region identified by convention Island – Piece of subcontinental land surrounded by water Mainland – Continental part of any polity or the main island within an island nation Mountain – Large natural elevation of the Earth's surface Mountain range – Geographic area containing several geologically related mountains Peninsula – Landform that extends from a mainland and is surrounded by water on most sides. Subcontinent – A large, relatively self-contained landmass forming a subdivision of a continent Natural body of water types Natural bodies of water – Any significant accumulation of water, generally on a planet's surface Bodies of seawater – Water from a sea or an ocean Channel – Narrow body of water Firth – Scottish word used for various coastal inlets and straits Harbor – Sheltered body of water where ships may shelter Inlet – Indentation of a shoreline Bay – Recessed, coastal body of water connected to an ocean or lake Bight – Shallowly concave bend or curve in a coastline, river, or other geographical feature Gulf – Recessed, coastal body of",
    "label": 0
  },
  {
    "text": "Inlet – Indentation of a shoreline Bay – Recessed, coastal body of water connected to an ocean or lake Bight – Shallowly concave bend or curve in a coastline, river, or other geographical feature Gulf – Recessed, coastal body of water connected to an ocean or lakePages displaying short descriptions of redirect targets Cove – Small sheltered bay or coastal inlet Creek (tidal) – Inlet or estuary that is affected by ebb and flow of ocean tidesPages displaying short descriptions of redirect targets Estuary – Partially enclosed coastal body of brackish water Fjord – Long, narrow inlet with steep sides or cliffs, created by glacial activity Kettle – Depression or hole in an outwash plain formed by retreating glaciers or draining floodwaters Kill – Creek, tidal inlet, river, strait, or arm of the sea Lagoon – Shallow body of water separated from a larger one by a narrow landform Barachois – Coastal lagoon partially or totally separated from the ocean by a sand or shingle bar Loch – Irish and Scottish Gaelic word for a lake or sea inlet Arm of the sea – Mere – Shallow lake, pond, or wetland Ocean – Body of salt water covering most of Earth Phytotelma – Small water-filled cavity in a terrestrial plant Salt marsh – Coastal ecosystem between land and open saltwater that is regularly flooded Sea – Large body of salt water Types of sea: Mediterranean sea – Mostly enclosed sea with limited exchange with outer oceansPages displaying short descriptions of redirect targets Sound – Long, wide body of water, connecting two larger bodies Sea components or extensions: Sea loch – Scottish Gaelic and Irish word for a sea inlet Sea lough – Anglicised version of Scottish Gaelic and Irish word for a sea inlet Strait – Waterway that connects two",
    "label": 0
  },
  {
    "text": "two larger bodies Sea components or extensions: Sea loch – Scottish Gaelic and Irish word for a sea inlet Sea lough – Anglicised version of Scottish Gaelic and Irish word for a sea inlet Strait – Waterway that connects two larger bodies of water Bodies of fresh water Bayou – Body of water in flat, low-lying areas Lake – Large inland body of relatively still water Lists of lakes Oxbow lake – U-shaped lake or pool left by an ancient river meander Subglacial lake – Lake under a glacier Tarn – Mountain lake or pool in a glacial cirque Pool – Deep and slow-moving stretch of a watercourse Pond – Relatively small body of standing water Billabong – Australian term for an oxbow lake or other waterhole Tide pool – Rocky pool on a seashore, separated from the sea at low tide, filled with seawater Vernal pool – Seasonal pools of water that provide habitat Puddle – Small accumulation of liquid, usually water, on a surface River – Natural flowing freshwater stream Lists of rivers Parts of a river: Rapids – River section with increased velocity and turbulence Source – Starting point of a riverPages displaying short descriptions of redirect targets Waterfall – Point in a river or stream where water flows over a vertical drop List of waterfalls Roadstead – Open anchorage affording some protection, but less than a harbor Spring – A point at which water emenges from an aquifer to the surface Boil - Stream – Body of surface water flowing down a channel Beck – Body of surface water flowing down a channel Brook – Body of surface water flowing down a channel Burn – Term of Scottish origin for a small river Creek – Body of surface water flowing down a channel Arroyo (watercourse) –",
    "label": 0
  },
  {
    "text": "of surface water flowing down a channel Brook – Body of surface water flowing down a channel Burn – Term of Scottish origin for a small river Creek – Body of surface water flowing down a channel Arroyo (watercourse) – Dry watercourse with flow after rain Wash – Dry watercourse with flow after rain Draw – Dry watercourse with flow after rain Run – Body of surface water flowing down a channel Wetland – Ecosystem that is flooded or saturated with water Freshwater marsh – Low-lying and seasonally waterlogged land Slough (wetland) – Forested wetland Mangrove swamp – Shrub growing in brackish water Artificial geographic features Artificial geographic feature – a thing that was made by humans that may be indicated on a map. It may be physical and exist in the real world (like a bridge or city), or it may be abstract and exist only on maps (such as the Equator, which has a defined location, but cannot be seen where it lies). Settlement – Community of any size, in which people live Rural area – Geographic area outside towns and cities Hamlet (place) – Small human settlement in a rural area – rural settlement which is too small to be considered a village. Historically, when a hamlet became large enough to justify building a church, it was then classified as a village. One example of a hamlet is a small cluster of houses surrounding a mill. Village – Human settlement smaller than a town – clustered human settlement or community, larger than a hamlet with the population ranging from a few hundred to a few thousand (sometimes tens of thousands). Town – Type of human settlement – human settlement larger than a village but smaller than a city. The size a settlement must be in order to",
    "label": 0
  },
  {
    "text": "ranging from a few hundred to a few thousand (sometimes tens of thousands). Town – Type of human settlement – human settlement larger than a village but smaller than a city. The size a settlement must be in order to be called a \"town\" varies considerably in different parts of the world, so that, for example, many American \"small towns\" seem to British people to be no more than villages, while many British \"small towns\" would qualify as cities in the United States. Urban hierarchy – Rank of cities based on their population – ranks the structure of towns within an area. 1st-order towns – bare minimum of essential services, such as bread and milk. 2nd-order towns 3rd-order towns 4th-order towns City – Large permanent human settlement – relatively large and permanent settlement. In many regions, a city is distinguished from a town by attainment of designation according to law, for instance being required to obtain articles of incorporation or a royal charter. Financial centre – Location with high concentration of commerce activity Primate city – Disproportionately largest city of a country or region – the leading city in its country or region, disproportionately larger than any others in the urban hierarchy. Metropolis – Large city or conurbation – very large city or urban area which is a significant economic, political and cultural center for a country or region, and an important hub for regional or international connections and communications. Metropolitan area – Dense urban core together with its satellite cities – region consisting of a densely populated urban core and its less-populated surrounding territories, sharing industry, infrastructure, and housing. Global city – City important to the world economy – city that is deemed to be an important node in the global economic system. Globalization is largely created, facilitated and",
    "label": 0
  },
  {
    "text": "and its less-populated surrounding territories, sharing industry, infrastructure, and housing. Global city – City important to the world economy – city that is deemed to be an important node in the global economic system. Globalization is largely created, facilitated and enacted in strategic geographic locales (including global cities) according to a hierarchy of importance to the operation of the global system of finance and trade. Megalopolis – Grouping of neighbouring metropolisesPages displaying short descriptions of redirect targets – chain of roughly adjacent metropolitan areas. An example is the huge metropolitan area along the eastern seaboard of the U.S. extending from Boston, Massachusetts through New York City; Philadelphia, Pennsylvania; Baltimore, Maryland and ending in Washington, D.C.. Eperopolis – Hypothetical planet-spanning cityPages displaying short descriptions of redirect targets – theoretical \"continent city\". The world does not have one yet. Will Europe become the first one? Ecumenopolis – Hypothetical planet-spanning city – theoretical \"world city\". Will the world ever become so urbanized as to be called this? Engineered construct – built feature of the landscape such as a highway, bridge, airport, railroad, building, dam, or reservoir. See also construction engineering and infrastructure. Artificial landforms Artificial dwelling hill – Raised ground to provide a refuge from flooding Artificial island – Island constructed by people Artificial reef – Human-made underwater structure that functions as a reef Airport – Facility with a runway for aircraft – place where airplanes can take off and land, including one or more runways and one or more passenger terminals. Aqueduct – Structure constructed to convey waterPages displaying short descriptions of redirect targets – artificial channel that is constructed to convey water from one location to another. Breakwater – Coastal defense structure – construction designed to break the force of the sea to provide calm water for boats or ships, or",
    "label": 0
  },
  {
    "text": "redirect targets – artificial channel that is constructed to convey water from one location to another. Breakwater – Coastal defense structure – construction designed to break the force of the sea to provide calm water for boats or ships, or to prevent erosion of a coastal feature. Bridge – Structure built to span physical obstacles – structure built to span a valley, road, body of water, or other physical obstacle such as a canyon, for the purpose of providing passage over the obstacle. Building – Structure, typically with a roof and walls, standing more or less permanently in one place – closed structure with walls and a roof. Canal – Artificial channel for water – artificial waterway, often connecting one body of water with another. Causeway – Route raised up on an embankment Dam – Barrier that stops or restricts the flow of surface or underground streams – structure placed across a flowing body of water to stop the flow, usually to use the water for irrigation or to generate electricity. Dike – Ridge or wall to hold back waterPages displaying short descriptions of redirect targets – barrier of stone or earth used to hold back water and prevent flooding. Levee – Ridge or wall to hold back water – artificial slope or wall to regulate water levels, usually earthen and often parallel to the course of a river or the coast. Farm – Area of land used to produce food and plants – place where agricultural activities take place, especially the growing of crops or the raising of livestock. Manmade harbor – Sheltered body of water where ships may shelter – harbor that has deliberately constructed breakwaters, sea walls, or jetties, or which was constructed by dredging. Industrial region – Geographical region with a high proportion of industrial use",
    "label": 0
  },
  {
    "text": "Manmade harbor – Sheltered body of water where ships may shelter – harbor that has deliberately constructed breakwaters, sea walls, or jetties, or which was constructed by dredging. Industrial region – Geographical region with a high proportion of industrial use Marina – Dock with moorings and facilities for yachts and small boats Orchard – Intentionally planted trees or shrubs that are maintained for food production Parking lot – Cleared area for parking vehicles Pier – Raised structure in a body of water Pipeline – Pumping fluids or gas through pipesPages displaying short descriptions of redirect targets Port – Maritime facility where ships may dock to load and discharge passengers and cargo Railway – Structure comprising rails on a foundation intended to carry trains Ranch – Large area of land for raising livestock Reservoir – Bulk storage space for water Road – Land route Highway – Public road or other public way on land Race track – Facility built for racing of animals, vehicles, or athletes Street – Public thoroughfare in a built environment Subsidence crater – Hole or depression left on the surface over the site of an underground explosion Ski resort – Resort developed for skiing, snowboarding, and other winter sports Train station – Railway facility for loading or unloading trains Tree farm – Type of forest planted for high volume production of woodPages displaying short descriptions of redirect targets Tunnel – Underground passage made for traffic Viaduct – Multiple-span bridge crossing an extended lower area Wharf – Shoreside structure where ships dock Abstract geographic feature – does not exist physically in the real world, yet has a location by definition and may be displayed on maps. Geographical zone – Major regions of Earth's surface demarcated by latitude Hardiness zone – Region defined by minimum temperature relevant to the",
    "label": 0
  },
  {
    "text": "exist physically in the real world, yet has a location by definition and may be displayed on maps. Geographical zone – Major regions of Earth's surface demarcated by latitude Hardiness zone – Region defined by minimum temperature relevant to the plant survival Time zone – Area that observes a uniform standard time Political division – A territorial entity for administration purposes Nation – Community based on common ethnic, cultural or political identity Administrative division – Territorial entity for administration purposes – a designated territory created within a country for administrative or identification purposes. Examples of the types of administrative divisions: Bailiwick – Area of jurisdiction of a bailiff Canton – Type of administrative division of a country Commune – An urban administrative division having corporate status County – Geographical and administrative region in some countries Department – Administrative or political division in some countriesPages displaying short descriptions of redirect targets District – Administrative division in some countries, managed by a local government Duchy – Territory ruled by, or representing the title of, a duke or duchess Emirate – Territory ruled by an emir Federal state – Type of political entity Parish – Ecclesiastical subdivision of a diocese Prefecture – Administrative jurisdiction or subdivision in various countries Province – Administrative division within a country or state Region – Two or three-dimensionally defined space, mainly in terrestrial and astrophysics sciences Rural district – Former type of local government area in England, Wales, and Ireland Settlement – Community of any size, in which people live Municipality – Local government area City – Large permanent human settlement Borough – Administrative division in some English-speaking countries Township – Type of settlement or urban area Village – Human settlement smaller than a town Shire – Traditional British term for county Special Economic Zone – Region with",
    "label": 0
  },
  {
    "text": "permanent human settlement Borough – Administrative division in some English-speaking countries Township – Type of settlement or urban area Village – Human settlement smaller than a town Shire – Traditional British term for county Special Economic Zone – Region with specialized business and trade lawsPages displaying short descriptions of redirect targets State – Territorial and constitutional community forming part of a federal unionPages displaying short descriptions of redirect targets Subdistrict Subprefecture – Administrative division of a country that is below prefecture Voivodeship – Administrative division in several countries of central and eastern Europe Wilayat – Administrative division approximating a state or province Cartographical feature – theoretical construct used specifically on maps that doesn't have any physical form apart from its location. Latitude line – Geographic coordinate specifying north-south position Equator – Imaginary line halfway between Earth's North and South poles Longitude line – East-West geographic coordinate Prime Meridian – A line of longitude, at which longitude is defined to be 0° Geographical pole – Points on a rotating astronomical body where the axis of rotation intersects the surface North Pole – Northernmost point on Earth South Pole – Southernmost point on Earth Geographic features that include the natural and artificial Waterway – Any navigable body of water List of waterways Geography awards Some awards and competitions in the field of geography: Geography Cup – 2006 online geography competition Founder's Medal – List of geography award winnersPages displaying short descriptions of redirect targets Patron's Medal – Award presented by the Royal Geographical SocietyPages displaying short descriptions of redirect targets Hubbard Medal – Medal awarded by the National Geographic Society National Geographic World Championship – Biennial, two-day-long international geography competition Victoria Medal – British geography award Geographical organizations See: List of geographical societies European Geography Association EUROGEO-European Association of Geographers Gamma Theta",
    "label": 0
  },
  {
    "text": "– Medal awarded by the National Geographic Society National Geographic World Championship – Biennial, two-day-long international geography competition Victoria Medal – British geography award Geographical organizations See: List of geographical societies European Geography Association EUROGEO-European Association of Geographers Gamma Theta Upsilon International Geographical Union Geographical publications Geographical magazines Al Arab Arizona Highways Asian Geographic Atlas (magazine) Australian Geographic Canadian Geographic Chinese National Geography Le Congo illustré GEO (magazine) Géographica Geographical Icelandic Geographic Le Mouvement Géographique National Geographic National Geographic Adventure National Geographic Kids National Geographic Traveler New Zealand Geographic Podróże Revista Geográfica Española Rhythms Monthly Vokrug sveta Walkabout The Wide World Magazine Persons influential in geography A geographer is a scientist who studies Earth's physical environment and human habitat. Geographers are historically known for making maps, the subdiscipline of geography known as cartography. They study the physical details of the environment and also its effect on human and wildlife ecologies, weather and climate patterns, economics, and culture. Geographers focus on the spatial relationships between these elements. Influential physical geographers Eratosthenes (276 – 194 BC) – who made the first known reliable estimation of the Earth's size. He is considered the father of geodesy. Ptolemy (c. 90 – c. 168) – who compiled Greek and Roman knowledge to produce the book Geographia. Abū Rayhān Bīrūnī (973 – 1048 AD) – considered the father of geodesy. Ibn Sina (Avicenna, 980–1037) – whose observations in Kitab Al-Shifa contributed to later formulations of the law of superposition and concept of uniformitarianism. Muhammad al-Idrisi (Dreses, 1100 – c.1165) – who drew the Tabula Rogeriana, the most accurate world map in pre-modern times. Piri Reis (1465 – c.1554) – whose Piri Reis map is the oldest surviving world map to include the Americas and possibly Antarctica Gerardus Mercator (1512–1594) – an innovative cartographer and originator of",
    "label": 0
  },
  {
    "text": "most accurate world map in pre-modern times. Piri Reis (1465 – c.1554) – whose Piri Reis map is the oldest surviving world map to include the Americas and possibly Antarctica Gerardus Mercator (1512–1594) – an innovative cartographer and originator of the Mercator projection. Bernhardus Varenius (1622–1650) – Wrote his important work \"General Geography\" (1650) – first overview of the geography, the foundation of modern geography. Mikhail Lomonosov (1711–1765) – father of Russian geography and founded the study of glaciology. Alexander von Humboldt (1769–1859) – considered the father of modern geography. Published Kosmos and founded the study of biogeography. Arnold Henry Guyot (1807–1884) – who noted the structure of glaciers and advanced the understanding of glacial motion, especially in fast ice flow. Louis Agassiz (1807–1873) – the author of a glacial theory which disputed the notion of a steady-cooling Earth. Alfred Russel Wallace (1823–1913) – founder of modern biogeography and the Wallace line. Vasily Dokuchaev (1846–1903) – patriarch of Russian geography and founder of pedology. Wladimir Peter Köppen (1846–1940) – developer of most important climate classification and founder of Paleoclimatology. William Morris Davis (1850–1934) – father of American geography, founder of Geomorphology and developer of the geographical cycle theory. Walther Penck (1888–1923) – proponent of the cycle of erosion and the simultaneous occurrence of uplift and denudation. Sir Ernest Shackleton (1874–1922) – Antarctic explorer during the Heroic Age of Antarctic Exploration. Robert E. Horton (1875–1945) – founder of modern hydrology and concepts such as infiltration capacity and overland flow. J Harlen Bretz (1882–1981) – pioneer of research into the shaping of landscapes by catastrophic floods, most notably the Bretz (Missoula) floods. Willi Dansgaard (1922–2011) – palaeoclimatologist and quaternary scientist, instrumental in the use of oxygen-isotope dating and co-identifier of Dansgaard–Oeschger events. Hans Oeschger (1927–1998) – palaeoclimatologist and pioneer in ice core",
    "label": 0
  },
  {
    "text": "landscapes by catastrophic floods, most notably the Bretz (Missoula) floods. Willi Dansgaard (1922–2011) – palaeoclimatologist and quaternary scientist, instrumental in the use of oxygen-isotope dating and co-identifier of Dansgaard–Oeschger events. Hans Oeschger (1927–1998) – palaeoclimatologist and pioneer in ice core research, co-identifier of Dansgaard-Orschger events. Richard Chorley (1927–2002) – a key contributor to the quantitative revolution and the use of systems theory in geography. Sir Nicholas Shackleton (1937–2006) – who demonstrated that oscillations in climate over the past few million years could be correlated with variations in the orbital and positional relationship between the Earth and the Sun. Stefan Rahmstorf (born 1960) – professor of abrupt climate changes and author on theories of thermohaline dynamics. Influential human geographers Carl Ritter (1779–1859) – considered to be one of the founding fathers of modern geography and first chair in geography at the Humboldt University of Berlin, also noted for his use of organic analogy in his works. Friedrich Ratzel (1844–1904) – environmental determinist, invented the term Lebensraum Paul Vidal de la Blache (1845–1918) – founder of the French School of geopolitics and possibilism. Sir Halford John Mackinder (1861–1947) – author of The Geographical Pivot of History, co-founder of the London School of Economics, along with the Geographical Association. Carl O. Sauer (1889–1975) – critic of environmental determinism and proponent of cultural ecology. Walter Christaller (1893–1969) – economic geographer and developer of the central place theory. Richard Hartshorne (1899–1992) – scholar of the history and philosophy of geography. Torsten Hägerstrand (1916–2004) – critic of the quantitative revolution and regional science, noted figure in critical geography. Milton Santos (1926–2001) winner of the Vautrin Lud prize in 1994, one of the most important geographers in South America. Waldo R. Tobler (1930–2018) – developer of the First law of geography. Yi-Fu Tuan (1930–2022) A Chinese-American geographer.",
    "label": 0
  },
  {
    "text": "critical geography. Milton Santos (1926–2001) winner of the Vautrin Lud prize in 1994, one of the most important geographers in South America. Waldo R. Tobler (1930–2018) – developer of the First law of geography. Yi-Fu Tuan (1930–2022) A Chinese-American geographer. David Harvey (born 1935) – world's most cited academic geographer and winner of the Lauréat Prix International de Géographie Vautrin Lud, also noted for his work in critical geography and critique of global capitalism. Evelyn Stokes (1936–2005). Professor of geography at the University of Waikato in New Zealand. Known for recognizing inequality with marginalized groups, including women and Māori using geography. Allen J. Scott (born 1938) – winner of Vautrin Lud Prize in 2003 and the Anders Retzius Gold medal 2009; author of numerous books and papers on economic and urban geography, known for his work on regional development, new industrial spaces, agglomeration theory, global city-regions and the cultural economy. Edward Soja (1941–2015) – noted for his work on regional development, planning, and governance, along with coining the terms synekism and postmetropolis. Doreen Massey (1944–2016) – key scholar in the space and places of globalization and its pluralities, winner of the Vautrin Lud Prize. Michael Watts, Class of 1963 Professor of Geography and Development Studies, University of California, Berkeley Nigel Thrift (born 1949) – developer of non-representational theory. Derek Gregory (born 1951) – famous for writing on the Israeli, U.S. and UK actions in the Middle East after 9/11, influenced by Edward Said and has contributed work on imagined geographies. Cindi Katz (born 1954) – who writes on social reproduction and the production of space. Writing on children's geographies, place and nature, everyday life and security. Gillian Rose (born 1962) – most famous for her critique: Feminism & Geography: The Limits of Geographical Knowledge (1993) – which was one of",
    "label": 0
  },
  {
    "text": "and the production of space. Writing on children's geographies, place and nature, everyday life and security. Gillian Rose (born 1962) – most famous for her critique: Feminism & Geography: The Limits of Geographical Knowledge (1993) – which was one of the first moves towards development of feminist geography. Geography educational frameworks Educational frameworks upon which primary and secondary school curricula for geography are based include: Five themes of geography – Educational tool Location – Point or an area on Earth's surface or elsewherePages displaying short descriptions of redirect targets – a position or point that something occupies on the Earth's surface. Place – Point or an area on Earth's surface or elsewherePages displaying short descriptions of redirect targets Human-environment interaction – Study of interactions between societies and their natural environments movement – Region – Two or three-dimensionally defined space, mainly in terrestrial and astrophysics sciences The six \"essential elements\" identified by the Geography Education Standards Project, under which the National Geography Standards they developed are organized: The World in spatial terms Places and regions Physical systems Human systems Environment and society The uses of geography The three content areas of geography from the 2010 National Assessment of Educational Progress (U.S.): Space and place Environment and society Spatial dynamics and connections See also References External links Pidwirny, Michael. (2014). Glossary of Terms for Physical Geography. Planet Earth Publishing, Kelowna, Canada. ISBN 9780987702906. Available on Google Play. Pidwirny, Michael. (2014). Understanding Physical Geography. Planet Earth Publishing, Kelowna, Canada. ISBN 9780987702944. Available on Google Play.",
    "label": 0
  },
  {
    "text": "A bioregion is a geographical area defined not by administrative boundaries, but by distinct characteristics such as plant and animal species, ecological systems, soils and landforms, human settlements, and topographic features such as drainage basins (also referred to as \"watersheds\"). A bioregion can be on land or at sea. The idea of bioregions was adopted and popularized in the mid-1970s by a school of philosophy called bioregionalism, which includes the concept that human culture can influence bioregional definitions due to its effect on non-cultural factors. Bioregions are part of a nested series of ecological scales, generally starting with local watersheds, growing into larger river systems, then Level III or IV ecoregions (or regional ecosystems), bioregions, then biogeographical realm, followed by the continental-scale and ultimately the biosphere. Within the life sciences, there are numerous methods used to define the physical limits of a bioregion based on the spatial extent of mapped ecological phenomena—from species distributions and hydrological systems (i.e. Watersheds) to topographic features (e.g. landforms) and climate zones (e.g. Köppen classification). Bioregions also provide an effective framework in the field of Environmental history, which seeks to use \"river systems, ecozones, or mountain ranges as the basis for understanding the place of human history within a clearly delineated environmental context\". A bioregion can also have a distinct cultural identity defined, for example, by Indigenous Peoples whose historical, mythological and biocultural connections to their lands and waters shape an understanding of place and territorial extent. Within the context of bioregionalism, bioregions can be socially constructed by modern-day communities for the purposes of better understanding a place \"with the aim to live in that place sustainably and respectfully.\" Bioregions have practical applications in the study of biology, biocultural anthropology, biogeography, biodiversity, bioeconomics, bioregionalism, bioregional mapping, community health, ecology, environmental history, environmental science, foodsheds, geography,",
    "label": 0
  },
  {
    "text": "understanding a place \"with the aim to live in that place sustainably and respectfully.\" Bioregions have practical applications in the study of biology, biocultural anthropology, biogeography, biodiversity, bioeconomics, bioregionalism, bioregional mapping, community health, ecology, environmental history, environmental science, foodsheds, geography, natural resource management, urban Ecology, and urban planning. References to the term \"bioregion\" in scholarly literature have grown exponentially since the introduction of the term—from a single research paper in 1971 to approximately 65,000 journal articles and books published to date. Governments and multilateral institutions have utilized bioregions in mapping Ecosystem Services and tracking progress towards conservation objectives, such as ecosystem representation. Background The first confirmed use of the term \"bioregion\" in academic literature was by E. Jarowski in 1971, a marine biologist studying the blue crab populations of Louisiana. The author used the term sensu stricto to refer to a \"biological region\"—the area within which a crab can be provided with all the resources needed throughout its entire life cycle. The term was quickly adopted by other biologists, but eventually took on a broader set of definitions to encompass a range of macro-ecological phenomena. The term bioregion as it relates to bioregionalism is credited to Allen Van Newkirk, a Canadian poet and biogeographer. In this field, the idea of \"bioregion\" likely goes back much earlier than published material suggests, being floated in early published small press zines by Newkirk, as well as in conversational dialogue. This can be exemplified by the fact that Newkirk had met Peter Berg (another early scholar on bioregionalism) in San Francisco in 1969 and again in Nova Scotia in 1971 where he shared the idea with Berg. He would go on to found the Institute for Bioregional Research and issue a series of short papers using the term bioregion as early as 1970. Peter",
    "label": 0
  },
  {
    "text": "again in Nova Scotia in 1971 where he shared the idea with Berg. He would go on to found the Institute for Bioregional Research and issue a series of short papers using the term bioregion as early as 1970. Peter Berg, who would go on to found the Planet Drum foundation, and become a leading proponent of \"bioregions\" learned of the term in 1971 while Judy Goldhaft and Peter Berg were staying with Allen Van Newkirk, before Berg attended the first United Nations Conference on the Human Environment in Stockholm during June 1972. The Planet Drum Foundation published their first Bioregional Bundle in that year, that also included a definition of a bioregion. Helping refine this definition, Author Kirkpatrick Sale wrote in 1974 that \"a bioregion is a part of the earth's surface whose rough boundaries are determined by natural rather than human dictates, distinguishable from other areas by attributes of flora, fauna, water, climate, soils and landforms, and human settlements and cultures those attributes give rise to. Several other marine biology papers picked up the term, and in 1974 the International Union for Conservation of Nature (IUCN) published its first global-scale biogeographical map entitled \"Biotic Provinces of the World\". However, in their 1977 article \"Reinhabiting California\", director of the IUCN and founder of the Man and Biosphere project Raymond Dasmann and Peter Berg pushed back against these global bodies that were attempting to use the term bioregion in a strictly ecological sense, which separated humans from the ecosystems they lived in, specifically naming that Biotic Provinces of the World Map, was not a map of bioregions. \"Reinhabitation involves developing a bioregional identity, something most North Americans have lost or have never possessed. We define bioregion in a sense different from the biotic provinces of Raymond Dasmann (1973) or the",
    "label": 0
  },
  {
    "text": "Map, was not a map of bioregions. \"Reinhabitation involves developing a bioregional identity, something most North Americans have lost or have never possessed. We define bioregion in a sense different from the biotic provinces of Raymond Dasmann (1973) or the biogeographical province of Miklos Udvardy. The term refers both to geographical terrain and a terrain of consciousness—to a place and the ideas that have developed about how to live in that place. Within a bioregion, the conditions that influence life are similar, and these, in turn, have influenced human occupancy.\"This article defined bioregions as distinct from biogeographical and biotic provinces that ecologists and geographers had been developing by adding a human and cultural lens to the strictly ecological idea. In 1975, A. Van Newkirk published a paper entitled \"Bioregions: Towards Bioregional Strategy for Human Cultures\" in which he advocates for the incorporation of human activity (\"occupying populations of the culture-bearing animal\") within bioregional definitions. Etymology Bioregion as a term comes from the Greek bios (life), and the French region (region), itself from the Latin regia (territory) and earlier regere (to rule or govern). Etymologically, bioregion means \"life territory\" or \"place-of-life\". Bioregionalism Bioregions became a foundational concept within the philosophical system called Bioregionalism. A key difference between ecoregions and biogeography and the term bioregion is that while ecoregions are based on general biophysical and ecosystem data, human settlement and cultural patterns play a key role in how a bioregion is defined. A bioregion is defined along the watershed, and hydrological boundaries, and uses a combination of bioregional layers, beginning with the oldest \"hard\" lines: geology, topography, tectonics, wind, fracture zones and continental divides, working its way through the \"soft\" lines: living systems such as soil, ecosystems, climate, marine life, and flora and fauna, and lastly the \"human\" lines: human geography, energy,",
    "label": 0
  },
  {
    "text": "oldest \"hard\" lines: geology, topography, tectonics, wind, fracture zones and continental divides, working its way through the \"soft\" lines: living systems such as soil, ecosystems, climate, marine life, and flora and fauna, and lastly the \"human\" lines: human geography, energy, transportation, agriculture, food, music, language, history, Indigenous cultures, and ways of living within the context set into a place, and its limits to determine the final edges and boundaries. This is summed up well by David McCloskey, author of the Cascadia Bioregion map: \"A bioregion may be analyzed on physical, biological, and cultural levels. First, we map the landforms, geology, climate, and hydrology and how these environmental factors work together to create a common template for life in that particular place. Second, we map flora and fauna, especially the characteristic vegetative communities, and link them to their habitats. Third, we look at native peoples, western settlement, and current land-use patterns and problems, in interaction with the first two levels.\" A bioregion is defined as the largest physical boundaries where connections based on that place will make sense. The basic units of a bioregion are watersheds and hydrological basins, and a bioregion will always maintain the natural continuity and full extent of a watershed. While a bioregion may stretch across many watersheds, it will never divide or separate a water basin. As conceived by Van Newkirk, bioregionalism is presented as a technical process of identifying \"biogeographically interpreted culture areas called bioregions\". Within these territories, resident human populations would \"restore plant and animal diversity,\" \"aid in the conservation and restoration of wild eco-systems,\" and \"discover regional models for new and relatively non-arbitrary scales of human activity in relation to the biological realities of the natural landscape\". His first published article in a mainstream magazine was in 1975 in his article Bioregions: Towards",
    "label": 0
  },
  {
    "text": "eco-systems,\" and \"discover regional models for new and relatively non-arbitrary scales of human activity in relation to the biological realities of the natural landscape\". His first published article in a mainstream magazine was in 1975 in his article Bioregions: Towards Bioregional Strategy in Environmental Conservation. In the article, Allen Van Newkirk tentatively defines a bioregion as: \"biologically significant areas of the Earth's surface which can be mapped and discussed as distinct existing patterns of plant, animal, and habitat distributions as related to range patterns and… deformations, attributed to one or more successive occupying populations of the culture-bearing animal (aka humans)....Towards this end a group of projects relating to bioregions or themes of applied human biogeography is envisaged. For Newkirk, the term \"bioregion\" was a way to combine human culture with earlier work on biotic provinces. So, he called this new field \"regional human biogeography\" and was the first to use terms such as \"bioregional strategies\" and \"bioregional framework\" for adapting human cultures into a place. This idea was carried forward and developed by ecologist Raymond Dasmann and Peter Berg in an article they co-authored called Reinhabiting California in 1977, which rebuked earlier ecologist efforts to only use biotic provinces, and biogeography, which excluded humans from the definition of bioregion. Peter Berg and Judy Goldhaft founded the Planet Drum Foundation in 1973, located in San Francisco which celebrated its 50th anniversary in 2023. Planet Drum, from their website, defines a bioregion as \"a geographical area with coherent and interconnected plant and animal communities, and other natural characteristics (often defined by a watershed) plus the cultural values that humans have developed for living in harmony with these natural systems. Because it is a cultural idea, the description of a specific bioregion uses information from both the natural sciences and other sources. Each",
    "label": 0
  },
  {
    "text": "watershed) plus the cultural values that humans have developed for living in harmony with these natural systems. Because it is a cultural idea, the description of a specific bioregion uses information from both the natural sciences and other sources. Each bioregion is a whole 'life-place' with unique requirements for human inhabitation so that it will not be disrupted and injured. People are counted as an integral aspect of a place's life.\" At a 1991 Symposium on Biodiversity of Northwestern California, Peter Berg stated \"A bioregion can be determined initially by the use of climatology, physiography, animal and plant geography, natural history and other descriptive natural sciences. The final boundaries of a bioregion are best described by the people who have lived within it, through human recognition of the realities of living-in-place. All life on the planet is interconnected in a few obvious ways, and in many more that remain barely explored. But there is a distinct resonance among living things and the factors which influence them that occurs specifically within each separate place on the planet. Discovering and describing that resonance is a way to describe a bioregion.\" Thomas Berry, an educator, environmentalist, activist, and priest, who authored the United Nations World Charter for Nature, and historian of the Hudson River Valley, was also deeply rooted in the bioregional movement, and helping bioregionalism spread to the east coast of North America. In 1984 he wrote \"A bioregion is simply an indenfidable geographic area whose life systems are self-contained, self- sustaining and self renewing. A bioregion, you might say, is a basic unit within the natural system of earth. Another way to define a bioregion is in terms of watersheds. Bioregions must develop human populations that accord with their natural context. The human is not exempt from being part of the",
    "label": 0
  },
  {
    "text": "basic unit within the natural system of earth. Another way to define a bioregion is in terms of watersheds. Bioregions must develop human populations that accord with their natural context. The human is not exempt from being part of the basic inventory in a bioregion.\" Kirkpatrick Sale another early pioneer of the idea of bioregions, wrote in his book Dwellers in the Land, \"A bioregion is a part of the earth's surface whose rough boundaries are determined by natural and human dictates, distinguishable from other areas by attributes of flora, fauna, water, climate, soils and land-forms, and human settlements and cultures those attributes give rise to. The borders between such areas are usually not rigid – nature works with more flexibility and fluidity than that – but the general contours of the regions themselves are not hard to identify, and indeed will probably be felt, understood, sensed or in some way known to many inhabitants, and particularly those still rooted in the land.\" One of the other early proponents of bioregionalism, and who helped define what a bioregion is, was American biologist and environmental scientist Raymond F. Dasmann. Dasmann studied at UC Berkeley under the legendary wildlife biologist Aldo Leopold, and earned his Ph.D. in zoology in 1954. He began his academic career at Humboldt State University, where he was a professor of natural resources from 1954 until 1965. During the 1960s, he worked at the Conservation Foundation in Washington, D.C., as Director of International Programs and was also a consultant on the development of the 1972 Stockholm Conference on the Human Environment. In the 1970s he worked with UNESCO where he initiated the Man and the Biosphere Programme(MAB), an international research and conservation program. During the same period he was Senior Ecologist for the International Union for Conservation of",
    "label": 0
  },
  {
    "text": "the Human Environment. In the 1970s he worked with UNESCO where he initiated the Man and the Biosphere Programme(MAB), an international research and conservation program. During the same period he was Senior Ecologist for the International Union for Conservation of Nature in Switzerland, initiating global conservation programs which earned him the highest honors awarded by The Wildlife Society, and the Smithsonian Institution. Working with Peter Berg, and also contemporary with Allen Van Newkirk, Dasmann was one of the pioneers in developing the definition for the term \"Bioregion\", as well as conservation concepts of \"Eco-development\" and \"biological diversity,\" and identified the crucial importance of recognizing indigenous peoples and their cultures in efforts to conserve natural landscapes. Because it is a cultural idea, the description of a specific bioregion is drawn using information from not only the natural sciences but also many other sources. It is a geographic terrain and a terrain of consciousness. Anthropological studies, historical accounts, social developments, customs, traditions, and arts can all play a part. Bioregionalism utilizes them to accomplish three main goals: restore and maintain local natural systems; practice sustainable ways to satisfy basic human needs such as food, water, shelter, and materials; and support the work of reinhabitation. The latter is accomplished through proactive projects, employment and education, as well as by engaging in protests against the destruction of natural elements in a life-place.Bioregional goals play out in a spectrum of different ways for different places. In North America, for example, restoring native prairie grasses is a basic ecosystem-rebuilding activity for reinhabitants of the Kansas Area Watershed Bioregion in the Midwest, whereas bringing back salmon runs has a high priority for Shasta Bioregion in northern California. Using geothermal and wind as a renewable energy source fits Cascadia Bioregion in the rainy Pacific Northwest. Less cloudy skies",
    "label": 0
  },
  {
    "text": "Watershed Bioregion in the Midwest, whereas bringing back salmon runs has a high priority for Shasta Bioregion in northern California. Using geothermal and wind as a renewable energy source fits Cascadia Bioregion in the rainy Pacific Northwest. Less cloudy skies in the Southwest's sparsely vegetated Sonoran Desert Bioregion make direct solar energy a more plentiful alternative there. Education about local natural characteristics and conditions varies diversely from place to place, along with bioregionally significant social and political issues. Bioregional mapping An important part of bioregionalism is bioregional mapping. Instructions for how to map a bioregion were first laid out in a book Mapping for Local Empowerment, written by University of British Columbia by Douglas Aberley in 1993, followed by the mapping handbook Giving the Land a Voice in 1994. This grew from the Tsleil-Waututh First Nation, Nisga'a, Tsilhqotʼin, Wetʼsuwetʼen first nations who used Bioregional Mapping to create some of the first bioregional atlases as part of court cases to defend their sovereignty in the 1980s and 1990s, one such example being the Tsilhqotʼin Nation v British Columbia. In these resources, there are two types of maps: Bioregional Maps and maps of Bioregions, which both include physical, ecological and human lines. A bioregional map can be any scale, and is a community and participatory process to map what people care about. Bioregional maps and atlases can be considered tools and jumping off points for helping guide regenerative activities of a community. Mapping a bioregion is considered a specific type of bioregional map, in which many layers are brought together to map a \"whole life place\", and is considered an 'optimal zone of interconnection for a species to thrive', i.e. for humans, or a specific species such as salmon, and uses many different layers to see what boundaries \"emerge\" and make sense",
    "label": 0
  },
  {
    "text": "a \"whole life place\", and is considered an 'optimal zone of interconnection for a species to thrive', i.e. for humans, or a specific species such as salmon, and uses many different layers to see what boundaries \"emerge\" and make sense as frameworks of stewardship. A good example of this is the Salmon Nation bioregion, which is the Pacific Northwest and northwest rim of the Pacific Ocean as defined through the historic and current range of the salmon, as well as the people and ecosystem which have evolved over millennia to depend on them. This style of bioregional mapping can also be found in the works of Henry David Thoreau who when hired to make maps by the United States government, chose instead to create maps \"that charts and delineates the local ecology and its natural history as well as its intersection with a human community\". This type of mapping is consistent with, and aligns with an indigenous and western worldview. This is put well by Douglas Aberley and chief Michael George noting that: \"Once the bioregional map atlas is completed it becomes the common foundation of knowledge from which planning scenarios can be prepared, and decisions ultimately made. Complex information that is otherwise difficult to present is clearly depicted. The community learns about itself in the process of making decisions about its future.\" Sheila Harrington, in the introduction to Islands of the Salish Sea: A Community Atlas goes one step further, noting that: \"The atlas should be used as a jumping off place for decision making about the future. From the holistic image of place that the maps collectively communicate, what actions could be adopted to achieve sustainable prosperity? What priorities emerge from a survey of damaged lands and unsolved social ills? What underutilized potentials can be put to work",
    "label": 0
  },
  {
    "text": "the holistic image of place that the maps collectively communicate, what actions could be adopted to achieve sustainable prosperity? What priorities emerge from a survey of damaged lands and unsolved social ills? What underutilized potentials can be put to work to help achieve sustainability? The atlas can become a focus for discussions setting a proactive plan for positive change.\" Defining a specific bioregion Mapping a bioregion consists of: Choosing and creating a base map and a scale, generally using a hydrological or watershed map. Because bioregional maps are whole systems maps, they can travel across watersheds, but will not divide them. Survey and decide what needs to be mapped in this area. What are the physical and ecological communities in that place that need to be included? This can include watershed information, animal communities, vegetation types, and physiographic (landform) regions. In addition to the above, \"it is necessary to make human occupation of any land area a part of the bioregion definition equation. [By so doing] this approach captures the essence of the bioregional ideal: to irrevocably human activity into processes of sustainable land, plant, and atmospheric interaction\" Include time. From points 2 and 3, how has this information changed from glacial times (or before) to the present? in this way, we map not only the present interactions between humans, ecosystems, physical landforms and water cycles, but also can map such changes over a long period of time. Examine the extent of what it is that you would like to map. Is the area big enough? Small enough? Does the area consist of a whole system? Is this space large enough to support the inhabitant populations and nutrient cycling? Outline the different extents and boundaries you are including, and layer them together to see what \"emerges\". Your final map will",
    "label": 0
  },
  {
    "text": "area consist of a whole system? Is this space large enough to support the inhabitant populations and nutrient cycling? Outline the different extents and boundaries you are including, and layer them together to see what \"emerges\". Your final map will generally help demarcate a bioregion, or life place. Methodology and classification While references to bioregions (or biogeographical regions) have become increasingly common in scholarly literature related to life sciences, \"there is little agreement on how to best classify and name such regions, with several conceptually related terms being used, often interchangeably.\" Bioregions can take many forms and operate at many scales – from very small ecosystems or 'biotopes' to ecoregions (which can be nested at different scales) to continent-scale distributions of plants and animals, like biomes or realms. All of them, technically, can be considered types of bioregions sensu lato and are often referred to as such in academic literature. In 2014, J. Marrone documented a history of 13 biogeographical concepts in \"On Biotas and their names\". A recent review of scholarly literature finds 20 unique biotic methods to define bioregions—based on populations of specific plant and animals species or species assemblages. These range from global and continental scales to sub-continental and regional scales to sub-regional and local scales. In addition, 5 abiotic methods have been utilized to inform the delineation of biogeographical extents. Ecoregions Ecoregions are one of the primary building blocks of bioregions, which are made up of \"clusters of biotically related ecoregions\". An Ecoregion (ecological region) is an ecologically and geographically defined area that is smaller than a bioregion, which in turn is smaller than a biogeographic realm. Ecoregions cover relatively large areas of land or water, and contain characteristic, geographically distinct assemblages of natural communities and species. They can include geology physiography, vegetation, climate, hydrology, terrestrial",
    "label": 0
  },
  {
    "text": "a bioregion, which in turn is smaller than a biogeographic realm. Ecoregions cover relatively large areas of land or water, and contain characteristic, geographically distinct assemblages of natural communities and species. They can include geology physiography, vegetation, climate, hydrology, terrestrial and aquatic fanua, and soils, and may or may not include the impacts of human activity (e.g. land use patterns, vegetation changes etc.). The biodiversity of flora, fauna and ecosystems that characterize an ecoregion tends to be distinct from that of other ecoregions. The phrase \"ecological region\" was widely used throughout the 20th century by biologists and zoologists to define specific geographic areas in research. In the early 1970s the term 'ecoregion' was introduced (short for ecological region), and R.G. Bailey published the first comprehensive map of U.S. ecoregions in 1976. The term was used widely in scholarly literature in the 1980s and 1990s, and in 2001 scientists at the U.S. conservation organization World Wildlife Fund (WWF) codified and published the first global-scale map of Terrestrial Ecoregions of the World (TEOW), led by D. Olsen, E. Dinerstein, E. Wikramanayake, and N. Burgess. While the two approaches are related, the Bailey ecoregions (nested in four levels) give more importance to ecological criteria and climate zones, while the WWF ecoregions give more importance to biogeography, that is, the distribution of distinct species assemblages. Ecoregions can change gradually, and have soft transition areas known as ecotones. Because of this, there can be some variation in how ecoregions are defined. The US Environmental Protection Agency has four ranking systems they use, which lists there being 12 type one ecoregions, and 187 type III ecoregions in North America while another study on the Biodiversity of the Klamath-Siskiyou Ecoregion, researchers found that North America contains 116 ecoregions nested within 10 major habitat types. The TEOW framework",
    "label": 0
  },
  {
    "text": "being 12 type one ecoregions, and 187 type III ecoregions in North America while another study on the Biodiversity of the Klamath-Siskiyou Ecoregion, researchers found that North America contains 116 ecoregions nested within 10 major habitat types. The TEOW framework originally delineated 867 terrestrial ecoregions nested into 14 major biomes, contained with the world's 8 major biogeographical realms. Subsequent regional papers by the co-authors covering Africa, Indo-Pacific, and Latin America differentiate between ecoregions and bioregions, referring to the latter as \"geographic clusters of ecoregions that may span several habitat types, but have strong biogeographic affinities, particularly at taxonomic levels higher than the species level (genus, family)\". In 2007, a comparable set of Marine Ecoregions of the World (MEOW) was published, led by M. Spalding, and in 2008 a set of Freshwater Ecoregions of the World (FEOW) was published, led by R. Abell. In 2017, an updated version of the terrestrial ecoregions dataset was released in the paper \"An Ecoregion-Based Approach to Protecting Half the Terrestrial Realm\" led by E. Dinerstein with 48 co-authors. Using recent advances in satellite imagery the ecoregion perimeters were refined and the total number reduced to 846 (and later 844), which can be explored on a web application developed by Resolve and Google Earth Engine. For conservation practitioners and organizations monitoring progress towards the goals of the United Nations Convention on Biological Diversity (CBD), in particular the goal of ecosystem representation in Protected Area networks, the most widely used bioregional delineations include the Resolve Ecoregions and the IUCN Global Ecosystem Typology. In bioregionalism, an ecoregion can also use geography, ecology, and culture as part of its definition. See also All pages with titles containing Bioregion Ecological classification Ecology terminology Interim Biogeographic Regionalisation for Australia Cascadia (bioregion)–a sample bioregion References",
    "label": 0
  },
  {
    "text": "Economic restructuring is used to indicate changes in the constituent parts of an economy in a very general sense. In the western world, it is usually used to refer to the phenomenon of urban areas shifting from a manufacturing to a service sector economic base. It has profound implications for productive capacities and competitiveness of cities and regions. This transformation has affected demographics including income distribution, employment, and social hierarchy; institutional arrangements including the growth of the corporate complex, specialized producer services, capital mobility, informal economy, nonstandard work, and public outlays; as well as geographic spacing including the rise of world cities, spatial mismatch, and metropolitan growth differentials. Demographic impact As cities experience a loss of manufacturing jobs and growth of services, sociologist Saskia Sassen affirms that a widening of the social hierarchy occurs where high-level, high-income, salaried professional jobs expands in the service industries alongside a greater incidence of low-wage, low-skilled jobs, usually filled by immigrants and minorities. A \"missing middle\" eventually develops in the wage structure. Several effects of this social polarization include the increasing concentration of poverty in large U.S. cities, the increasing concentration of black and Hispanic populations in large U.S. cities, and distinct social forms such as the underclass, informal economy, and entrepreneurial immigrant communities. In addition, the declining manufacturing sector leaves behind strained blue-collared workers who endure chronic unemployment, economic insecurity, and stagnation due to the global economy's capital flight. Wages and unionization rates for manufacturing jobs also decline. One other qualitative dimension involves the feminization of the job supply as more and more women enter the labor force usually in the service sector. Both costs and benefits are associated with economic restructuring. Greater efficiency, job creation, gentrification, and enhanced national competitiveness are associated with social exclusion and inclusion. The low-skilled, low-income population faces",
    "label": 0
  },
  {
    "text": "women enter the labor force usually in the service sector. Both costs and benefits are associated with economic restructuring. Greater efficiency, job creation, gentrification, and enhanced national competitiveness are associated with social exclusion and inclusion. The low-skilled, low-income population faces the loss of opportunities, full participation in society, lack of access in labor market and school, weak position in housing markets, limited political participation, and restricted social-cultural integration. Conversely, high-skilled, high-income professionals enjoy social inclusion with modern amenities, conveniences, social participation, and full access to public resources. Furthermore, sociologist William Julius Wilson argues that the deindustrialization of manufacturing employment have exacerbated joblessness in impoverished African American communities correlating with a rise in single-mother households, high premature mortality rates, and increasing incarceration rates among African American males. With some African Americans gaining professional upward mobility through affirmative action and equal opportunity sanctions in education and employment, African Americans without such opportunities fall behind. This creates a growing economic class division among the African American demographic accentuated by global economic restructuring without government response to the disadvantaged. Furthermore, Wilson asserts that as the black middle class leave the predominantly black inner city neighborhoods, informal employment information networks are eroded. This isolates poor, inner city residents from the labor market compounding the concentration of poverty, welfare dependency, rise of unemployment, and physical isolation in these areas. City youth are also affected such as in New York City. The declines in education, health care, and social services and the dearth of jobs for those with limited education and training along with the decay of public environments for outdoor play and recreation have all contributed to fewer autonomous outdoor play or \"hanging out\" places for young people. This in turn affects their gross motor development, cultural build-up, and identity construction. Children become prisoners of home",
    "label": 0
  },
  {
    "text": "public environments for outdoor play and recreation have all contributed to fewer autonomous outdoor play or \"hanging out\" places for young people. This in turn affects their gross motor development, cultural build-up, and identity construction. Children become prisoners of home relying on television and other outlets for companionship. Contemporary urban environments restricts the opportunities for children to forge and negotiate peer culture or acquire necessary social skills. Overall, their ecologies have eroded in recent years brought about by global restructuring. Institutional arrangements When the 1973 oil crisis affected the world capitalist economy, economic restructuring was used to remedy the situation by geographically redistributing production, consumption, and residences. City economies across the globe moved from goods-producing to service-producing outlets. Breakthroughs in transportation and communications made industrial capital much more mobile. Soon, producer services emerged as a fourth basic economic sector where routine low-wage service employment moved to low-cost sites and advanced corporate services centralized in cities. These technological upheavals brought about changes in institutional arrangements with the prominence of large corporations, allied business and financial services, nonprofit and public sector enterprises. Global cities such as New York and London become centers for international finance and headquarters for multinational corporations offering cross currency exchange services as well as buildup of foreign banking and trading. Other cities become regional headquarters centers of low-wage manufacturing. In all these urban areas the corporate complex grows offering banking, insurance, advertising, legal council, and other service functions. Economic restructuring allows markets to expand in size and capacity from regional to national to international scopes. Altogether, these institutional arrangements buttressed by improved technology reflect the interconnectedness and internationalization of firms and economic processes. Consequently, capital, goods, and people rapidly flow across borders. Where the mode of regulation began with Fordism and Taylorization in the industrial age then to",
    "label": 0
  },
  {
    "text": "arrangements buttressed by improved technology reflect the interconnectedness and internationalization of firms and economic processes. Consequently, capital, goods, and people rapidly flow across borders. Where the mode of regulation began with Fordism and Taylorization in the industrial age then to mass consumption of Keynesian economics policies, it evolves to differentiated and specialized consumption through international competition. Additionally, in the labor market, nonstandard work arrangements develop in the form of part-time work, temporary agency and contract company employment, short-term employment, contingent work, and independent contracting. Global economic changes and technological improvements in communications and information systems encouraged competitive organizations to specialize in production easily and assemble temporary workers quickly for specific projects. Thus, the norm of standard, steady employment unravels beginning in the mid-1970s. Another shift in institutional arrangement involves public resources. As economic restructuring encourages high-technology service and knowledge-based economies, massive public de-investment results. Across many parts of the U.S. and the industrialized Western nations, steep declines in public outlays occur in housing, schools, social welfare, education, job training, job creation, child care, recreation, and open space. To remedy these cutbacks, privatization is installed as a suitable measure. Though it leads to some improvements in service production, privatization leads to less public accountability and greater unevenness in the distribution of resources. With this reform in privatizing public services, neoliberalism has become the ideological platform of economic restructuring. Free market economic theory has dismantled Keynesian and collectivists’ strategies and promoted the Reagan and Thatcher politics of the 1980s. Soon free trade, flexible labor, and capital flight are used from Washington D.C. to London to Moscow. Moreover, economic restructuring requires decentralization as states hand down power to local governments. Where the federal government focuses on mainly warfare-welfare concerns, local governments focus on productivity. Urban policy reflects this market-oriented shift from once supporting",
    "label": 0
  },
  {
    "text": "to London to Moscow. Moreover, economic restructuring requires decentralization as states hand down power to local governments. Where the federal government focuses on mainly warfare-welfare concerns, local governments focus on productivity. Urban policy reflects this market-oriented shift from once supporting government functions to now endorsing businesses. Geographic impact Urban landscapes especially in the U.S. have significantly altered in response to economic restructuring. Cities such as Baltimore, Detroit, St. Louis and others face population losses which result in thousands of abandoned homes, unused buildings, and vacant lots, contributing to urban decay. Such transformations frustrate urban planning and revitalization, fostering deviance in the forms of drug-related activity and vagrancy. Older, compact, industrial U.S. cities have been rendered obsolete. Urban spaces become playgrounds for the urban gentry, wastelands for low-paid service workers, and denizens for the underground economy. In some areas, gentrification projects have caused displacement of poverty-stricken residents. Sunbelt cities such as Miami and Atlanta rise to become key business centers while Snowbelt cities such as Buffalo and Youngstown decline. Even housing markets respond to economic restructuring with decaying housing stocks, escalating housing prices, depleting tax base, changes in financing, and reduction in federal support for housing. Soon, spatial divisions among wealthy and poor households exacerbate. Moreover, with the movement of blue-collared employment from central cities, geographically entrenched housing discrimination, and suburban land use policy, African American youths in inner cities become victims of spatial mismatch, where their residences provide only weak and negative employment growth and they usually lack access to intrametropolitan mobility. High-order services, an expanding sector in the industrialized world, become spatially concentrated in a relative small number of large metropolitan areas, particularly in suburban office agglomerations. Meaning In cultural terms, economic restructuring has been associated with postmodernity as its counterpart concerning flexible accumulation. Additionally, the term carries with",
    "label": 0
  },
  {
    "text": "Exploration is the process of exploring, an activity which has some expectation of discovery. Organised exploration is largely a human activity, but exploratory activity is common to most organisms capable of directed locomotion and the ability to learn, and has been described in, amongst others, social insects foraging behaviour, where feedback from returning individuals affects the activity of other members of the group. Types Geographical Geographical exploration, sometimes considered the default meaning for the more general term exploration, is the practice of discovering lands and regions of the planet Earth remote or relatively inaccessible from the origin of the explorer. The surface of the Earth not covered by water has been relatively comprehensively explored, as access is generally relatively straightforward, but underwater and subterranean areas are far less known, and even at the surface, much is still to be discovered in detail in the more remote and inaccessible wilderness areas. Two major eras of geographical exploration occurred in human history: The first, covering most of Human history, saw people moving out of Africa, settling in new lands, and developing distinct cultures in relative isolation. Early explorers settled in Europe and Asia; about 14,000 years ago, some crossed the Ice Age land bridge from Siberia to Alaska, and moved southwards to settle in the Americas. For the most part, these cultures were ignorant of each other's existence. The second period of exploration, occurring over the last 10,000 years, saw increased cross-cultural exchange through trade and exploration, and marked a new era of cultural intermingling, and more recently, convergence. Early writings about exploration date back to the 4th millennium B.C. in ancient Egypt. One of the earliest and most impactful thinkers on exploration was Ptolemy in the 2nd century AD. Between the 5th century and 15th century AD, most exploration was done",
    "label": 0
  },
  {
    "text": "exploration date back to the 4th millennium B.C. in ancient Egypt. One of the earliest and most impactful thinkers on exploration was Ptolemy in the 2nd century AD. Between the 5th century and 15th century AD, most exploration was done by Chinese and Arab explorers. This was followed by the Age of Discovery after European scholars rediscovered the works of early Latin and Greek geographers. While the Age of Discovery was partly driven by land routes outside of Europe becoming unsafe, and a desire for conquest, the 17th century also saw exploration driven by nobler motives, including scientific discovery and the expansion of knowledge about the world. This broader knowledge of the world's geography meant that people were able to make world maps, depicting all land known. The first modern atlas was the Theatrum Orbis Terrarum, published by Abraham Ortelius, which included a world map that depicted all of Earth's continents. Underwater Underwater exploration is the exploration of any underwater environment, either by direct observation by the explorer, or by remote observation and measurement under the direction of the investigators. Systematic, targeted exploration, with simultaneous survey, and recording of data, followed by data processing, interpretation and publication, is the most effective method to increase understanding of the ocean and other underwater regions, so they can be effectively managed, conserved, regulated, and their resources discovered, accessed, and used. Less than 10% of the ocean has been mapped in any detail, even less has been visually observed, and the total diversity of life and distribution of populations is similarly incompletely known. Space Space exploration is the use of astronomy and space technology to explore outer space. While the exploration of space is currently carried out mainly by astronomers with telescopes, its physical exploration is conducted both by uncrewed robotic space probes and",
    "label": 0
  },
  {
    "text": "Space exploration is the use of astronomy and space technology to explore outer space. While the exploration of space is currently carried out mainly by astronomers with telescopes, its physical exploration is conducted both by uncrewed robotic space probes and human spaceflight. Space exploration, like its classical form astronomy, is one of the main sources for space science. While the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical extraterrestrial exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries. Urban Urban exploration is the exploration of manmade structures, usually abandoned ruins or hidden components of the manmade environment. Photography and historical interest/documentation are heavily featured in the hobby, sometimes involving trespassing onto private property. The activity presents various risks, including physical danger and, if done illegally and/or without permission, the possibility of arrest and punishment. Some activities associated with urban exploration violate local or regional laws and certain broadly interpreted anti-terrorism laws, or can be considered trespassing or invasion of privacy. Geological Traditionally, mineral exploration relied on direct observation of mineralisation in rock outcrops or in sediments. More recently, however, mineral exploration also includes the use of geologic, geophysical, and geochemical tools to search for anomalies, which can narrow the search area. The area to be prospected should be covered sufficiently to minimize the risk of missing something important, but it can take into account previous experience that certain geological evidence correlates with a very low probability of finding the desired minerals. Other evidence indicates a high probability, making it efficient to concentrate on the",
    "label": 0
  },
  {
    "text": "of missing something important, but it can take into account previous experience that certain geological evidence correlates with a very low probability of finding the desired minerals. Other evidence indicates a high probability, making it efficient to concentrate on the areas of high probability when they are found, and for the skipping areas of very low probability. Once an anomaly has been identified and interpreted to be a prospect, more detailed exploration of the potential reserve can be done by soil sampling, drilling, seismic surveys, and similar methods to assess the most appropriate method and type of mining and the economic potential. Modes Systematic examination or investigation Systematic investigation is done in an orderly and organised manner, generally following a plan, though it should be a flexible plan, which is amenable to rational adaptation to suit circumstances, as the concept of exploration accepts the possibility of the unexpected being encountered, and the plan must survive such encounters to remain useful. Diagnostical examination Diagnosis is the identification of the nature and cause of a given phenomenon. Diagnosis is used in many different disciplines, such as medicine, forensic science and engineering failure analysis, with variations in the use of logic, analytics, and experience, to determine causality. A diagnostic examination explores the available evidence to try to identify likely causes for observed effects, and may also investigate further with the intention to discover additional relevant evidence. This is an instance of inspective and extrinsic exploration. To seek experience first hand Exploration as the pursuit of first hand experience and knowledge is often an example of diversive and intrinsic exploration when done for personal satisfaction and entertainment, though it may also be for purposes of learning or verifying the information provided by others, which is an extrinsic motivation, and which is likely to be",
    "label": 0
  },
  {
    "text": "example of diversive and intrinsic exploration when done for personal satisfaction and entertainment, though it may also be for purposes of learning or verifying the information provided by others, which is an extrinsic motivation, and which is likely to be characterised by a relatively systematic approach. As the personal aspect of the experience is central to this type of exploration, the same region or range of experiences may be explored repeatedly by different people, for each can have a reasonable expectation of personal discovery. Exploratory behavior in animals Exploratory behavior has been defined as behavior directed toward getting information about the environment, or to locate things such as food or individuals. Exploration usually follows a sequence, in which four stages can be identified. The first phase is search, in which the subject moves around to contact relevant stimuli, to which the subject pays attention, and may approach and investigate. The sequence may be interrupted by flight if danger is recognised, or a return to search if the stimulus is not interesting or useful. In all these definitions there is an implication of novelty, or unfamiliarity or the expectation of discovery in the exploration, whereas a survey implies directed examination, but not necessarily discovery of any previously unknown or unexpected information. The activities are not mutually exclusive, and often occur simultaneously to a variable extent. The same field of investigation or region may be explored at different times by different explorers with different motivations, who may make similar or different discoveries. See also Exploration geophysics – Applied branch of geophysics and economic geology Exploration problem – Use of a robot to maximize the knowledge over a particular area Intrinsic motivation (artificial intelligence) – Mechanism for enabling artificial agents to exhibit curiosity Motivation – Inner state causing goal-directed behavior Overlanding – Travel",
    "label": 0
  },
  {
    "text": "An extreme environment is a habitat that is considered very hard to survive in due to its considerably extreme conditions such as temperature, accessibility to different energy sources or under high pressure. For an area to be considered an extreme environment, it must contain certain conditions and aspects that are considered very hard for other life forms to survive. Pressure conditions may be extremely high or low; high or low content of oxygen or carbon dioxide in the atmosphere; high levels of radiation, acidity, or alkalinity; absence of water; water containing a high concentration of salt; the presence of sulphur, petroleum, and other toxic substances. Examples of extreme environments include the geographical poles, very arid deserts, volcanoes, deep ocean trenches, upper atmosphere, outer space, and the environments of every planet in the Solar System except the Earth. Any organisms living in these conditions are often very well adapted to their living circumstances, which is usually a result of long-term evolution. Physiologists have long known that organisms living in extreme environments are especially likely to exhibit clear examples of evolutionary adaptation because of the presumably intense past natural selection they have experienced. On Earth The distribution of extreme environments on Earth has varied through geological time. Humans generally do not inhabit extreme environments. There are organisms referred to as extremophiles that do live in such conditions and are so well-adapted that they readily grow and multiply. Extreme environments are usually hard to survive in. Beyond Earth Most of the moons and planets in the Solar System are also extreme environments. Astrobiologists have not yet found life in any environments beyond Earth, though experiments have shown that tardigrades can survive the harsh vacuum and intense radiation of outer space. The conceptual modification of conditions in locations beyond Earth, to make them more",
    "label": 0
  },
  {
    "text": "have not yet found life in any environments beyond Earth, though experiments have shown that tardigrades can survive the harsh vacuum and intense radiation of outer space. The conceptual modification of conditions in locations beyond Earth, to make them more habitable by humans and other terrestrial organisms, is known as terraforming. Types Among extreme environments are places that are alkaline, acidic, or unusually hot or cold or salty, or without water or oxygen. There are also places altered by humans, such as mine tailings or oil impacted habitats. Alkaline: broadly conceived as natural habitats above pH 9 whether persistently, or with regular frequency or for protracted periods of time. Acidic: broadly conceived as natural habitats below pH 5 whether persistently, or with regular frequency or for protracted periods of time. Extremely cold: broadly conceived habitats periodically or consistently below -17 °C either persistently, or with regular frequency or for protracted periods of time. Includes montane sites, polar sites, and deep ocean habitats. Extremely hot: broadly conceived habitats periodically or consistently in excess of 40 °C either persistently, or with regular frequency or for protracted periods of time. Includes sites with geothermal influences such as Yellowstone and comparable locations worldwide or deep-sea vents. Hypersaline: environments with salt concentrations greater than that of seawater, that is, >3.5%. Includes salt lakes. Under pressure: broadly conceived as habitats under extreme hydrostatic pressure – i.e. aquatic habitats deeper than 2000 meters and enclosed habitats under pressure. Includes habitats in oceans and deep lakes. Radiation: broadly conceived as habitats exposed to abnormally high radiation or of radiation outside the normal range of light. Includes habitats exposed to high UV and IR radiation. Without water: broadly conceived as habitats without free water whether persistently, or with regular frequency or for protracted periods of time. Includes hot and",
    "label": 0
  },
  {
    "text": "radiation outside the normal range of light. Includes habitats exposed to high UV and IR radiation. Without water: broadly conceived as habitats without free water whether persistently, or with regular frequency or for protracted periods of time. Includes hot and cold desert environments, and some endolithic habitats Without oxygen: broadly conceived as habitats without free oxygen – whether persistently, or with regular frequency, or for protracted periods of time. Includes habitats in deeper sediments. Altered by humans: i.e. anthropogenically impacted habitats. Includes mine tailings, oil impacted habitats, and pollution by heavy metals or organic compounds. Without light: deep ocean environments and habitats such as caves. Void of food: areas on earth that lack an abundance of food such as the vast ocean, desert and high country. Extreme pressure: deep ocean areas Extreme habitats Many different habitats can be considered extreme environments, such as the polar ice caps, the driest spots in deserts, and abysmal depths in the ocean. Many different places on the Earth demand that species become highly specialized if they are to survive. In particular, microscopic organisms that can't be seen with the naked eye often thrive in surprising places. Polar regions Owing to the dangerously low temperatures, the number of species that can survive in these remote areas is very slim. Over years of evolution and adaptation to this extremely cold environment, both microscopic and larger species have survived and thrived no matter what conditions they have faced. By changing their eating patterns and due to their dense pelt or their body fat, only a few species have been capable of adapting to such harsh conditions and have learned how to thrive in these cold environments. Deserts A desert is known for its extreme temperatures and extremely dry climate. The type of species that live in this",
    "label": 0
  },
  {
    "text": "have been capable of adapting to such harsh conditions and have learned how to thrive in these cold environments. Deserts A desert is known for its extreme temperatures and extremely dry climate. The type of species that live in this area have adapted to these harsh conditions over years and years. Species that are able to store water and have learned how to protect themselves from the Sun's harsh rays are the only ones that are capable of surviving in these extreme environments. Oceans The oceans depths and temperatures contains some of the most extreme conditions for any species to survive. The deeper one travels, the higher the pressure and the lower the visibility gets, causing completely blacked out conditions. Many of these conditions are too intense for humans to travel to, so instead of sending humans down to these depths to collect research, scientists are using smaller submarines or deep sea drones to study these creatures and extreme environments. Types of species in extreme environments There are many different species that are either commonly known or not known amongst many people. These species have either adapted over time into these extreme environments or they have resided their entire life no matter how many generations. The different species are able to live in these environments because of their flexibility with adaptation. Many can adapt to different climate conditions and hibernate, if need be, to survive. The following list contains only a few species that live in extreme environments. Examples Giant kangaroo rat Certain species of frogs Thermotolerant worms (Alvinella pompejana) Devil worms, Halicephalobus mephisto Greenland shark Marine microorganism Bdelloidea Tardigrade (waterbear) Himalayan jumping spider, Euophrys omnisuperstes Cockroach Extreme environment examples Antarctica Dead Sea Mammoth Hot Springs Mariana Trench Mono Lake Mount Everest Sahara Picture gallery See also Adaptation Ecology Ecophysiology",
    "label": 0
  },
  {
    "text": "Geocriticism is a method of literary analysis and literary theory that incorporates the study of geographic space. The term designates a number of different critical practices. In France, Bertrand Westphal has elaborated the concept of géocritique in several works. In the United States, Robert Tally has argued for a geocriticism as a critical practice suited to the analysis of what he has termed \"literary cartography\". Origins Some of the first expressly geocritical writings emerged from symposia organized by Westphal at the University of Limoges. Westphal's foundational essay, \"Pour une approche géocritique des textes\" constitutes a manifesto for geocriticism. Westphal's theory is elaborated in greater detail in his Geocriticism: Real and Fictional Spaces, translated by Tally, who also provides a brief introduction. But there are also many works addressing similar themes and using similar methods that might be considered geocritical, even if the term \"geocriticism\" is not used. Theory In Westphal's theory, geocriticism is based on three theoretical concepts: spatio-temporality, transgressivity, and referentiality. The idea that space and time form a continuum (space-time) is a tenet of modern physics. In the field of literary theory, geocriticism is an interdisciplinary method of literary analysis that focuses not only on such temporal data as relations between the life and times of the author (as in biographical criticism), the history of the text (as in textual criticism), or the story (as studied by narratology), but also on spatial data. Geocriticism therefore has affinities with geography, architecture, urban studies, and so on; it also correlates to philosophical concepts such as deterritorialization. Following the work of Michel Foucault, Gilles Deleuze, Henri Lefebvre and Mikhail Bakhtin, among others, a geocritical approach to literature recognizes that representations of space are often transgressive, crossing the boundaries of established norms while also reestablishing new relations among people, places, and things.",
    "label": 0
  },
  {
    "text": "Michel Foucault, Gilles Deleuze, Henri Lefebvre and Mikhail Bakhtin, among others, a geocritical approach to literature recognizes that representations of space are often transgressive, crossing the boundaries of established norms while also reestablishing new relations among people, places, and things. Cartography is no longer seen as the exclusive province of the state or the government; rather, various agents or groups may be responsible for representing the geographic spaces at the same time and with different effects. In practice, therefore, geocriticism is multifocal, examining a variety of topics at once, thus differentiating itself from practices that focus on the singular point of view of the traveller or protagonist. Geocriticism also assumes a literary referentiality between world and text, or, in other words, between the referent and its representation. By questioning the relations between a given space's nature and its actually existing condition, the geocritical approach allows for a study of fiction that points also to the theory of possible worlds, such as may be seen in the work on third space by the American geographer Edward Soja (Thirdspace). Tally's book Spatiality, an introduction to spatiality studies in literature and critical theory, includes a chapter on geocriticism. Critical practices Geocriticism frequently involves the study of places described in the literature by various authors, but it can also study the effects of literary representations of a given space. An example of the range of geocritical practices can be found in Tally's collection Geocritical Explorations: Space, Place, and Mapping in Literary and Cultural Studies. Geocriticism derives some of its practices from precursors whose theoretical work helped establish space as a valid topic for literary analysis. For example, in The Poetics of Space and elsewhere, Gaston Bachelard studied literary works to develop a typology of places according to their connotations. Maurice Blanchot's writings have legitimized",
    "label": 0
  },
  {
    "text": "work helped establish space as a valid topic for literary analysis. For example, in The Poetics of Space and elsewhere, Gaston Bachelard studied literary works to develop a typology of places according to their connotations. Maurice Blanchot's writings have legitimized the idea of literary space, an imaginary place for the creation of the work of literature. One might also look at the developments of cultural studies and especially postcolonial studies, such as Raymond Williams's The Country and the City or Edward Said's Culture and Imperialism, which employ what Said has called a \"geographical inquiry into historical experience.\" Fredric Jameson's concept of cognitive mapping and his theoretical engagement with the postmodern condition also highlight the importance of spatial representation and aesthetic productions, including literature, film, architecture, and design. In The Atlas of European Novel, 1800-1900, Franco Moretti has examined the diffusion of literary spaces in Europe, focusing on the complex relationship between the text and space. Moretti has also promulgated a theory of literary history, or literary geography, that would use maps to bring to light new connections between the texts studied and their social spaces. And, in his study of Herman Melville's literary cartography, Robert Tally has offered a geocritical approach to certain texts. Geocriticism has intellectual and methodological affiliations with such fields as Literature and the Environment or ecocriticism, regional literature, urban studies, sociological and philosophical approaches to literature, and utopian studies. References Notes Further reading Bachelard, Gaston. The Poetics of Space. Trans. Maria Jolas. Boston: Beacon Press, 1969. Blanchot, Maurice. The Space of Literature. Trans. Ann Smock. Lincoln, NE: University of Nebraska Press, 1989. Foucault, Michel. \"Of Other Spaces\". Harvey, David. The Condition of Postmodernity. Oxford: Blackwell, 1989. Jameson, Fredric. Postmodernism, or, The Cultural Logic of Late Capitalism. Durham, NC: Duke University Press. 1991. Jameson, Fredric. The Geopolitical",
    "label": 0
  },
  {
    "text": "NE: University of Nebraska Press, 1989. Foucault, Michel. \"Of Other Spaces\". Harvey, David. The Condition of Postmodernity. Oxford: Blackwell, 1989. Jameson, Fredric. Postmodernism, or, The Cultural Logic of Late Capitalism. Durham, NC: Duke University Press. 1991. Jameson, Fredric. The Geopolitical Aesthetic: Cinema and Space in the World System. Bloomington: Indiana University Press. 1992. Moretti, Franco. An Atlas of the European Novel, 1800-1900. London: Verso, 1998. Moretti, Franco. Graphs, Maps, Trees: Abstract Models for a Literary History. London: Verso, 2005. Said, Edward. Culture and Imperialism. New York: Knopf, 1993. Soja, Edward. Postmodern Geographies. London: Verso, 1989. Soja, Edward. Thirdspace: Journeys to Los Angeles and Other Real-and-Imagined Places. Oxford: Blackwell, 1996. Tally, Robert T. (ed.) Geocritical Explorations: Space, Place, and Mapping in Literary and Cultural Studies. New York: Palgrave Macmillan, 2011. Tally, Robert T. \"Geocriticism and Classic American Literature.\" Archived 2012-02-19 at the Wayback Machine Tally, Robert T. Melville, Mapping and Globalization: Literary Cartography in the American Baroque Writer. London: Continuum Books, 2009. Tally, Robert T. Spatiality. The New Critical Idiom. London and New York: Routledge, 2013 Tally, Robert T. Utopia in the Age of Globalization: Space, Representation, and the World System. New York: Palgrave Macmillan, 2013 Wells-Lynn, Amy. \"The Intertextual, Sexually-Coded Rue Jacob: A Geocritical Approach to Djuna Barnes, Natalie Barney, and Radclyffe Hall.\" South Central Review 22.3 (Fall 2005) 78-112. Westphal, Bertrand. Le Monde plausible, Espace, Lieu, Carte, Paris, Éditions de Minuit, 2011. Westphal, Bertrand. Flabbi, Lorenzo. Espaces, tourismes, esthétiques, Limoges, Pulim, 2010. Westphal, Bertrand. Geocriticism: Real and Fictional Spaces, trans. Robert T. Tally Jr. New York: Palgrave Macmillan, 2011. Westphal, Bertrand. La Géocritique, Réel, Fiction, Espace, Paris, Éditions de Minuit, 2007. Westphal, Bertrand. Bertrand Westphal, Pour une approche géocritique des textes. Vox Poetica, 2005.",
    "label": 0
  },
  {
    "text": "Geography of aging or gerontological geography is an emerging field of knowledge of human geography that analyzes the socio-spatial implications of aging of the population from the understanding of the relationships between the physical-social environment and the elderly, at different scales, micro (city, region, country), etc. Since the 1970s in a number of developed countries such as the United States, Canada, the United Kingdom, Germany, Sweden, France, Spain, Australia, New Zealand and Japan, there have been increasing studies focusing on the understanding of spatial patterns of aging population, as well as aspects related to residential changes and provision of health and social services. Among the geographers of aging is S. Harper, who identified the phenomenon of aging associated with the social construction of old age and the processes of residential mobility of this group to the urban periphery, mainly nursing homes and sheltered housing. The contribution of geographers of aging, such as Graham D. Rowles, SM. Golant, S. Harper, G. Laws, are contributing to environmental gerontology by understanding the environmental aspects of gerontology in developed and developing countries. Also in Spain, some geographers, such as Gloria Fernández-Mayoralas, Fermina Rojo-Pérez and Vicente Rodríguez-Rodríguez, have made outstanding contributions to the study of residential strategies, access to health services, and, in general, quality of Life of the elderly, as well as the impacts of Northern European retirees on the Costa del Sol, Spain. In Latin America and Spain, Diego Sánchez-González has shed light on the deepening of issues such as the physical-built and social environment and the quality of life of the elderly; the importance of the natural environment (therapeutic natural landscape) on active and healthy aging in the place; residential strategies for the maintenance of the elderly in the communities; the socio-environmental vulnerability of the elderly in the face of climate change;",
    "label": 0
  },
  {
    "text": "the importance of the natural environment (therapeutic natural landscape) on active and healthy aging in the place; residential strategies for the maintenance of the elderly in the communities; the socio-environmental vulnerability of the elderly in the face of climate change; as well as issues related to the attachment to the place (identity and public space); elderly people with disabilities and social exclusion; leisure and tourism of elderly; and the planning of gerontological and geriatric services. References Abellán García, A., (1999), “Movilidad residencial y género entre las personas de edad: una aproximación a las estrategias residenciales en Madrid”, Documents d’anàlisi geogràfica, nr. 34, pp. 143-159. Andrews, Gavin J.; Phillips, David R. (2005). Ageing and Place: Perspectives, Policy, Practice. Routledge. Escudero, JM., (2003), “Los viejos en su casa, en su ciudad”, Scripta Nova. Revista electrónica de geografía y ciencias sociales, vol. VII, núm. 146 (203), Universidad de Barcelona, Barcelona. Eyles, J., (2004), “The geography of everyday life”, in D. Gregory and R. Walford (eds): Horizons in human geography, London, MacMillan, pp. 102-117. García Ballesteros, A., Ortiz-Álvarez, M.I., y Gómez Escobar, M.C. (2003), “El envejecimiento de las poblaciones: los casos de España y México”, Anales de Geografía de la Universidad Complutense, N° 23, pp. 75-102. Golant, SM., (2002), “The housing problems of the future elderly population”. Commission on Affordable Housing and Health Facility Needs for seniors in the 21st Century (ed.): A quiet crisis in America: A report to Congress. Washington, DC: US Government Printing Office, pp. 189-370. Golant, SM. and Salmon, JR., (2004), “The unequal availability of affordable assisted living units in Florida’s Counties”, Journal of Applied Gerontology, Dec., vol. 23, pp. 349-369. Hanson, S. and Pratt, G., (1992), “Dynamic dependencies: A geographic investigation of local labour markets”, Economic Geography, vol. 68, nr. 4, October, pp. 373-405. Harper, S. and Laws, G.,",
    "label": 0
  },
  {
    "text": "Florida’s Counties”, Journal of Applied Gerontology, Dec., vol. 23, pp. 349-369. Hanson, S. and Pratt, G., (1992), “Dynamic dependencies: A geographic investigation of local labour markets”, Economic Geography, vol. 68, nr. 4, October, pp. 373-405. Harper, S. and Laws, G., (1995), “Rethinking the geography of ageing”, Progress in Human Geography, nr. 19, SAGE Publications, London, pp. 199-221. Katz, C., and J. Monk, (eds.) (1993). Full circles. London: Routledge. Laws, G. (1994). Aging, contested meanings, and the built environment. Environment and Planning A, 26:1787–802. López Jiménez, J.J. (1993), El proceso de envejecimiento urbano y sus implicaciones en el municipio de Madrid. Tesis doctoral, Universidad Complutense, Madrid. Peet, JR. and Rowles, GD., (1974), “Geographical aspects of aging”, Geographical Review, vol. LXIV, nr. 2, The American Geographical Society of New York, New York, pp. 287-289. Phillips, D. et al., (2005), \"The Impacts of Dwelling Conditions on Older Persons' Psychological Well-being in Hong Kong: The Mediating Role of Residential Satisfaction\", Social Science & Medicine, 60, 12, June 2005, pp. 2785-2797. Puga González, MD. y Abellán García, A., (2007), “Las escalas territoriales del envejecimiento”, Semata: Ciencias sociais e humanidades, núm. 18, pp. 121-141. Rodríguez, V. et al., (2003), “Envejecimiento y salud: diez años de investigación en el CSIC”, en Revista Multidisciplinar de Gerontología, vol. 13, nº 1, Nexus, Barcelona, pp. 43-46. Rojo, F., Fernández-Mayoralas, G., Pozo, E., (2000), “Envejecer en casa: los predictores de la satisfacción con la casa, el barrio y el vecindario como componentes de la calidad de vida de los mayores en Madrid”, Revista Multidisciplinar de Gerontología, 10, 4, pp. 222-233. Rowles, Graham D. (1978). Prisoners of space? Exploring the geographical experience of older people. Boulder, C., Westview Press. Rowles, Graham D.; Chaudhury, Habib (2005). Home and identity in late life: international perspectives. New York: Springer Publishing Company. Rowles, Graham D.; Bernard,",
    "label": 0
  },
  {
    "text": "Graham D. (1978). Prisoners of space? Exploring the geographical experience of older people. Boulder, C., Westview Press. Rowles, Graham D.; Chaudhury, Habib (2005). Home and identity in late life: international perspectives. New York: Springer Publishing Company. Rowles, Graham D.; Bernard, Miriam (2012). Environmental Gerontology: Making Meaningful Places in Old Age. New York: Springer Publishing Company. Sánchez-González, D. (2005). The situation of older people in the city of Granada. Geographical Survey. Granada: Universidad de Granada. p. 2089. ISBN 978-84-338-3570-3. Sánchez-González, D. (2009). «Environmental context and spatial experience of aging in place: the case of Granada». Papeles de Población 15 (60): 175-213. ISSN 1405-7425. Sánchez-González, D. (2011). Geografía del envejecimiento y sus implicaciones en Gerontología. Contribuciones geográficas a la Gerontología Ambiental y el envejecimiento de la población. Saarbrücken: Editorial Académica Española-Lambert Academic Publishing. p. 264. ISBN 978-3-8443-4692-3. Sánchez-González, D. (2015): \"Physical-social environments and aging population from environmental gerontology and geography. Socio-spatial implications in Latin America\", Revista de Geografía Norte Grande, Nr. 60, May 2015, pp. 97-114 Sánchez-González, D.; Rodríguez-Rodríguez, V. (2016). Environmental Gerontology in Europe and Latin America. Policies and perspectives on environment and aging. New York: Springer Publishing Company. p. 306. ISBN 978-3-319-21418-4. Smith, GC., (1998), “Residential separation and patterns of interaction between elderly parents and their adult children”. Progress in Human Geography, Jun, vol. 22, pp. 368-384. Stewart, JE., (2003), “Geographic Information Systems in Community-Based Gerontological Research and Practice”, Journal of Applied Gerontology, vol. 22, nr. 1, pp. 134-151. Warnes, AM., (1990), “Geographical questions in gerontology: needed directions for research”, Progress in Human Geography, nr. 14, SAGE Publications, London, pp. 24-56.",
    "label": 0
  },
  {
    "text": "Governmentality is a theory of power developed by French philosopher Michel Foucault, which analyses \"governmental\" power through both the power states have over the population and the means by which subjects govern themselves. As a form of power, governmentality differs from state discipline or punishment, which relies upon coercion to force individuals into specific action. Rather, governmentality also comprises the power that individuals have within a population to self-govern, which the state may influence or guide through non-coercive means such as education. The concept of governmentality has found application and reception in the fields of anthropology, history, law, philosophy, political science, and sociology. Prominent scholars include Peter Miller, Nikolas Rose and Mitchell Dean. History The concept of \"governmentality\" was developed by Michel Foucault roughly between 1977 and his death in 1984, particularly in his lectures at the Collège de France during this time. Governmentality is a neologism, which some commentators have argued combines the terms \"government\" and \"mentality\" or \"rationality\", in this context, referring to modes of thinking. However, this view is disputed, with others arguing that the term is a mere combination of \"government\" and the suffix \"-ality\" to form the abstract noun \"governmentality\". In using the word \"government\", Foucault did not only use the strictly political definition of \"government\" popular today, but he drew upon a broader definition of governing or government that was employed until the eighteenth century. For Foucault, \"'government' also signified problems of self-control, guidance for the family and for children, management of the household, directing the soul, etc.\" The development of governmentality segues from Foucault's ethical, political and historical thoughts from the late 1970s to the early 1980s. His most widely known formulation of this notion is his lecture entitled \"Security, territory and population\" (1978). Foucault elaborated on this concept in his course on",
    "label": 0
  },
  {
    "text": "Foucault's ethical, political and historical thoughts from the late 1970s to the early 1980s. His most widely known formulation of this notion is his lecture entitled \"Security, territory and population\" (1978). Foucault elaborated on this concept in his course on \"The Birth of Biopolitics\" at the Collège de France in 1978–1979. The course was first published in French in 2004. Foucault's governmentality was part of a wider analysis on the topic of disciplinary institutions, neoliberalism, the \"Rule of Law\", the \"microphysics of power\" and also on what Foucault called biopolitics and power-knowledge. In a series of lectures and articles, he posed questions about the nature of contemporary social orders, the conceptualization of power, human freedom and the limits, and possibilities and sources of human actions that were linked to his understanding of the notion of \"governmentality\". The notion of governmentality first gained wider attention in the English-speaking academic world through the edited book The Foucault Effect (1991), which contained a series of essays on the notion of governmentality, together with a translation of Foucault's 1978 short text on \"gouvernementalité\". Definition Governmentality is most popularly defined as the \"conduct of conduct\". It has also been described as the organized practices (mentalities, rationalities, and techniques) through which subjects are governed, the \"art of government\", the calculated means of directing how we behave and act), \"governmental rationality\", \"a 'guideline' for the analysis that Michel Foucault offers by way of historical reconstructions embracing a period starting from Ancient Greece right through to modernity and neo-liberalism\", \"the techniques and strategies by which a society is rendered governable\", and the \"reasoned way of governing best and, at the same time, reflection on the best possible way of governing\". Foucault In his lecture titled Governmentality, Foucault defines governmentality as: \"1. The ensemble formed by the institutions, procedures,",
    "label": 0
  },
  {
    "text": "rendered governable\", and the \"reasoned way of governing best and, at the same time, reflection on the best possible way of governing\". Foucault In his lecture titled Governmentality, Foucault defines governmentality as: \"1. The ensemble formed by the institutions, procedures, analyses and reflections, the calculations and tactics that allow the exercise of this very specific albeit complex form of power, which has as its target: population, as its principal form of knowledge: political economy, and as its essential technical means: apparatuses of security. 2. The tendency which, over a long period and throughout the West, has steadily led towards the pre-eminence over all other forms (sovereignty, discipline, etc) of this type of power which may be termed government, resulting, on the one hand, in formation of a whole series of specific governmental apparatuses, and, on the other, in the development of a whole complex of savoirs. 3. The process, or rather the result of the process, through which the state of justice of the Middle Ages, transformed into the administrative state during the fifteenth and sixteenth centuries, gradually becomes 'governmentalized'.\"The last part of Foucault's definition of governmentality describes the evolution from the Medieval state, which maintained its territory and an ordered territorial society through the blunt and simple imposition of laws upon subjects, to the early Renaissance state, which became more concerned with the \"disposing of things\", and so began to employ strategies and tactics to maintain a content and thus stable society, or in other words to \"render a society governable\". Read with the first two definitions, the third definition describes the process through which a form of government with specific ends, means to these ends (\"apparatuses of security\"), and with a particular type of knowledge (\"political economy\") to achieve these ends, evolved from a medieval state of justice",
    "label": 0
  },
  {
    "text": "definition describes the process through which a form of government with specific ends, means to these ends (\"apparatuses of security\"), and with a particular type of knowledge (\"political economy\") to achieve these ends, evolved from a medieval state of justice to a modern administrative state with complex bureaucracies. Further development Hunt and Wickham, in their work Foucault and Law (1994) begin the section on governmentality with a very basic definition derived from Foucault: \"governmentality is the dramatic expansion in the scope of government, featuring an increase in the number and size of the governmental calculation mechanisms\". In giving this definition, Hunt and Wickham conceive of the term as consisting of two parts 'governmental' and '–ity', with 'governmental' pertaining to the government of a country; and the suffix –ity meaning the study of. They also contextualise the definition in broader Foucaultian theory referring to concepts such as the reason of state, the problem of population, modern political economy, liberal securitisation, and the emergence of the human sciences. Kerr's approach to the term is more complex. He conceives of the term as an abbreviation of \"governmental rationality\" [1999:174]. In other words, it is a way of thinking about the government and the practices of the government. To him it is not \"a zone of critical-revolutionary study, but one that conceptually reproduces capitalist rule\" [1999:197] by asserting that some form of government (and power) will always be necessary to control and constitute society. Dean Dean's understanding of the term incorporates both other forms of governance and the idea of mentalities of government, as well as Hunt and Wickham's, and Kerr's approaches to the term. In line with Hunt and Wickham's approach, Dean acknowledges that in a very narrow sense, governmentality can be used to describe the emergence of a government that saw that",
    "label": 0
  },
  {
    "text": "well as Hunt and Wickham's, and Kerr's approaches to the term. In line with Hunt and Wickham's approach, Dean acknowledges that in a very narrow sense, governmentality can be used to describe the emergence of a government that saw that the object of governing power was to optimise, use and foster living individuals as members of a population [1999:19]. He also includes the idea of government rationalities, seeing governmentality as one way of looking at the practices of government. In addition to the above, he sees government as anything to do with conducting oneself or others: \"Governmentality: How we think about governing others and ourselves in a wide variety of contexts\" including the analysis of those mechanisms that try to shape, sculpt, mobilise and work through the choices, desires, aspirations, needs, wants and lifestyles of individuals and groups [Dean, 1999:12]. Dean's main contribution to the definition of the term, however, comes from the way he breaks the term up into 'govern' 'mentality', or mentalities of governing—mentality being a mental disposition or outlook. This means that the concept of governmentality is not just a tool for thinking about government and governing but also incorporates how and what people who are governed think about the way they are governed. He defines thinking as a \"collective activity\" [1999:16], that is, the sum of the knowledge, beliefs and opinions held by those who are governed. He also raises the point that a mentality is not usually \"examined by those who inhabit it\" [1999:16]. This raises the point that those who are governed may not understand the unnaturalness of both the way they live and the fact that they take this way of life for granted—that the same activity in which they engage in \"can be regarded as a different form of practice depending on",
    "label": 0
  },
  {
    "text": "understand the unnaturalness of both the way they live and the fact that they take this way of life for granted—that the same activity in which they engage in \"can be regarded as a different form of practice depending on the mentalities that invest it\" [1999:17]. Dean highlights another important feature of the concept of governmentality—its reflexivity. He explains: On the one hand, we govern others and ourselves according to what we take to be true about who we are, what aspects of our existence should be worked upon, how, with what means, and to what ends. On the other hand, the ways in which we govern and conduct ourselves give rise to different ways of producing truth. [1999:18] According to Dean, any definition of governmentality should incorporate all of Foucault's intended ideas. A complete definition of the term governmentality must include not only government in terms of the state, but government in terms of any \"conduct of conduct\" [Dean, 1999:10]. It must incorporate the idea of mentalities and the associations that go with that concept: that it is an attitude towards something, and that it is not usually understood \"from within its own perspective\" [1999:16], and that these mentalities are collective and part of a society's culture. It must also include an understanding of the ways in which conduct is governed, not just by governments, but also by others as well. Components Mentality of rule A mentality of rule is any relatively systematic way of thinking about government. It delineates a discursive field in which the exercise of power is 'rationalised' [Lemke, 2001:191]. For example, neo-liberalism is a mentality of rule because it represents a method of rationalising the exercise of government, a rationalisation that obeys the internal rule of maximum economy [Foucault, 1997:74]. Fukuyama [in Rose, 1999: 63]",
    "label": 0
  },
  {
    "text": "is 'rationalised' [Lemke, 2001:191]. For example, neo-liberalism is a mentality of rule because it represents a method of rationalising the exercise of government, a rationalisation that obeys the internal rule of maximum economy [Foucault, 1997:74]. Fukuyama [in Rose, 1999: 63] writes \"a liberal State is ultimately a limited State, with governmental activity strictly bounded by the sphere of individual liberty\". However, as Rose notes, only a certain type of liberty, a certain way of understanding and exercising freedom, is compatible with neo-liberalism; if neo-liberalist government is to fully realize its goals, individuals must come to recognize and act upon themselves as both free and responsible [Rose, 1999:68]. As Lemke states, a mentality of government \"is not pure, neutral knowledge that simply re-presents the governing reality\" [Lemke, 2001:191]. Through the transformation of subjects with duties and obligations, into individuals, with rights and freedoms, modern individuals are not merely 'free to choose' but obliged to be free, \"to understand and enact their lives in terms of choice\" [Rose, 1999:87]. From governmentality to neoliberal governmentality: cartography Governmentality and cartography Cartographic mapping has historically been a key strategy of governmentality. Harley, drawing on Foucault, affirms that State-produced maps \"extend and reinforce the legal statutes, territorial imperatives, and values stemming from the exercise of political power\". Typically, State-led mapping conforms to Bentham's concept of a panopticon, in which 'the one views the many'. From a Foucauldian vantage point, this was the blueprint for disciplinary power. Neoliberal governmentality and cartography Through processes of neoliberalism, the State has \"hollowed out\" some of its cartographic responsibilities and delegated power to individuals who are at a lower geographical scale. 'People's cartography' is believed to deliver a more democratic spatial governance than traditional top-down State-distribution of cartographic knowledge. Joyce challenges Foucauldian notions of Panopticism, contending that neoliberal governmentality is more",
    "label": 0
  },
  {
    "text": "power to individuals who are at a lower geographical scale. 'People's cartography' is believed to deliver a more democratic spatial governance than traditional top-down State-distribution of cartographic knowledge. Joyce challenges Foucauldian notions of Panopticism, contending that neoliberal governmentality is more adequately conceptualised by an omniopticon - 'the many surveilling the many'. Collaborative mapping initiatives utilising GPS technology are arguably omniopticons, with the ability to reverse the panoptic gaze. Self-governing capabilities Through freedom, particular self-governing capabilities can be installed in order to bring our own ways of conducting and evaluating ourselves into alignment with political objectives [Rose, 1996:155]. These capabilities are enterprise and autonomy. \"Enterprise\" designates an array of rules for the conduct of one's everyday existence: energy, initiative, ambition, calculation, and personal responsibility. The enterprising self will make an enterprise of its life, seek to maximize its own human capital, project itself a future, and seek to shape life in order to become what it wishes to be. The enterprising self is thus both an active self and a calculating self, a self that calculates about itself and that acts upon itself in order to better itself [Rose, 1996:154]. \"Autonomy\" refers to the adoption of undertakings, definition of goals, and planning to achieve needs through the powers of self [Rose, 1996:159]. Seem from this view, the autonomy of the self is thus not the eternal antithesis of political power, but one of the objectives and instruments of modern mentalities for the conduct of conduct [Rose, 1996:155]. Technologies of power Technologies of power are those \"technologies imbued with aspirations for the shaping of conduct in the hope of producing certain desired effects and averting certain undesired ones\" [Rose, 1999:52]. The two main groups of technologies of power are technologies of the self, and technologies of the market. Foucault defined technologies of",
    "label": 0
  },
  {
    "text": "shaping of conduct in the hope of producing certain desired effects and averting certain undesired ones\" [Rose, 1999:52]. The two main groups of technologies of power are technologies of the self, and technologies of the market. Foucault defined technologies of the self as techniques that allow individuals to effect by their own means a certain number of operations on their own bodies, minds, souls, and lifestyle, so as to transform themselves in order to attain a certain state of happiness, and quality of life. Technologies of the market are those technologies based around the buying and selling of goods that enable persons to define who they are, or want to be. These two technologies are not always completely distinct Technologies of the self Technologies of the self refer to the practices and strategies by which individuals represent to themselves their own ethical self-understanding. One of the main features of technologies of self is that of expertise. In governmentalist theory, expertise has three important aspects. First, its grounding of authority in a claim to scientificity and objectivity creates distance between self-regulation and the state that is necessary with liberal democracies. Second, expertise can \"mobilise and be mobilised within political argument in distinctive ways, producing a new relationship between knowledge and government. Expertise comes to be accorded a particular role in the formulation of programs of government and in the technologies that seek to give them effect\" [Rose, 1996:156]. Third, expertise operates through a relationship with the self-regulating abilities of individuals. The plausibility inherent in a claim to scientificity binds \"subjectivity to truth and subjects to experts\" [Rose, 1996:156]. Expertise works through a logic of choice, through a transformation of the ways in which individuals constitute themselves, through \"inculcating desires for self-development that expertise itself can guide and through claims to be",
    "label": 0
  },
  {
    "text": "truth and subjects to experts\" [Rose, 1996:156]. Expertise works through a logic of choice, through a transformation of the ways in which individuals constitute themselves, through \"inculcating desires for self-development that expertise itself can guide and through claims to be able to allay the anxieties generated when the actuality of life fails to live up to its image [Rose, 1999:88]. Responsibilisation In line with its desire to reduce the scope of government (e.g. welfare) Neo-liberalism characteristically develops indirect techniques for leading and controlling individuals without being responsible for them. The main mechanism is through the technology of responsibilisation. This entails subjects becoming responsibilised by making them see social risks such as illness, unemployment, poverty, and public safety not as the responsibility of the state, but actually lying in the domain for which the individual is responsible and transforming it into a problem of 'self-care' [Lemke, 2001:201] and of 'consumption'. Healthism Healthism links the \"public objectives for the good health and good order of the social body with the desire of individuals for health and well-being\" [Rose, 1999:74]. Healthy bodies and hygienic homes may still be objectives of the state, but it no longer seeks to discipline, instruct, moralise or threaten subjects into compliance. Rather, \"individuals are addressed on the assumption that they want to be healthy and enjoined to freely seek out the ways of living most likely to promote their own health\" [Rose, 1999:86-87]. Normalisation Another technology of power arising from the social sciences is that of normalisation. The technology of norms was given a push by the new methods of measuring population. A norm is that \"which is socially worthy, statistically average, scientifically healthy and personally desirable\". The important aspect of normality, is that while the norm is natural, those who wish to achieve normality will do so",
    "label": 0
  },
  {
    "text": "methods of measuring population. A norm is that \"which is socially worthy, statistically average, scientifically healthy and personally desirable\". The important aspect of normality, is that while the norm is natural, those who wish to achieve normality will do so by working on themselves, controlling their impulses in everyday conduct and habits, and inculcating norms of conduct into their children, under the guidance of others. Norms are enforced through the calculated administration of shame. Shame entails an anxiety over the exterior behaviour and appearance of the self, linked to an injunction to care for oneself in the name of achieving quality of life [Rose, 1999:73]. Self-esteem Self-esteem is a practical and productive technology linked to the technology of norms, which produces of certain kinds of selves. Self-esteem is a technology in the sense that it is a specialised knowledge of how to esteem ourselves to estimate, calculate, measure, evaluate, discipline, and to judge our selves. The 'self-esteem' approach considers a wide variety of social problems to have their source in a lack of self-esteem on the part of the persons concerned. 'Self-esteem' thus has much more to do with self-assessment than with self-respect, as the self continuously has to be measured, judged and disciplined in order to gear personal 'empowerment' to collective yardsticks. These collective yardsticks are determined by the norms previously discussed. Self-esteem is a technology of self for \"evaluating and acting upon ourselves so that the police, the guards and the doctors do not have to do so\". Technologies of the market The technologies of the market can be described as the technology of desire, and the technology of identity through consumption. The technology of desire is a mechanism that induces desires that individuals work to satisfy. Marketers create wants and artificial needs in us through advertising goods,",
    "label": 0
  },
  {
    "text": "be described as the technology of desire, and the technology of identity through consumption. The technology of desire is a mechanism that induces desires that individuals work to satisfy. Marketers create wants and artificial needs in us through advertising goods, experiences and lifestyles that are tempting. These advertisements seek to convey the sense of individual satisfaction brought about by the purchase or use of this product. The technology of identity through consumption utilises the power of goods to shape identities. Each commodity is imbued with a particular meaning, which is reflected upon those who purchase it, illuminating the kind of person they are or want to be. Consumption is thus portrayed as placing an individual within a certain form of life. These technologies of the market and of the self are the particular mechanisms whereby individuals are induced into becoming free, enterprising individuals who govern themselves and thus need only limited direct governance by the state. The implementation of these technologies is greatly assisted by experts from the social sciences. These experts operate a regime of the self, where success in life depends on continual exercise of freedom, and where life is understood not in terms of fate or social status, but in terms of our success or failure in acquiring the skills and making the choices to actualise ourself. The parts of self that are sought to be worked upon, the means by which one does so, and the eventual hope of becoming a new self, all vary according to the nature of the technology of power by which one is motivated [Dean, 1999:17]. Applications and extensions Ecogovernmentality Ecogovernmentality (or eco-governmentality) is the application of Foucault's concepts of biopower and governmentality to the analysis of the regulation of social interactions with the natural world. Timothy W. Luke theorized this",
    "label": 0
  },
  {
    "text": "one is motivated [Dean, 1999:17]. Applications and extensions Ecogovernmentality Ecogovernmentality (or eco-governmentality) is the application of Foucault's concepts of biopower and governmentality to the analysis of the regulation of social interactions with the natural world. Timothy W. Luke theorized this as environmentality and green governmentality. Ecogovernmentality began in the mid-1990s with a small body of theorists (Luke, Darier, and Rutherford) the literature on ecogovernmentality grew as a response to the perceived lack of Foucauldian analysis of environmentalism and in environmental studies. Crises of governmentality According to Foucault, there are several instances where the Western, \"liberal art of government\" enters into a period of crisis, where the logic of ensuring freedom (which was defined against the background of risk or danger) necessitates actions \"which potentially risk producing exactly the opposite.\" The inherently contradictory logics that lead to such contradictions are identified by Foucault as: Liberalism depends on the socialization of individuals to fear the constant presence of danger, e.g., public campaigns advocating savings banks, public hygiene, and disease prevention, the development of detective novels as a genre and of news stories of crime, and sexual anxieties surrounding \"degeneration\". Liberal freedom requires disciplinary techniques that manage the individual's behaviour and everyday life so as to ensure productivity and the increase in profit through efficient labour, e.g., Bentham's Panopticon surveillance system. Liberalism claims to supervise the natural mechanisms of behaviour and production, but must intervene when it notices \"irregularities.\" Liberalism must force individuals to be free: control and intervention becomes the entire basis of freedom. Freedom must ultimately be manufactured by control rather than simply \"counterweighted\" by it. Examples of this contradictory logic which Foucault cites are the policies of the Keynesian welfare state under F.D. Roosevelt, the thought of the German liberals in the Freiburg school, and the thought of American libertarian",
    "label": 0
  },
  {
    "text": "than simply \"counterweighted\" by it. Examples of this contradictory logic which Foucault cites are the policies of the Keynesian welfare state under F.D. Roosevelt, the thought of the German liberals in the Freiburg school, and the thought of American libertarian economists such as the Chicago School which attempt to free individuals from the lack of freedom perceived to exist under socialism and fascism, but did so by using state interventionist models. These governmental crises may be triggered by phenomena such as a discursive concern with increasing economic capital costs for the exercise of freedom, e.g., prices for purchasing resources, the need for excessive state coercion and interventionism to protect market freedoms, e.g., anti-trust and anti-monopoly legislation that leads to a \"legal strait-jacket\" for the state, local protests rejecting the disciplinary mechanisms of the market society and state. and finally, the destructive and wasteful effects of ineffective mechanisms for producing freedom. Application to health care Scholars have recently suggested that the concept of governmentality may be useful in explaining the operation of evidence-based health care and the internalization of clinical guidelines relating to best practice for patient populations, such as those developed by the American Agency for Health Care Research and Quality and the British National Institute for Health and Clinical Excellence (NICE). Research has also explored potential micro-level resistance to governmentality in health care and how governmentally is enacted into health care practice, drawing on Foucault's notion of pastoral power. Beyond the West Jeffreys and Sigley (2009) highlight that governmentality studies have focused on advanced liberal democracies, and preclude considerations of non-liberal forms of governmentality in both western and non-western contexts. Recent studies have broken new ground by applying Foucault's concept of governmentality to non-western and non-liberal settings, such as China. Jeffreys (2009) for example provides a collection of essay",
    "label": 0
  },
  {
    "text": "of non-liberal forms of governmentality in both western and non-western contexts. Recent studies have broken new ground by applying Foucault's concept of governmentality to non-western and non-liberal settings, such as China. Jeffreys (2009) for example provides a collection of essay on China's approach to governance, development, education, the environment, community, religion, and sexual health where the notion of 'Chinese governmentally' is based not on the notion of 'freedom and liberty' as in the western tradition but rather, on a distinct rational approach to planning and administration. Another well-known study is Li (2007), an account of development in action. Focusing on attempts to improve landscapes and livelihoods in Indonesia, Li exposes the practices that enable experts to diagnose problems and devise interventions, and the agency of people whose conduct is targeted for reform. Katomero (2017) also employs governmentality in a development context, this time to describe accountability practices in the water supply sector in Tanzania. Some studies illustrate that in societies that are influenced by global capitalism, the model of governmentality is limited to specific spaces and practices rather than dictating a wholesome ethos of citizenship. For example, in China, people who practice “self-cultivation” through various educational programs in psychology or communication skills often treat these activities as a place where they can perform individualistic personalities in contrast to their ordinary social responsibilities. Moreover, these activities may be oriented at promoting social change as much as they aim to regulate the capacities of the self. See also Interpellation (philosophy) Inverted totalitarianism Political power Rationality and power Therapeutic governance References Further reading Cruikshank, B. (1996), 'Revolutions within: self-government and self-esteem', in Andrew Barry, Thomas Osborne & Nikolas Rose (eds.) (1996), Foucault and Political Reason: Liberalism, Neo-Liberalism, and Rationalities of Government, Chicago, IL: University of Chicago Press. Dean, M. (1999), Governmentality: Power and",
    "label": 0
  },
  {
    "text": "Cruikshank, B. (1996), 'Revolutions within: self-government and self-esteem', in Andrew Barry, Thomas Osborne & Nikolas Rose (eds.) (1996), Foucault and Political Reason: Liberalism, Neo-Liberalism, and Rationalities of Government, Chicago, IL: University of Chicago Press. Dean, M. (1999), Governmentality: Power and Rule in Modern Society. London: Sage. Foucault, M.(1982), 'Technologies of the Self', in Luther H. Martin, Huck Gutman and Patrick H. Hutton (eds) Technologies of the Self: A Seminar with Michel Foucault, pp. 16–49. Amherst: The University of Massachusetts Press, 1988. Foucault, M.(1984), The History of Sexuality Vol. 2: The Use of Pleasure, trans. Robert Hurley. New York: Random House, 1985. Foucault, M.(1984), The History of Sexuality Vol. 3: The Care of the Self, trans. Robert Hurley. New York: Vintage Books, 1988. Foucault, M. (1997), Ethics: Subjectivity and Truth, edited by Paul Rabinow, New York: New Press. Foucault, M. (2004), Naissance de la biopolitique: cours au Collège de France (1978-1979). Paris: Gallimard & Seuil. Foucault, M., (2008), The birth of biopolitics. Lectures at the College de France, 1978‐79. Palgrave MacMillan Hunt, H. & Wickham, G. (1994), Foucault and Law. London. Pluto Press. Inda, J. X. (2005), \"Anthropologies of Modernity: Foucault, Governmentality, and Life Politics\". Malden, MA: Wiley-Blackwell. Joyce, P. (2003), The Rule of Freedom: Liberalism and the Modern City. London: Verso. Kerr, D. (1999), 'Beheading the king and enthroning the market: A critique of Foucauldian governmentality' in Science & Society, New York: v.63, i.2; p. 173-203 (accessed through Expanded Academic Index). Luke, T.W. (1997) Ecocritique: Contesting the Politics of Nature, Economy and Culture. Minneapolis: University of Minnesota Press. Nagl, D. (2013), 'The Governmentality of Slavery in Colonial Boston, 1690-1760' in American Studies, 58.1, pp. 5–26. Rivera Vicencio, E. (2014), 'The firm and corporative governmentality. From the perspective of Foucault', Int. J. Economics and Accounting, Vol. 5, No. 4, pp. 281–305.",
    "label": 0
  },
  {
    "text": "The international date line in Judaism is used to demarcate the change of one calendar day to the next in the Jewish calendar. It is not necessarily the same as the internationally recognised International Date Line (IDL - which is 180° from the Greenwich Meridian, passing through London, UK). On the east side of the IDL it is one day, on the west side it is the next. However, the conventional International Date Line is a relatively recent geographic and political construct whose exact location has moved from time to time depending on the needs of different interested parties. While it is well-understood why the conventional date line is located in the Pacific Ocean, there are not really objective criteria for its exact placement within the Pacific. In that light, it cannot be taken for granted that the conventional International Date Line can (or should) be used as a date line under Jewish law. In practice, within Judaism the halakhic date line is similar to, but not necessarily identical with, the conventional Date Line, and the differences can have consequences under religious law. Location of the date line Theoretical basis for the discussion Many of the opinions about the halakhic date line are structured as a response to the question of what days someone should observe as Shabbat and Jewish holidays. Shabbat occurs every seven days at any location on Earth. It is normally thought to occur on Saturday—or more precisely, from Friday at sundown to Saturday at nightfall. But if the halakhic date line is not identical to the conventional Date Line, it is possible that what is Saturday with respect to the conventional Date Line is not Saturday with respect to the halakhic date line, at least in some places. There are several opinions regarding where exactly the",
    "label": 0
  },
  {
    "text": "conventional Date Line, it is possible that what is Saturday with respect to the conventional Date Line is not Saturday with respect to the halakhic date line, at least in some places. There are several opinions regarding where exactly the halakhic date line should be according to Jewish law, and at least one opinion that says that no halakhic date line really exists. 1. 90 degrees east of Jerusalem. The concept of a halakhic date line is mentioned in the Baal HaMeor, a 12th-century Talmudic commentary, which seems to indicate that the day changes in an area where the time is six hours ahead of Jerusalem (90 degrees east of Jerusalem, about 125.2°E, a line now known to run through Australia, the Philippines, China and Russia). This line, which he refers to as the K'tzai Hamizrach (the easternmost line), is used to calculate the day of Rosh Hashanah, the Jewish New Year. According to some sources it is alluded to in both the Babylonian Talmud (Rosh Hashanah and Eruvin) and in the Jerusalem Talmud. The Kuzari of Yehuda Halevi, also a 12th-century work, seems to agree with this ruling. Later decisors like the Chazon Ish (twentieth century) fundamentally agree with this ruling. However, they recognize practical issues associated with the pure use of a line of longitude for this purpose. As an example, 125.2°E passes directly through Dongfeng Street in Changchun, China. If this line of longitude were used strictly, people could simply avoid Shabbat altogether by crossing the street. To prevent that, the Chazon Ish rules that the contiguous land masses to the east of that line of longitude are considered secondary (tafel) to the land masses west of that line. As a result, he rules that the date line runs along 125.2°E when over water, but curves around",
    "label": 0
  },
  {
    "text": "land masses to the east of that line of longitude are considered secondary (tafel) to the land masses west of that line. As a result, he rules that the date line runs along 125.2°E when over water, but curves around the eastern coast of mainland Asia and Australia. By this view, Russia, China and mainland Australia are west of the date line and observe Shabbat on local Saturday. Japan, New Zealand and Tasmania are east of the date line and should observe Shabbat on local Sunday, as defined by the conventional International Date Line. By this view, the Philippines and Indonesia would have portions west of the line and portions east of the line. 2. 180 degrees east of Jerusalem. Rabbi Yechiel Michel Tucazinsky ruled that the International Date Line is 180 degrees east of Jerusalem. That would mean that the date line, rather than being near 180°, would be at 144.8°W. By this view, places east of the conventional International Date Line but west of 144.8°W—Alaska, Hawaii and a variety of archipelagos in the Pacific—would observe Shabbat on the local Friday instead of the local Saturday. It is possible (but not certain) under this view to apply the principle of tafel described above as well. In that event, mainland Alaska would be east of the date line, but the Aleutian Islands would be west of the date line. 3. Mid-Pacific. A variety of decisors rule that the date line runs in the middle of the Pacific Ocean, close to (but not necessarily the same as) the conventional International Date Line. According to this point of view, all of the major populated areas of the Pacific (such as New Zealand, Japan, Alaska and Hawaii) observe Shabbat on local Saturday (that is, consistent with the conventional International Date Line). Only certain",
    "label": 0
  },
  {
    "text": "Line. According to this point of view, all of the major populated areas of the Pacific (such as New Zealand, Japan, Alaska and Hawaii) observe Shabbat on local Saturday (that is, consistent with the conventional International Date Line). Only certain Pacific islands, generally having few or no permanent Jewish residents, might not observe Shabbat on local Saturday. 4. Following local custom/There is no fixed date line. According to Rabbi Menachem Mendel Kasher, there is no clear tradition or Talmudic source dictating any of the preceding opinions as binding. For that reason, and consistent with a responsum of the Radbaz, Rabbi Kasher starts with the default law that a Jew not knowing the proper day for Shabbat should count days from the last time s/he observed Shabbat, and that every seven days is Shabbat. In his view, established Jewish communities are presumed to have fixed their calendars according to this principle. Therefore, Shabbat in an established community is whatever day the community has established. Rabbi Isser Zalman Meltzer and Rabbi Zvi Pesach Frank apparently agree with this position. This position does not in and of itself require a formal date line to be established, and Rabbi Kasher does not seem to think that it is necessary to do so. But the de facto result of this position is consistent with the conventional International Date Line, at least anywhere there is an established Jewish community. Practical rulings In practice, the conventional International Date Line (or another mid-Pacific line near it) is the de facto date line under Jewish law, at least for established Jewish communities. The communities of Japan, New Zealand, Hawaii, and French Polynesia all observe Shabbat on local Saturday (i.e., Friday night until Saturday night). No known Jewish community observes Shabbat on a day other than local Saturday. However there",
    "label": 0
  },
  {
    "text": "established Jewish communities. The communities of Japan, New Zealand, Hawaii, and French Polynesia all observe Shabbat on local Saturday (i.e., Friday night until Saturday night). No known Jewish community observes Shabbat on a day other than local Saturday. However there are those in Hawaii who celebrate Kabbalah Shabbat on a Thursday evening. However, that practical conclusion is reached in two different ways, resulting in somewhat different practice patterns in each case. Following local custom/There is no fixed date line. As noted above, according to this point of view, Shabbat is simply observed on the date previously established as Shabbat by the local community—uniformly, local Saturday—without any need for any further observance. This appears to be the default practice for residents of such places as Japan, New Zealand and Hawaii. At minimum, it is difficult to find evidence of other practices by residents of those areas. Establishment of a date line by a majority among three halakhic positions. The travelers' guide of the Star-K kosher supervision service, compiled according to the rulings of its rabbinic administrator, Rabbi Moshe Heinemann, uses the following approach, which is also cited by others. According to this approach, the first three numbered sections above constitute three valid, parallel, halakhic rulings. Shabbat is consequently fully observed on whichever day is consistent with the majority view among those opinions (two out of three). However, out of respect to the minority view of the third ruling, and with an eye toward not desecrating Shabbat, Torah-level prohibitions are to be avoided on the day consistent with the minority view, although that day is otherwise considered a weekday. According to this rule, practice is as follows: In New Zealand and Japan, the local Saturday is Shabbat according to the majority opinion (sections 2 and 3 above), and it should therefore be",
    "label": 0
  },
  {
    "text": "that day is otherwise considered a weekday. According to this rule, practice is as follows: In New Zealand and Japan, the local Saturday is Shabbat according to the majority opinion (sections 2 and 3 above), and it should therefore be fully observed as Shabbat. However, since according to the minority opinion (section 1 above), Shabbat is on the local Sunday, one should not perform any Torah-level Shabbat prohibitions on Sunday. Nevertheless, on Sunday, one should pray the regular weekday prayers, donning tefillin during morning prayers. In Hawaii and French Polynesia, the local Saturday is Shabbat according to majority opinion (sections 1 and 3 above), and it should therefore be fully observed as Shabbat. However, since according to the minority opinion (section 2 above), Shabbat is on the local Friday, one should not perform any Torah-level Shabbat prohibitions on Friday. Friday prayers are weekday prayers, and preparation for Shabbat must be done with an eye to avoiding Torah-level Shabbat prohibitions. In principle, a zone of pure doubt exists in approximately the area of 169°W–177°E, including places like Tonga, Samoa and American Samoa. The rulings of the Mid-Pacific group above are not perfectly consistent in this area, so there is no possibility of an unambiguous \"two out of three\" consensus here. Rabbi Dovid Heber of the Star-K suggests that in such locations one might need to observe Shabbat fully for two days every week. The Star-K's international kosher supervision staff follows this approach, and there is evidence that some other travelers also do. Authorities suggesting this approach often advise travelers to avoid the zone of doubt entirely near weekends, or to consult with a competent rabbinical authority directly. Crossing the halakhic date line The issues discussed in the previous section apply per se to individuals or communities in fixed locations. However, the",
    "label": 0
  },
  {
    "text": "the zone of doubt entirely near weekends, or to consult with a competent rabbinical authority directly. Crossing the halakhic date line The issues discussed in the previous section apply per se to individuals or communities in fixed locations. However, the act of crossing the date line (wherever it may be drawn) introduces a number of additional issues under Jewish law. Questions potentially affected include: In some cases, crossing the date line (wherever it may be drawn) has a specific impact on practice or prohibitions under Jewish law. In others, an individual's count of days (by the experience of sunset and sunrise) is the determining factor, regardless of the crossing of the date line. Details around specific questions, cases and rulings of Jewish law are beyond the scope of this article. Considerations for astronauts Before Israeli astronaut Ilan Ramon flew on the Space Shuttle Columbia in 2003, he decided (after consultation with rabbis) to observe Shabbat according to time in his last residence, Cape Canaveral, since he would be crossing the date line and observing sunset many times per day. Judith Resnik, the first American Jewish astronaut in space, lit (electronic) Shabbat candles according to the time in Houston, TX, her home and the location of Mission Control. See also Jewish law in the polar regions Zmanim Style note References",
    "label": 0
  },
  {
    "text": "The landlocked developing countries (LLDC) are developing countries that are landlocked. Due to the economic and other disadvantages suffered by such countries, the majority of landlocked countries are least developed countries (LDCs), with inhabitants of these countries occupying the bottom billion tier of the world's population in terms of poverty. Outside of Europe, there is not a single highly developed landlocked country as measured by the Human Development Index (HDI), and nine of the twelve countries with the lowest HDI scores are landlocked. Landlocked European countries are exceptions in terms of development outcomes due to their close integration with the regional European market. Landlocked countries that rely on transoceanic trade usually suffer a cost of trade that is double that of their maritime neighbours. Landlocked countries experience economic growth 6% less than non-landlocked countries, holding other variables constant. 32 out of the world's 44 landlocked countries, including all the landlocked countries in Africa, Asia, and South America, have been classified as Landlocked Developing Countries by the United Nations. As of 2012, about 442.8 million people lived in these LLDCs. UN-OHRLLS The United Nations has an Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States (UN-OHRLLS). It mainly holds the view that high transport costs due to distance and terrain result in the erosion of competitive edge for exports from landlocked countries. In addition, it recognizes the constraints on landlocked countries to be mainly physical, including lack of direct access to the sea, isolation from world markets and high transit costs due to physical distance. It also attributes geographic remoteness as one of the most significant reasons why developing landlocked nations cannot alleviate themselves, while European landlocked cases are mostly developed because of short distances to the sea through well-developed countries. One",
    "label": 0
  },
  {
    "text": "to physical distance. It also attributes geographic remoteness as one of the most significant reasons why developing landlocked nations cannot alleviate themselves, while European landlocked cases are mostly developed because of short distances to the sea through well-developed countries. One other commonly cited factor is the administrative burdens associated with border crossings, as there is a heavy load of bureaucratic procedures, paperwork, custom charges, and most importantly, traffic delay due to border wait times, which affect delivery contracts. Delays and inefficiency compound geographically: thus a two- to three-week wait due to border customs between Uganda and Kenya makes it impossible to book ships ahead of time in Mombasa, furthering delivery contract delays. Despite these explanations, it is also important to consider the transit countries that neighbour LLDCs, from whose ports the goods of LLDCs are exported. Dependency problems Although Adam Smith and traditional thought hold that geography and transportation are the culprits for keeping LLDCs from realizing development gains, Faye, Sachs and Snow hold the argument that no matter the advancement of infrastructure or lack of geographic distance to a port, landlocked nations are still dependent on their neighbouring transit nations. Outlying this specific relationship of dependency, Faye et al. insist that though LLDCs vary across the board in terms of HDI index scores, LLDCs almost uniformly straddle at the bottom of HDI rankings in terms of region, suggesting a correlated dependency relationship of development for landlocked countries with their respective regions. In fact, HDI levels decrease as one moves inland along the major transit route that runs from the coast of Kenya, across the country before going through Uganda, Rwanda and then finally Burundi. Just recently, it has been economically modeled that if the economic size of a transit country is increased by just 1%, a subsequent increase of",
    "label": 0
  },
  {
    "text": "coast of Kenya, across the country before going through Uganda, Rwanda and then finally Burundi. Just recently, it has been economically modeled that if the economic size of a transit country is increased by just 1%, a subsequent increase of at least 2% is experienced by the landlocked country, which shows that there is hope for LLDCs if the conditions of their transit neighbours are addressed. In fact, some LLDCs are seeing the brighter side of such a relationship, with the Central Asian nations geographic location between three BRICS nations (China, Russia and India) hungry for the region's oil and mineral wealth serving to boost economic development. The three major factors that LLDCs are dependent on their transit neighbours are dependence on transit infrastructure, dependence on political relations with neighbours, and dependence on internal peace and stability within transit neighbours. Burundi Burundi has relatively good internal road networks, but it cannot export its goods using the most direct route to the sea since the inland infrastructure of Tanzania is poorly connected to the port of Dar es Salaam. Thus Burundi relies on Kenya's port of Mombasa for export; but this route was severed briefly in the 1990s when political relations with Kenya deteriorated. Further, Burundi's exports could not pass through Mozambique around the same time due to the Mozambican Civil War (1977–1992). Thus, Burundi had to export its goods using a 4500 km route, crossing several borders and changing transport modes, to reach the port of Durban in South Africa. Other African countries Mali had problems exporting goods in the 1990s as nearly all its transit neighbours (Algeria, Togo, Sierra Leone, Liberia, Guinea and Côte d'Ivoire) were engaged in civil war around the same time: the Algerian civil war (1991–2002), the Sierra Leone civil war (1991–2002), the Guinea-Bissau civil war",
    "label": 0
  },
  {
    "text": "1990s as nearly all its transit neighbours (Algeria, Togo, Sierra Leone, Liberia, Guinea and Côte d'Ivoire) were engaged in civil war around the same time: the Algerian civil war (1991–2002), the Sierra Leone civil war (1991–2002), the Guinea-Bissau civil war involving Guinea (1999), the First Liberian Civil War (1989–1997), the Second Liberian Civil War (1999) and the First Ivorian Civil War (2002–2007). The lone exception was Ghana, which was under military rule but did not have an active civil war at the time. The Central African Republic's export routes are seasonal: in the rainy season Cameroon's roads are too poor to use; and during the dry season the Democratic Republic of Congo's Oubangui River water levels are too low for river travel. Central Asia The mineral resource-rich countries of Central Asia and Mongolia offer a unique set of landlocked cases, as these are nations where economic growth has grown exceptionally in recent years. In Central Asia, oil and coal deposits have influenced development: Kazakhstan’s GDI per capita in purchasing power parity was five times greater than Kyrgyzstan's in 2009. Despite substantial development growth, these nations are not on a stable and destined path to being well developed, as the exploitation of their natural resources translates into an overall low average income and disparity of income, and because their limited deposits of resources allow growth only in the short term, and most importantly because dependence on unprocessed materials increases the risk of shocks due to variations in market prices. And though it is widely conceived that free trade can permit faster economic growth, Mongolia is now subjected to a new geopolitical game which concerns the traffic on its railway lines between China and Russia. Russian Railways now effectively owns 50% of Mongolia's rail infrastructure, which could mean more efficient modernization and",
    "label": 0
  },
  {
    "text": "economic growth, Mongolia is now subjected to a new geopolitical game which concerns the traffic on its railway lines between China and Russia. Russian Railways now effectively owns 50% of Mongolia's rail infrastructure, which could mean more efficient modernization and the laying of new rail lines, but in reality also translates into powerful leverage to pressure the government of Mongolia to concede unfair terms for license grants of coal, copper, and gold mines. Thus, it can be argued that these nations with extraordinary mineral wealth should pursue economic diversification. All of these nations possess education qualifications, as they are inheritors of the Soviet Union's social education system. This implies that it is due to poor economic policies that more than 40% of the labour force is bogged down in the agricultural sector instead of being diverted into secondary or tertiary economic activity. However, Mongolia benefits exceptionally from its proximity to the large economies of Russia and China, resulting in a rapid development of railway ports along its borders, especially along the Chinese border, as the Chinese seek to direct coking coal from Mongolia to China's northwestern industrial core, and, as well as for transportation southeast towards Japan and South Korea, resulting in revenue generation through the seaport of Tianjin. Armenia The Republic of Armenia is a landlocked country with geographic disadvantages, and faces limitations on foreign policy options. It needs to transport its goods via coastal neighbors to access ports to participate in international trade, to which Azerbaijan and Turkey are hostile and deny its access. Therefore, Armenia mainly depends on the Georgian ports of Batumi and Poti and the Georgian railway system to participate in international trade. Armenia also shares a short border with neighboring Iran, through which it trades despite American sanctions. Armenia remains heavily dependent on imports",
    "label": 0
  },
  {
    "text": "on the Georgian ports of Batumi and Poti and the Georgian railway system to participate in international trade. Armenia also shares a short border with neighboring Iran, through which it trades despite American sanctions. Armenia remains heavily dependent on imports from Russia and exports of moderately unsophisticated goods to Russia. While Russia remained Armenia's dominant trade partner, trade with the EU in 2020 accounted for around 18% of Armenia's total trade. As of 2020, the European Union is Armenia's third-biggest export market, with a 17% share of total Armenian exports, and the second largest source of Armenian imports, with an 18.6% share in total Armenian imports. Nepal Nepal is a landlocked country with extreme dependency on its transit neighbour India. India does not have poor relations with Nepal, nor does it lack relevant transport infrastructure or internal stability. However, there have been two cases of economic blockades imposed by the government of India on Nepal – the official 1989 blockade and the unofficial 2015 blockade – both of which left the nation in severe economic crisis. In the 1970s, Nepal suffered from large commodity concentration and a high geographic centralization in its export trade: over 98% of its exports were to India, and 90% of its imports came from India. As a result of all this, Nepal had a poor trade bargaining position. In the 1950s, Nepal was forced to comply with India's external tariffs as well as the prices of India's exports. This was problematic since the two countries have different levels of development, resulting in greater gains for India which was larger, more advanced and with more resources. It was feared that a parasitic relationship might emerge, since India had a head start in industrialization, and dominated Nepal in manufacturing, which could reduce Nepal to being just a",
    "label": 0
  },
  {
    "text": "India which was larger, more advanced and with more resources. It was feared that a parasitic relationship might emerge, since India had a head start in industrialization, and dominated Nepal in manufacturing, which could reduce Nepal to being just a supplier of raw materials. Because of these problems, and Nepal's inability to develop its own infant industries (as it could not compete with Indian manufactures) treaties were drafted in 1960 and 1971, with amendments to the equal tariffs conditions, and terms of trade have since progressed. Almaty Ministerial Conference In August, 2003, the International Ministerial Conference of Landlocked and Transit Developing Countries and Donor Countries on Transit Transport Cooperation (Almaty Ministerial Conference) was held in Almaty, Kazakhstan, setting the necessities of LLDCs in a universal document whereas there were no coordinated efforts on the global scale to serve the unique needs of LLDCs in the past. Other than acknowledging the main forms of dependency that must be addressed, it also acknowledged the additional dependency issue where neighbouring transit countries are often observed to export the same products as their landlocked neighbours. One result of the conference was a direct call for donor countries to step in to direct aid into setting up suitable infrastructure of transit countries to alleviate the burden of supporting LLDCs in regions of poor development in general. The general objectives of the Almaty Program of Action is as follows: Reduce customs processes and fees to minimize costs and transport delays Improve infrastructure with respect to existing preferences of local transport modes, where road should be focused in Africa and rail in South Asia Implement preferences for landlocked countries’ commodities to boost their competitiveness in the international market To establish relationships between donor countries with landlocked and transit countries for technical, financial and policy improvements Third UN",
    "label": 0
  },
  {
    "text": "in Africa and rail in South Asia Implement preferences for landlocked countries’ commodities to boost their competitiveness in the international market To establish relationships between donor countries with landlocked and transit countries for technical, financial and policy improvements Third UN Conference on Landlocked Developing Countries In August 2025, the third UN Conference on Landlocked Developing Countries (LLDC3) was held in Awaza, Turkmenistan, with the agenda to strengthen transit, trade corridors, economic resilience and financing for landlocked developing countries. The Conference was postponed twice, having been originally scheduled to be held in August 2024 in Rwanda, and December 2024 in Gaborone, Botswana. The Programme of Action for Landlocked Developing Countries for the Decade 2024-2034 (renamed Awan Programme of Action) was adotped by the General Assmelby in December 2024. The Programme of Action identifies five priorities: promoting sustainable economic growth through innovation and structural transformation, enhancing regional trade integration, improving transport connectivity, building climate resilience, and ensuring effective implementation strategies. Current LLDCs Africa (16 countries) Botswana Burkina Faso Burundi Central African Republic Chad Eswatini Ethiopia Lesotho Malawi Mali Niger Rwanda South Sudan Uganda Zambia Zimbabwe Asia (12 countries) Afghanistan Armenia Azerbaijan Bhutan Kazakhstan Kyrgyzstan Laos Mongolia Nepal Tajikistan Turkmenistan Uzbekistan Europe (2 countries) Moldova North Macedonia South America (2 countries) Bolivia Paraguay See also Small Island Developing States References Notes Bibliography Bulag, U. E. (2010). Mongolia in 2009: From Landlocked to Land-linked Cosmopolitan. Asian Survey, 50(1), 97-103. Farra, F. (2012). OECD Presents... Unlocking Central Asia. Harvard International Review, 33(4), 76-79 Faye, M. L., McArthur, J. W., Sachs, J. D., & Snow, T. (2004). The Challenges Facing Landlocked Developing Countries. Journal of Human Development, 5(1), 31-68. Hagen, J. (2003). Trade Routes for Landlocked Countries. UN Chronicle, 40(4), 13-14. Jayaraman, T. K., Shrestha, O. L. (1976). Some Trade Problems of Landlocked Nepal. Asian Survey,",
    "label": 0
  },
  {
    "text": "Rank–size distribution is the distribution of size by rank, in decreasing order of size. For example, if a data set consists of items of sizes 5, 100, 5, and 8, the rank-size distribution is 100, 8, 5, 5 (ranks 1 through 4). This is also known as the rank–frequency distribution, when the source data are from a frequency distribution. These are particularly of interest when the data vary significantly in scales, such as city size or word frequency. These distributions frequently follow a power law distribution, or less well-known ones such as a stretched exponential function or parabolic fractal distribution, at least approximately for certain ranges of ranks; see below. A rank-size distribution is not a probability distribution or cumulative distribution function. Rather, it is a discrete form of a quantile function (inverse cumulative distribution) in reverse order, giving the size of the element at a given rank. Simple rank–size distributions In the case of city populations, the resulting distribution in a country, a region, or the world will be characterized by its largest city, with other cities decreasing in size respective to it, initially at a rapid rate and then more slowly. This results in a few large cities and a much larger number of cities orders of magnitude smaller. For example, a rank 3 city would have one-third the population of a country's largest city, a rank 4 city would have one-fourth the population of the largest city, and so on. Segmentation A rank-size (or rank–frequency) distribution is often segmented into ranges. This is frequently done somewhat arbitrarily or due to external factors, particularly for market segmentation, but can also be due to distinct behavior as rank varies. Most simply and commonly, a distribution may be split in two pieces, termed the head and tail. If a distribution",
    "label": 0
  },
  {
    "text": "or due to external factors, particularly for market segmentation, but can also be due to distinct behavior as rank varies. Most simply and commonly, a distribution may be split in two pieces, termed the head and tail. If a distribution is broken into three pieces, the third (middle) piece has several terms, generically middle, also belly, torso, and body. These frequently have some adjectives added, most significantly long tail, also fat belly, chunky middle, etc. In more traditional terms, these may be called top-tier, mid-tier, and bottom-tier. The relative sizes and weights of these segments (how many ranks in each segment, and what proportion of the total population is in a given segment) qualitatively characterize a distribution, analogously to the skewness or kurtosis of a probability distribution. Namely: is it dominated by a few top members (head-heavy, like profits in the recorded music industry), or is it dominated by many small members (tail-heavy, like internet search queries), or distributed in some other way? Practically, this determines strategy: where should attention be focused? These distinctions may be made for various reasons. For example, they may arise from differing properties of the population, as in the 90–9–1 principle, which posits that in an internet community, 90% of the participants of a community only view content, 9% of the participants edit content, and 1% of the participants actively create new content. As another example, in marketing, one may pragmatically consider the head as all members that receive personalized attention, such as personal phone calls; while the tail is everything else, which does not receive personalized attention, for example receiving form letters; and the line is simply set at a point that resources allow, or where it makes business sense to stop. Purely quantitatively, a conventional way of splitting a distribution into head and",
    "label": 0
  },
  {
    "text": "receive personalized attention, for example receiving form letters; and the line is simply set at a point that resources allow, or where it makes business sense to stop. Purely quantitatively, a conventional way of splitting a distribution into head and tail is to consider the head to be the first p portion of ranks, which account for 1 − p {\\displaystyle 1-p} of the overall population, as in the 80:20 Pareto principle, where the top 20% (head) comprises 80% of the overall population. The exact cutoff depends on the distribution – each distribution has a single such cutoff point—and for power, laws can be computed from the Pareto index. Segments may arise naturally due to actual changes in the behavior of the distribution as rank varies. Most common is the king effect, where the behavior of the top handful of items does not fit the pattern of the rest, as illustrated at the top for country populations, and above for most common words in English Wikipedia. For higher ranks, behavior may change at some point, and be well-modeled by different relations in different regions; on the whole by a piecewise function. For example, if two different power laws fit better in different regions, one can use a broken power law for the overall relation; the word frequency in English Wikipedia (above) also demonstrates this. The Yule–Simon distribution that results from preferential attachment (intuitively, \"the rich get richer\" and \"success breeds success\") simulates a broken power law and has been shown to \"very well capture\" word frequency versus rank distributions. It originated from trying to explain the population versus rank in different species. It has also been shown to fit city population versus rank better. Rank–size rule The rank-size rule (or law) describes the remarkable regularity in many phenomena, including the",
    "label": 0
  },
  {
    "text": "originated from trying to explain the population versus rank in different species. It has also been shown to fit city population versus rank better. Rank–size rule The rank-size rule (or law) describes the remarkable regularity in many phenomena, including the distribution of city sizes, the sizes of businesses, the sizes of particles (such as sand), the lengths of rivers, the frequencies of word usage, and wealth among individuals. All are real-world observations that follow power laws, such as Zipf's law, the Yule distribution, or the Pareto distribution. If one ranks the population size of cities in a given country or in the entire world and calculates the natural logarithm of the rank and of the city population, the resulting graph will show a linear pattern. This is the rank-size distribution. Known exceptions to simple rank–size distributions While Zipf's law works well in many cases, it tends to not fit the largest cities in many countries; one type of deviation is known as the King effect. A 2002 study found that Zipf's law was rejected in 53 of 73 countries, far more than would be expected based on random chance. The study also found that variations of the Pareto exponent are better explained by political variables than by economic geography variables like proxies for economies of scale or transportation costs. A 2004 study showed that Zipf's law did not work well for the five largest cities in six countries. In the richer countries, the distribution was flatter than predicted. For instance, in the United States, although its largest city, New York City, has more than twice the population of second-place Los Angeles, the two cities' metropolitan areas (also the two largest in the country) are much closer in population. In metropolitan-area population, New York City is only 1.3 times larger than",
    "label": 0
  },
  {
    "text": "City, has more than twice the population of second-place Los Angeles, the two cities' metropolitan areas (also the two largest in the country) are much closer in population. In metropolitan-area population, New York City is only 1.3 times larger than Los Angeles. In other countries, the largest city would dominate much more than expected. For instance, in the Democratic Republic of the Congo, the capital, Kinshasa, is more than eight times larger than the second-largest city, Lubumbashi. When considering the entire distribution of cities, including the smallest ones, the rank-size rule does not hold. Instead, the distribution is log-normal. This follows from Gibrat's law of proportionate growth. Because exceptions are so easy to find, the function of the rule for analyzing cities today is to compare the city systems in different countries. The rank-size rule is a common standard by which urban primacy is established. A distribution such as that in the United States or China does not exhibit a pattern of primacy, but countries with a dominant \"primate city\" clearly vary from the rank-size rule in the opposite manner. Therefore, the rule helps to classify national (or regional) city systems according to the degree of dominance exhibited by the largest city. Countries with a primate city, for example, have typically had a colonial history that accounts for that city pattern. If a normal city distribution pattern is expected to follow the rank-size rule (i.e. if the rank-size principle correlates with central place theory), then it suggests that those countries or regions with distributions that do not follow the rule have experienced some conditions that have altered the normal distribution pattern. For example, the presence of multiple regions within large nations such as China and the United States tends to favor a pattern in which more large cities appear than",
    "label": 0
  },
  {
    "text": "have experienced some conditions that have altered the normal distribution pattern. For example, the presence of multiple regions within large nations such as China and the United States tends to favor a pattern in which more large cities appear than would be predicted by the rule. By contrast, small countries that had been connected (e.g. colonially/economically) to much larger areas will exhibit a distribution in which the largest city is much larger than would fit the rule, compared with the other cities—the excessive size of the city theoretically stems from its connection with a larger system rather than the natural hierarchy that central place theory would predict within that one country or region alone. See also Pareto principle Long tail References Further reading Brakman, S.; Garretsen, H.; Van Marrewijk, C.; Van Den Berg, M. (1999). \"The Return of Zipf: Towards a Further Understanding of the Rank–Size Distribution\". Journal of Regional Science. 39 (1): 183–213. Bibcode:1999JRegS..39..183B. doi:10.1111/1467-9787.00129. S2CID 56011475. Guérin-Pace, F. (1995). \"Rank–Size Distribution and the Process of Urban Growth\". Urban Studies. 32 (3): 551–562. Bibcode:1995UrbSt..32..551G. doi:10.1080/00420989550012960. S2CID 154660734. Reed, W.J. (2001). \"The Pareto, Zipf and other power laws\". Economics Letters. 74 (1): 15–19. doi:10.1016/S0165-1765(01)00524-9. Douglas R. White, Laurent Tambayong, and Nataša Kejžar. 2008. Oscillatory dynamics of city-size distributions in world-historical systems. Globalization as an Evolutionary Process: Modeling Global Change. Ed. by George Modelski, Tessaleno Devezas, and William R. Thompson. London: Routledge. ISBN 978-0-415-77361-4 The Use of Agent-Based Models in Regional Science—an agent-based simulation study that explains rank–size distribution. External links Media related to Rank-size distribution at Wikimedia Commons",
    "label": 0
  },
  {
    "text": "Religion and geography is the study of the impact of geography, i.e. place and space, on religious belief. Another aspect of the relationship between religion and geography is religious geography, in which geographical ideas are influenced by religion, such as early map-making, and the biblical geography that developed in the 16th century to identify places from the Bible. Research traditions Traditionally, the relationship between geography and religion can clearly be seen by the influences of religion in shaping cosmological understandings of the world. From the 16th and 17th centuries, the study of geography and religion mainly focused on mapping the spread of Christianity (ecclesiastical geography), though in the later half of the 17th century, the influences and spread of other religions were also taken into account. Other traditional approaches to the study of the relationship between geography and religion involved the theological explorations of the workings of nature – a highly environmentally deterministic approach which identified the role of geographical environments in determining the nature and evolution of different religious traditions. Thus, geographers are less concerned about religion per se, but are more sensitive to how religion as a cultural feature affects social, cultural, political and environmental systems. The point of focus is not the specifics of religious beliefs and practices, but how these religious beliefs and practices are internalised by adherents, and how these processes of internalisation influence, and is influenced by, social systems. Sacred places Traditional cultural geographical approaches to the study of religion mainly seek to determine religion's impact on the landscape. A more contemporary approach to the study of the intersections of geography and religion not only highlights the role of religion in affecting landscape changes and in assigning sacred meanings to specific places, but also acknowledges how in turn, religious ideology and practice at specific",
    "label": 0
  },
  {
    "text": "the study of the intersections of geography and religion not only highlights the role of religion in affecting landscape changes and in assigning sacred meanings to specific places, but also acknowledges how in turn, religious ideology and practice at specific spaces are guided and transformed by their location. Religious experiences and the belief in religious meanings transforms physical spaces into sacred spaces. These perceptions and imaginings influence the way such spaces are used, and the personal, spiritual meanings developed in using such sacred spaces. These religiously significant spaces go beyond officially religious/spiritual spaces (such as places of worship) to include non-official religious spaces such as homes, schools and even bodies. These works have focused on both material aspects of spaces (such as architectural distinctiveness) and socially constructed spaces (such as rituals and demarcation of sacred spaces) to present religious meaning and significance. A key focus in the study of sacred places is the politics of identity, belonging and meaning that are ascribed to sacred sites, and the constant negotiations for power and legitimacy. Particularly in multicultural settings, the contestation for legitimacy, public approval, and negotiations for use of particular spaces are at the heart of determining how communities understand, internalise and struggle to compete for the right to practice their religious traditions in public spaces. Community and identity Religion may be a starting point to examine issues of ethnic identity formation and the construction of ethnic identity Geographers studying the negotiations of religious identity within various communities are often concerned with the overt articulation of religious identity, for example, how adherents in different locations establish their distinctive (religious and cultural) identities through their own understandings of the religion, and how they externally present their religious adherence (in terms of religious practice, ritual and behaviour). As an overarching theme, the articulation",
    "label": 0
  },
  {
    "text": "adherents in different locations establish their distinctive (religious and cultural) identities through their own understandings of the religion, and how they externally present their religious adherence (in terms of religious practice, ritual and behaviour). As an overarching theme, the articulation of religious identity is concerned with material aspects of symbolizing religious identity (such as architecture and the establishment of a physical presence), with negotiations and struggles in asserting religious identity in the face of persecution and exclusion and with personal practices of religious ritual and behaviour that re-establishes one's religious identity New geographies of religion As research on geography and religion has grown, one of the new focuses of geographical research examines the rise of religious fundamentalism, and the resulting impact this has on the geographical contexts in which it develops. In addition, migration processes have resulted in the development of religious pluralism in numerous countries, and the landscape changes that accompany the movement and settlement of communities defined by religion is a key focus in the study of geography and religion. More work needs to be done to examine the intersections and collisions that occur due to the movement of communities (for example, the migration of Muslim communities to western countries) and highlight how these communities negotiate their religious experiences in new spaces. Recent research in this area has been published by Barry A. Vann who analyzes Muslim population shifts in the Western world and the theological factors that play into these demographic trends. Another new area of interest in the study of geography and religion explores different sites of religious practice beyond the ‘officially sacred’ – sites such as religious schools, media spaces, banking and financial practices (for example, Islamic banking) and home spaces are just some of the different avenues that take into account informal, everyday spaces",
    "label": 0
  },
  {
    "text": "of religious practice beyond the ‘officially sacred’ – sites such as religious schools, media spaces, banking and financial practices (for example, Islamic banking) and home spaces are just some of the different avenues that take into account informal, everyday spaces that intersect with religious practice and meaning. Geopolitics The geographical distinctiveness of religion has also been discussed by scholars in the past. Environmental determinists developed racial justifications for the relationship between regional habitats, climates, and religions around the beginning of the 20th century. Geographical approaches to religion in the past have subsequently studied the intersections between history, place, and religion, condemning and rejecting these studies' simplistic emphasis on the influence of the environment on cultural behaviors. Many have evaluated how regional differences in religious doctrine, practice, and theologies are influenced by local social, cultural, and political settings. See also Ethnic religion Religion by country Religious demographics Notes References Douglas, H. Paul. (1926) 1000 City Churches Phases of Adaptation to Urban Environment; in United States. online free Archived 2017-03-12 at the Wayback Machine Knott, Kim (2005). The location of religion: a spatial analysis. Equinox Publishing Ltd. ISBN 9781904768753. Park, Chris (1994). Sacred worlds: an introduction to geography and religion. Routledge. ISBN 9780415090124. External links Media related to Geography of religion at Wikimedia Commons Maps of religious belief in the US",
    "label": 0
  },
  {
    "text": "The Small Island Developing States (SIDS) are a grouping of developing countries which are small island countries and small states that tend to share similar sustainable development challenges. These include small but growing populations, limited resources, remoteness, susceptibility to natural disasters, vulnerability to external shocks, excessive dependence on international trade, and fragile environments. Their growth and development are also held back by high communication, energy and transportation costs, irregular international transport volumes, disproportionately expensive public administration and infrastructure due to their small size, and little to no opportunity to create economies of scale. They consist of some of the most vulnerable countries to anthropogenic climate change. The SIDS were first recognized as a distinct group of developing countries at the United Nations Conference on Environment and Development in June 1992. The Barbados Programme of Action was produced in 1994 to assist the SIDS in their sustainable development efforts. The United Nations Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States (UN-OHRLLS) represents the group of states. List of SIDS As of 2023, the United Nations Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States (UN-OHRLLS) lists 57 such nations (39 sovereign states and 18 dependent territories). These nations are grouped into three geographical regions: the Caribbean; the Pacific; and Africa, Indian Ocean, Mediterranean and South China Sea (AIMS), including 18 Associate Members of the United Nations Regional Commissions. Each of these regions has a regional cooperation body: the Caribbean Community, the Pacific Islands Forum, and the Indian Ocean Commission respectively, which many SIDS are members or associate members of. In addition, most (but not all) SIDS are members of the Alliance of Small Island States (AOSIS), which performs lobbying and negotiating functions",
    "label": 0
  },
  {
    "text": "Islands Forum, and the Indian Ocean Commission respectively, which many SIDS are members or associate members of. In addition, most (but not all) SIDS are members of the Alliance of Small Island States (AOSIS), which performs lobbying and negotiating functions for the SIDS within the United Nations System. Notes Impacts of climate change The SIDS are some of the regions most vulnerable to anthropogenic climate change. Due to their oceanic environment, SIDS are especially vulnerable to the marine effects of climate change like sea level rise, ocean acidification, marine heatwaves, and the increase in cyclone intensity. Changing precipitation patterns could also cause droughts. Many citizens of SIDS live near a coastline, meaning that they have a high risk exposure to the effects of marine climate change. Additional climate change vulnerability comes through their economies: many SIDS have economies that are based on natural resources, such as ecotourism, fishing, or agriculture. Phenomena like sea level rise, coastal erosion, and severe storms have the potential to severely impact their economies. In addition to these vulnerabilities, the energy sector in Small Island Developing States faces unique challenges and opportunities, particularly in the least-electrified regions. According to the March 2024 IRENA report, \"Small Island Developing States at a Crossroads: Towards Equitable Energy Access in Least-Electrified Countries,\" these states, which contribute less than 1% to global greenhouse gas emissions, are exploring decentralized renewable energy solutions to address their energy access issues. Technologies such as biomass gasification, small-scale hydro, and solar PV not only offer paths to reduce their carbon footprint but also enhance resilience against climate impacts. The report highlights case studies from Guinea-Bissau, Papua New Guinea, and Vanuatu, showing significant socio-economic benefits from improved energy access, including increased income opportunities for rural women and reduced indoor air pollution. These advancements are crucial as they",
    "label": 0
  },
  {
    "text": "climate impacts. The report highlights case studies from Guinea-Bissau, Papua New Guinea, and Vanuatu, showing significant socio-economic benefits from improved energy access, including increased income opportunities for rural women and reduced indoor air pollution. These advancements are crucial as they not only provide immediate relief but also contribute to long-term sustainability and resilience against future climate challenges.\" Sustainable Development Goals Small island developing states are mentioned in several of the Sustainable Development Goals. For example, Target 7 of Sustainable Development Goal 14 (\"Life below Water\") states: \"By 2030, increase the economic benefits to small island developing States and least developed countries from the sustainable use of marine resources, including through sustainable management of fisheries, aquaculture and tourism\". Several Small Island Developing States (SIDS), including Palau and Orkney, have launched renewable energy initiatives to improve energy efficiency and reduce reliance on fossil fuels; Palau has set a target of transitioning to 45% renewable energy by 2025, while Orkney is expanding its use of tidal turbines to create a 50 MW array. Deep-sea mining Small Island Developing States (SIDS) are complex actors within the field of deep-sea mining. SIDS are small island communities that hold the status of developing countries due to their economic, political, and social limitations and challenges. Despite their small size and relatively limited political and economic capital, their geographic position means that they are often perceived as so-called ‘Large Ocean States’. This self-perception is rooted in the fact that many SIDS lay territorial claims to the vast areas of ocean surrounding their small islands and the resources that belong to these areas. Given the global distribution of seabed resources, it is particularly important that a substantial share of the raw materials and critical minerals found on the ocean floor lies within, or in close proximity to, the Exclusive",
    "label": 0
  },
  {
    "text": "belong to these areas. Given the global distribution of seabed resources, it is particularly important that a substantial share of the raw materials and critical minerals found on the ocean floor lies within, or in close proximity to, the Exclusive Economic Zones (EEZs) of many SIDS. This geographic advantage elevates these islands as increasingly important political and economic actors in the emerging deep-sea mining industry. Their positions, however, are also shaped by historical legacies, most notably the enduring impacts of slavery and colonialism, which in many cases resulted in the severe depletion of local resources and long-term constraints on wealth and development. As a result, many of these states feel a strong imperative to avoid once again being exploited for their natural resources, and they therefore seek to maximize their influence through participation in supranational organizations. SIDS and Deep-sea mining in the Clarion-Clipperton Zone There are 57 SIDS worldwide, according to the United Nations. Several of these states are geographically located near potentially lucrative deep-sea mining zones. One of the most promising of these areas is the Clarion-Clipperton Zone, whose resource potential has prompted a number of SIDS to initiate or explore early-stage deep-sea mining projects. According to economic assessments published by The Metals Company in 2025, mining polymetallic nodules in the Clarion-Clipperton Zone could potentially generate substantial economic returns. In its public release, the company states that two separate economic studies estimate a combined net present value in the high tens of billions of USD for the Clarion-Clipperton Zone as a whole, and approximately 18 billion USD for the areas sponsored by Nauru and Tonga. The assessments also suggest that extraction of the first minerals could take place as early as 2027. However, this projection should be interpreted with considerable caution. The Metals Company makes it clear that these",
    "label": 0
  },
  {
    "text": "areas sponsored by Nauru and Tonga. The assessments also suggest that extraction of the first minerals could take place as early as 2027. However, this projection should be interpreted with considerable caution. The Metals Company makes it clear that these assessments rely on a series of assumptions regarding technological feasibility, regulatory permissions, and future market prices. Moreover, researchers question whether deep sea minerals can be lifted to the surface in a commercially profitable way at all, and there is no scientific consensus on the economic viability of large scale extraction. According to the most recent assessment from 2025, the following 5 SIDS have entered into agreements or sponsorship arrangements related to early-stage deep-sea mining companies in the Clarion-Clipperton Zone: A total of five Small Island Developing States (SIDS) currently sponsor exploration activities in the Clarion-Clipperton Zone (CCZ): the Cook Islands, Kiribati, Nauru, Tonga, and Jamaica. Although all five states play a role in enabling access to seabed minerals in this region, they do so through very different institutional arrangements and with varying degrees of legal and financial exposure. The overview shows that the Cook Islands have adopted a comparatively state-led model through a joint-venture structure, which provides them with greater regulatory control and fewer investor constraints. In contrast, Nauru and Tonga rely on sponsorship agreements with subsidiaries of The Metals Company that include strong investor-protection mechanisms, potentially limiting their policy flexibility and increasing the risk of investor-tate disputes. Kiribati’s participation is organized through a state-owned enterprise whose contractual arrangements with DeepGreen were terminated in 2024, leaving the country formally present in the CCZ but without an active commercial partner. Jamaica, meanwhile, sponsors a subsidiary of Blue Minerals Switzerland and faces distinctive risks due to the absence of domestic deep-sea mining legislation combined with binding bilateral investment treaties. Opportunities for SIDS",
    "label": 0
  },
  {
    "text": "in the CCZ but without an active commercial partner. Jamaica, meanwhile, sponsors a subsidiary of Blue Minerals Switzerland and faces distinctive risks due to the absence of domestic deep-sea mining legislation combined with binding bilateral investment treaties. Opportunities for SIDS in deep-sea mining The technological and political developments in deep-sea mining mean that many SIDS suddenly gain access to resources on a scale previously unavailable. It is a well-known phenomenon that states can act as economic maximalists in the sense that a desire and tendency arise to claim areas as their own when there is an economic gain to be made. However, there is also a significant importance in the fact that technological development now promises to make it possible to extract these materials and resources. Therefore, it is predictable that in recent years we are seeing an increasing number of countries, including SIDS, claiming and protecting maritime areas and their resources. Since many of these SIDS possess potentially valuable Exclusive Economic Zones (EEZ), there is significant interest from both major countries and corporations in developing the production and extraction of these minerals and resources from the seabed. This interest means that many members of the Global South and developing states, such as SIDS, find themselves in a favorable negotiating position with the potential to boost their economies. In this context, they are brought into contact with multinational corporations and powerful states that show interest in their resources. For countries that have previously experienced limited resources and export opportunities, this is a situation that holds the possibility and potential to propel their nations toward a brighter future. However, there is not a single one of these SIDS that is strong enough to stand on its own and negotiate with either larger nations or multinational corporations alone. Therefore, it is also",
    "label": 0
  },
  {
    "text": "propel their nations toward a brighter future. However, there is not a single one of these SIDS that is strong enough to stand on its own and negotiate with either larger nations or multinational corporations alone. Therefore, it is also observed that, despite their limited landmass and economic capacity, SIDS are pivotal actors in the International Seabed Authority (ISA) and other supranational organisation, sponsoring exploration contracts, shaping governance debates, and reframing how the global commons are managed. Despite their relatively small landmasses, these countries have a strong interest in ensuring that the surrounding areas are not exploited or extracted without their involvement. These nations are deeply dependent on the potential of the seabed, as in many cases it constitutes the vast majority of their economic potential. Therefore, international cooperation is not just an option but a necessity for these states. A stronger ISA will make it easier for them to stand up to foreign powers and corporations that seek to profit at their expense. Through their engagement in the ISA, these states focus particularly on three articles in the United Nations Convention on the Law of the Sea (UNCLOS), the UN’s international law governing the oceans. A central dimension of the political agency of SIDS in deep-sea mining governance lies in their ability to draw on different parts of UNCLOS. Two distinct legal regimes are particularly significant: the regime governing The Area, which refers to the seabed beyond national jurisdiction, and the regime governing the EEZ. Together these regimes shape the formal authority, political leverage, and environmental responsibilities of SIDS within global ocean governance. The regime governing The Area is articulated most clearly in Articles 136, 137(2), and 145. Article 136 establishes that the deep seabed and its resources constitute the common heritage of mankind, a principle that underpins the",
    "label": 0
  },
  {
    "text": "within global ocean governance. The regime governing The Area is articulated most clearly in Articles 136, 137(2), and 145. Article 136 establishes that the deep seabed and its resources constitute the common heritage of mankind, a principle that underpins the ISA mandate to ensure collective benefit sharing. Article 137(2) specifies that no state may claim sovereignty or exclusive rights over resources in The Area, which creates a legal barrier against unilateral appropriation by powerful states and corporations. Article 145 requires the protection of the marine environment from harmful effects of mineral exploitation. Together these provisions create a governance architecture in which SIDS can appeal to international law to justify claims to participation, oversight, and equitable benefit distribution in deep-sea mining. They allow SIDS to argue that their lack of economic or technological capacity does not preclude them from having a voice in decisions that affect the global commons, reinforcing the broader Global South emphasis on equity and sovereignty. However, the legal position of SIDS does not depend solely on The Area. The second foundational regime is the EEZ, defined primarily through Articles 55 to 57, and especially Article 56. These provisions grant coastal states exclusive sovereign rights to explore, exploit, manage, and conserve natural resources within two hundred nautical miles of their shorelines. For SIDS, these zones are extraordinarily large relative to their land area. Some island states possess EEZs thousands of times larger than their terrestrial territory, a reality that has contributed to the political discourse of SIDS as Large Ocean States. The EEZ regime thus represents a major source of both economic potential and environmental responsibility, providing these states with a legally recognized spatial domain in which they can assert considerable authority. Although the EEZ regime and the regime of The Area operate under different legal principles, they",
    "label": 0
  },
  {
    "text": "source of both economic potential and environmental responsibility, providing these states with a legally recognized spatial domain in which they can assert considerable authority. Although the EEZ regime and the regime of The Area operate under different legal principles, they intersect in important political, ecological, and strategic ways. Deep-sea mining activities in The Area can have ecological spillover effects that reach well into national jurisdictions, particularly due to ecological connectivity across mid ocean ridges and abyssal plains. For SIDS, whose economies and food security often depend heavily on marine ecosystems, these spillovers heighten the importance of the environmental protections outlined in Article 145. At the same time, the principles embedded in Articles 136 and 137(2) support SIDS in making claims for increased transparency, environmental safeguards, financial compensation mechanisms, and technological capacity building within the ISA system, since these articles mandate that the benefits of the global commons are to be shared equitably. The combination of these two legal regimes therefore plays a crucial role in how SIDS navigate global ocean politics. Their EEZ rights provide them with concrete sovereign authority over vast ocean spaces, strengthening their position in regional fisheries, conservation policies, and blue economy initiatives. Meanwhile, their participation in The Area’s governance enables them to influence decisions about deep sea mining beyond their national borders and to articulate broader Global South concerns regarding justice, environmental protection, and postcolonial vulnerability. In this dual framework, SIDS emerge not merely as small or vulnerable states, but as states whose legal entitlements under UNCLOS grant them both territorial authority and normative leverage. Their EEZs anchor their identity as stewards of large ocean territories, while the principles governing The Area enable them to challenge inequitable distributions of power and benefit in the governance of the global commons. Together, these regimes form the basis for",
    "label": 0
  },
  {
    "text": "EEZs anchor their identity as stewards of large ocean territories, while the principles governing The Area enable them to challenge inequitable distributions of power and benefit in the governance of the global commons. Together, these regimes form the basis for the political strategies through which SIDS seek to shape the governance of deep-sea mining in ways that reflect their development priorities, environmental concerns, and long standing struggles for recognition within international law. In this sense, deep-sea mining has been presented as a tool to narrow global economic inequalities, offering resource-dependent island states a stake in an emerging industry otherwise dominated by developed economies. Challenges and risks associated with deep-sea mining While deep-sea mining holds the promise of substantial economic gains for SIDS, it is not without significant structural and legal risks. Most SIDS lack the technological capacity, financial resources, and institutional infrastructure to undertake deep-sea extraction independently. Under the legal framework of UNCLOS, participation in activities in The Area is restricted to states, state enterprises, or private entities that are formally sponsored by a state (UNCLOS Article 153(2)(b); Annex III, Article 4). As a consequence, some SIDS have established their own state-owned enterprises in order to act as the official sponsoring entities for private corporations. This has been the case for Nauru, Tonga, and Kiribati, which created Nauru Ocean Resources Inc., Tonga Offshore Mining Limited, and Marawa Research and Exploration Ltd., respectively, to meet ISA requirements for sponsorship. Because SIDS do not possess the capacity to conduct extraction themselves, these state-owned enterprises function primarily as legal intermediaries that enable partnerships with multinational corporations or larger states possessing the necessary expertise, capital, and equipment. The industry is already highly concentrated, dominated by a small number of corporate operators due to extremely high technological and financial barriers to entry. As a result,",
    "label": 0
  },
  {
    "text": "with multinational corporations or larger states possessing the necessary expertise, capital, and equipment. The industry is already highly concentrated, dominated by a small number of corporate operators due to extremely high technological and financial barriers to entry. As a result, when SIDS enter this sector, they often confront asymmetrical negotiations with a few highly powerful corporate actors. Their limited experience with large scale contract management or natural resource governance further weakens their bargaining position. This imbalance produces a considerable risk that SIDS may accept agreements that undervalue their natural resources, diminish their regulatory autonomy, or create long term dependencies that echo earlier patterns of colonial extraction. Their vulnerability is not only economic but also institutional. Postcolonial legacies, constrained administrative capacity, and fragile regulatory structures leave SIDS highly exposed to the influence of powerful corporate and state backed actors in this emerging industry. In this context, the legal requirement to sponsor deep-sea mining companies places a disproportionate burden on SIDS, who must assume the liabilities of sponsorship while lacking the leverage to shape outcomes on equal terms. SIDS are particularly vulnerable in negotiations due to the slow, fragmented, and politically contested development of common guidelines on deep-sea mining by the ISA. The ongoing negotiations over the Mining Code, which is still incomplete after more than a decade, create a regulatory gap in which key issues such as environmental thresholds, liability, benefit sharing, and enforcement mechanisms remain uncertain. This legal uncertainty disproportionately affects SIDS, who often lack the diplomatic capacity, in-house legal expertise, and scientific infrastructure needed to navigate such emerging governance regimes. In the absence of finalized ISA rules, SIDS are frequently left to negotiate bilateral contracts and joint ventures directly with multinational corporations, most of which have far greater financial resources, legal teams, and technical knowledge. This structural asymmetry increases",
    "label": 0
  },
  {
    "text": "regimes. In the absence of finalized ISA rules, SIDS are frequently left to negotiate bilateral contracts and joint ventures directly with multinational corporations, most of which have far greater financial resources, legal teams, and technical knowledge. This structural asymmetry increases the risk that SIDS may accept contractual terms that expose them to long-term liabilities or grant excessive influence to companies over national decision-making. Remittances Remittances are payments made in foreign countries, contributing to household income in home nations. It is particularly prevalent in SIDS as many workers see better income, migrating for work purposes. For example, in Haiti, many locals travel to countries such as the Dominican Republic and the United States for work. This is because Haiti's population has an average income of less than $2.50 a day. Which is below the poverty line of $3 a day set by the World Bank. When as there is an average daily salary of $31 and $250 in the Dominican Republic and the United States, respectively. This lack of available income is due to the dire economic, social, and political circumstances within the country. Mainly caused by the fact Haiti is on a Fault Line between the Caribbean plate and North American plate, so sees earthquakes occurring regularly, harming its infrastructure. Continually, politics in Haiti remains unstable, their president Jovenel Moïse was assassinated in 2021. And to this day, Haiti sees one of the highest corruption rates in the world. See also 2010 United Nations Climate Change Conference Organisation of African, Caribbean and Pacific States (OACPS) Group of 77 Islands First (environmental organization) Landlocked developing countries List of island countries Alliance of Small Island States References External links Media related to Small Island Developing States at Wikimedia Commons About SIDS, United Nations Office of the High Representative for the Least Developed",
    "label": 0
  },
  {
    "text": "Spatial citizenship describes the ability of individuals and groups to interact and participate in societal spatial decision making through the reflexive production and use of geo-media (geographic media such as maps, virtual globes, GIS, and the Geoweb). Spatial citizens are lay users who are able to use geo-media to question existing perspectives on action in space (e.g. social rules, spatial planning) and to produce, communicate, and negotiate alternative spatial visions. Spatial citizenship is an educational approach at the intersection of citizenship education and geography education. Its main theoretical reference points are emancipatory forms of citizenship and the \"reflexive appropriation of space\". Reference points in citizenship education Spatial citizenship can be distinguished from traditional citizenship education approaches in many respects: Spatial citizenship respects multiple institutional and locational setups. It is de-linked from traditional national \"citizenship\" as it is not tied to the nation or local state as predefined spatial entities. Spatial citizenship is based upon the principles of human rights and democratic negotiation to ensure a basis for conciliation and compromise. It is based on the concept of activist citizenship (in contrast to active citizenship) formulated by Katharyne Mitchell and Sarah Elwood, who challenge unquestioned social rules that limit participation. Spatial citizenship refers to an open and flexible conception of social institutions. It supplants the notion of belonging to one specific place with the notion of belonging to multiple and fluid communities that may or may not be linked with a particular location on the Earth's surface. Angharad Stephens and Vicki Squire have argued that societal negotiation processes have shifted away from fixed communities due to new information technologies. Spatial citizenship draws special attention to web communities and geo-social networks. A spatial citizen is a 'self-actualizing citizen' who, in contrast to the 'dutiful citizen' concept of the past, is able to",
    "label": 0
  },
  {
    "text": "from fixed communities due to new information technologies. Spatial citizenship draws special attention to web communities and geo-social networks. A spatial citizen is a 'self-actualizing citizen' who, in contrast to the 'dutiful citizen' concept of the past, is able to use Web 2.0 and cloud-based applications to compare different and potentially contradicting information sources and communicate his/her own alternative ideas via collaborative web tools. Reference points in social geography: appropriation of space Spatial citizenship has become a conceptual reference point in theories of action-oriented social geography and new cultural geography. These approaches contend that human beings constantly appropriate spaces, as they attach meanings to geographically located physical matter in order to prepare it for their own actions. In these theories, spaces are regarded as being socially constructed. To a large extent, the attachment of meaning works unconsciously, following socially accepted, mainstream categories and discourses. Meanings given to physical objects determine the actions deemed possible. For instance, a field of asphalt in a city centre might have multiple meanings: it may be interpreted as a parking area as well as a place for ball games, with both meanings competing for dominance. As soon as one meaning becomes superior, which is a result of social power relations, the other meaning may decline, become invisible, and eventually is not used anymore. The superiority of a specific meaning over another one might be supported by artifacts representing meanings attached, such as signs on buildings, structural modifications of the physical environment, or symbols and explanations of the socio-cultural significance of places and objects in spatial representations visualized via geo-media. A mature appropriation of space therefore includes the conscious attachment of meaning as well as awareness of meanings being attached to places by others. It includes a sensibility to the multitude of meanings transported and hidden",
    "label": 0
  },
  {
    "text": "representations visualized via geo-media. A mature appropriation of space therefore includes the conscious attachment of meaning as well as awareness of meanings being attached to places by others. It includes a sensibility to the multitude of meanings transported and hidden by a mainstream discourse. Keys to the mature appropriation of space are therefore the deconstruction of socially produced meanings, as well as the ability to communicate one's own, potentially contradictory meanings and negotiate them with others. This process is often mediated through the formation of a collective spatialized identity. Given that space – its uses and symbolic significance – is often the site of social and political struggle, it becomes the container for action while at the same time shaping the group development of a \"us\". In particular, when groups re-imagine public space for political usages, this expression of spatial citizenship is the outcome of a spatially informed collective identity. Spatial citizenship in the geoinformation society Geographic media (geo-media) are especially important for attaching meaning to places as they clearly connect location, information and visualization. In addition to this, geo-media represents mainly single meanings out of the many that are possible. Nowadays, geo-media have become more and more present in everyday life due to mobile computing in combination with Geoweb applications. For instance, maps on smart phones guide people in their everyday actions, but at the same time limit their opportunities for action by limiting the variety of potential meanings. Scholars of spatial citizenship understand geo-media as instruments of reflection and communication. Reflection on geo-media means reflecting on the limitations of previously given meanings by using theories of critical cartography to ask which aspects of potential relevance for a certain spatial problem or decision making are included and excluded in the given meanings. At the same time, self-reflection requires being",
    "label": 0
  },
  {
    "text": "of previously given meanings by using theories of critical cartography to ask which aspects of potential relevance for a certain spatial problem or decision making are included and excluded in the given meanings. At the same time, self-reflection requires being aware of one's own, subjective hypothesis construction while using geo-media. Both aspects allow for a more mature appropriation of space with geo-media, while being aware of and gaining insight into the construction process of meanings attached to space. Communication with geo-media means communicating using spatial representations. Thanks to user-friendly web mapping tools within the Geoweb, users (prosumers) can easily produce their own geo-media and share it web-wide. Collaboration features allow for the negotiation of constructions of space with other users, for example through volunteered geographic information (VGI). The study of spatial citizenship examines subjectivity, impact on everyday action, social power relations, competition, and negotiation in VGI. Spatial citizenship also aims at increasing awareness of geographic information produced involuntarily by users through automatic data collection that many Geoweb platforms (especially mobile phones) include to support the interests of service providers (see location-based services). Education The goal of education for spatial citizenship is to enable learners to achieve a reflexive appropriation of space as the basis for mature action in space by reflective geo-media use and active, reflective geo-media production. Using a broad variety of learning environments orientated toward the learners' needs, the educational approach of spatial citizenship is applicable at different levels from primary to tertiary education. Apart from technological proficiency, spatial citizenship education aims at two additional main competencies: Being able to achieve a reflective use of geo-media, while understanding the process of the social construction of spaces, with the result of either consciously accepting given meanings or producing alternative meanings. Being able to communicate alternative meanings effectively with geo-media",
    "label": 0
  },
  {
    "text": "able to achieve a reflective use of geo-media, while understanding the process of the social construction of spaces, with the result of either consciously accepting given meanings or producing alternative meanings. Being able to communicate alternative meanings effectively with geo-media and using geo-media as instruments to support argumentation in negotiation processes on contradictory meanings. Scientific response The European Commission-funded project SPACIT furthers education for spatial citizenship by developing teacher training standards, curricula, and learning modules for teacher education. Another EU-funded project, digital-earth.eu, linked with the SPACIT project by connecting stakeholders using or interested in using geo-media in education. It supported spatial citizenship through the creation of educational standards, the collection of best-practice examples, and the provision of learning environments applicable to teachers in everyday classroom situations. Digital-earth.eu also promoted these concepts related to spatial citizenship in political circles concerned with the development of the Europe2020 goals. References Further reading Atzmanstorfer, Karl; Resl, Richard; Eitzinger, Anton; Izurieta, Xiomara (May 2014). \"The GeoCitizen-approach: community-based spatial planning – an Ecuadorian case study\". Cartography and Geographic Information Science. 41 (3): 248–259. Bibcode:2014CGISc..41..248A. doi:10.1080/15230406.2014.890546. PMC 4786845. PMID 27019644. Gryl, Inga; Schulze, Uwe; Kanwischer, Detlef (2013). \"Spatial citizenship: the concept of competence\". In Jekel, Thomas; Car, Adrijana; Strobl, Josef; Griesebner, Gerald (eds.). Creating the GISociety: conference proceedings. Berlin; Offenbach: Herbert Wichmann Verlag. pp. 282–293. ISBN 9783879075324. OCLC 854921245. Sbicca, Joshua; Perdue, Robert Todd (July 2014). \"Protest through presence: spatial citizenship and identity formation in contestations of neoliberal crises\" (PDF). Social Movement Studies. 13 (3): 309–327. doi:10.1080/14742837.2013.822782. S2CID 143659468.",
    "label": 0
  },
  {
    "text": "Spatial justice is a concept that links the principles of social justice to the spatial organisation of society. It examines how power, resources, rights, and opportunities are distributed across space, and how these spatial arrangements reflect, reproduce, or challenge structural inequalities. While the idea has deep roots in political philosophy and planning thought, it gained conceptual traction in the 1970s through the work of critical geographers, particularly David Harvey and Edward W. Soja. Harvey (1973) argued that urban space is both shaped by and productive of social relations, particularly under capitalism, where processes of uneven development and spatial segregation reflect broader patterns of economic and social injustice. Soja (2010) later expanded this framework, proposing spatial justice as a distinctive analytical category, emphasising that space is not merely a backdrop for social processes but an active medium through which justice is negotiated, contested, and potentially realised. More recently, spatial justice has been developed as a multi-dimensional framework encompassing distributive, procedural, and recognitional dimensions. The distributive dimension concerns the fair allocation of spatial resources—such as housing, transport, and green space—while the procedural dimension focuses on the inclusiveness and transparency of decision-making processes that shape space. The recognitional dimension, drawing on the work of Nancy Fraser and Iris Marion Young, addresses the visibility, dignity, and representation of marginalised identities in spatial governance. Spatial justice is a core concern of the academic tradition known as critical geography, which interrogates the spatial manifestations of power and inequality. It also plays an increasingly prominent role in fields such as spatial planning, regional development, environmental justice, and housing policy. In practice, spatial justice frameworks are used to assess and guide policy and interventions that seek to make cities and regions more equitable, inclusive, and democratic. Redistributive, procedural and recognitional dimensions of spatial justice Building on the work",
    "label": 0
  },
  {
    "text": "housing policy. In practice, spatial justice frameworks are used to assess and guide policy and interventions that seek to make cities and regions more equitable, inclusive, and democratic. Redistributive, procedural and recognitional dimensions of spatial justice Building on the work of several influential theorists of justice, notably John Rawls (1971), Nancy Fraser (1999, 2000, 2010) and Iris Marion Young (1990, 2000), contemporary understandings of spatial justice integrate three mutually-constitutive dimensions: redistributive, procedural and recognitional. The redistributive dimension concerns the equitable spatial distribution of material benefits and burdens—such as housing, transport, green spaces, and public services—within cities and communities. It involves assessing the geography of access to essential resources that support the reproduction of social life and wellbeing. This perspective emphasises how spatial arrangements can perpetuate or mitigate socio-economic inequalities. The procedural dimension focuses on the fairness of decision-making processes that shape spatial outcomes. This includes questions such as: who participates in planning and policy-making? Whose voices are heard or silenced? These concerns have been strongly influenced by the work of Nancy Fraser, particularly her theory of participatory parity, which argues that justice requires institutional arrangements that enable all social actors to interact as peers in decision-making processes. By combining both dimensions, spatial justice offers a framework that addresses not only the outcomes of urban development, but also the democratic legitimacy of the processes that produce them. Nancy Fraser has further expanded the understanding of spatial justice by incorporating a third dimension: recognitional justice. According to Fraser, participatory parity—the condition in which all individuals can interact as peers in social life—requires more than equitable distribution and fair procedures. It also demands the recognition of diverse identities, histories, and lived experiences that have been structurally devalued or rendered invisible in dominant planning and governance frameworks. From this perspective, redistribution without recognition can",
    "label": 0
  },
  {
    "text": "more than equitable distribution and fair procedures. It also demands the recognition of diverse identities, histories, and lived experiences that have been structurally devalued or rendered invisible in dominant planning and governance frameworks. From this perspective, redistribution without recognition can reinforce cultural hierarchies by ignoring the specific ways in which injustice is experienced. Conversely, recognition without redistribution risks reducing justice to symbolic gestures or identity affirmation, without addressing the material inequalities that underlie exclusion. Therefore, Fraser argues for an integrated approach that links economic, cultural, and political dimensions of justice. Distributive Dimensions of Spatial Justice A first approach to spatial justice focuses on the distribution of resources, services, and environmental conditions across space. This distributive perspective evaluates justice by examining how material and immaterial goods—such as housing, employment opportunities, clean air, green space, and healthcare—are unevenly distributed across urban and regional geographies. These spatial patterns often reflect deeper social and economic inequalities, shaped by historical processes of segregation, disinvestment, and exclusion. Distributive spatial injustices are particularly acute in contexts where marginalised groups lack the means to relocate or influence spatial decision-making, due to poverty, systemic discrimination, or political coercion. For example, under South Africa's apartheid regime, pass laws were used to enforce spatial segregation and restrict the mobility of Black populations. However, spatial injustices are not confined to authoritarian regimes. In democratic societies, structural barriers can also limit access to urban resources and public space. Geographer Don Mitchell has shown how the privatisation of once-public urban land—such as parks, plazas, and sidewalks—can result in the exclusion of unhoused people and other vulnerable populations, effectively removing their right to the city. At the urban scale, issues such as accessibility, walkability, and transport equity are increasingly recognised as matters of spatial justice. The ability to move freely, access essential services, and participate",
    "label": 0
  },
  {
    "text": "other vulnerable populations, effectively removing their right to the city. At the urban scale, issues such as accessibility, walkability, and transport equity are increasingly recognised as matters of spatial justice. The ability to move freely, access essential services, and participate in social life depends not only on the physical layout of the city but also on the underlying social and economic structures that determine who benefits from urban development and who is excluded. Procedural Dimensions of Spatial Justice Another key dimension of spatial justice focuses on the decision-making processes that shape urban and regional space. This procedural approach examines how spatial decisions are made, who participates in them, and how power circulates through institutions, governance structures, and design practices. Procedural spatial justice calls for inclusive, transparent, and accountable planning processes that enable all affected communities—particularly those historically marginalised—to meaningfully participate in shaping their environments. This perspective also raises questions about the representation of space, as well as the role of territorial and cultural identities in shaping spatial practices and claims. Analysing how minority groups experience, contest, or are excluded from spatial governance can reveal structural injustices that a universalist or technocratic approach might otherwise obscure. Architect and urbanist Liz Ogbu has argued that spatial justice requires planners and designers to “engage people who don’t have a seat at the table and think about them as co-designers in the process.” This view aligns with urban scholar Faranak Miraftab's distinction between invited and invented spaces of participation. While the former refers to formal opportunities structured by institutions, the latter emerge from grassroots struggles to reclaim agency and voice in spatial decision-making. Recognitional Dimensions of Spatial Justice A third key dimension of spatial justice concerns the politics of recognition. While distributive justice focuses on the allocation of material goods and procedural justice on",
    "label": 0
  },
  {
    "text": "struggles to reclaim agency and voice in spatial decision-making. Recognitional Dimensions of Spatial Justice A third key dimension of spatial justice concerns the politics of recognition. While distributive justice focuses on the allocation of material goods and procedural justice on participatory processes, recognitional spatial justice examines how social identities, cultural practices, and group-based experiences are acknowledged—or misrecognised—within spatial planning and governance frameworks. Philosopher Nancy Fraser has argued that full justice requires more than economic redistribution or procedural inclusion; it also demands cultural recognition, especially of those whose identities have been historically devalued or stigmatised in public institutions. In Fraser’s framework, misrecognition is not simply a psychological injury, but a form of institutionalised subordination that can impede one’s ability to participate as a peer in social life—a concept she calls participatory parity. In the spatial domain, misrecognition manifests when the lived experiences, spatial practices, or territorial claims of minoritised groups—such as Indigenous peoples, informal settlement dwellers, or racialised communities—are rendered invisible, stereotyped, or pathologised in planning processes and urban policy. This can occur through cultural erasure, displacement, or the imposition of spatial norms that privilege dominant social groups. Political theorist Iris Marion Young similarly argued that justice requires attending to the specificity of group-based oppression and to the ways in which spatial relations are implicated in the reproduction of domination. From this perspective, a purely universalist model of spatial planning—one that treats all subjects as formally equal without regard for historical injustice or cultural specificity—can perpetuate spatial injustices by failing to account for the differentiated needs, voices, and rights of affected groups. Recognitional spatial justice thus calls for an ethic of pluralism, historical awareness, and spatial sensitivity. This involves designing policies and spaces that acknowledge the presence and dignity of diverse groups, engage with their epistemologies and practices, and redress symbolic",
    "label": 0
  },
  {
    "text": "groups. Recognitional spatial justice thus calls for an ethic of pluralism, historical awareness, and spatial sensitivity. This involves designing policies and spaces that acknowledge the presence and dignity of diverse groups, engage with their epistemologies and practices, and redress symbolic and material exclusions. In this view, recognition is not merely a matter of respect or inclusion, but a political act that reshapes the spatial ordering of society in more just and democratic ways. See also Hermann Knoflacher Transport Justice References Bibliography Key Articles and Journals Brawley, Lisa (2009). « The Practice of Spatial Justice in Crisis », Justice Spatiale - Spatial Justice, n°01, September. Bret, Bernard (2009). « Rawlsian Universalism Confronted with the Diversity of Reality », Justice Spatiale - Spatial Justice, n°01, September. Coll. (2007). \"Spatial Justice\", Critical Planning, Vol. 14, Summer. Coll. (2009). « Justice spatiale », Annales de Géographie, n°665–666, Jan–April. Dikeç, Mustafa (2009). « Space, Politics and (In)justice », Justice Spatiale - Spatial Justice, n°01, September. Fainstein, Susan S. (2009). « Spatial Justice and Planning », Justice Spatiale - Spatial Justice, n°01, September. Fiedler, Johannes; Humann, Melanie; & Kölke, Manuela (2012). \"Radical Standard for the Implementation of Spatial Justice in Urban Planning and Design\". Centre for Gender Studies, TU Braunschweig. Marcuse, Peter (2009). « Spatial Justice: Derivative but Causal of Social Injustice », Justice Spatiale - Spatial Justice, n°01, September. Soja, Edward W. (2009). « The City and Spatial Justice », Justice Spatiale - Spatial Justice, n°01, September. Foundational and Theoretical Works Fraser, N. (1990). Rethinking the Public Sphere: A Contribution to the Critique of Actually Existing Democracy. Social Text 25-26(1): 56-80. Fraser, Nancy (2000). Rethinking Recognition. In Osborne, Peter (Ed.), Recognition and Power: Axel Honneth and the Tradition of Critical Social Theory, Routledge, pp. 107–120. Fraser, Nancy (2003). Social Justice in the Age of Identity",
    "label": 0
  },
  {
    "text": "Existing Democracy. Social Text 25-26(1): 56-80. Fraser, Nancy (2000). Rethinking Recognition. In Osborne, Peter (Ed.), Recognition and Power: Axel Honneth and the Tradition of Critical Social Theory, Routledge, pp. 107–120. Fraser, Nancy (2003). Social Justice in the Age of Identity Politics: Redistribution, Recognition, and Participation. In Fraser, N. & Honneth, A. (Eds.), Redistribution or Recognition? London: Verso. Fraser, N. (2010). Scales of Justice: Reimagining Political Space in a Globalizing World, Columbia University Press. Harvey, David (1973). Social Justice and the City. London: Edward Arnold. Harvey, David (1992). \"Social Justice, Postmodernism and the City\". International Journal of Urban and Regional Research, 16(4), pp. 588–601. Lefebvre, Henri (1968). Le Droit à la ville. Paris: Anthropos. Lefebvre, Henri (1972). Espace et politique. Paris: Anthropos. Miraftab, Faranak (2009). \"Insurgent Planning: Situating Radical Planning in the Global South\". Planning Theory, 8(1), pp. 32–50. https://doi.org/10.1177/1473095208099297 Mitchell, Don (2003). The Right to the City: Social Justice and the Fight for Public Space. New York: Guilford Press. Pirie, Gordon (1983). \"On Spatial Justice\". Environment and Planning A, 15, pp. 465–473. Rawls, John (1971). A Theory of Justice. Cambridge: Harvard University Press. Reynaud, Alain (1981). Société, espace et justice. Paris: PUF. Rocco, Roberto (2026). Spatial Justice: The Basics. London: Routledge. Smith, D. M. (1994). Geography and Social Justice. Oxford: Blackwell. Soja, Edward W. (2000). Postmetropolis: Critical Studies of Cities and Regions. Oxford: Blackwell. Soja, Edward W. (2010). Seeking Spatial Justice. Minneapolis: University of Minnesota Press. Young, Iris Marion (1990). Justice and the Politics of Difference. Princeton: Princeton University Press. Young, Iris Marion (2000). Inclusion and Democracy. Oxford: Oxford University Press. Ogbu, Liz (2018). \"Social Impact Design as a New Practice\". In Bell, Susan & Wakeford, Ralph (Eds.), Listening to the City, Routledge, pp. 65–78. External links What Makes Justice Spatial? What Makes Spaces Just? / Three Interviews on the",
    "label": 0
  },
  {
    "text": "Literature is any collection of written work, but it is also used more narrowly for writings specifically considered to be an art form, especially novels, plays, and poems. It includes both print and digital writing. In recent centuries, the definition has expanded to include oral literature, much of which has been transcribed. Literature is a method of recording, preserving, and transmitting knowledge and entertainment. It can also have a social, psychological, spiritual, or political role. Literary criticism is one of the oldest academic disciplines, and is concerned with the literary merit or intellectual significance of specific texts. The study of books and other texts as artifacts or traditions is instead encompassed by textual criticism or the history of the book. \"Literature\", as an art form, is sometimes used synonymously with literary fiction, fiction written with the goal of artistic merit, but can also include works in various non-fiction genres, such as biography, diaries, memoirs, letters, and essays. Within this broader definition, literature includes non-fictional books, articles, or other written information on a particular subject. Developments in print technology have allowed an ever-growing distribution and proliferation of written works, while the digital era has blurred the lines between online electronic literature and other forms of modern media. Definitions Definitions of literature have varied over time. In Western Europe, prior to the 18th century, literature denoted all books and writing. It can be seen as returning to older, more inclusive notions, so that cultural studies, for instance, include, in addition to canonical works, popular and minority genres. The word is also used in reference to non-written works: to \"oral literature\" and \"the literature of preliterate culture\". Etymologically, the term derives from Latin literatura/litteratura, \"learning, writing, grammar,\" originally \"writing formed with letters,\" from litera/littera, \"letter.\" In spite of this, the term has also",
    "label": 0
  },
  {
    "text": "in reference to non-written works: to \"oral literature\" and \"the literature of preliterate culture\". Etymologically, the term derives from Latin literatura/litteratura, \"learning, writing, grammar,\" originally \"writing formed with letters,\" from litera/littera, \"letter.\" In spite of this, the term has also been applied to spoken or sung texts. Literature is often referred to synecdochically as \"writing,\" especially creative writing, and poetically as \"the craft of writing\" (or simply \"the craft\"). Syd Field described his discipline, screenwriting, as \"a craft that occasionally rises to the level of art.\" A value judgment definition of literature considers it as consisting solely of high quality writing that forms part of the belles-lettres (\"fine writing\") tradition. An example of this is in the 1910–1911 Encyclopædia Britannica, which classified literature as \"the best expression of the best thought reduced to writing\". History Oral literature The use of the term \"literature\" here poses some issues due to its origins in the Latin littera, \"letter,\" essentially writing. Alternatives such as \"oral forms\" and \"oral genres\" have been suggested, but the word literature is widely used. Australian Aboriginal culture has thrived on oral traditions and oral histories passed down through tens of thousands of years. In a study published in February 2020, new evidence showed that both Budj Bim and Tower Hill volcanoes erupted between 34,000 and 40,000 years ago. Significantly, this is a \"minimum age constraint for human presence in Victoria\", and also could be interpreted as evidence for the oral histories of the Gunditjmara people, an Aboriginal Australian people of south-western Victoria, which tell of volcanic eruptions being some of the oldest oral traditions in existence. An axe found underneath volcanic ash in 1947 had already proven that humans inhabited the region before the eruption of Tower Hill. Oral literature is an ancient human tradition found in \"all",
    "label": 0
  },
  {
    "text": "some of the oldest oral traditions in existence. An axe found underneath volcanic ash in 1947 had already proven that humans inhabited the region before the eruption of Tower Hill. Oral literature is an ancient human tradition found in \"all corners of the world.\" Modern archaeology has been unveiling evidence of the human efforts to preserve and transmit arts and knowledge that depended completely or partially on an oral tradition, across various cultures: The Judeo-Christian Bible reveals its oral traditional roots; medieval European manuscripts are penned by performing scribes; geometric vases from archaic Greece mirror Homer's oral style. (...) Indeed, if these final decades of the millennium have taught us anything, it must be that oral tradition never was the other we accused it of being; it never was the primitive, preliminary technology of communication we thought it to be. Rather, if the whole truth is told, oral tradition stands out as the single most dominant communicative technology of our species as both a historical fact and, in many areas still, a contemporary reality. The earliest poetry is believed to have been recited or sung, employed as a way of remembering history, genealogy, and law. In Asia, the transmission of folklore, mythologies as well as scriptures in ancient India, in different Indian religions, was by oral tradition, preserved with precision with the help of elaborate mnemonic techniques. The early Buddhist texts are also generally believed to be of oral tradition, with the first by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbia and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down. According to Goody, the Vedic texts likely involved both a written and",
    "label": 0
  },
  {
    "text": "Serbia and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down. According to Goody, the Vedic texts likely involved both a written and oral tradition, calling it a \"parallel products of a literate society\". All ancient Greek literature was to some degree oral in nature, and the earliest literature was completely so. Homer's epic poetry, states Michael Gagarin, was largely composed, performed and transmitted orally. As folklores and legends were performed in front of distant audiences, the singers would substitute the names in the stories with local characters or rulers to give the stories a local flavor and thus connect with the audience by making the historicity embedded in the oral tradition as unreliable. The lack of surviving texts about the Greek and Roman religious traditions have led scholars to presume that these were ritualistic and transmitted as oral traditions, but some scholars disagree that the complex rituals in the ancient Greek and Roman civilizations were an exclusive product of an oral tradition. Writing systems are not known to have existed among Native North Americans (north of Mesoamerica) before contact with Europeans. Oral storytelling traditions flourished in a context without the use of writing to record and preserve history, scientific knowledge, and social practices. While some stories were told for amusement and leisure, most functioned as practical lessons from tribal experience applied to immediate moral, social, psychological, and environmental issues. Stories fuse fictional, supernatural, or otherwise exaggerated characters and circumstances with real emotions and morals as a means of teaching. Plots often reflect real life situations and may be aimed at particular people known by the story's audience. In this way, social pressure could be exerted without directly causing embarrassment or social exclusion.",
    "label": 0
  },
  {
    "text": "and morals as a means of teaching. Plots often reflect real life situations and may be aimed at particular people known by the story's audience. In this way, social pressure could be exerted without directly causing embarrassment or social exclusion. For example, rather than yelling, Inuit parents might deter their children from wandering too close to the water's edge by telling a story about a sea monster with a pouch for children within its reach. The enduring significance of oral traditions is underscored in a systemic literature review on indigenous languages in South Africa, within the framework of contemporary linguistic challenges. Oral literature is crucial for cultural preservation, linguistic diversity, and social justice, as evidenced by the postcolonial struggles and ongoing initiatives to safeguard and promote South African indigenous languages. Oratory Oratory or the art of public speaking was considered a literary art for a significant period of time. From ancient Greece to the late 19th century, rhetoric played a central role in Western education in training orators, lawyers, counselors, historians, statesmen, and poets. Writing Around the 4th millennium BC, the complexity of trade and administration in Mesopotamia outgrew human memory, and writing became a more dependable method of recording and presenting transactions in a permanent form. Though in both ancient Egypt and Mesoamerica, writing may have already emerged because of the need to record historical and environmental events. Subsequent innovations included more uniform, predictable legal systems, sacred texts, and the origins of modern practices of scientific inquiry and knowledge-consolidation, all largely reliant on portable and easily reproducible forms of writing. Early written literature Ancient Egyptian literature, along with Sumerian literature, are considered the world's oldest literatures. The primary genres of the literature of ancient Egypt—didactic texts, hymns and prayers, and tales—were written almost entirely in verse; By the Old",
    "label": 0
  },
  {
    "text": "writing. Early written literature Ancient Egyptian literature, along with Sumerian literature, are considered the world's oldest literatures. The primary genres of the literature of ancient Egypt—didactic texts, hymns and prayers, and tales—were written almost entirely in verse; By the Old Kingdom (26th century BC to 22nd century BC), literary works included funerary texts, epistles and letters, hymns and poems, and commemorative autobiographical texts recounting the careers of prominent administrative officials. It was not until the early Middle Kingdom (21st century BC to 17th century BC) that a narrative Egyptian literature was created. Many works of early periods, even in narrative form, had a covert moral or didactic purpose, such as the Sanskrit Panchatantra (200 BC – 300 AD), based on older oral tradition. Drama and satire also developed as urban cultures, which provided a larger public audience, and later readership for literary production. Lyric poetry (as opposed to epic poetry) was often the speciality of courts and aristocratic circles, particularly in East Asia where songs were collected by the Chinese aristocracy as poems, the most notable being the Shijing or Book of Songs (1046–c. 600 BC). In ancient China, early literature was primarily focused on philosophy, historiography, military science, agriculture, and poetry. China, the origin of modern paper making and woodblock printing, produced the world's first print cultures. Much of Chinese literature originates with the Hundred Schools of Thought period that occurred during the Eastern Zhou dynasty (769‒269 BC). The most important of these include the Classics of Confucianism, of Daoism, of Mohism, of Legalism, as well as works of military science (e.g. Sun Tzu's The Art of War, c. 5th century BC) and Chinese history (e.g. Sima Qian's Records of the Grand Historian, c. 94 BC). Ancient Chinese literature had a heavy emphasis on historiography, with often very detailed",
    "label": 0
  },
  {
    "text": "military science (e.g. Sun Tzu's The Art of War, c. 5th century BC) and Chinese history (e.g. Sima Qian's Records of the Grand Historian, c. 94 BC). Ancient Chinese literature had a heavy emphasis on historiography, with often very detailed court records. An exemplary piece of narrative history of ancient China was the Zuo Zhuan, which was compiled no later than 389 BC, and attributed to the blind 5th-century BC historian Zuo Qiuming. In ancient India, literature originated from stories that were originally orally transmitted. Early genres included drama, fables, sutras and epic poetry. Sanskrit literature begins with the Vedas, dating back to 1500–1000 BC, and continues with the Sanskrit Epics of Iron Age India. The Vedas are among the oldest sacred texts. The Samhitas (vedic collections) date to roughly 1500–1000 BC, and the \"circum-Vedic\" texts, as well as the redaction of the Samhitas, date to c. 1000‒500 BC, resulting in a Vedic period, spanning the mid-2nd to mid-1st millennium BC, or the Late Bronze Age and the Iron Age. The period between approximately the 6th to 1st centuries BC saw the composition and redaction of the two most influential Indian epics, the Mahabharata and the Ramayana, with subsequent redaction progressing down to the 4th century AD such as Ramcharitmanas. The earliest known Greek writings are Mycenaean (c. 1600–1100 BC), written in the Linear B syllabary on clay tablets. These documents contain prosaic records largely concerned with trade (lists, inventories, receipts, etc.); no real literature has been discovered. Michael Ventris and John Chadwick, the original decipherers of Linear B, state that literature almost certainly existed in Mycenaean Greece, but it was either not written down or, if it was, it was on parchment or wooden tablets, which did not survive the destruction of the Mycenaean palaces in the twelfth century",
    "label": 0
  },
  {
    "text": "that literature almost certainly existed in Mycenaean Greece, but it was either not written down or, if it was, it was on parchment or wooden tablets, which did not survive the destruction of the Mycenaean palaces in the twelfth century BC. Homer's epic poems, the Iliad and the Odyssey, are central works of ancient Greek literature. It is generally accepted that the poems were composed at some point around the late eighth or early seventh century BC. Modern scholars consider these accounts legendary. Most researchers believe that the poems were originally transmitted orally. From antiquity until the present day, the influence of Homeric epic on Western civilization has been significant, inspiring many of its most famous works of literature, music, art and film. The Homeric epics were the greatest influence on ancient Greek culture and education; to Plato, Homer was simply the one who \"has taught Greece\" – ten Hellada pepaideuken. Hesiod's Works and Days (c.700 BC) and Theogony are some of the earliest and most influential works of ancient Greek literature. Classical Greek genres included philosophy, poetry, historiography, comedies and dramas. Plato (428/427 or 424/423 – 348/347 BC) and Aristotle (384–322 BC) authored philosophical texts that are regarded as the foundation of Western philosophy, Sappho (c. 630 – c. 570 BC) and Pindar were influential lyric poets, and Herodotus (c. 484 – c. 425 BC) and Thucydides were early Greek historians. Although drama was popular in ancient Greece, of the hundreds of tragedies written and performed during the classical age, only a limited number of plays by three authors still exist: Aeschylus, Sophocles, and Euripides. The plays of Aristophanes (c. 446 – c. 386 BC) provide the only real examples of a genre of comic drama known as Old Comedy, the earliest form of Greek Comedy, and are in",
    "label": 0
  },
  {
    "text": "still exist: Aeschylus, Sophocles, and Euripides. The plays of Aristophanes (c. 446 – c. 386 BC) provide the only real examples of a genre of comic drama known as Old Comedy, the earliest form of Greek Comedy, and are in fact used to define the genre. The Hebrew religious text, the Torah, is widely seen as a product of the Persian period (539–333 BC, probably 450–350 BC). This consensus echoes a traditional Jewish view which gives Ezra, the leader of the Jewish community on its return from Babylon, a pivotal role in its promulgation. This represents a major source of Christianity's Bible, which has had a major influence on Western literature. The beginning of Roman literature dates to 240 BC, when a Roman audience saw a Latin version of a Greek play. Literature in Latin would flourish for the next six centuries, and includes essays, histories, poems, plays, and other writings. The Qur'an (610 AD to 632 AD), the main holy book of Islam, had a significant influence on the Arab language, and marked the beginning of Islamic literature. Muslims believe it was transcribed in the Arabic dialect of the Quraysh, the tribe of Muhammad. As Islam spread, the Quran had the effect of unifying and standardizing Arabic. Theological works in Latin were the dominant form of literature in Europe typically found in libraries during the Middle Ages. Western Vernacular literature includes the Poetic Edda and the sagas, or heroic epics, of Iceland, the Anglo-Saxon Beowulf, and the German Song of Hildebrandt. A later form of medieval fiction was the romance, an adventurous and sometimes magical narrative with strong popular appeal. Controversial, religious, political and instructional literature proliferated during the European Renaissance as a result of the Johannes Gutenberg's invention of the printing press around 1440, while the Medieval romance",
    "label": 0
  },
  {
    "text": "romance, an adventurous and sometimes magical narrative with strong popular appeal. Controversial, religious, political and instructional literature proliferated during the European Renaissance as a result of the Johannes Gutenberg's invention of the printing press around 1440, while the Medieval romance developed into the novel. Publishing Publishing became possible with the invention of writing but became more practical with the invention of printing. Prior to printing, distributed works were copied manually, by scribes. The Chinese inventor Bi Sheng made movable type of earthenware c. 1045 and was spread to Korea later. Around 1230, Koreans invented a metal type movable printing. East metal movable type was spread to Europe between the late 14th century and early 15th century. In c. 1450, Johannes Gutenberg invented movable type in Europe. This invention gradually made books less expensive to produce and more widely available. Early printed books, single sheets, and images created before 1501 in Europe are known as incunables or incunabula. \"A man born in 1453, the year of the fall of Constantinople, could look back from his fiftieth year on a lifetime in which about eight million books had been printed, more perhaps than all the scribes of Europe had produced since Constantine founded his city in A.D. 330.\" Eventually, printing enabled other forms of publishing besides books. The history of newspaper publishing began in Germany in 1609, with the publishing of magazines following in 1663. University discipline In England In late 1820s England, growing political and social awareness, \"particularly among the utilitarians and Benthamites, promoted the possibility of including courses in English literary study in the newly formed London University\". This further developed into the idea of the study of literature being \"the ideal carrier for the propagation of the humanist cultural myth of a well educated, culturally harmonious nation\". America Women",
    "label": 0
  },
  {
    "text": "literary study in the newly formed London University\". This further developed into the idea of the study of literature being \"the ideal carrier for the propagation of the humanist cultural myth of a well educated, culturally harmonious nation\". America Women and literature The widespread education of women was not common until the nineteenth century, and because of this, literature until recently was mostly male dominated. There were few English-language women poets whose names are remembered until the twentieth century. In the nineteenth century some notable individuals include Emily Brontë, Elizabeth Barrett Browning, and Emily Dickinson (see American poetry). But while generally women are absent from the European canon of Romantic literature, there is one notable exception, the French novelist and memoirist Amantine Dupin (1804 – 1876) best known by her pen name George Sand. One of the more popular writers in Europe in her lifetime, being more renowned than both Victor Hugo and Honoré de Balzac in England in the 1830s and 1840s, Sand is recognised as one of the most notable writers of the European Romantic era. Jane Austen (1775 – 1817) is the first major English woman novelist, while Aphra Behn is an early female dramatist. Nobel Prizes in Literature have been awarded between 1901 and 2020 to 117 individuals: 101 men and 16 women. Selma Lagerlöf (1858 – 1940) was the first woman to win the Nobel Prize in Literature, which she was awarded in 1909. Additionally, she was the first woman to be granted a membership in The Swedish Academy in 1914. Feminist scholars have since the twentieth century sought to expand the literary canon to include more women writers. Children's literature A separate genre of children's literature only began to emerge in the eighteenth century, with the development of the concept of childhood. The earliest",
    "label": 0
  },
  {
    "text": "the twentieth century sought to expand the literary canon to include more women writers. Children's literature A separate genre of children's literature only began to emerge in the eighteenth century, with the development of the concept of childhood. The earliest of these books were educational books, books on conduct, and simple ABCs—often decorated with animals, plants, and anthropomorphic letters. Study and criticism Literary theory A fundamental question of literary theory is \"what is literature?\" – although many contemporary theorists and literary scholars believe either that \"literature\" cannot be defined or that it can refer to any use of language. Literary fiction Literary fiction is a term used to describe fiction that explores any facet of the human condition, and may involve social commentary. It is often regarded as having more artistic merit than genre fiction, especially the most commercially oriented types, but this has been contested in recent years, with the serious study of genre fiction within universities. The following, by the British author William Boyd on the short story, might be applied to all prose fiction: [short stories] seem to answer something very deep in our nature as if, for the duration of its telling, something special has been created, some essence of our experience extrapolated, some temporary sense has been made of our common, turbulent journey towards the grave and oblivion. The very best in literature is annually recognized by the Nobel Prize in Literature, which is awarded to an author from any country who has, in the words of the will of Swedish industrialist Alfred Nobel, produced \"in the field of literature the most outstanding work in an ideal direction\" (original Swedish: den som inom litteraturen har producerat det mest framstående verket i en idealisk riktning). The value of imaginative literature Some researchers suggest that literary fiction",
    "label": 0
  },
  {
    "text": "\"in the field of literature the most outstanding work in an ideal direction\" (original Swedish: den som inom litteraturen har producerat det mest framstående verket i en idealisk riktning). The value of imaginative literature Some researchers suggest that literary fiction can play a role in an individual's psychological development. Psychologists have also been using literature as a therapeutic tool. Psychologist Hogan argues for the value of the time and emotion that a person devotes to understanding a character's situation in literature; that it can unite a large community by provoking universal emotions, as well as allowing readers access to different cultures, and new emotional experiences. One study, for example, suggested that the presence of familiar cultural values in literary texts played an important impact on the performance of minority students. Psychologist Maslow's ideas help literary critics understand how characters in literature reflect their personal culture and the history. The theory suggests that literature helps an individual's struggle for self-fulfillment. Aesthetic value The influence of religious texts Religion has had a major influence on literature, through works like the Vedas, the Torah, the Bible, and the Quran. The King James Version of the Bible has been called \"the most influential version of the most influential book in the world, in what is now its most influential language\", \"the most important book in English religion and culture\", and \"arguably the most celebrated book in the English-speaking world,\" principally because of its literary style and widespread distribution. Prominent atheist figures such as the late Christopher Hitchens and Richard Dawkins have praised the King James Version as being \"a giant step in the maturing of English literature\" and \"a great work of literature\", respectively, with Dawkins adding: \"A native speaker of English who has never read a word of the King James Bible is",
    "label": 0
  },
  {
    "text": "James Version as being \"a giant step in the maturing of English literature\" and \"a great work of literature\", respectively, with Dawkins adding: \"A native speaker of English who has never read a word of the King James Bible is verging on the barbarian\". Societies in which preaching has great importance, and those in which religious structures and authorities have a near-monopoly of reading and writing and/or a censorship role (as, for example, in the European Middle Ages), may impart a religious gloss to much of the literature those societies produce or retain. The traditions of close study of religious texts has furthered the development of techniques and theories in literary studies. Types Poetry Poetry has traditionally been distinguished from prose by its greater use of the aesthetic qualities of language, including musical devices such as assonance, alliteration, rhyme, and rhythm, and by being set in lines and verses rather than paragraphs, and more recently its use of other typographical elements. This distinction is complicated by various hybrid forms such as digital poetry, sound poetry, concrete poetry and prose poem, and more generally by the fact that prose possesses rhythm. Abram Lipsky refers to it as an \"open secret\" that \"prose is not distinguished from poetry by lack of rhythm\". Prior to the 19th century, poetry was commonly understood to be something set in metrical lines: \"any kind of subject consisting of Rhythm or Verses\". Possibly as a result of Aristotle's influence (his Poetics), \"poetry\" before the 19th century was usually less a technical designation for verse than a normative category of fictive or rhetorical art. As a form it may pre-date literacy, with the earliest works being composed within and sustained by an oral tradition; hence it constitutes the earliest example of literature. Prose As noted above, prose generally",
    "label": 0
  },
  {
    "text": "category of fictive or rhetorical art. As a form it may pre-date literacy, with the earliest works being composed within and sustained by an oral tradition; hence it constitutes the earliest example of literature. Prose As noted above, prose generally makes far less use of the aesthetic qualities of language than poetry. However, developments in modern literature, including free verse and prose poetry have tended to blur the differences, and poet T.S. Eliot suggested that while \"the distinction between verse and prose is clear, the distinction between poetry and prose is obscure\". There are verse novels, a type of narrative poetry in which a novel-length narrative is told through the medium of poetry rather than prose. Eugene Onegin (1831) by Alexander Pushkin is the most famous example. On the historical development of prose, Richard Graff notes that, in the case of ancient Greece, \"recent scholarship has emphasized the fact that formal prose was a comparatively late development, an 'invention' properly associated with the classical period\". Latin was a major influence on the development of prose in many European countries. Especially important was the great Roman orator Cicero. It was the lingua franca among literate Europeans until quite recent times, and the great works of Descartes (1596 – 1650), Francis Bacon (1561 – 1626), and Baruch Spinoza (1632 – 1677) were published in Latin. Among the last important books written primarily in Latin prose were the works of Swedenborg (d. 1772), Linnaeus (d. 1778), Euler (d. 1783), Gauss (d. 1855), and Isaac Newton (d. 1727). Novel A novel is a long fictional narrative, usually written in prose. In English, the term emerged from the Romance languages in the late 15th century, with the meaning of \"news\"; it came to indicate something new, without a distinction between fact or fiction. The romance",
    "label": 0
  },
  {
    "text": "fictional narrative, usually written in prose. In English, the term emerged from the Romance languages in the late 15th century, with the meaning of \"news\"; it came to indicate something new, without a distinction between fact or fiction. The romance is a closely related long prose narrative. Walter Scott defined it as \"a fictitious narrative in prose or verse; the interest of which turns upon marvelous and uncommon incidents\", whereas in the novel \"the events are accommodated to the ordinary train of human events and the modern state of society\". Other European languages do not distinguish between romance and novel: \"a novel is le roman, der Roman, il romanzo\", indicates the proximity of the forms. Although there are many historical prototypes, so-called \"novels before the novel\", the modern novel form emerges late in cultural history—roughly during the eighteenth century. Initially subject to much criticism, the novel has acquired a dominant position amongst literary forms, both popularly and critically. Novella The publisher Melville House classifies the novella as \"too short to be a novel, too long to be a short story\". Publishers and literary award societies typically consider a novella to be between 17,000 and 40,000 words. Short story A dilemma in defining the \"short story\" as a literary form is how to, or whether one should, distinguish it from any short narrative and its contested origin, that include the Bible, and Edgar Allan Poe. Graphic novel Graphic novels and comic books present stories told in a combination of artwork, dialogue, and text. Electronic literature Electronic literature is a literary genre consisting of works created exclusively on and for digital devices. Non-fiction Common literary examples of non-fiction include, the essay; travel literature; biography, autobiography and memoir; journalism; letter; diary; history, philosophy, economics; scientific, nature, and technical writings. Non-fiction can fall within",
    "label": 0
  },
  {
    "text": "consisting of works created exclusively on and for digital devices. Non-fiction Common literary examples of non-fiction include, the essay; travel literature; biography, autobiography and memoir; journalism; letter; diary; history, philosophy, economics; scientific, nature, and technical writings. Non-fiction can fall within the broad category of literature as \"any collection of written work\", but some works fall within the narrower definition \"by virtue of the excellence of their writing, their originality and their general aesthetic and artistic merits\". Drama Drama is literature intended for performance. The form is combined with music and dance in opera and musical theatre (see libretto). A play is a written dramatic work by a playwright that is intended for performance in a theatre; it comprises chiefly dialogue between characters. A closet drama, by contrast, is written to be read rather than to be performed; the meaning of which can be realized fully on the page. Nearly all drama took verse form until comparatively recently. The earliest form of which there exists substantial knowledge is Greek drama. This developed as a performance associated with religious and civic festivals, typically enacting or developing upon well-known historical, or mythological themes, In the twentieth century, scripts written for non-stage media have been added to this form, including radio, television and film. Law Law and literature The law and literature movement focuses on the interdisciplinary connection between law and literature. Copyright Copyright is a type of intellectual property that gives its owner the exclusive right to make copies of a creative work, usually for a limited time. The creative work may be in a literary, artistic, educational, or musical form. Copyright is intended to protect the original expression of an idea in the form of a creative work, but not the idea itself. United Kingdom Literary works have been protected by copyright",
    "label": 0
  },
  {
    "text": "in a literary, artistic, educational, or musical form. Copyright is intended to protect the original expression of an idea in the form of a creative work, but not the idea itself. United Kingdom Literary works have been protected by copyright law from unauthorized reproduction since at least 1710. Literary works are defined by copyright law to mean \"any work, other than a dramatic or musical work, which is written, spoken or sung, and accordingly includes (a) a table or compilation (other than a database), (b) a computer program, (c) preparatory design material for a computer program, and (d) a database.\" Literary works are all works of literature; that is all works expressed in print or writing (other than dramatic or musical works). United States The copyright law of the United States has a long and complicated history, dating back to colonial times. It was established as federal law with the Copyright Act of 1790. This act was updated many times, including a major revision in 1976. European Union The copyright law of the European Union is the copyright law applicable within the European Union. Copyright law is largely harmonized in the Union, although country to country differences exist. The body of law was implemented in the EU through a number of directives, which the member states need to enact into their national law. The main copyright directives are the Copyright Term Directive, the Information Society Directive and the Directive on Copyright in the Digital Single Market. Copyright in the Union is furthermore dependent on international conventions to which the European Union is a member (such as the TRIPS Agreement and conventions to which all Member States are parties (such as the Berne Convention)). Copyright in communist countries Copyright in Japan Japan was a party to the original Berne convention in",
    "label": 0
  },
  {
    "text": "Union is a member (such as the TRIPS Agreement and conventions to which all Member States are parties (such as the Berne Convention)). Copyright in communist countries Copyright in Japan Japan was a party to the original Berne convention in 1899, so its copyright law is in sync with most international regulations. The convention protected copyrighted works for 50 years after the author's death (or 50 years after publication for unknown authors and corporations). However, in 2004 Japan extended the copyright term to 70 years for cinematographic works. At the end of 2018, as a result of the Trans-Pacific Partnership negotiations, the 70-year term was applied to all works. This new term is not applied retroactively; works that had entered the public domain between 1999 and 2018 by expiration would remain in the public domain. Censorship Censorship of literature is employed by states, religious organizations, educational institutions, etc., to control what can be portrayed, spoken, performed, or written. Generally such bodies attempt to ban works for political reasons, or because they deal with other controversial matters such as race, or sex. A notable example of censorship is James Joyce's novel Ulysses, which has been described by Russian-American novelist Vladimir Nabokov as a \"divine work of art\" and the greatest masterpiece of 20th century prose. It was banned in the United States from 1921 until 1933 on the grounds of obscenity. Nowadays it is a central literary text in English literature courses, throughout the world. Awards There are numerous awards recognizing achievements and contributions in literature. Given the diversity of the field, awards are typically limited in scope, usually on: form, genre, language, nationality and output (e.g. for first-time writers or debut novels). The Nobel Prize in Literature was one of the six Nobel Prizes established by the will of Alfred",
    "label": 0
  },
  {
    "text": "The following outline is provided as an overview of and topical guide to literature: Literature – prose, written or oral, including fiction and non-fiction, drama, and poetry. See also the Outline of poetry. What type of thing is literature? Literature can be described as all of the following: Communication – activity of conveying information. Communication requires a sender, a message, and an intended recipient, although the receiver need not be present or aware of the sender's intent to communicate at the time of communication; thus communication can occur across vast distances in time and space. Written communication (writing) – representation of language in a textual medium through the use of a set of signs or symbols (known as a writing system). Subdivision of culture – shared attitudes, values, goals, and practices that characterizes an institution, organization, or group. One of the arts – imaginative, creative, or nonscientific branch of knowledge, especially as studied academically. Essence of literature Composition – World literature – Creative writing – Forms of literature Oral literary genres Oral literature Oral poetry – Epic poetry – Legend – Mythology – Ballad – Folktale – Oral Narrative – Oral History – Urban legend – Christian literature – Written literary genres Cordel Literature Children's literature – Constrained writing – Erotic literature – Electronic literature – Literary fiction and poetry that uses the capabilities of computers and networks Digital poetry – Audiobook – Interactive fiction – Hypertext fiction – literary fiction written with hypertextual links Fan fiction Cell phone novel Chatbot – e-reader – e-books – Podcast – although not a form of literature in the traditional sense. It is often attributed by many as the neo-\"Digital Oratory\" form of literature. Poetry (see that article for an extensive list of subgenres and types) Aubade – Clerihew – Epic – Grook",
    "label": 0
  },
  {
    "text": "a form of literature in the traditional sense. It is often attributed by many as the neo-\"Digital Oratory\" form of literature. Poetry (see that article for an extensive list of subgenres and types) Aubade – Clerihew – Epic – Grook – form of short aphoristic poem invented by the Danish poet and scientist Piet Hein, who wrote more than 7,000 of them. Haiku – form of short Japanese poetry consisting of three lines. Instapoetry Tanka – classical Japanese poetry of five lines. Lied – Limerick – a kind of a witty, humorous, or nonsense poem, especially one in five-line anapestic or amphibrachic meter with a strict rhyme scheme (aabba), which is sometimes obscene with humorous intent. Lyric – Ode – Rhapsody – Song – Sonnet – Speculative poetry – Prison literature – Rhymed prose – Saj' Maqama Fu (literature) Rayok Non-fiction Non-fiction Autobiography – Biography – History – Diaries and Journals – Newspaper – Literary criticism – Memoir – Outdoor literature – Self-Help – Spiritual autobiography – Travel literature – Dictionary – Fiction genres Fiction Manga – Adventure novel – Airport novels – Comedy – Parody – Satire – Crime fiction – Detective fiction – Hardboiled – Whodunit – Newgate novel – Erotica – Fable – Fairy tale – Family saga – Gothic – Southern Gothic – Historical fiction – Inspirational fiction – Invasion literature – Mystery – Philosophical literature – Inspirational fiction (religious literature) – Psychological fiction – Psychological thriller – Romance (heroic literature) – Romance – Historical romance – Regency romance – Inspirational romance – Paranormal romance – Saga – Speculative fiction – Alternate history – Fantasy – (for more details see Fantasy subgenres, fantasy literature) Epic fantasy – Science fantasy – Steampunk – Urban fantasy – Weird fantasy – Horror – Lovecraftian horror – Weird menace –",
    "label": 0
  },
  {
    "text": "Saga – Speculative fiction – Alternate history – Fantasy – (for more details see Fantasy subgenres, fantasy literature) Epic fantasy – Science fantasy – Steampunk – Urban fantasy – Weird fantasy – Horror – Lovecraftian horror – Weird menace – Science fiction – (for more details see Science fiction genres and related topics Cyberpunk – Hard science fiction – Space opera – Supernatural fiction – Sensation novel – Slave narrative – Thriller – Conspiracy fiction – Legal thriller – Spy fiction/Political thriller – Techno-thriller – Western fiction – Literature by region and country Asia East Asian literature Chinese literature Japanese literature Korean literature Mongolian literature Taiwanese literature South Asian literature Bangladeshi literature Bhutanese literature Indian literature Assamese literature Bengali literature Bhojpuri language#Bhojpuri literature Indian English literature Gujarati literature Hindi literature Kannada literature Kashmiri literature Konkani literature Malayalam literature Maithili literature Meitei literature Marathi literature Mizo literature Nepali literature Odia literature Punjabi literature Rajasthani literature Sanskrit literature Sindhi literature Tamil literature Telugu literature Urdu literature Maldivian literature Nepalese literature Pakistani literature Sri Lankan literature Southeast Asian literature Brunei literature Burmese literature Cambodian literature Indonesian literature Laotian literature Malaysian literature Philippine literature Singaporean literature Thai literature Timoran literature Vietnamese literature Central Asian literature Kazakh literature Kyrgyz literature Tajik literature Turkmen literature Uzbek literature Europe Albanian literature Andorran literature Armenian literature Aromanian literature Austrian literature Azerbaijani literature Basque literature Belarusian literature Belgian literature Flemish literature Bosnian literature Bulgarian literature British literature Cornish literature English literature Manx literature Jèrriais literature Scottish literature Scots-language literature Scottish Gaelic literature Ulster literature Welsh literature in English Welsh-language literature Croatian literature Cypriot literature Turkish Cypriot literature Czech literature Danish literature Faroese literature Greenlandic literature Dutch literature Frisian literature Esperanto literature Estonian literature Finnish literature Åland literature French literature - also Francophone literature Breton literature Occitan literature Georgian literature",
    "label": 0
  },
  {
    "text": "literature Croatian literature Cypriot literature Turkish Cypriot literature Czech literature Danish literature Faroese literature Greenlandic literature Dutch literature Frisian literature Esperanto literature Estonian literature Finnish literature Åland literature French literature - also Francophone literature Breton literature Occitan literature Georgian literature Abkhaz literature Chechen literature Ossetian literature German literature Greek literature Hungarian literature Icelandic literature Irish literature Gaelic literature Literature of Northern Ireland Italian literature Friulian literature Sardinian literature Venetian literature Western Lombard literature Kazakh literature Kosovar literature Latvian literature Liechtensteiner literature Lithuanian literature Luxembourg literature Macedonian literature Maltese literature Moldovan literature Monégasque literature Montenegrin literature Norwegian literature Polish literature Portuguese literature Romanian literature Russian literature Sammarinese literature Serbian literature Slovak literature Slovene literature Spanish literature Aragonese literature Asturian literature Catalan literature Galician-language literature Swedish literature Swiss literature Turkish literature Ukrainian literature Yiddish literature Middle East and North Africa Afghan literature Algerian literature Arabic literature Bahraini literature Egyptian literature Ethiopian literature Emirati literature Iranian literature Iraqi literature Israeli literature Jordanian literature Kuwaiti literature Kurdish literature Lebanese literature Libyan literature Moroccan literature Oman literature Pakistani literature Palestinian literature Persian literature Qatari literature Saudi literature Syrian literature Tunisian literature Turkish literature Yemeni literature North and South America North American literature American literature African American literature Native American literature Southern literature Deaf American literature Canadian literature Quebec literature Mexican literature Caribbean literature Cuban literature Dominican Republic literature Guadeloupean Literature Haitian literature Jamaican literature Martinican Literature Puerto Rican literature Barthélemois literature Trinidad and Tobago literature Central American literature Costa Rican literature Salvadoran literature Guatemalan literature Honduran literature Nicaraguan literature Panamanian literature South American literature Argentine literature Bolivian literature Brazilian literature Chilean literature Colombian literature Ecuadorean literature Guyanese literature Paraguayan literature Peruvian literature Uruguayan literature Venezuelan literature Oceania Oceanian literature Australian literature Fijian literature Kiribati literature Marshall Islands literature Micronesian literature Nauran literature New Zealand literature",
    "label": 0
  },
  {
    "text": "literature Bolivian literature Brazilian literature Chilean literature Colombian literature Ecuadorean literature Guyanese literature Paraguayan literature Peruvian literature Uruguayan literature Venezuelan literature Oceania Oceanian literature Australian literature Fijian literature Kiribati literature Marshall Islands literature Micronesian literature Nauran literature New Zealand literature Papua New Guinean literature Palau literature Samoan literature Solomon Islands literature Tongan literature Tuvalan literature Vanuatu literature Sub-Saharan Africa East African literature Burundian literature Comorian literature Djibouti literature Eritrean literature Kenyan literature Madagascar literature Malawian literature Mauritian literature Mozambique literature Réunion literature Rwandan literature Seychelles literature Somalian literature Somaliland literature South Sudanese literature Sudanese literature Tanzanian literature Ugandan literature Zambian literature Zimbabwean literature Central African literature Angolan literature Cameroonian literature Literature of Central African Republic Chadian literature Congolese literature Equatorial Guinea literature Gabonese literature São Tomé and Príncipe literature Southern African literature Botswanan literature Swazi literature Lesotho literature Namibian literature South African literature Afrikaans literature West African literature Beninese literature Burkina Faso literature Literature of Cape Verde Gambian literature Ghanaian literature Guinean literature Guinea-Bissau literature Ivory Coast literature Liberian literature Malian literature Mauritanian literature Literature of Niger Nigerian literature Yoruba literature Senegalese literature Sierra Leonean literature Togo literature History of literature History of literature History of the book History of theater History of science fiction History of ideas Intellectual history Literature by written language Bronze Age literature Sumerian Ancient Egyptian Akkadian Classical literature Avestan Chinese Greek Hebrew Latin Pali Prakrit Sanskrit Syriac Sangam literature Middle Persian literature Medieval literature Medieval Dutch literature Medieval French literature Byzantine literature Medieval Bulgarian literature Old English literature Middle English literature Medieval German literature Old Irish literature Old Norse literature Georgian literature Catalan literature Medieval Welsh literature Renaissance literature Early Modern literature Baroque English literature French literature German literature Italian literature Spanish literature Bengali literature Hindi literature Kannada literature Newari literature Telugu literature Chinese literature",
    "label": 0
  },
  {
    "text": "Irish literature Old Norse literature Georgian literature Catalan literature Medieval Welsh literature Renaissance literature Early Modern literature Baroque English literature French literature German literature Italian literature Spanish literature Bengali literature Hindi literature Kannada literature Newari literature Telugu literature Chinese literature Japanese literature Korean literature Arabic literature Persian literature Armenian literature Turkish literature Literature by century Ancient literature - until the 6th century CE Early medieval literature - 6th through 9th centuries 10th century in literature 11th century in literature 12th century in literature 13th century in literature 14th century in literature 15th century in literature 16th century in literature 17th century in literature 18th century in literature 19th century in literature 20th century in literature 21st century in literature Literature by year List of years in literature Table of years in literature General literature concepts Book Western canon – Teaching of writing: Composition – Rhetoric – Poetry – Prosody – Meter – Scansion – Constrained writing – Poetics – Villanelle – Sonnet – Sestina – Ghazal – Ballad – Blank verse – Free verse – Epic poetry – Prose – Fiction – Non-fiction – Biography – Prose genres – Essay – Flash prose – Hypertext fiction – Journalism – Novel – Novella – Short story – Theater – History of theater – Rhetoric – Metaphor – Metonymy – Symbol – Allegory – Basic procedural knowledge Poetry analysis – effective reasoning in argument writing Narratology Frame tale – Anecdote – In Medias Res – Point of view – Literary criticism – an application of literary theory Marxist literary criticism – Semiotic literary interpretation – Psychoanalytic literary interpretation – Feminist literary interpretation – New historicism – Queer literary interpretation – Literary awards List of literary awards List of poetry awards Persons influential in the field of literature List of authors Category:Literary critics",
    "label": 0
  },
  {
    "text": "Allusion, or alluding, is a figure of speech that makes a reference to someone or something by name (a person, object, location, etc.) without explaining how it relates to the given context, so that the audience must realize the connection in their own minds. When a connection is directly and explicitly explained (as opposed to indirectly implied), it is instead often simply termed a reference. In the arts, a literary allusion puts the alluded text in a new context under which it assumes new meanings and denotations. Literary allusion is closely related to parody and pastiche, which are also \"text-linking\" literary devices. In a wider, more informal context, an allusion is a passing or casually short statement indicating broader meaning. It is an incidental mention of something, either directly or by implication, such as \"In the stock market, he met his Waterloo.\" Scope of the term In the most traditional sense, allusion is a literary term, though the word has also come to encompass indirect references to any source, including allusions in film or the visual arts. In literature, allusions are used to link concepts that the reader already has knowledge of, with concepts discussed in the story. It is not possible to predetermine the nature of all the new meanings and inter-textual patterns that an allusion will generate. In the field of film criticism, a filmmaker's intentionally unspoken visual reference to another film is also called an homage. It may even be sensed that real events have allusive overtones, when a previous event is inescapably recalled by a current one. \"Allusion is bound up with a vital and perennial topic in literary theory, the place of authorial intention in interpretation\", William Irwin observed, in asking \"What is an allusion?\" Without the hearer or reader comprehending the author's intention, an",
    "label": 0
  },
  {
    "text": "one. \"Allusion is bound up with a vital and perennial topic in literary theory, the place of authorial intention in interpretation\", William Irwin observed, in asking \"What is an allusion?\" Without the hearer or reader comprehending the author's intention, an allusion becomes merely a decorative device. Allusion is an economical device, a figure of speech that uses a relatively short space to draw upon the ready stock of ideas, cultural memes or emotion already associated with a topic. Thus, an allusion is understandable only to those with prior knowledge of the covert reference in question, a mark of their cultural literacy. Allusion as cultural bond The origin of allusion is from the Latin noun allusionem \"a playing with, a reference to\", from alludere \"to play, jest, make fun of\", a compound of ad \"to\" + ludere \"to play\". Recognizing the point of allusion's condensed riddle also reinforces cultural solidarity between the maker of the allusion and the hearer: their shared familiarity with allusion bonds them. Ted Cohen finds such a \"cultivation of intimacy\" to be an essential element of many jokes. Some aspect of the referent must be invoked and identified for the tacit association to be made; the allusion is indirect in part because \"it depends on something more than mere substitution of a referent\". The allusion depends as well on the author's intent; a reader may search out parallels to a figure of speech or a passage, of which the author was unaware, and offer them as unconscious allusions—coincidences that a critic might not find illuminating. Addressing such issues is an aspect of hermeneutics. William Irwin remarks that allusion moves in only one direction: \"If A alludes to B, then B does not allude to A. The Bible does not allude to Shakespeare, though Shakespeare may allude to",
    "label": 0
  },
  {
    "text": "issues is an aspect of hermeneutics. William Irwin remarks that allusion moves in only one direction: \"If A alludes to B, then B does not allude to A. The Bible does not allude to Shakespeare, though Shakespeare may allude to the Bible.\" Irwin appends a note: \"Only a divine author, outside of time, would seem capable of alluding to a later text.\" This is the basis for Christian readings of Old Testament prophecy, which asserts that passages are to be read as allusions to future events. Allusion differs from the similar term intertextuality in that it is an intentional effort on the author's part. The success of an allusion depends in part on at least some of its audience \"getting\" it. Allusions may be made increasingly obscure, until at last they are understood by the author alone, who thereby retreats into a private language. Academic analysis of the concept of allusions In discussing the richly allusive poetry of Virgil's Georgics, R. F. Thomas distinguished six categories of allusive reference, which are applicable to a wider cultural sphere. These types are: Casual reference, \"the use of language which recalls a specific antecedent, but only in a general sense\" that is relatively unimportant to the new context; Single reference, in which the hearer or reader is intended to \"recall the context of the model and apply that context to the new situation\"; such a specific single reference in Virgil, according to Thomas, is a means of \"making connections or conveying ideas on a level of intense subtlety\"; Self-reference, where the locus is in the poet's own work; Corrective allusion, where the imitation is clearly in opposition to the original source's intentions; Apparent reference \"which seems clearly to recall a specific model but which on closer inspection frustrates that intention\"; and Multiple reference",
    "label": 0
  },
  {
    "text": "in the poet's own work; Corrective allusion, where the imitation is clearly in opposition to the original source's intentions; Apparent reference \"which seems clearly to recall a specific model but which on closer inspection frustrates that intention\"; and Multiple reference or conflation, which refers in various ways simultaneously to several sources, fusing and transforming the cultural traditions. A type of literature has grown round explorations of the allusions in such works as Alexander Pope's The Rape of the Lock or T. S. Eliot's The Waste Land. Examples In Homer, brief allusions could be made to mythic themes of generations previous to the main narrative because they were already familiar to the epic's hearers: one example is the theme of the Calydonian boarhunt. In Hellenistic Alexandria, literary culture and a fixed literary canon known to readers and hearers made a densely allusive poetry effective; the poems of Callimachus offer the best-known examples. Martin Luther King Jr., alluded to the Gettysburg Address in starting his \"I Have a Dream\" speech by saying \"Five score years ago...\"; his hearers were immediately reminded of Abraham Lincoln's \"Four score and seven years ago\", which opened the Gettysburg Address. King's allusion effectively called up parallels in two historic moments without overwhelming his speech with details. A sobriquet is an allusion. By metonymy one aspect of a person or other referent is selected to identify it, and it is this shared aspect that makes a sobriquet evocative: for example, \"the city that never sleeps\" is a sobriquet of (and therefore an allusion to) New York. References Bibliography Ben-Porot, Ziva (1976) The Poetics of Literary Allusion, p. 108, in PTL: A Journal for descriptive poetics and theory of literature 1 Irwin, William (2001). \"What Is an Allusion?\" The Journal of Aesthetics and Art Criticism, 59 (3): 287–297. Irwin,",
    "label": 0
  },
  {
    "text": "Comedy of menace is the body of plays written by David Campton, Nigel Dennis, N. F. Simpson, and Harold Pinter. The term was coined by drama critic Irving Wardle, who borrowed it from the subtitle of Campton's play The Lunatic View: A Comedy of Menace, in reviewing Pinter's and Campton's plays in Encore in 1958. (Campton's subtitle Comedy of Menace is a jocular play-on-words derived from comedy of manners—menace being manners pronounced with somewhat of an English accent.) Background Citing Wardle's original publications in Encore magazine (1958), Susan Hollis Merritt points out that in \"Comedy of Menace\" Wardle \"first applies this label to Pinter's work … describ[ing] Pinter as one of 'several playwrights who have been tentatively lumped together as the \"non-naturalists\" or \"abstractionists\" ' (28)\". His article \"Comedy of Menace,\" Merritt continues, centers on The Birthday Party because it is the only play of Pinter's that Wardle had seen [and reviewed] at the time, yet he speculates on the basis of \"descriptions of [Pinter's] other plays, 'The Room' and 'The Dumb Waiter', [that Pinter] is a writer dogged by one image—the womb\" (33). Mentioning the acknowledged \"literary influences\" on Pinter's work—\"Beckett, Kafka and American gangster films\"—Wardle argues that \" 'The Birthday Party' exemplifies the type of comic menace which gave rise to this article.\" (225) In \"Comedy of Menace\", as Merritt observes, on the basis of his experience of The Birthday Party and others' accounts of the other two plays, Wardle proposes that \"Comedy enables the committed agents and victims of destruction to come on and off duty; to joke about the situation while oiling a revolver; to display absurd or endearing features behind their masks of implacable resolution; to meet … in paper hats for a game of blind man's buff\"; he suggests how \"menace\" in Pinter's plays",
    "label": 0
  },
  {
    "text": "joke about the situation while oiling a revolver; to display absurd or endearing features behind their masks of implacable resolution; to meet … in paper hats for a game of blind man's buff\"; he suggests how \"menace\" in Pinter's plays \"stands for something more substantial: destiny,\" and that destiny, \"handled in this way—not as an austere exercise in classicism, but as an incurable disease which one forgets about most of the time and whose lethal reminders may take the form of a joke—is an apt dramatic motif for an age of conditioned behaviour in which orthodox man is a willing collaborator in his own destruction\". \"Just two years later\" (1960), however, Wardle retracted \"Comedy of Menace\" in his review of The Caretaker, stating: \"On the strength of 'The Birthday Party' and the pair of one-acters, I rashly applied the phrase 'comedy of menace' to Pinter's writing. I now take it back\". After Wardle's retraction of comedy of menace as he had applied it to Pinter's writing, Pinter himself also occasionally disavowed it and questioned its relevance to his work (as he also did with his own offhand but apt statement that his plays are about \"the weasel under the cocktail cabinet\"). For example, in December 1971, in his interview with Pinter about Old Times, Mel Gussow recalled that \"After The Homecoming [Pinter] said that [he] 'couldn't any longer stay in the room with this bunch of people who opened doors and came in and went out. Landscape and Silence [the two short poetic memory plays that were written between The Homecoming and Old Times] are in a very different form. There isn't any menace at all.' \" Later, when Gussow asked Pinter to expand on his view that he had \"tired\" of \"menace\", Pinter added: \"when I said that I",
    "label": 0
  },
  {
    "text": "Homecoming and Old Times] are in a very different form. There isn't any menace at all.' \" Later, when Gussow asked Pinter to expand on his view that he had \"tired\" of \"menace\", Pinter added: \"when I said that I was tired of menace, I was using a word that I didn't coin. I never thought of menace myself. It was called 'comedy of menace' quite a long time ago . I never stuck categories on myself, or on any of us [playwrights]. But if what I understand the word menace to mean is certain elements that I have employed in the past in the shape of a particular play, then I don't think it's worthy of much more exploration\". Despite Wardle's retraction of comedy of menace (and Pinter's later qualifications), Comedy of menace and comedies of menace caught on and have been prevalent since the late 1950s in advertisements and in critical accounts, notices, and reviews to describe Pinter's early plays and some of his later work as well. As Merritt points out, among other examples of critics' usage of this and similar categories of Pinter's work, after Gussow's 1971 \"conversation\" with Pinter, \"Though he echoes Wardle's concept, Gussow seems to avoid using comedy of menace when reviewing the CSC Repertory Theatre's 1988 production of The Birthday Party. While still emphasizing Pinter's 'terrors' and the 'shiver beneath the laughter,' Gussow describes the play as \"a play of intrigue, with an underlying motif of betrayal' … [and] [Bernard F.] Dukore calls the play 'a comedy (of menace or otherwise)'\". Selected examples The Birthday Party (1958) In discussing the first production of Pinter's first full-length play, The Birthday Party (1958), which followed his first play, The Room (1957), his authorised official biographer Michael Billington points out that Wardle \"once excellently\" described",
    "label": 0
  },
  {
    "text": "examples The Birthday Party (1958) In discussing the first production of Pinter's first full-length play, The Birthday Party (1958), which followed his first play, The Room (1957), his authorised official biographer Michael Billington points out that Wardle \"once excellently\" described its setting (paraphrasing Wardle), as \"a banal living-room [which] opens up to the horrors of modern history\". The Dumb Waiter (1960) In Pinter's second one-act play, The Dumb Waiter (1960), as accentuated through the 2008 film by Martin McDonagh closely resembling and markedly influenced by it, In Bruges, \"Pinter conveys the idea of political terror through the staccato rhythms of music-hall cross-talk and the urban thriller: Hackney Empire cross-fertilises with Hemingway's The Killers \", one of Pinter's own acknowledged early influences, along with Franz Kafka (348–49); Elizabethan and Jacobean dramatists, such as William Shakespeare, John Webster, and Cyril Tourneur, whose work his schoolmaster Joseph Brearley had introduced to him; Samuel Beckett (mostly his novels ); and black-and-white American movies of the 1940s and 1950s. \"A near-perfect play about the testiness of a collapsing partnership and the divide-and-rule tactics of authority,\" according to Billington, The Dumb Waiter focuses on two characters, Gus and Ben; Gus is \"the man who questions the agreed system and who is ultimately destroyed by his quest for meaning\"; Ben, \"the man who blindly obeys orders and thereby places himself at risk. (If the system can arbitrarily dispose of his partner, why not of him?)\". As Pinter's The Dumb Waiter has been categorised as a \"comedy of menace,\" so may be McDonagh's In Bruges, as it closely resembles it; yet, despite the comedy and the sense of threat growing out of the menace, these works of Pinter and McDonagh are, in Pinter's words to Billington, also \"doing something which can be described as political\". At the same",
    "label": 0
  },
  {
    "text": "resembles it; yet, despite the comedy and the sense of threat growing out of the menace, these works of Pinter and McDonagh are, in Pinter's words to Billington, also \"doing something which can be described as political\". At the same time, [Pinter] had – and still [in 1996 through to the time of his death in 2008] has – an acute sense of the fragility of earthly happiness and of the terrors that haunt us even from infancy\". The \"punning title\" of The Dumb Waiter, Billington observes, \"carries several layers of meaning\": \"It obviously refers to the antique serving-hatch that despatches [sic] ever more grotesque orders for food to these bickering gunmen\"—the dumbwaiter; \"But it also applies to Gus, who, troubled by the nature of the mission [their next job as hitmen] to realise he is its chosen target; or, indeed to Ben, who, by his total obedience to a higher authority that forces him to eliminate his partner, exposes his own vulnerability\". As Gus \"dumbly\" awaits his fate, he may be a subservient partner who awaits orders from the \"senior partner\" Ben, but Ben too is subservient to The Powers That Be, a contemporary variation on Deus ex machina, manipulating both the mechanical dumbwaiter and them through its increasingly extravagant and thus comically inconvenient \"orders\" for increasingly exotic dishes, unnerving both of them. Billington adds: This being Pinter, the play has a metaphorical openness. You can interpret it as an Absurdist comedy – a kind of Godot in Birmingham – about two men passing the time in a universe without meaning or purpose. You can see it as a cry of protest against a whimsically cruel God who treats man as His plaything – even the twelve matches that are mysteriously pushed under the door have been invested with",
    "label": 0
  },
  {
    "text": "without meaning or purpose. You can see it as a cry of protest against a whimsically cruel God who treats man as His plaything – even the twelve matches that are mysteriously pushed under the door have been invested with religious significance [by critics]. But it makes much more sense if seen as a play about the dynamics of power and the nature of partnership. Ben and Gus are both victims of some unseen authority and a surrogate married couple quarrelling, testing, talking past each other and raking over old times. The comedy in this \"comedy of menace\" often derives from such arguments between Gus and Ben, especially the one that occurs when \"Ben tells Gus to go and light the kettle,\" a \"semantic nit-picking that is a standard part of music-hall comedy\": \"All the great stage and film double acts – Jewel and Warriss, Abbott and Costello – fall into this kind of verbal worrying in which the bullying 'male' straight man issues instructions which are questioned by the more literal-minded 'female' partner\" — Gus: Light what?Ben: The kettle.Gus: You mean the gas.Ben: Who does?Gus: You do.Ben (his eyes narrowing): What do you mean, I mean the gas?Gus: Well, that's what you mean, don't you? The gas.Ben (powerfully): If I say go and light the kettle I mean go and light the kettle.Gus: How can you light a kettle?Ben: It's a figure of speech! Light the kettle. It's a figure of speech! As Billington observes further, This kind of comic pedantry has precise echoes of the great Sid Field – ironically [since the city is the setting of this play] a Birmingham comic – who had a famous sketch in which he played a virgin of the greens being hectored by Jerry Desmonde's golf pro who would cry, in",
    "label": 0
  },
  {
    "text": "– ironically [since the city is the setting of this play] a Birmingham comic – who had a famous sketch in which he played a virgin of the greens being hectored by Jerry Desmonde's golf pro who would cry, in exasperation, 'When I say \"Slowly Back\" I don't mean \"Slowly Back\", I mean \"Slowly Back.\" ' At another moment, the bullying pro would tell the hapless Sid to get behind the ball and he would vainly protest 'But it's behind all round it'. But, where in a music-hall sketch this kind of semantic by-play was its own justification, in Pinter it becomes a crucial part of the power-structure. … The pay-off comes when Gus, having dogmatically insisted that the accurate phrase is 'put on the kettle', suddenly finds an irritated Ben adopting the right usage. \"Everything\" in The Dumb Waiter, Billington observes, \"contributes towards a necessary end\"; for, \"the image, as Pete says in [Pinter's only novel] The Dwarfs, stands in exact correspondence and relation to the idea\". In this example, the central image and central metaphor, the dumbwaiter, while \"despatching ever more unlikely orders,\" serves as \"both a visual gag and a metaphor for manipulative authority\", and therein lies its menace. When Ben instructs Gus verbally, while practicing their \"routine\" for killing their next victim, he leaves out the most important line, which instructs Gus to \"take out\" his \"gun\": [Ben frowns and presses his forehead]Gus: You've missed something out.Ben: I know. What?Gus: I haven't taken my gun out, according to you.Ben: You take your gun out—Gus: After I've closed the door.Ben: After you've closed the door.Gus: You've never missed that out before, you know that? The crucial significance of the omission becomes clear only at the very end of the play, when \"Gus enters through the door stage-right",
    "label": 0
  },
  {
    "text": "closed the door.Ben: After you've closed the door.Gus: You've never missed that out before, you know that? The crucial significance of the omission becomes clear only at the very end of the play, when \"Gus enters through the door stage-right – the one marked for the intended victim – stripped of his gun and holster\"; it becomes clear that he is going to be \"Ben's target\", as Ben's \"revolver [is] levelled at the door\", though the play ends before Ben fires any shot. The Caretaker (1960) In an entry on Pinter for the 1969 edition of The Encyclopedia of World Drama cited by Merritt, Wardle repeats and updates some of his first perspective on comedy of menace as he had applied it initially to Pinter's writing: Early in his writing career Pinter admitted to three influences: Franz Kafka, American gangster films, and Samuel Beckett. . . . At that time his plays, more than those of any other playwright's [sic], were responsible for the newly coined term 'comedy of menace.' This phrase certainly makes sense when applied to The Birthday Party . . . or to The Dumb Waiter. . . . But 'menace' is hardly the word for The Caretaker, and still less for subsequent plays in which Pinter increasingly exchanged his derelict settings and down-and-out characters for environments of moneyed elegance (657–58). Despite those more-recent caveats regarding applying the phrase that he himself initially coined for Pinter's writing to The Caretaker—only the second of Pinter's full-length plays produced by then and the one that launched his career as a successful playwright in 1960 —and to Pinter's later plays, scenes in both acts of The Caretaker in which Mick confronts an unsuspecting Davies and scares him almost speechless also epitomise how comedy and menace still co-exist in Pinter's text",
    "label": 0
  },
  {
    "text": "a successful playwright in 1960 —and to Pinter's later plays, scenes in both acts of The Caretaker in which Mick confronts an unsuspecting Davies and scares him almost speechless also epitomise how comedy and menace still co-exist in Pinter's text and on Pinter's stage. The comic aspects of this play multiply, reaching a crescendo in Mick's monologue in Act Two describing his \"deepest wishes\" for decorating the attic room (161, 173) and falling with Davies, a tramp taken in out of the cold by his brother, suggesting that \"if\" he can \"just get down to Sidcup\" to get his \"papers\" and \"sort\" himself \"out\" (113–16, 164), his refrain and excuse for everything (153, 175–79), he might just be able to accomplish Mick's hyperbolic pipe dream and \"decorate the attic room out for [Mick]\" (164), leading Mick to accuse Davies of misrepresenting himself as \"an experienced first-class professional interior and exterior decorator\" (172–74), an absurd conclusion, given the tangible evidence of the down-and-out Davies before Mick (and the audience). Pinter's friend the late film and stage director David Jones, who directed the play for the Roundabout Theatre, in New York City, in 2003 (having previously directed Pinter's 1983 film of Betrayal, as well as other works by or featuring him), reminds his audience that Pinter himself said, in a widely quoted statement, that The Caretaker is only \"funny, up to a point\" and that \"beyond that point\" is why he wrote it: There is always mischief lurking in the darkest corners. The world of The Caretaker is a bleak one, its characters damaged and lonely. But they are all going to survive. And in their dance to that end they show a frenetic vitality and a wry sense of the ridiculous that balance heartache and laughter. Funny, but not too funny.",
    "label": 0
  },
  {
    "text": "characters damaged and lonely. But they are all going to survive. And in their dance to that end they show a frenetic vitality and a wry sense of the ridiculous that balance heartache and laughter. Funny, but not too funny. As Pinter wrote, back in 1960: \"As far as I am concerned The Caretaker IS funny, up to a point. Beyond that point, it ceases to be funny, and it is because of that point that I wrote it.\" (Jones) \"Beyond the point\" of the comedy (the \"funny\") lies the scary territory that threatens one's very existence, which Wardle and others commonly have \"labeled\" or \"pigeonholed\" (depending on one's perspective) as \"menace\". Pinter's later plays Though \"comedy of menace\" is generally applied to Pinter's early work of the late 1950s through the middle of the 1960s, including The Collection (1961), The Lover (1963), Tea Party (1965, 1968), and The Homecoming (1965), even Pinter's late plays, like Ashes to Ashes (1996) and Celebration (2000), his last two full-length stage plays, exhibit his characteristic amalgam of the comic and the menacing, a sense of threat or impending doom; there is less comedy and more menace in Ashes to Ashes, in which heavy echoes of the Holocaust predominate; a more comedy than menace in Celebration, where heightened comic dialogue outweighs the frightening undercurrents of terror, the terrifying, or the terrible. Celebration (2000) While reviewers and other audience members describe Celebration as hilarious (\"one of Pinter's funniest plays\", according to Billington), the nature of the relationships of two sets of diners (three couples) having dinner in an upscale restaurant (which some critics assume that Pinter modeled on The Ivy, in London's West End) – \"this is the best and most expensive restaurant in the whole of Europe\" – remains characteristically ambiguous; Billington describes one",
    "label": 0
  },
  {
    "text": "dinner in an upscale restaurant (which some critics assume that Pinter modeled on The Ivy, in London's West End) – \"this is the best and most expensive restaurant in the whole of Europe\" – remains characteristically ambiguous; Billington describes one set of couples as \"a strangely rootless bunch with a depleted sense of family\". One set (the two couples seated at Table One) consists of brothers Lambert and Matt and their wives, Prue and Julie, who are sisters; the second set of diners (the couple seated at Table Two) consists of a banker and his young wife, Suki, who comically turns out to have had an affair with Lambert when she was 18. As the \"maître d'hôtel\" emits platitudes geared to elevate the nouveaux riches in their own imagined esteem (\"I believe the concept of this restaurant rests in that public house of my childhood\"), the \"maîtress d'hôtel\" appears to dwell on a peculiar past family and sex life, while the Waiter engages in \"interjections\" spinning fantasied impossible memories of a grandfather who knew writers, other artists, and various other public figures of multiple decades and geographical locations too far apart to have been experienced personally in one man's lifetime. Lambert and Matt reveal themselves to be rather uncouth bullies (\"Teddy boys\", according to some London reviews)—who describe themselves as \"consultants … Strategy consultants. … It means we don't carry guns. … We don't have to! … We're peaceful strategy consultants. … Worldwide. Keeping the peace. Strategy consultant could be a euphemistic catchall for warmonger, manager of terrorism, purveyor of counter-terrorism, or orderer of covert operations. [Lambert's mobile phone rings]Lambert: Who the fuck's this?[He switches it on]Lambert: Yes? What?\" (He listens briefly.) \"I said no calls! It's my fucking wedding anniversary![He switches it off]Lambert: Cunt. As the banker Russell interprets",
    "label": 0
  },
  {
    "text": "counter-terrorism, or orderer of covert operations. [Lambert's mobile phone rings]Lambert: Who the fuck's this?[He switches it on]Lambert: Yes? What?\" (He listens briefly.) \"I said no calls! It's my fucking wedding anniversary![He switches it off]Lambert: Cunt. As the banker Russell interprets their explanation, peaceful strategy consultants seems vaguely menacing: \"We need more people like you. Taking responsibility. Taking charge. Keeping the peace. Enforcing the peace. Enforcing peace. We need more like you\". This speech stressing \"force\" (in the repetitions of Enforcing) occurs after Russell has already revealed his own ilk: Russell: Look at me. I'm basically a totally disordered personality; some people would describe me as a psychopath.\" (to Suki) \"Am I right?Suki: Yes.Russell: But when I'm sitting in this restaurant I suddenly find I have no psychopathic tendencies at all. I don't feel like killing everyone in sight, I don't feel like putting a bomb under everyone's arse. I feel something quite different, I have a sense of equilibrium, of harmony, I love my fellow diners. Now this is very unusual for me. Normally I feel—as I've just said—absoute malice and hatred towards everyone within spitting distance—but here I feel love. How do you explain it?Suki: It's the ambience. Lambert and Matt are distantly reminiscent of Gus and Ben from The Dumb Waiter; but distinctly less polite while living much higher on the hog. One imagines such characters \"strategically\" plotting the \"peaceful\" rendition of others without any qualms while sipping Perrier and simultaneously planning their next wedding-anniversary dinner celebration (perhaps with a different set of wives) on their mobiles. As the Waiter says in his apparently penultimate \"interjection\", in which one might detect intimations of mortality: Waiter: My grandfather introduced me to the mystery of life and I'm still in the middle of it. I can't find the door to",
    "label": 0
  },
  {
    "text": "the Waiter says in his apparently penultimate \"interjection\", in which one might detect intimations of mortality: Waiter: My grandfather introduced me to the mystery of life and I'm still in the middle of it. I can't find the door to get out. My grandfather got out of it. He left it behind him and he didn't look back.He got that absolutely right.And I'd like to make one further interjection.[He stands still.][Slow fade.] \"Apart From That\" (Sketch) (2006) Pinter mocks mobile phones comically in an ostensibly-trivial wireless conversation, while still suggesting a residual bit of menace in the unsaid, developed as his last revue sketch Apart From That (2006). As Billington observes, this dramatic sketch inspired by Pinter's strong aversion to mobile phones is \"very funny\", but \"as two people trade banalities over their mobile phones[,] there is a hint of something ominous and unspoken behind the clichéd chat\", as illustrated in the following excerpt: Gene: How are you?Lake: Very well. And you? Are you well?Gene: I'm terribly well. How about you?Lake: Really well. I'm really well.Gene: I'm so glad.Lake: Apart from ... oh you know ...Gene: I know.Lake: Apart from ... oh you know ...Gene: I do know. But apart from that ...?Lake: How about you? Gene: Oh you know ... all things considered ...Lake: I know. But apart from that ...?Gene: Sorry. I've lost you. [...] See also Comedy of manners Film noir Grotesque Kafkaesque Music hall Theatre of the Absurd Vaudeville Notes Works cited External links \"Plays by Harold Pinter\" at HaroldPinter.org: The Official Website of the International Playwright Harold Pinter (Index of Harold Pinter's plays; includes dates of composition and productions).",
    "label": 0
  },
  {
    "text": "A film adaptation transfers the details or story of an existing source text, such as a novel, into a feature film. This transfer can involve adapting most details of the source text closely, including characters or plot points, or the original source can serve as loose inspiration, with the implementation of only a few details. Although often considered a type of derivative work, film adaptation has been conceptualized recently by academic scholars such as Robert Stam as a dialogic process. While the most common form of film adaptation is the use of a novel as the basis, other works adapted into films include non-fiction (including journalism), autobiographical works, comic books, scriptures, plays, historical sources and even other films. Adaptation from such diverse resources has been a ubiquitous practice of filmmaking since the earliest days of cinema in nineteenth-century Europe. In contrast to when making a remake, movie directors usually take more creative liberties when creating a film adaptation, changing the context of factors such as audience or genre. Fidelity The Fidelity Argument One aspect that is usually considered for analyzing adaptations is the fidelity argument. This regards the discussion between scholars, reviewers, or fans, evaluating the effectiveness and success of an adaptation based on how faithfully it follows the original plot and details. This can involve the common discourse of the \"book being better than the movie\" without considering the efforts that go towards a film adaptation. A film can choose to stay faithful, or stray from the original source text details, but regardless, this medium is created with the use of more elements. Unlike a piece of media like a novel, a film has many aspects to consider, including the script, the music, the actors and their performances, images and shots, and sound effects. All these elements must work",
    "label": 0
  },
  {
    "text": "use of more elements. Unlike a piece of media like a novel, a film has many aspects to consider, including the script, the music, the actors and their performances, images and shots, and sound effects. All these elements must work together to translate the source material into a feature film. As a type of media that is composed of several elements, a film's runtime often impacts the final product too. A film adaptation may go against adhering to fidelity for a number of reasons, including the desire for a shift in the time period, cultural context, or perspective of the original plot, and a shift in overall audience. On the other hand, a film adaptation may consider strong fidelity to help promote the work if it is already tied to a popular writer or existing title, especially since it can help financially. It can be difficult to judge or account for the effectiveness of a film adaptation based solely on fidelity, especially since an adaptation also exists as its own entity outside of the original source text. Although one can consider the original ideas and themes being consistently transposed, an adaptation can also be examined for the new elements it brings audiences. Types of Fidelity One way to think of fidelity as other than a measure of success is as different levels of faithfulness to help facilitate discourse. Although not all adaptations aim towards focusing on fidelity for their final product, these labels may help consider how much of a story translates through history. Close Adaptation A close adaptation would help define a work that adheres to fidelity closely, implementing most, if not all, details from the original source text. This would include embedding a majority of the original characters, plot points, and timeline. An example of this would include",
    "label": 0
  },
  {
    "text": "help define a work that adheres to fidelity closely, implementing most, if not all, details from the original source text. This would include embedding a majority of the original characters, plot points, and timeline. An example of this would include To Kill a Mockingbird (1962) that closely adapted a majority of the details from Harper Lee's 1960 novel of the same name, To Kill a Mockingbird. Loose Adaptation A loose adaptation would include any work that mainly uses the original source text as inspiration, straying far from fidelity. This can include instances of only implementing one or two elements, such as the protagonist or title of the original text. This type of adaptation is where the most creative liberties are taken. An example of this would include Clueless (1995), that was loosely adapted from Jane Austen's 1815 novel, Emma, into a modern day context. Intermediate Adaptation An intermediate adaptation would consist of a work that falls in between a close and loose adaptation, as it both embeds original source text details, takes creative liberties to incorporate new elements (such as new characters or plot points), and/or excludes certain original details. An example of this would include What's Eating Gilbert Grape (1993) that was adapted from Peter Hodge's 1991 novel of the same name, What's Eating Gilbert Grape, that closely adapts a few details, while also excluding certain original characters. Elision and interpolation In 1924, Erich von Stroheim attempted a literal adaptation of Frank Norris's novel McTeague with his film Greed. The resulting film was 9½ hours long, and was cut to four hours at studio insistence. It was then cut again (without Stroheim's input) to around two hours. The result was a film that was largely incoherent. Since that time, few directors have attempted to put everything in a novel",
    "label": 0
  },
  {
    "text": "to four hours at studio insistence. It was then cut again (without Stroheim's input) to around two hours. The result was a film that was largely incoherent. Since that time, few directors have attempted to put everything in a novel into a film. Therefore, elision is all but essential. In some cases, film adaptations also interpolate scenes or invent characters. This is especially true when a novel is part of a literary saga. Incidents or quotations from later or earlier novels will be inserted into a single film. Additionally and far more controversially, filmmakers will invent new characters or create stories that were not present in the source material at all. Given the anticipated audience for a film, the screenwriter, director or movie studio may wish to increase character time or to invent new characters. For example, William J. Kennedy's Pulitzer Prize-winning novel Ironweed included a short appearance by a prostitute named Helen. Because the film studio anticipated a female audience for the film and had Meryl Streep for the role, Helen became a significant part of the film. However, characters are also sometimes invented to provide the narrative voice. Interpretation as adaptation There have been several notable cases of massive inventive adaptation, including the Roland Joffe adaptation of The Scarlet Letter with explicit sex between Hester Prynn and the minister and Native American obscene puns into a major character and the film's villain. The Charlie Kaufman and \"Donald Kaufman\" penned Adaptation, credited as an adaptation of the novel The Orchid Thief, was an intentional satire and commentary on the process of film adaptation itself. All of those are cases of Nathaniel Hawthorne's point. The creators of the Gulliver's Travels miniseries interpolated a sanity trial to reflect the ongoing scholarly debate over whether or not Gulliver himself is sane at",
    "label": 0
  },
  {
    "text": "process of film adaptation itself. All of those are cases of Nathaniel Hawthorne's point. The creators of the Gulliver's Travels miniseries interpolated a sanity trial to reflect the ongoing scholarly debate over whether or not Gulliver himself is sane at the conclusion of Book IV. In those cases, adaptation is a form of criticism and recreation as well as translation. Change is essential and practically unavoidable, mandated both by the constraints of time and medium, but how much is always a balance. Some film theorists have argued that a director should be entirely unconcerned with the source, as a novel is a novel and a film is a film, and the two works of art must be seen as separate entities. Since a transcription of a novel into film is impossible, even holding up a goal of \"accuracy\" is absurd. Others argue that what a film adaptation does is change to fit (literally, adapt), and the film must be accurate to the effect (aesthetics), the theme, or the message of a novel and that the filmmaker must introduce changes, if necessary, to fit the demands of time and to maximize faithfulness along one of those axes. In most cases adaptation, the films are required to create identities (for example, a characters' costume or set decor) since they are not specified in the original material. Then, the influence of film-makers may go unrecognized because there is no comparison in the original material even though the new visual identities will affect narrative interpretation. Peter Jackson's adaptations of The Lord of the Rings trilogy and The Hobbit by author JRR Tolkien represent an unusual case since many visual and stylistic details were specified by Tolkien. For the Harry Potter film series, author JK Rowling was closely consulted by the filmmakers, and she provided",
    "label": 0
  },
  {
    "text": "trilogy and The Hobbit by author JRR Tolkien represent an unusual case since many visual and stylistic details were specified by Tolkien. For the Harry Potter film series, author JK Rowling was closely consulted by the filmmakers, and she provided production designer Stuart Craig with a map of Hogwarts' grounds and also prevented director Alfonso Cuarón from adding a graveyard scene because the graveyard would appear elsewhere in a later novel. An often overlooked aspect of film adaptation is the inclusion of sound and music. In a literary text, a specific sound effect can often be implied or specified by an event, but in the process of adaptation, filmmakers must determine specific the sound characteristics that subliminally affects narrative interpretation. In some cases of adaptation, music may have been specified in the original material (usually diegetic music). In Stephenie Meyer's 2005 Twilight novel, the characters Edward Cullen and Bella Swan both listen to Debussy's Clair de lune and Edward composes the piece Bella's Lullaby for Bella. While Clair de lune was a pre-existing piece of music, Bella's Lullaby was not and required original music to be composed for the 2008 movie adaptation. In the 2016 sci-fi film 2BR02B: To Be or Naught to Be adapted from the story by Kurt Vonnegut, the film-makers decided to abandon Vonnegut's choice of music. They stated that they felt that it worked in his prose only because it was not actually heard. Filmmakers' test screenings found that Vonnegut's style of music confused audiences and detracted from narrative comprehension. The film's composer, Leon Coward, stated, \"You can try to be as true to Vonnegut's material as possible, but at the end of the day also you’re working with the material that you as a team have generated, not just Vonnegut's, and that’s what you've got",
    "label": 0
  },
  {
    "text": "\"You can try to be as true to Vonnegut's material as possible, but at the end of the day also you’re working with the material that you as a team have generated, not just Vonnegut's, and that’s what you've got to make work.\" Theatrical adaptation Stage plays are frequent sources for film adaptations. Many of William Shakespeare's plays, including Hamlet, Romeo and Juliet, and Othello, have been adapted into films. The first sound adaptation of any Shakespeare play was the 1929 production of The Taming of the Shrew, starring Mary Pickford and Douglas Fairbanks. It was later adapted as both a musical play called Kiss Me, Kate, which opened on Broadway in 1948, and as the 1953 Hollywood musical of the same name. The Taming of the Shrew was again retold in 1999 as a teen comedy set in a high school in 10 Things I Hate about You, and also in 2003 as an urban romantic comedy, Deliver Us from Eva. The 1961 musical film West Side Story was adapted from Romeo and Juliet, with its first incarnation as a Broadway musical play that opened in 1957. The animated film The Lion King (1994) was inspired by Hamlet as well as various traditional African myths, and 2001's O was based on Othello. Film adaptations of Shakespeare's works in languages other than English are numerous, including Akira Kurosawa's films Throne of Blood (1957, an epic film version of Macbeth), The Bad Sleep Well (1960, inspired by Hamlet) and Ran (1985, based on King Lear); and Vishal Bhardwaj's \"Shakespearean trilogy\" consisting of Haider (2014, a retelling of Hamlet), Omkara (2006, based on Othello) and Maqbool (2003, based on Macbeth). Another way in which Shakespearean texts have been incorporated in films is to feature characters who are either actors performing those texts",
    "label": 0
  },
  {
    "text": "of Haider (2014, a retelling of Hamlet), Omkara (2006, based on Othello) and Maqbool (2003, based on Macbeth). Another way in which Shakespearean texts have been incorporated in films is to feature characters who are either actors performing those texts or characters who are somehow influenced or effected by seeing one of Shakespeare's plays, within a larger non-Shakespearean story. Generally, Shakespeare's basic themes or certain elements of the plot will parallel the main plot of the film or become part of a character's development in some way. Hamlet and Romeo and Juliet are the two plays which have most often been used in this way. Éric Rohmer's 1992 film Conte d'hiver (A Tale of Winter) is one example. Rohmer uses one scene from Shakespeare's A Winter's Tale as a major plot device within a story that is not based on the play at all. In Britain, where stage plays tend to be more popular as a form of entertainment than currently in the United States, many films began as a stage productions. Some British films and British/American collaborations that were based on successful British plays include Gaslight (1940), Blithe Spirit (1945), Rope (1948), Look Back in Anger (1959), Oh! What a Lovely War (1969), Sleuth (1972), The Rocky Horror Picture Show (1975), Shirley Valentine (1989), The Madness of King George (1994), The History Boys (2006), Quartet (2012), and The Lady in the Van (2015). Similarly, hit Broadway plays are often adapted into films, whether from musicals or dramas. Some examples of American film adaptations based on successful Broadway plays are Arsenic and Old Lace (1944), Born Yesterday (1950), Harvey (1950), A Streetcar Named Desire (1951), The Odd Couple (1968), The Boys in the Band (1970), Agnes of God (1985), Children of a Lesser God (1986), Glengarry Glen Ross (1992), Real",
    "label": 0
  },
  {
    "text": "Arsenic and Old Lace (1944), Born Yesterday (1950), Harvey (1950), A Streetcar Named Desire (1951), The Odd Couple (1968), The Boys in the Band (1970), Agnes of God (1985), Children of a Lesser God (1986), Glengarry Glen Ross (1992), Real Women Have Curves (2002), Rabbit Hole (2010), and Fences (2016). On one hand, theatrical adaptation does not involve as many interpolations or elisions as novel adaptation, but on the other, the demands of scenery and possibilities of motion frequently entail changes from one medium to the other. Film critics will often mention if an adapted play has a static camera or emulates a proscenium arch. Laurence Olivier consciously imitated the arch with his Henry V (1944), having the camera begin to move and to use color stock after the prologue, indicating the passage from physical to imaginative space. Sometimes, the adaptive process can continue after one translation. Mel Brooks' The Producers began as a film in 1967, was adapted into a Broadway musical in 2001, and then adapted again in 2005 as a musical film. Television adaptation Feature films are occasionally created from television series or television segments, or vice versa, a television series will derive from a film, such as in the case of Bates Motel and Chucky. In the former, the film will offer a longer storyline than the usual television program's format and/or expanded production values. During the 1970s, many British television series were turned into films including Dad's Army, On the Buses, Steptoe and Son and Porridge. In 1979, The Muppet Movie was a big success. In the adaptation of The X-Files to film, greater effects and a longer plotline were involved. Additionally, adaptations of television shows will offer the viewer the opportunity to see the television show's characters without broadcast restrictions. These additions (nudity, profanity,",
    "label": 0
  },
  {
    "text": "In the adaptation of The X-Files to film, greater effects and a longer plotline were involved. Additionally, adaptations of television shows will offer the viewer the opportunity to see the television show's characters without broadcast restrictions. These additions (nudity, profanity, explicit drug use, and explicit violence) are only rarely a featured adaptive addition (film versions of \"procedurals\" such as Miami Vice are most inclined to such additions as featured adaptations) – South Park: Bigger, Longer & Uncut is a notable example of a film being more explicit than its parent TV series. At the same time, some theatrically released films are adaptations of television miniseries events. When national film boards and state-controlled television networks co-exist, filmmakers can sometimes create very long films for television that they may adapt solely for time for theatrical release. Both Ingmar Bergman (notably with Fanny and Alexander but with other films as well) and Lars von Trier have created long television films that they then recut for international distribution. Even segments of television series have been adapted into feature films. The American television sketch comedy show Saturday Night Live has been the origin of a number of films, beginning with The Blues Brothers, which began as a one-off performance by Dan Aykroyd and John Belushi. Radio adaptation Radio narratives have also provided the basis of film adaptation. In the heyday of radio, radio segments were often translated to film, usually as shorts. Radio series turned into film series include Dr. Christian, Crime Doctor and The Whistler. Dialog-heavy stories and fantastic stories from radio were also adapted to film (e.g. Fibber McGee and Molly and The Life of Riley). The Hitchhiker's Guide to the Galaxy began as a radio series for the BBC and then became a novel that was adapted to film. Comic book adaptation",
    "label": 0
  },
  {
    "text": "adapted to film (e.g. Fibber McGee and Molly and The Life of Riley). The Hitchhiker's Guide to the Galaxy began as a radio series for the BBC and then became a novel that was adapted to film. Comic book adaptation American comic book characters, particularly superheroes, have long been adapted into film, beginning in the 1940s with Saturday movie serials aimed at children. Superman (1978) and Batman (1989) are two later successful movie adaptations of famous comic book characters. In the Philippines, superhero comics have been adapted numerous times into films such as Darna (1951), Captain Barbell (1964), and Lastik Man (1965). In addition, comics of various genres other than those involving superheroes such as romance, fantasy and drama have widely been used as a source for film adaptations such as Roberta (1951), Dyesebel (1953), Ang Panday (1980), Bituing Walang Ningning (1985) and Mars Ravelo's Bondying: The Little Big Boy (1989). In the early 2000s, blockbusters such as X-Men (2000) and Spider-Man (2002) have led to dozens of superhero films. The success of these films has also led to other comic books not necessarily about superheroes being adapted for the big screen, such as Ghost World (2001), From Hell (2001), American Splendor (2003), Sin City (2005), 300 (2007), Wanted (2008), and Whiteout (2009). The adaptation process for comics is different from that of novels. Many successful comic book series last for several decades and have featured several variations of the characters in that time. Films based on such series usually try to capture the back story and “spirit” of the character instead of adapting a particular storyline. Occasionally, aspects of the characters and their origins are simplified or modernized. Self-contained graphic novels, and miniseries many of which do not feature superheroes, can be adapted more directly, such as in the",
    "label": 0
  },
  {
    "text": "character instead of adapting a particular storyline. Occasionally, aspects of the characters and their origins are simplified or modernized. Self-contained graphic novels, and miniseries many of which do not feature superheroes, can be adapted more directly, such as in the case of Road to Perdition (2002) or V for Vendetta (2006). In particular, Robert Rodriguez did not use a screenplay for Sin City but utilized actual panels from writer/artist Frank Miller's series as storyboards to create what Rodriguez regards as a \"translation\" rather than an adaptation. Furthermore, some films based on long-running franchises use particular story lines from the franchise as a basis for a plot. The second X-Men film was loosely based on the graphic novel X-Men: God Loves, Man Kills and the third film on the storyline \"The Dark Phoenix Saga\". Spider-Man 2 was based on the storyline \"Spider-Man No More!\". Likewise, Batman Begins owes many of its elements to Miller's Batman: Year One and the film's sequel, The Dark Knight, uses subplots from Batman: The Long Halloween. The Marvel Cinematic Universe starting in 2008 is a shared universe with films combining characters from different works by Marvel Comics. The DC Extended Universe starting in 2013 uses the same model for DC Comics. The highest-grossing and most profitable comic book adaptations are Avengers: Endgame (2019) and Joker (2019), respectively. Video game adaptation A video game adaptation is primarily a film that is based on a video game, usually incorporating elements of the game's plot or gameplay, beginning in the mid-1980s. Tie-in with films Tie-in video games with films or other properties have existed since home consoles and arcade games of the early 1980s. Developers are usually limited by what they can do with the film property, and may be further limited in time as to produce the game",
    "label": 0
  },
  {
    "text": "or other properties have existed since home consoles and arcade games of the early 1980s. Developers are usually limited by what they can do with the film property, and may be further limited in time as to produce the game in time for the release of the film or other work. Related Films closely related to the computer and video game industries were also done in this time, such as Tron, Cloak & Dagger, Wreck-It Ralph, Pixels, Ready Player One and Free Guy but only after the release of several films based on well-known brands has this genre become recognized in its own right. Adaptations from other sources While documentary films have often been made from journalism and reportage, so too have some dramatic films, including: All the President's Men (1976, adapted from the 1974 book); Miracle, (2004, from an account published shortly after the 1980 \"miracle on ice\"); and Pushing Tin (1999, from a 1996 New York Times article by Darcy Frey). An Inconvenient Truth is Al Gore's film adaptation of his own Keynote multimedia presentation. The 2011 independent comedy film, Benjamin Sniddlegrass and the Cauldron of Penguins was based on Kermode and Mayo's Film Review of Percy Jackson & the Olympians: The Lightning Thief. Films adapted from songs include Coward of the County, Ode to Billy Joe, Convoy, and Pretty Baby (each from a song of the same name). Films based on toys include the Transformers franchise and the G.I. Joe films; there is a longer history of animated television series being created simultaneous to toy lines as a marketing tool. Hasbro's plans to for films based on their board games began with 2012's Battleship. While amusement park rides have often been based on action movies, conversely the 1967 Pirates of the Caribbean ride at Disneyland was adapted",
    "label": 0
  },
  {
    "text": "a marketing tool. Hasbro's plans to for films based on their board games began with 2012's Battleship. While amusement park rides have often been based on action movies, conversely the 1967 Pirates of the Caribbean ride at Disneyland was adapted into Pirates of the Caribbean: The Curse of the Black Pearl in 2003. Remakes and film sequels are technically adaptations of the original film. Less direct derivations include The Magnificent Seven from The Seven Samurai, Star Wars from The Hidden Fortress, and 12 Monkeys from La Jetée. Many films have been made from mythology and religious texts. Both Greek mythology and the Bible have been adapted frequently. Homer's works have been adapted multiple times in several nations. In these cases, the audience already knows the story well, and so the adaptation will de-emphasize elements of suspense and concentrate instead on detail and phrasing. Awards Many major film award programs present an award for adapted screenplays, separate from the award for original screenplays. In the case of a film which was adapted from an unpublished work, however, different awards have different rules around which category the screenplay qualifies for. In 1983, the Canadian Genie Awards rescinded the Best Adapted Screenplay award they had presented to the film Melanie when they learned that the original work had been unpublished; and in 2017, the film Moonlight, which was adapted from an unpublished theatrical play, was classified and nominated as an adapted screenplay by some awards but as an original screenplay by others. Adaptation of films When a film's screenplay is original, it can also be the source of derivative works such as novels and plays. For example, movie studios will commission novelizations of their popular titles or sell the rights to their titles to publishing houses. These novelized films will frequently be written",
    "label": 0
  },
  {
    "text": "also be the source of derivative works such as novels and plays. For example, movie studios will commission novelizations of their popular titles or sell the rights to their titles to publishing houses. These novelized films will frequently be written on assignment and sometimes written by authors who have only an early script as their source. Consequently, novelizations are quite often changed from the films as they appear in theatres. Novelization can build up characters and incidents for commercial reasons (e.g., to market a card or computer game, to promote the publisher's \"saga\" of novels, or to create continuity between films in a series). There have been instances of novelists who have worked from their own screenplays to create novels at nearly the same time as a film. Both Arthur C. Clarke, with 2001: A Space Odyssey, and Graham Greene, with The Third Man, have worked from their own film ideas to a novel form (although the novel version of The Third Man was written more to aid in the development of the screenplay than for the purposes of being released as a novel). Both John Sayles and Ingmar Bergman write their film ideas as novels before they begin producing them as films, although neither director has allowed these prose treatments to be published. Finally, films have inspired and been adapted into plays. John Waters's films have been successfully mounted as plays; both Hairspray and Cry-Baby have been adapted, and other films have spurred subsequent theatrical adaptations. Spamalot is a Broadway play based on Monty Python films. In a rare case of a film being adapted from a stage musical adaptation of a film, in 2005, the film adaptation of the stage musical based on Mel Brooks' classic comedy film The Producers was released. See also Remake Literary adaptation Adaptation",
    "label": 0
  },
  {
    "text": "case of a film being adapted from a stage musical adaptation of a film, in 2005, the film adaptation of the stage musical based on Mel Brooks' classic comedy film The Producers was released. See also Remake Literary adaptation Adaptation (arts) Licensed game AACTA Award for Best Adapted Screenplay Academy Award for Best Adapted Screenplay BAFTA Award for Best Adapted Screenplay César Award for Best Adaptation Golden Horse Award for Best Adapted Screenplay Goya Award for Best Adapted Screenplay Satellite Award for Best Adapted Screenplay Writers Guild of America Award for Best Adapted Screenplay References Further reading Eisenstein, Sergei. \"Dickens, Griffith, and the Film Today.\" Film Form Dennis Dobson, trans. 1951. Literature/Film Quarterly, journal published by Salisbury University Journal of Adaptation in Film and Performance, published by Intellect Adaptation, journal published by Oxford University Press Movie Adaptation Database, UC Berkeley Media Resources Center The history of Erich von Stroheim's Greed, from welcometosilentmovies.com The Art of Adaptation from hollywoodlitsales.com Hutcheon, Linda, with Siobhan O’Flynn. A Theory of Adaptation. 2nd ed. London: Routledge, 2013. Leitch, Thomas (ed.) Oxford Handbook of Adaptation Studies. Oxford: OUP, 2017. Murray, Simone. The Adaptation Industry: The Cultural Economy of Contemporary Adaptation. New York: Routledge, 2012. Sanders, Julie. Adaptation and Appropriation. London: Routledge, 2006.",
    "label": 0
  },
  {
    "text": "Futurism is a modernist avant-garde movement in literature and part of the Futurism art movement that originated in Italy in the early 20th century. It made its official literature debut with the publication of Filippo Tommaso Marinetti's Manifesto of Futurism (1909). Futurist poetry is characterised by unexpected combinations of images and by its hyper-concision (in both economy of speech and actual length). Futurist theatre also played an important role within the movement and is distinguished by scenes that are only a few sentences long, an emphasis on nonsensical humour, and attempts to examine and subvert traditions of theatre via parody and other techniques. Longer forms of literature, such as the novel, have little place in the Futurist aesthetic of speed and compression, although there are exceptions like Marinetti's Mafarka the Futurist (1909) and Aldo Palazzeschi's Man of Smoke (1911). Futurist literature primarily focuses on seven aspects: intuition, analogy, irony, abolition of syntax, metrical reform, onomatopoeia, and essential/synthetic lyricism. The ideals of the futurists expanded to their sculptures and painting styles as well; they were not fond of the cubism movement in France or the renaissance era progression (in their point of view, emasculation) and would often preach going back to old fashioned values in their manifestos and articles as well as their artwork. Although the movement was founded with manifestos written by men there were responses to Marinetti in particular from women whom considered themselves traditional feminists and did not see the previous renaissance movement as a shift towards emasculation, but relied too much on the traditional titles of \"men\" and \"women\" that pigeon holed society into believing they couldn't be empathetic and that a woman couldn't be vigorous. Methodology Intuition In Marinetti's 1909 manifesto, Marinetti calls for the reawakening of \"divine intuition\" that \"after hours of relentless toil\" allows",
    "label": 0
  },
  {
    "text": "and \"women\" that pigeon holed society into believing they couldn't be empathetic and that a woman couldn't be vigorous. Methodology Intuition In Marinetti's 1909 manifesto, Marinetti calls for the reawakening of \"divine intuition\" that \"after hours of relentless toil\" allows for the \"creative spirit seems suddenly to shake off its shackles and become prey to an incomprehensible spontaneity of conception and execution\". Soffici had a more earthly reasoning. Intuition was the means by which creation took place. He believed that there could be no abstraction of the values of futurist literature in logical terms. Rather, art was a language in and of itself that could only be expressed in that language. Any attempt to extrapolate from the literature resulted \"in the evaluation not of artistic qualities but of extraneous matters\". As such, the spontaneous creation brought by intuition freed one from abstracting (and therefore adding erroneous material into the literature) and allowed on to speak in the language of art. In this way, Futurists rallied against \"intellectualistic literature…[and] intelligible poetry\". However, this idea is different from anti-intellectualism. They were not hostile to intellectual approaches, but just the specific intellectual approach that poetry had taken for so many years. Therefore, they often rejected any form of tradition as it had been tainted with the previous intellectual approaches of the past. Analogy Analogy's purpose in Futurist writing was to show that everything related to one another. They helped to unveil this true reality lying underneath the surface of existence. That is to say, despite what the experience might show one, everything is in fact interconnected. The more startling the comparison, the more successful it is. The means for creating these analogies is intuition. This intuition is \"the poet's peculiar quality in that it enables him to discover analogies which, hidden to reason,",
    "label": 0
  },
  {
    "text": "in fact interconnected. The more startling the comparison, the more successful it is. The means for creating these analogies is intuition. This intuition is \"the poet's peculiar quality in that it enables him to discover analogies which, hidden to reason, are yet the essentials of art\". The discovering of analogies is made possible by intuition. Marinetti believed that analogies had always existed, but earlier poets had not reached out enough to bring appropriately disparate entities together. By creating a communion of two (or more) seemingly unrelated objects, the poet pierces to the \"essence of reality\". The farther the poet has to reach in terms of logical remoteness is in direct proportion to its efficacy. As analogy thus plays such an important role, it \"offers a touchstone to gauge poetical values…: the power to startle. The artistic criterion derived from analogy is stupefaction\". While an ordinary person's vision is colored by convention and tradition, the poet can brush away this top layer to reveal the reality below. The process of communicating the surprise is art while the \"stupefaction\" is the reaction to this discovery. Thus, analogies are the essence of poetry for the Futurists. Irony As the Futurists advocated the aforementioned intuition and the bucking of tradition, one might assume that they would suppress the use of irony. On the contrary, irony proved to be \"so old and forgotten that it looked almost new when the dust was brushed away from it. What was new and untried, at least more so than their principles and theories, were the futurists' stylistic devices\". Abolition of syntax Futurists believed that the constraints of syntax were inappropriate to modern life and that it did not truly represent the mind of the poet. Syntax would act as a filter in which analogies had to be processed",
    "label": 0
  },
  {
    "text": "Abolition of syntax Futurists believed that the constraints of syntax were inappropriate to modern life and that it did not truly represent the mind of the poet. Syntax would act as a filter in which analogies had to be processed and so analogies would lose their characteristic \"stupefaction.\" By abolishing syntax, the analogies would become more effective. The practical realization of this ideal meant that many parts of speech were discarded: Adjectives were thought to bring nuance in \"a universe which is…black and white\"; the infinitive provided all the idea of an action one needed without the hindrances of conjugation; substantives followed their linked substantives without other words (by the notion of analogy). Punctuation, moods, and tenses, also disappeared in order to be consistent with analogy and \"stupefaction.\" However, the Futurists were not truly abolishing syntax. White points out that since \"The OED defines 'syntax' as 'the arrangement of words in their proper forms' by which their connection and relation in a sentence are shown\". The Futurists were not destroying syntax in that sense. Marinetti in truth advocated a number of \"substantial, but nevertheless selective modifications to existing syntax\" and that the \"Russian Futurists' idea that they were 'shaking syntax loose'\" is more accurate. Metrical reform Early Futurist poetry relied on free verse as their poetical vehicle. However, free verse \"was too thoroughly bound up with tradition and too fond of producing…stale effects\" to be effective. Furthermore, by using free verse, the Futurist realized they would be working under the rules of syntax and therefore interfering with intuition and inspiration. In order to break free of the shackles of meter, they resorted to what they called \"parole in libertá (word autonomy)\". Essentially, all ideas of meter were rejected and the word became the main unit of concern instead of the",
    "label": 0
  },
  {
    "text": "In order to break free of the shackles of meter, they resorted to what they called \"parole in libertá (word autonomy)\". Essentially, all ideas of meter were rejected and the word became the main unit of concern instead of the meter. In this way, the Futurists managed to create a new language free of syntax punctuation, and metrics that allowed for free expression. For example, in the poem entitled \"Studio\" by Soffici, he \"describes the artist's studio—and by extension, modern man himself—as becoming a 'radiotelefantastic cabin open to all messages', the sense of wonder her being transmitted by the portmanteau neologism: 'readotelefantastica'\". Here all notions of familiar language have been abandoned and in their place a new language has emerged with its own vocabulary. Onomatopoeia There were four forms of onomatopoeia that the Futurists advocated: direct, indirect, integral, and abstract. The first of these four is the usually onomatopoeia seen in typical poetry, e.g. boom, splash, tweet. They convey the most realistic translation of sound into language. Indirect onomatopoeia \"expressed the subjective responses to external conditions\". Integral onomatopoeia was \"the introduction of any and every sound irrespective of its similarity to significant words\". This meant that any collection of letters could represent a sound. The final form of onomatopoeia did not reference external sounds or movements like the aforementioned versions of onomatopoeia. Rather, they tried to capture the internal motions of the soul. Essential/synthetic lyricism In order to better provide stark, contrasting, analogies, the Futurist literature promoted a kind of hyper-conciseness. It was dubbed essential and synthetic lyricism. The former refers to a paring down of any and all superfluous objects while the latter expresses an unnatural compactness of the language unseen elsewhere. This idea explains where poetry became the preferred literary medium of Futurism and why there are no",
    "label": 0
  },
  {
    "text": "refers to a paring down of any and all superfluous objects while the latter expresses an unnatural compactness of the language unseen elsewhere. This idea explains where poetry became the preferred literary medium of Futurism and why there are no Futurist novels (since novels are neither pared down nor compressed). Futurism in the theatre Traditional theatre often served as a target for Futurists because of its deep roots in classical societies. In its stead, the Futurists exalted the variety theatre, vaudeville, and music hall because, they argued, it \"had no tradition; it was a recent discovery\". Vaudevillian acts aligned themselves well to the notions of \"stupefaction\" as there was the desire to surprise and excite the audience. Furthermore, the heavy use of machinery attracted the Futurists, as well as Vaudevillian acts' tendency to \"destroy\" the masterpieces of the past through parody and other forms of depreciation. By adding other Futurist ideals mentioned above, they firmly rooted their beliefs into theatre. They wanted to blur the line between art and life in order to reach below the surface to reality. In practice, this manifested itself in various ways: \"Collaboration between the public and the actors was to be developed to the point of indistinction of roles—such cooperating confusion was to be partly impromptu…e.g. chairs were to be covered with glue so that ladies' gowns would stick to them; and tickets sold in such a way as to bring side by side men of the extreme right and those of the extreme left, prudes and prostitutes, teachers and pupils. Sneezing powders, sudden darkening of the hall, and alarm signals were all means to insure the proper functioning of this universal human farce\". However, the most important aspect of the work was the discrediting of the great works of theatre. These new theatrical",
    "label": 0
  },
  {
    "text": "darkening of the hall, and alarm signals were all means to insure the proper functioning of this universal human farce\". However, the most important aspect of the work was the discrediting of the great works of theatre. These new theatrical ideal of the Futurists helped to establish a new genre of theatre: the synthetic play. Synthetic play This type of play took the idea of compression to an extreme, where \"a brief performance in which entire acts were reduced to a few sentences, and scenes to a handful of words. No sentiments, no psychological development, no atmosphere, no suggestiveness. Common sense was banished, or rather, replaced by nonsense\". There did exist some plays similar to this before the Futurists, but they did not conform to the Futurist agenda. The creator of the first modern synthetic play is thought to be Verlaine, with his aptly titled work Excessive Haste. Futurism in Art Work It took some time during the development of the movement to take shape in all the areas the futurists wanted to explore in order to visually represent their message especially in two-dimensional paintings. Their hybrid-style of human and machine or showing a forward motion was common due to their worship of the modern machine and technology. One of the most popular representations of this is in the piece \"Dynamism of the dog by Giacomo Balla. This piece shows a dog being walked on a lease and rather than the typical four-legged representation the dog has multiple feet in a swishing type motion to suggest movement. This piece along with the others that followed was referred to as \"Dynamism\": \"No single object is separate from its background or another object.\" Examples Excerpt from Marinetti's free verse poem To a Racing Car Source: Veemente dio d’una razza d’acciaio, Automobile ebbra",
    "label": 0
  },
  {
    "text": "with the others that followed was referred to as \"Dynamism\": \"No single object is separate from its background or another object.\" Examples Excerpt from Marinetti's free verse poem To a Racing Car Source: Veemente dio d’una razza d’acciaio, Automobile ebbra di spazio, che scalpiti e fremi d’angoscia rodendo il morso con striduli denti Formidabile mostro giapponese, dagli occhi di fucina, nutrito di fiamma e d’olî minerali, avido d’orizzonti, di prede siderali Io scateno il tuo cuore che tonfa diabolicamente, scateno i tuoi giganteschi pneumatici, per la danza che tu sai danzare via per le bianche strade di tutto il mondo! Vehement god from a race of steel, Automobile drunk with space, Trampling with anguish, bit between your strident teeth! O formidable Japanese monster with forge, Nourished with flame and mineral oils, Hungry for horizons and sidereal prey, I unleash your heart to the diabolical vroom-vroom And your giant radials, for the dance You lead on the white roads of the world. Farfa's parole in Libertá poem Triangle Source: they were three three the he and the other the other and the other was a real triangle a true younger brother of the file of steel rusty stell that he in the frenzy of possession only his only his alone seized abruptly and thrust theeeeere into the pure white velvet of her belly plugging her new opening with his flesh with his virile flesh ah ah ah ahh ahh ahh ahe haaaahh List of futurist poets by region Source: Czechoslovakia Stanislav Kostka Neumann Vítězslav Nezval Jaroslav Seifert Italy Libero Altomare Paolo Buzzi Enrico Cardile Loris Catrizzi Enrico Cavacchioli Auroa D'Alba Escodame Farfa Fillia Luciano Folgore Filippo Tommaso Marinetti Armando Mazza Aldo Palazzeschi Giovanni Papini Bruno G. Sanzin Ruggero Vasari Poland Tytus Czyżewski Jan Hrynkowski Jerzy Jankowski Bruno Jasieński Anatol Stern Aleksander",
    "label": 0
  },
  {
    "text": "Intelligence literature, sometimes referred to as espionage nonfiction, is a genre of non-fiction or historical literature, written in any language, that focuses on the intelligence field, and its most popular subfield known as espionage. This field of literature includes biographies and autobiographies of intelligence officers, historical research and analysis of intelligence operations and missions, studies of undercover work, policy and legal studies surrounding the fields of intelligence law, intelligence history, and national security law, academic and professional journals, essays, textbooks, and more. Other works of intelligence literature include official histories, official reports, tradecraft and technical manuals, declassified documents and archival materials, and oral histories and interviews. The origins of intelligence literature Intelligence literature is rooted in the original works of myth, folklore, oral tradition, political science and philosophy, as spycraft has always been a tool of statecraft, warfare, and diplomacy, and for the majority of its history, intelligence literature was wrapped-up into these genres. In ancient cultures, there often was no distinction between fiction and nonfiction. It took hundreds to thousands of years for it to become its own, unique, genre of literature. Ancient Egypt The Story of Wenamun is a Hieratic Late Egyptian literary text that focuses on Wenamun, a priest of Amun at Karnak, is sent by the High Priest of Amun Herihor on a secret mission to the Phoenician city of Byblos to acquire lumber (probably cedar wood) to build a new ship to transport the cult image of Amun. After visiting Smendes (Nesbanebded in Egyptian) at Tanis, Wenamun stopped at the port of Dor ruled by the Tjeker prince Beder, where he was robbed. Upon reaching Byblos, he was shocked by the hostile reception he received there. When he finally gained an audience with Zakar-Baal, the local king, the latter refused to give the requested goods",
    "label": 0
  },
  {
    "text": "the Tjeker prince Beder, where he was robbed. Upon reaching Byblos, he was shocked by the hostile reception he received there. When he finally gained an audience with Zakar-Baal, the local king, the latter refused to give the requested goods for free, as had been the traditional custom, instead demanding payment. Wenamun had to send to Smendes for payment, a humiliating move that demonstrates the waning of Egyptian power over the Eastern Mediterranean; a causative factor of a new nature can be seen in this ebbing of Egyptian power — the rise of Assyria and its intrusion into Phoenicia around the year 1100 BCE. After a wait of almost a year at Byblos, Wenamun attempted to leave for Egypt, only to be blown off course to Alashiya (Cyprus), where he was almost killed by an angry mob before placing himself under the protection of the local queen, whom he called Hatbi. At this point the story breaks off. Rome Strategemata is a work by the Roman writer Frontinus, written in the 1st century, which focuses on military strategy, but does include discussions on deception and intelligence gathering. This book is more accurately defined as Military history, but includes within it sections on Intelligence history. The first section of this book is entitled \"On Concealing One's Plans,\" and the second is entitled \"On Discovering the Enemy's Plans,\" both of which are directly related to the fields of intelligence collection and counterintelligence. India Arthashastra (Sanskrit: अर्थशास्त्रम्, IAST: Kautiliyam Arthaśāstram) is an Ancient Indian Sanskrit treatise on statecraft, politics, economic policy and military strategy. The Arthashastra explores issues of social welfare, the collective ethics that hold a society together, advising the king that in times and in areas devastated by famine, epidemic and such acts of nature, or by war, he should initiate",
    "label": 0
  },
  {
    "text": "strategy. The Arthashastra explores issues of social welfare, the collective ethics that hold a society together, advising the king that in times and in areas devastated by famine, epidemic and such acts of nature, or by war, he should initiate public projectssuch as creating irrigation waterways and building forts around major strategic holdings and towns and exempt taxes on those affected. The text was influenced by Hindu texts such as the sections on kings, governance and legal procedures included in Manusmriti. China In China, the author Sun Tzu is perhaps most well known for his seminal work, The Art of War. In the Art of War, Sun Tzu suggests tactics and strategies for spies, but in the broader conception of warfighting. The Art of War is an early inspiration for many intelligence officers and leaders of intelligence agencies, but it is not, on its own, a work of intelligence literature. The Crusades During the Crusades, the Knights Templar, which was engaged in espionage activities in the Holy Land, might have published manuals on clandestine activities. However, after the Templar Order was disbanded in 1312 by Pope Clement V, many of their records were seized or destroyed. Some theorists speculate that if the Templars had intelligence manuals, they were either lost or kept secret by successor groups (e.g., the Freemasons or other knightly orders). The Renaissance: Raison d’état Italian writers in the 1500s, such as Niccolò Machiavelli, Giovanni Botero, and Giovan Battista Possevino (Giovan's French Wikipedia article) published many treatises and manuals for statecraft, which did include literature surrounding the intelligence field, but might not be considered works of intelligence literature, but still bound in political science and philosophy. Perhaps the most persistent author of this period is Machiavelli, with his collection of works including The Prince and The Art of",
    "label": 0
  },
  {
    "text": "field, but might not be considered works of intelligence literature, but still bound in political science and philosophy. Perhaps the most persistent author of this period is Machiavelli, with his collection of works including The Prince and The Art of War, both of which have inspired generations of diplomats, politicians, and spies. The Prince, upon its release, shocked the readers of Europe because it presented a moral case for ill deeds in the time of war. Scholars often note that Machiavelli glorifies instrumentality in state building, an approach embodied by the saying, often attributed to interpretations of The Prince, \"The ends justify the means\". Fraud and deceit are held by Machiavelli as necessary for a prince to use. Memoir and autobiography The first known intelligence memoir (in English) The first officially recognized, but not widely known, memoir in intelligence literature was Memoirs of Secret Service: , written by Matthew Smith in the 17th century. Prior to this, intelligence officers did not share their secrets – but Smith submitted his manuscript to the House of Commons of the United Kingdom in order to defend himself against accusations that he had been incompetent and in league with Jacobites. in which he had been called to testify in front of the House of Lords. In his memoir, he accused Charles Talbot, 1st Duke of Shrewsbury and his aide, James Vernon, of having been involved in a Jacobite assassination attempt. He opens his memoir with the following words: The book's bold accusations generated public interest and was even translated, but Parliament largely ignored it. In November 1699, Smith intensified his claims against the Earl of Shrewsbury in Remarks upon the D— of S—'s Letter to the House of Lords, which led to his brief imprisonment for breaching parliamentary privilege and the public burning of",
    "label": 0
  },
  {
    "text": "In November 1699, Smith intensified his claims against the Earl of Shrewsbury in Remarks upon the D— of S—'s Letter to the House of Lords, which led to his brief imprisonment for breaching parliamentary privilege and the public burning of the pamphlet. Despite setbacks, Smith continued publishing attacks, responding to Kingston's pamphlet with A Reply to an Unjust and Scandalous Libel in 1700. After William III’s death, he wrote a petition accusing Shrewsbury and James Vernon of treason and alleging that the late king had bribed MPs. However, fearing poor timing after William Fuller’s recent exposure as a fraud, he hesitated before submitting it to Speaker Robert Harley in March 1702. Harley ignored the claims, and with Shrewsbury overseas and Vernon out of office, the accusations faded. Smith’s fortunes later changed in 1703 when he was appointed judge-advocate of Jamaica, likely through the influence of the earl of Peterborough. However, little is known of his later years, and he is believed to have died before 1723. Memoirs of the Secret Service of John Macky John Macky (died 1726) was a Scottish spy and travel writer. Between 1688 and 1710 he ran a successful intelligence gathering network across the English Channel, principally concerned with Jacobite and French threats to England. He was also the author of several publications which reflected his travel, political outlook and access to leading figures of the period. Memoirs of the Secret Services of John Macky (1733) is a firsthand account of his espionage and political intelligence gathering activities during the late 17th and early 18th centuries. Macky, even though he was Scottish, provided intelligence to the English government about Jacobite activities and French military movements. His memoirs detail his observations and experiences during the reigns of William III and Queen Anne, offering insight into diplomatic maneuvering,",
    "label": 0
  },
  {
    "text": "Macky, even though he was Scottish, provided intelligence to the English government about Jacobite activities and French military movements. His memoirs detail his observations and experiences during the reigns of William III and Queen Anne, offering insight into diplomatic maneuvering, spy networks, and court intrigues. His work provides an early example of structured intelligence reporting, reflecting the growing professionalization of espionage in Britain. The text also serves as an important primary source for researchers studying Jacobitism, Anglo-French relations, and the broader development of intelligence operations in early modern Europe. Despite its contested reliability, Memoirs of the Secret Services remains an essential reference in the history of espionage. The Spycatcher Affair The most famous controversial memoir in intelligence literature is likely Spycatcher: the Candid Autobiography of a Senior Intelligence Officer, by the British spy Peter Wright. This book caused a great controversy in the United Kingdom when it was released, as it detailed aspects of the intelligence field in the era of the Cambridge Five that were damaging to the British government. In this memoir, Wright alleges the presence of a Soviet mole within MI5, specifically accusing former Director-General Sir Roger Hollis. The book also delves into various covert operations and internal dynamics of British intelligence. Upon its release in 1987, the British government sought to suppress its distribution, leading to legal battles and debates over press freedom. Recent revelations have shed light on the British government's internal handling of the affair. Declassified documents indicate that Prime Minister Margaret Thatcher approved the controlled leak of information regarding the suspected mole, Sir Roger Hollis, to journalist Chapman Pincher. This strategic leak aimed to manage public perception and preempt the disclosures in Wright's forthcoming book. Additionally, these documents reveal that Cabinet Secretary Robert Armstrong provided misleading testimony during the Spycatcher trial, denying the",
    "label": 0
  },
  {
    "text": "Roger Hollis, to journalist Chapman Pincher. This strategic leak aimed to manage public perception and preempt the disclosures in Wright's forthcoming book. Additionally, these documents reveal that Cabinet Secretary Robert Armstrong provided misleading testimony during the Spycatcher trial, denying the orchestrated leak under oath. The government's attempts to suppress Spycatcher inadvertently amplified its notoriety. While the book was banned in England, it remained available in other regions, including Scotland and Australia, leading to widespread public interest and debate. The affair not only highlighted internal tensions within British intelligence but also sparked discussions on press freedom, governmental transparency, and the public's right to information. Academic literature Intelligence studies Intelligence Studies is an interdisciplinary academic field that concerns intelligence assessment and intelligence analysis. Intelligence has been referred to as the \"lost dimension\" of the fields of international relations (IR) and diplomatic history, as the secretive nature of the subject means most intelligence successes are unknown. Intelligence studies comprise a small but major component of intelligence literature. Security studies Security studies, also known as international security studies, is an academic sub-field within the wider discipline of international relations that studies organized violence, military conflict, national security, and international security. Journals in intelligence National Security Law Journal Intelligence (journal) Political Science Quarterly Small Wars Journal Studies in Intelligence Periscope (discontinued) Intelligencer Journal American Intelligence Journal Intelligence Report Foreign Intelligence Literary Scene Intelligence literature curation and intelligence collections The largest collection of intelligence history remains the Intelligence History Collection (IHC), which is housed in the CIA Library, containing over 23,000 volumes. This collection was mostly gathered by Walter Pforzheimer, who was often referred to by his honorific title in the intelligence world as \"The Dean of Intelligence Literature.\" In 1954, Pforzheimer was assigned by Allen Dulles to develop the Historical Intelligence Collection, a role he",
    "label": 0
  },
  {
    "text": "gathered by Walter Pforzheimer, who was often referred to by his honorific title in the intelligence world as \"The Dean of Intelligence Literature.\" In 1954, Pforzheimer was assigned by Allen Dulles to develop the Historical Intelligence Collection, a role he held until his retirement in 1974. Pforzheimer's mandate was to develop a collection encompassing all aspects of intelligence operations and doctrine, providing a valuable resource for Agency personnel. Under Pforzheimer's leadership, the HIC expanded rapidly. In its inaugural year, the collection grew to include 3,570 volumes, with Pforzheimer personally acquiring 1,308 books during a European trip across ten countries, all for under $2,500. He defined the collection's scope to cover a wide range of topics, including military and national intelligence, espionage, counterespionage, unconventional warfare, cryptography, and various elements of intelligence tradecraft. This comprehensive approach ensured that the HIC became an unparalleled resource within the intelligence community. When he retired, the HIC had grown to 22,000 volumes, the largest professional intelligence collection in the world. His efforts not only provided CIA officers with a rich repository of historical intelligence materials but also laid the foundation for ongoing scholarly research in the field. The Collection is now a part of the CIA Library. Writers on World War II: 1939–1945 Writers on Cold War era: 1945–1991 Writers of other nationalities Michael Ross, The Volunteer: The Incredible True Story of an Israeli Spy on the Trail of International TerroristsMcClelland & Stewart 2007, rev. 2008 Jean-Marie Thiébaud, Dictionnaire Encyclopédique International des Abréviations, Singles et Acronyms, Armée et armament, Gendarmerie, Police, Services de renseignement et Services secrets français et étrangers, Espionage, Counterespionage, Services de Secours, Organisations révolutionnaires et terrorists, Paris, L'Harmattan, 2015, 827 pFrench journalist Gérard de Villiers began to write his SAS series in 1965. The franchise now extends to 200 titles and 150 million",
    "label": 0
  },
  {
    "text": "secrets français et étrangers, Espionage, Counterespionage, Services de Secours, Organisations révolutionnaires et terrorists, Paris, L'Harmattan, 2015, 827 pFrench journalist Gérard de Villiers began to write his SAS series in 1965. The franchise now extends to 200 titles and 150 million books. Julian Semyonov was an influential spy novelist, writing in the Eastern Bloc, whose range of novels and novel series featured a White Russian spy in the USSR; Max Otto von Stierlitz, a Soviet mole in the Nazi High Command, and Felix Dzerzhinsky, founder of the Cheka. In his novels, Semyonov covered much Soviet intelligence history, ranging from the Russian Civil War (1917–1923), through the Great Patriotic War (1941–45), to the Russo–American Cold War (1945–91). Swedish author Jan Guillou also began to write his Coq Rouge series, featuring Swedish spy Carl Hamilton, during this period, beginning in 1986. Further reading Surveys Andrew, Christopher. For the President's Eyes Only: Secret Intelligence and the American Presidency from Washington to Bush (1996) Black, Ian and Morris, Benny Israel's Secret Wars: A History of Israel's Intelligence Services (1991) Bungert, Heike et al. eds. Secret Intelligence in the Twentieth Century (2003) Archived 2012-07-16 at the Wayback Machine essays by scholars Dulles, Allen W. The Craft of Intelligence: America's Legendary Spy Master on the Fundamentals of Intelligence Gathering for a Free World (2006) Kahn, David The Codebreakers: The Comprehensive History of Secret Communication from Ancient Times to the Internet (1996), 1200 pages Lerner, K. Lee and Brenda Wilmoth Lerner, eds. Encyclopedia of Espionage, Intelligence and Security (2003), 1100 pages. 850 articles, strongest on technology Odom, Gen. William E. Fixing Intelligence: For a More Secure America, Second Edition (Yale Nota Bene) (2004) O'Toole, George. Honorable Treachery: A History of U.S. Intelligence, Espionage, Covert Action from the American Revolution to the CIA (1991) Owen, David. Hidden Secrets: A",
    "label": 0
  },
  {
    "text": "William E. Fixing Intelligence: For a More Secure America, Second Edition (Yale Nota Bene) (2004) O'Toole, George. Honorable Treachery: A History of U.S. Intelligence, Espionage, Covert Action from the American Revolution to the CIA (1991) Owen, David. Hidden Secrets: A Complete History of Espionage and the Technology Used to Support It (2002), popular Richelson, Jeffery T. A Century of Spies: Intelligence in the Twentieth Century (1997) Archived 2012-07-16 at the Wayback Machine Richelson, Jeffery T. The U.S. Intelligence Community (4th ed. 1999) Shulsky, Abram N. and Schmitt, Gary J. \"Silent Warfare: Understanding the World of Intelligence\" (3rd ed. 2002), 285 pages West, Nigel. MI6: British Secret Intelligence Service Operations 1909–1945 (1983) West, Nigel. Secret War: The Story of SOE, Britain's Wartime Sabotage Organization (1992) Wohlstetter, Roberta. Pearl Harbor: Warning and Decision (1962) World War I Beesly, Patrick. Room 40. (1982). Covers the breaking of German codes by RN intelligence, including the Turkish bribe, Zimmermann telegram, and failure at Jutland. May, Ernest (ed.) Knowing One's Enemies: Intelligence Assessment before the Two World Wars (1984) Tuchman, Barbara W. The Zimmermann Telegram (1966) Yardley, Herbert O. American Black Chamber (2004) World War II 1931–1945 Babington Smith, Constance. Air Spy: the Story of Photo Intelligence in World War II (1957) - originally published as Evidence in Camera in the UK Beesly, Patrick. Very Special Intelligence: the Story of the Admiralty's Operational Intelligence Centre, 1939–1945 (1977) Hinsley, F. H. British Intelligence in the Second World War (1996) (abridged version of multivolume official history) Jones, R. V. Most Secret War: British Scientific Intelligence 1939–1945 (2009) Kahn, David. Hitler's Spies: German Military Intelligence in World War II (1978) Kahn, David. Seizing the Enigma: the Race to Break the German U-Boat Codes, 1939–1943 (1991) Kitson, Simon. The Hunt for Nazi Spies: Fighting Espionage in Vichy France, Chicago: University",
    "label": 0
  },
  {
    "text": "David. Hitler's Spies: German Military Intelligence in World War II (1978) Kahn, David. Seizing the Enigma: the Race to Break the German U-Boat Codes, 1939–1943 (1991) Kitson, Simon. The Hunt for Nazi Spies: Fighting Espionage in Vichy France, Chicago: University of Chicago Press, (2008). ISBN 978-0-226-43893-1 Lewin, Ronald. The American Magic: Codes, Ciphers and the Defeat of Japan (1982) May, Ernest (ed.) Knowing One's Enemies: Intelligence Assessment before the Two World Wars (1984) Smith, Richard Harris. OSS: the Secret History of America's First Central Intelligence Agency (2005) Stanley, Roy M. World War II Photo Intelligence (1981) Wark, Wesley K. The Ultimate Enemy: British Intelligence and Nazi Germany, 1933–1939 (1985) Wark, Wesley K. \"Cryptographic Innocence: the Origins of Signals Intelligence in Canada in the Second World War\", in: Journal of Contemporary History 22 (1987) Cold War Era 1945–1991 Aldrich, Richard J. The Hidden Hand: Britain, America and Cold War Secret Intelligence (2002). Ambrose, Stephen E. Ike's Spies: Eisenhower and the Intelligence Establishment (1981). Andrew, Christopher and Vasili Mitrokhin. The Sword and the Shield: The Mitrokhin Archive and the Secret History of the KGB (1999) Archived 2012-07-16 at the Wayback Machine Andrew, Christopher, and Oleg Gordievsky. KGB: The Inside Story of Its Foreign Operations from Lenin to Gorbachev (1990). Bogle, Lori, ed. Cold War Espionage and Spying (2001), essays by scholars Boiling, Graham. Secret Students on Parade: Cold War Memories of JSSL, CRAIL, PlaneTree, 2005. ISBN 1-84294-169-0 Dorril, Stephen. MI6: Inside the Covert World of Her Majesty's Secret Intelligence Service (2000). Dziak, John J. Chekisty: A History of the KGB (1988) Elliott, Geoffrey and Shukman, Harold. Secret Classrooms. An Untold Story of the Cold War. London, St Ermin's Press, Revised Edition, 2003. ISBN 1-903608-13-9 Koehler, John O. Stasi: The Untold Story of the East German Secret Police (1999) Archived 2012-07-16 at the",
    "label": 0
  },
  {
    "text": "Literary language is the register of a language used when writing in a formal, academic, or particularly polite tone; when speaking or writing in such a tone, it can also be known as formal language. It may be the standardized variety of a language. It can sometimes differ noticeably from the various spoken lects, but the difference between literary and non-literary forms is greater in some languages than in others. If there is a strong divergence between a written form and the spoken vernacular, the language is said to exhibit diglossia. The understanding of the term differs from one linguistic tradition to another and is dependent on the terminological conventions adopted. Literary English For much of its history, there has been a distinction in the English language between an elevated literary language (written) and a colloquial or vernacular language (spoken, but sometimes also represented in writing). After the Norman conquest of England, for instance, Latin and French displaced English as the official and literary languages, and standardized literary English did not emerge until the end of the Middle Ages. At this time and into the Renaissance, the practice of aureation (the introduction of terms from classical languages, often through poetry) was an important part of the reclamation of status for the English language, and many historically aureate terms are now part of general common usage. Modern English no longer has quite the same distinction between literary and colloquial registers. English has been used as a literary language in countries that were formerly part of the British Empire, for instance in India up to the present day, Malaysia in the early 20th century and Nigeria, where English remains the official language. Written in Early Modern English, the King James Bible and works by William Shakespeare from the 17th century are defined",
    "label": 0
  },
  {
    "text": "India up to the present day, Malaysia in the early 20th century and Nigeria, where English remains the official language. Written in Early Modern English, the King James Bible and works by William Shakespeare from the 17th century are defined as prototype mediums of literary English and are taught in advanced English classes. Furthermore, many literary words that are used today are found in abundance in the works of Shakespeare and as well as in King James Bible, hence the literary importance of early modern English in contemporary English literature and English studies. Other languages Arabic Modern Standard Arabic is the contemporary literary and standard register of Classical Arabic used in writing across all Arabic-speaking countries and any governing body with Arabic as an official language. Many western scholars distinguish two varieties: the Classical Arabic of the Qur'an and early Islamic (7th to 9th centuries) literature; and Modern Standard Arabic (MSA), the standard language in use today. The modern standard language is closely based on the Classical language, and most Arabs consider the two varieties to be two registers of the same language. Literary Arabic or classical Arabic is the official language of all Arab countries and is the only form of Arabic taught in schools at all stages . The sociolinguistic situation of Arabic in modern times provides a prime example of the linguistic phenomenon of diglossia—the use of two distinct varieties of the same language, usually in different social contexts. Educated Arabic speakers are usually able to communicate in MSA in formal situations. This diglossic situation facilitates code-switching in which a speaker switches back and forth between the two varieties of the language, sometimes even within the same sentence. In instances in which highly educated Arabic-speakers of different nationalities engage in conversation but find their dialects mutually unintelligible",
    "label": 0
  },
  {
    "text": "in which a speaker switches back and forth between the two varieties of the language, sometimes even within the same sentence. In instances in which highly educated Arabic-speakers of different nationalities engage in conversation but find their dialects mutually unintelligible (e.g. a Moroccan speaking with a Kuwaiti), they are able to code switch into MSA for the sake of communication. Aramaic The Aramaic language has been diglossic for much of its history, with many different literary standards serving as the \"high\" liturgical languages, including Syriac language, Jewish Palestinian Aramaic, Jewish Babylonian Aramaic, Samaritan Aramaic language and Mandaic language, while the vernacular Neo-Aramaic languages serve as the vernacular language spoken by the common people like Northeastern Neo-Aramaic (Sureth, Bohtan Neo-Aramaic, Hértevin language, Koy Sanjaq Syriac language, Senaya language), Western Neo-Aramaic, Northeastern Neo-Aramaic, Central Neo-Aramaic (Mlahsô language, Turoyo language), Neo-Mandaic, Hulaulá language, Lishana Deni, Lishanid Noshan, Lishán Didán, Betanure Jewish Neo-Aramaic, and Barzani Jewish Neo-Aramaic. Armenian The Armenian language was a diglossic language for much of its history, with Classical Armenian serving as the \"high\" literary standard and liturgical language, and the Western Armenian and Eastern Armenian dialects serving as the vernacular language of the Armenian people. Western Armenian and Eastern Armenian were eventually standardized into their own literary forms. Bengali Standard Bengali has two forms: Chôlitôbhasha (চলিত ভাষা calita bhāṣā), the vernacular standard based on the elite speech of Kolkata. Shadhubhasha (সাধু ভাষা sādhu bhāṣā), the literary standard, which employs more Sanskritized vocabulary and longer prefixes and suffixes. Grammatically, the two forms are identical; differing forms, such as verb conjugations, are easily converted from one form to another. However, the vocabulary is quite different from one form to the other and must be learned separately. Among the works of Rabindranath Tagore are examples of both shadhubhasha (especially among his earlier works)",
    "label": 0
  },
  {
    "text": "easily converted from one form to another. However, the vocabulary is quite different from one form to the other and must be learned separately. Among the works of Rabindranath Tagore are examples of both shadhubhasha (especially among his earlier works) and chôlitôbhasha (especially among his later works). The national anthem of India was originally written in the shadhubhasha form of Bengali. Chinese Literary Chinese (文言文; wényánwén; 'written-speech writing') is the form of written Chinese used from the end of the Han dynasty to the early 20th century. Literary Chinese continually diverged from Classical Chinese, as the dialects of China became more disparate and as the classical written language became less representative of the spoken language. At the same time, Literary Chinese was based largely upon the Classical Chinese, and writers frequently borrowed from the classical language into their literary writings. Literary Chinese therefore shows a great deal of similarity to Classical Chinese, even though the similarity decreased over the centuries. Starting from early 20th century, written vernacular Chinese (simplified Chinese: 白话文; traditional Chinese: 白話文; pinyin: báihuàwén) became the literary standard. This is mostly aligned with a standardized form of Mandarin Chinese, which however means there exists considerable divergence between written vernacular Chinese and other Chinese variants like Cantonese, Shanghainese, Hokkien and Sichuanese. Some of these variants have their own literary form, but none of them are currently used in official formal registers, although they may be used in legal transcription, and in certain media and entertainment settings. Finnish The Finnish language has a literary variant, literary Finnish, and a spoken variant, spoken Finnish. Both are considered a form of non-dialectal standard language, and are used throughout the country. Literary Finnish is a consciously created fusion of dialects for use as a literary language, which is rarely spoken at all, being",
    "label": 0
  },
  {
    "text": "variant, spoken Finnish. Both are considered a form of non-dialectal standard language, and are used throughout the country. Literary Finnish is a consciously created fusion of dialects for use as a literary language, which is rarely spoken at all, being confined to writing and official speeches. Georgian The Georgian language has a literary liturgical form, the Old Georgian language, while the vernacular spoken varieties are the Georgian dialects and other related Kartvelian languages like Svan language, Mingrelian language, and Laz language. German German differentiates between Hochdeutsch/Standarddeutsch (Standard German) and Umgangssprache (everyday/vernacular language). Amongst the differences are the regular use of the genitive case and the simple past tense Präteritum in written language. In vernacular German, genitive phrases (\"des Tages\") are frequently replaced with a construction of \"von\" + dative object (\"von dem Tag\") — comparable to English \"the dog's tail\" vs. \"the tail of the dog\". Likewise, the Präteritum (\"ich ging\") can be substituted with the perfect (\"ich bin gegangen\") to a certain degree. The preterite and genitive cases are still used in daily language, if rarely. Their use in vernacular can depend on the regional dialect and education of the speaker. People of higher education use the genitive more regularly in their casual speech, and the use of perfect instead of Präteritum is especially common in southern Germany, where the Präteritum is considered somewhat declamatory. The German Konjunktiv I / II (\"er habe\" / \"er hätte\") is also used more often in written form, and is replaced by the conditional (\"er würde geben\") in spoken language, although in some southern German dialects the Konjunktiv II is used more often. Generally there is a continuum between more dialectical varieties and more standard varieties in German, while colloquial German nonetheless tends to increase analytic elements at the expense of synthetic elements.",
    "label": 0
  },
  {
    "text": "southern German dialects the Konjunktiv II is used more often. Generally there is a continuum between more dialectical varieties and more standard varieties in German, while colloquial German nonetheless tends to increase analytic elements at the expense of synthetic elements. Greek From the early nineteenth century until the mid-20th century, Katharevousa, a form of Greek, was used for literary purposes. In later years, Katharevousa was used only for official and formal purposes (such as politics, letters, official documents, and newscasting) while Dimotiki, 'demotic' or popular Greek, was the daily language. This created a diglossic situation until in 1976, Dimotiki was made the official language. Hebrew During the revival of the Hebrew language, spoken and literary Hebrew were revived separately, causing a dispersion between the two. The dispersion started to narrow sometime after the two movements merged, but substantial differences between the two still exist. Irish and Scottish Gaelic Early Modern Irish, also called Classical Gaelic or Classical Irish (Gaoidhealg) was a shared literary form of Gaelic that was in use by poets in Scotland and Ireland from the 13th century to the 18th century. Before that time, the vernacular dialects of Ireland and Scotland were considered to belong to a single language, and in the late 12th century a highly formalized standard variant of that language was created for the use in Irish bardic poetry. The standard was created by medieval Gaelic poets based on the vernacular usage of the late 12th century and allowed a lot of dialectal forms that existed at that point in time, but was kept conservative and was taught virtually unchanged throughout later centuries. The grammar and metrical rules were described in a series of grammatical tracts and linguistic poems used for teaching in bardic schools. Italian Standard Italian evolved as a literary language, based",
    "label": 0
  },
  {
    "text": "conservative and was taught virtually unchanged throughout later centuries. The grammar and metrical rules were described in a series of grammatical tracts and linguistic poems used for teaching in bardic schools. Italian Standard Italian evolved as a literary language, based principally on the Tuscan dialect, in part due to the prestige enjoyed by Florentine authors like Dante, Petrarch, Boccaccio, Machiavelli, and Francesco Guicciardini. Different languages were spoken throughout Italy, almost all of which were Romance languages which had developed in every region, due to the political and cultural fragmentation of the peninsula. Now, it is the standard language of Italy, due to modern media and education, and many of Italy's other languages and dialects are dying out. Japanese Until the late 1940s, the prominent literary language in Japan was the Classical Japanese language (文語, bungo), which is based on the language spoken in Heian period (Late Old Japanese) and is different from the contemporary Japanese language in grammar and some vocabulary. It still has relevance for historians, literary scholars, and lawyers (many Japanese laws that survived World War II are still written in bungo, although there are ongoing efforts to modernize their language). Bungo grammar and vocabulary are occasionally used in modern Japanese for effect, and fixed form poetries like Haiku and Tanka are still mainly written in this form. In the Meiji period, some authors started to use the colloquial form of the language in their literature. Following the government policy after the World War II, the standard form of contemporary Japanese language is used for most literature published since the 1950s. The standard language is based on the colloquial language in Tokyo area, and its literary stylistics in polite form differs little from its formal speech. Notable characteristics of literary language in contemporary Japanese would include more frequent",
    "label": 0
  },
  {
    "text": "since the 1950s. The standard language is based on the colloquial language in Tokyo area, and its literary stylistics in polite form differs little from its formal speech. Notable characteristics of literary language in contemporary Japanese would include more frequent use of Chinese origin words, less use of expressions against prescriptive grammar (such as \"ら抜き言葉\"), and use of non-polite normal form (\"-だ/-である\") stylistics that are rarely used in colloquial language. Javanese In the Javanese language, alphabet characters derived from the alphabets used to write Sanskrit, no longer in ordinary use, are used in literary words as a mark of respect. Kannada Kannada exhibits a strong diglossia, like Tamil, also characterised by three styles: a classical literary style modelled on the ancient language, a modern literary and formal style, and a modern colloquial form. These styles shade into each other, forming a diglossic continuum. The formal style is generally used in formal writing and speech. It is, for example, the language of textbooks, of much of Kannada literature and of public speaking and debate. Novels, even popular ones, will use the literary style for all description and narration and use the colloquial form only for dialogue, if they use it at all. In recent times, however, the modern colloquial form has been making inroads into areas that have traditionally been considered the province of the modern literary style: for instance most cinema, theatre and popular entertainment on television and radio. There are also many dialects of Kannada, Which are Dharwad Kannada of North Karnataka, Arebhashe of Dakshina Kannada and Kodagu, Kundakannada of Kundapura, Havyaka Kannada are major dialects. Latin Classical Latin was the literary register used in writing from 75 BC to the 3rd century AD, while Vulgar Latin was the common, spoken variety used across the Roman Empire. The Latin",
    "label": 0
  },
  {
    "text": "of Kundapura, Havyaka Kannada are major dialects. Latin Classical Latin was the literary register used in writing from 75 BC to the 3rd century AD, while Vulgar Latin was the common, spoken variety used across the Roman Empire. The Latin brought by Roman soldiers to Gaul, Iberia, or Dacia was not identical to the Latin of Cicero, and differed from it in vocabulary, syntax, and grammar. Some literary works with low-register language from the Classical Latin period give a glimpse into the world of early Vulgar Latin. The works of Plautus and Terence, being comedies with many characters who were slaves, preserve some early basilectal Latin features, as does the recorded speech of the freedmen in the Cena Trimalchionis by Petronius Arbiter. At the Third Council of Tours in 813, priests were ordered to preach in the vernacular language—either in the rustica lingua romanica (Vulgar Latin), or in the Germanic vernaculars—since the common people could no longer understand formal Latin. Malay The Malay language exists in a classical variety, two modern standard variety and several vernacular dialects. Maltese Maltese has a variety of dialects (including the Żejtun dialect, Qormi dialect and Gozitan amongst others) that co-exist alongside Standard Maltese. Literary Maltese, unlike Standard Maltese, features a preponderance of Semitic vocabulary and grammatical patterns; however, this traditional separation between Semitic and Romance influences in Maltese literature (especially Maltese poetry and Catholic liturgy on the island) is changing. Manchu Standard Manchu was based on the language spoken by the Jianzhou Jurchens during Nurhaci's time, while other unwritten Manchu dialects, such as that of Aigun and Sanjiazi, were spoken in addition to the related Xibe language. Mongolian The Classical Mongolian language was the high register used for religious and official purposes, while the various Mongolian dialects served as the low register, like Khalkha",
    "label": 0
  },
  {
    "text": "of Aigun and Sanjiazi, were spoken in addition to the related Xibe language. Mongolian The Classical Mongolian language was the high register used for religious and official purposes, while the various Mongolian dialects served as the low register, like Khalkha Mongolian, Chakhar Mongolian, Khorchin Mongolian, Kharchin Mongolian, Baarin Mongolian, Ordos Mongolian and the Buryat language. The Tibetan Buddhist canon was translated into Classical Mongolian. The Oirat Mongols who spoke the Oirat language and dialects like Kalmyk language or Torgut Oirat used a separate standard written with the Clear script. The Mongolian language, based on Khalkha Mongolian, now serves as the high register in Mongolia itself while in Inner Mongolia a standard Mongolian based on Chakhar Mongolian serves as the high register for all Mongols in China. The Buryat language, which is seen as part of the Mongolian language, has been turned into a standard literary form in Russia. N'Ko N'Ko is a literary language devised by Solomana Kante in 1949 as a writing system for the Mande languages of West Africa. It blends the principal elements of the partially mutually intelligible Manding languages. The movement promoting N'Ko literacy was instrumental in shaping the Maninka cultural identity in Guinea, and has also strengthened the Mande identity in other parts of West Africa. N'Ko publications include a translation of the Qur'an, a variety of textbooks on subjects such as physics and geography, poetic and philosophical works, descriptions of traditional medicine, a dictionary, and several local newspapers. Persian Persian or New Persian has been used continually as the literary language of major areas in Western Asia, the Caucasus, Central Asia and South Asia. The language written today remains essentially the same as that used by Ferdowsi despite variant colloquial dialects and forms. For many centuries, people belonging to the educated classes from the",
    "label": 0
  },
  {
    "text": "in Western Asia, the Caucasus, Central Asia and South Asia. The language written today remains essentially the same as that used by Ferdowsi despite variant colloquial dialects and forms. For many centuries, people belonging to the educated classes from the Bosphorus to the Bay of Bengal would be expected to know some Persian. It was once the language of culture (especially of poetry), from the Balkans to the Deccan, functioning as a lingua franca. Until the late 18th century, Persian was the dominant literary language of Georgia's elite. Persian was the second major vehicle after Arabic in transmitting Islamic culture and has a particularly prominent place in Sufism. Serbian Slavonic-Serbian (slavenosrpski) was the literary language of Serbs in the Habsburg monarchy used from the mid-18th century to 1825. It was a linguistic blend of Church Slavonic of the Russian recension, vernacular Serbian (Štokavian dialect), and Russian. At the beginning of the 19th century, it was severely attacked by Vuk Karadžić and his followers, whose reformatory efforts formed modern literary Serbian based on the popular language, known as Serbo-Croatian. Tagalog Tagalog was the basis of the Filipino language; both share the same vocabulary and grammatical system and are mutually intelligible. However, there is a significant political and social history that underlies the reasons for differentiating between Tagalog and Filipino. Modern Tagalog is derived from Archaic Tagalog, which was likely spoken during the Classical period, it was the language of the Mai State, Tondo Dynasty (according to the Laguna Copperplate Inscription) and southern Luzon. It was written using Baybayin, a syllabary which is a member of the Brahmic family, before the Spanish Romanised the alphabet beginning in the late 15th century. Tagalog was also the spoken language of the 1896 Philippine Revolution. The 1987 Constitution maintains that Filipino is the country's national",
    "label": 0
  },
  {
    "text": "is a member of the Brahmic family, before the Spanish Romanised the alphabet beginning in the late 15th century. Tagalog was also the spoken language of the 1896 Philippine Revolution. The 1987 Constitution maintains that Filipino is the country's national language and one of two official languages, alongside English. Today, Filipino is considered the proper term for the language of the Philippines, especially by Filipino-speakers who are not of Tagalog origin, with many referring to the Filipino language as \"Tagalog-based\". The language is taught in schools throughout the country and is the official language of education and business. Native Tagalog-speakers meanwhile comprise one of the largest linguistic and cultural groups of the Philippines, numbering an estimated 14 million. Slavic languages Notably, in Eastern European and Slavic linguistics, the term \"literary language\" has also been used as a synonym of \"standard language\". Tamil Tamil exhibits a strong diglossia, characterised by three styles: a classical literary style modelled on the ancient language, a modern literary and formal style and a modern colloquial form. These styles shade into each other, forming a diglossic continuum. The modern literary style is generally used in formal writing and speech. It is, for example, the language of textbooks, of much of Tamil literature and of public speaking and debate. Novels, even popular ones, will use the literary style for all description and narration and use the colloquial form only for dialogue, if they use it at all. In recent times, however, the modern colloquial form has been making inroads into areas that have traditionally been considered the province of the modern literary style: for instance most cinema, theatre and popular entertainment on television and radio. Tibetan Classical Tibetan was the high register used universally by all Tibetans while the various mutually unintelligible Tibetic languages serve as the",
    "label": 0
  },
  {
    "text": "province of the modern literary style: for instance most cinema, theatre and popular entertainment on television and radio. Tibetan Classical Tibetan was the high register used universally by all Tibetans while the various mutually unintelligible Tibetic languages serve as the low register vernacular, like Central Tibetan language in Ü-Tsang (Tibet proper), Khams Tibetan in Kham, Amdo Tibetan in Amdo, Ladakhi language in Ladakh and Dzongkha in Bhutan. Classical Tibetan was used for official and religious purposes, such as in Tibetan Buddhist religious texts like the Tibetan Buddhist canon and taught and learned in monasteries and schools in Tibetan Buddhist regions. Now, Standard Tibetan, based on the Lhasa dialect, serves as the high register in China. In Bhutan, the Tibetan Dzongkha language has been standardised and replaced Classical Tibetan for official purposes and education, in Ladakh, the standard official language learned are now the unrelated languages Urdu and English, and in Baltistan, the Tibetan Balti language serves as the low register while the unrelated Urdu is the official language. Uzbek and Uyghur The Turkic Chagatai language served as the high register literary standard for Central Asian Turkic peoples, while the vernacular low register languages were the Uzbek language and Eastern Turki (Modern Uyghur). The Soviet Union abolished Chagatai as the literary standard and had the Uzbek language standardized as a literary language for, and the Taranchi dialect of Ili was chosen as the literary standard for Modern Uyghur, while other dialects like the Kashgar and Turpan dialects continue to be spoken. Welsh Like other languages, the modern spoken language tends to use simplified forms, for example using auxiliary verbs, as in English, to form tenses, in contrast to verb forms for each tense etc., similar to Latin. Yorùbá Standard Yoruba is the literary form of the Yoruba language of West Africa,",
    "label": 0
  },
  {
    "text": "use simplified forms, for example using auxiliary verbs, as in English, to form tenses, in contrast to verb forms for each tense etc., similar to Latin. Yorùbá Standard Yoruba is the literary form of the Yoruba language of West Africa, the standard variety learnt at school and that spoken by newsreaders on the radio. Standard Yoruba has its origin in the 1850s, when Samuel A. Crowther, native Yoruba and the first African Anglican Bishop in Nigeria, published a Yoruba grammar and started his translation of the Bible. Though for a large part based on the Ọyọ and Ibadan dialects, Standard Yoruba incorporates several features from other dialects. Additionally, it has some features peculiar to itself only, for example the simplified vowel harmony system, as well as foreign structures, such as calques from English which originated in early translations of religious works. The first novel in the Yorùbá language was Ogboju Ode ninu Igbo Irunmale (The Forest of A Thousand Demons), written in 1938 by Chief Daniel O. Fagunwa (1903–1963). Other writers in the Yorùbá language include: Senator Afolabi Olabimtan (1932–1992) and Akinwunmi Isola. See also Aureation Classical language Official language Sacred language Standard language Written language Acrolect List of languages by first written accounts References Bibliography Crystal, David (ed.), The Cambridge Encyclopedia of the English Language (Cambridge, 2003) ISBN 0-521-53033-4 Gould, Rebecca Ruth (2018). \"Sweetening the Heavy Georgian Tongue: Jāmī in the Georgian-Persianate World\". In d'Hubert, Thibaut; Papas, Alexandre (eds.). Jāmī in Regional Contexts: The Reception of ʿAbd al-Raḥmān Jāmī's Works in the Islamicate World, ca. 9th/15th-14th/20th Century. Brill. ISBN 978-9004386600. Matthee, Rudi (2009). \"Was Safavid Iran an Empire?\". Journal of the Economic and Social History of the Orient. 53 (1–2). Brill: 233–265. doi:10.1163/002249910X12573963244449. S2CID 55237025. McArthur, Tom (ed.), The Oxford Companion to the English Language (Oxford, 1992), ISBN 0-19-280637-8",
    "label": 0
  },
  {
    "text": "Liberature is literature in which the material form is considered an important part of the whole and essential to understanding the work. Description of liberature Liberature refers to a new kind of literature, a trans-genre, in which the text and the material form of a book constitute an inseparable whole. The term itself is derived from the word 'literature', but draws from the Latin liber, meaning \"a book\" and \"the free one\", as well as libra meaning \"measurement\" or \"writing as a measurement of words.\" In a work of liberature, text does not serve as the sole source of meaning; the shape and the construction of the book, its format, the number of pages, its typographical layout, the size and type of the font applied, pictures and photographs integrated with the text, and type of paper or other material used in the process of creation of the book are all taken into consideration. The reader confronts a work of liberature as a total package, which often assumes a non-traditional shape, the quality of which, in practice, sometime involves a radical separation from the traditional design of the book. Its textual message dictates the physical shape that the work finally assumes. All of this lends a level of intent and control to the creator of liberature that surpasses that of other genres. The demands posed by liberature shape a new kind of reader. In traditional texts, the reader empirically processes the text of a work in order to attain a desired level of understanding; one follows the steps of a model reader, whose purpose is to bring oneself closer to one's virtual model. Certain works diverge from this concept of linear, textual reading such as those by Umberto Eco or Roland Barthes. Such works introduce uncertainty into these structured expectations and",
    "label": 0
  },
  {
    "text": "reader, whose purpose is to bring oneself closer to one's virtual model. Certain works diverge from this concept of linear, textual reading such as those by Umberto Eco or Roland Barthes. Such works introduce uncertainty into these structured expectations and programs of behavior. Neither the works of Eco nor Barthes constitute liberature, but they exemplify a difference in the quality of the programming of the reader's experience. The printed works of liberature, although diverse in the application, share the following variables of the traversal function: Only in several instances the work may breach the above-mentioned scheme of characteristic variables of liberature. In all other cases, readers are responsible for configuring their reading of the text. Texts of liberature that generate multiple statements are not limited to literary works, and definitely not to electronic texts – the category embraces computer games and other forms of both ergodic and non‐ergodic literature. The former requires a non‐trivial effort to traverse the text. In a classical, non-ergodic literary work, such as The Odyssey, the reader is required only to turn the pages and to interpret the text. In permutative works – those in which the order of text is inconsequential - instead of static function defining the dynamics, something else occurs. An intratextonic dynamic, one that guides the reader throughout the text, occurs. The text is also not determinate, meaning that it lacks a specificity which would hinder interpretation and different readings. The ergodic character of the work is usually determined by the exploratory function of the reader. The parallel character of the works is written into the very core of liberature; the order in which the reader perceives the text governs the way the work is rendered. The interactive capacity of the work becomes, if not an aesthetic category, a way of the",
    "label": 0
  },
  {
    "text": "is written into the very core of liberature; the order in which the reader perceives the text governs the way the work is rendered. The interactive capacity of the work becomes, if not an aesthetic category, a way of the reader's behavior that is written into the text. The work of liberature disrupts the structure of expectations based on a syntagmatic order as well as the strategies characteristic of a linear text. The works of liberature refer to the experience of simultaneousness. Furthermore, liberature as a hybrid genre incorporating the features of numerous media at the same time, including the arts, assumes a quality of simultaneousness as a dominating one. Creation of the concept. Liberature avant la lettre In 1999, Zenon Fajfer postulated the genre of liberature to describe the yet-undefinable work Oka-leczenie that he and Katarzyna Bazarnik had been working on. In a translated explanation of the concept, Fajfer describes liberature as \"a type or genre of literature in which the text is integrated with the physical space of the book into a meaningful whole and in which all elements (from the graphic ones to the kinds of paper (or other material) and the physical shape of the book) may contribute to its meaning\". Fajfer specifically cites the necessity of creating the genre because he too often sees non-traditional literary works judged only as works of art, but not as literature. After conception, the idea was developed by Katarzyna Bazarnik who, basing her reflections on the analysis of the works by James Joyce, demonstrated that the similarity of the text and the form creates iconicity. By exploring and dynamically developing the study of iconicity, a common feature of liberature, Bazarnik contributed to the consideration of liberature in the literary community at large. Fajfer's 'liberature' is in no immediate denotational",
    "label": 0
  },
  {
    "text": "text and the form creates iconicity. By exploring and dynamically developing the study of iconicity, a common feature of liberature, Bazarnik contributed to the consideration of liberature in the literary community at large. Fajfer's 'liberature' is in no immediate denotational relation with various earlier uses of the word, including the English legal synonym for “livery”, the name of a program of Czech Radio, and the term coined by Julián Ríos for the novel Larva (1983). Considering liberature anachronistically, it is noteworthy that writers began experimenting with literary forms as early as in the Baroque period. The unusual structure of the works forces the readers to focus their attention, to choose the beginning and the end of the text, and – most importantly – form another level of signification. It is an invitation to play, act, and interact with the text; this enables the text to transcend the boundaries of a given work. Its form \"does not only consider a literary text a symbolic expression of a person’s subjectivity but also considered a text as determined by the level of programming and processing of signs\". In the 17th and 18th centuries, word‐games (such as rebuses, palindromes, and anagrams), all of which date back to antiquity, were employed. There were also text‐machines, texts that after being programmed were able to generate new texts. The similarity of various literary works from this period was a result of fashion, but also of using the same Latin books on poetry and rhetoric in which creating elaborate poetry, and later, prose, was one of the practice exercises. The works that may be considered liberature avant la lettre were penned by such writers as William Blake, Blaise Cendrars, B.S. Johnson, James Joyce, Stéphane Mallarmé, Raymond Queneau, Laurence Sterne, or Stanisław Wyspiański. In the case of American postmodernist",
    "label": 0
  },
  {
    "text": "exercises. The works that may be considered liberature avant la lettre were penned by such writers as William Blake, Blaise Cendrars, B.S. Johnson, James Joyce, Stéphane Mallarmé, Raymond Queneau, Laurence Sterne, or Stanisław Wyspiański. In the case of American postmodernist writers, Raymond Federman, Robert Gass and Ronald Sukenick come to the fore of what can be called liberature. In Poland, some of Radosław Nowakowski's works predated the coinage of the term. Owing to discussions with Fajfer, Nowakowski found the term useful and started considering himself a writer of liberature. One of Nowakowski's most prominent works is Ulica Sienkiewicza w Kielcach (Sienkiewicz Street in Kielce) published in 2002. Another of Nowakowski's books, Nieposkładana teoria sztuki (Noncompleted Theory of Art), first published in 1994, is composed of a pile of separate pages placed in a box which, according to the author, mirrors the \"theory of puzzle[...] [w]ith the exception that not all of the elements fit[...]”. In the works where Nowakowski provides the reader with three versions of the text – Polish, English and Esperanto – one could observe certain differences in the rendition of the same topic, the technique which contributes to the further differentiation of the book's meaning. Development of liberature in Poland In an organized manner, liberature in Poland, including both Polish texts and translated work, has been presented in the Liberatura series published by Korporacja Ha!art seated in Kraków. The first publications are: (1) Katarzyna Bazarnik and Zenon Fajfer. (O)patrzenie. 2003. (2) Zenon Fajfer. Spoglądając przez ozonową dziurę. 2004. Followed by its English translation, But Eyeing Like Ozone Whole. by Krzysztof Bartnicki. 2004. (3) Stéphane Mallarmé. Rzut kośćmi nie zniesie przypadku. Polish translation of Un coup de des jamais n'abolira le hasard by Tomasz Różycki. 2005. (4) Stanisław Czycz. Arw. 2007. (5) B. S. Johnson. Nieszczęśni. Polish translation",
    "label": 0
  },
  {
    "text": "Whole. by Krzysztof Bartnicki. 2004. (3) Stéphane Mallarmé. Rzut kośćmi nie zniesie przypadku. Polish translation of Un coup de des jamais n'abolira le hasard by Tomasz Różycki. 2005. (4) Stanisław Czycz. Arw. 2007. (5) B. S. Johnson. Nieszczęśni. Polish translation of The Unfortunates by Katarzyna Bazarnik. 2009. (6) Raymond Queneau. Sto tysięcy miliardów wierszy. Polish translation of Cent mille milliards de poèmes by Jan Gondowicz. 2009. (7) Georges Perec. Życie instrukcja obsługi. Polish translation of La Vie mode d'emploi by Wawrzyniec Brzozowski. 2009. (8) Katarzyna Bazarnik and Zenon Fajfer. Oka-leczenie. 2009. (9) Perec instrukcja obsługi. Various authors. 2010. (10) Zenon Fajfer. dwadzieścia jeden liter. 2010. (11) Zenon Fajfer. ten letters. English translation of (10) by Katarzyna Bazarnik. 2010. (12) Zenon Fajfer. Liberatura czyli literatura totalna. Teksty zebrane z lat 1999–2009. With English translation Liberature or Total Literature. Collected Essays 1999-2009 by Katarzyna Bazarnik. Bilingual edition. 2010. (13) Paweł Dunajko. untitled text. 2010. (14) Krzysztof Bartnicki. Prospekt emisyjny. 2010. (15) Herta Müller. Strażnik bierze swój grzebień. Polish translation of The Guard Takes His Comb by Artur Kożuch. Bilingual edition. 2010. (16) Raymond Federman. Podwójna wygrana jak nic. Polish translation of Double or Nothing by Jerzy Kutnik. 2010. (17) B. S. Johnson. Przełożona w normie. Polish translation of House Mother Normal by Katarzyna Bazarnik. 2011. (18) Sonnet of Sonnets. Various authors. 2012. (19) James Joyce. Finneganów tren. Polish translation of Finnegans Wake by Krzysztof Bartnicki. 2012. Jonathan Safran Foer Liberature in the Anglo-Saxon countries is represented by Jonathan Safran Foer. Foer's hybrid, postmodern novel Tree of Codes (2010) is an example of visual writing, utilizing typesetting, images, spaces and blank pages to give the book a visual dimension beyond the prose narrative. Foer has taken his favorite book, The Street of Crocodiles by Bruno Schulz, and used it as a canvas, cutting",
    "label": 0
  },
  {
    "text": "visual writing, utilizing typesetting, images, spaces and blank pages to give the book a visual dimension beyond the prose narrative. Foer has taken his favorite book, The Street of Crocodiles by Bruno Schulz, and used it as a canvas, cutting into and out of the pages, to arrive at an original new story. In one interview the author emphasises that, although the originality of the story brought to life in such an innovative way is a very compelling feature, it was the form of the book that captivated him the most: “[…] I was more interested in subtracting than adding, and also in creating a book with a three-dimensional life. On the brink of the end of paper, I was attracted to the idea of a book that can’t forget it has a body\". Liberature in other media Although the authors creating liberature frequently apply the traditional ways of publication (chiefly paper books), they also appreciate other possibilities. Liberatic texts whose primary medium is the Internet are representative of 'e-liberature'. Other examples include Radosław Nowakowski's works, such as the transformation of the hypertext model of the book by Nowakowski into Hala 1000 Ton, set in the old Norblin's factory (Warsaw) (2005). The project called Libro 2N, was defined as a \"journey into BLIN concept\", which was enacted inside a book represented by the old workshop. References External links Tree of Codes page at the official site of Visual Editions Liberatura. Wstąp do liberatury (available only in Polish version) Zenon Fajfer. \"Liberatura. Aneks od słownika terminów literackich (available only in Polish version) Katarzyna Bazarnik, \"Krótkie wprowadzenie do liberatury\" (available only in Polish version)",
    "label": 0
  },
  {
    "text": "Outdoor literature is a literature genre about or involving the outdoors. Outdoor literature encompasses several different subgenres including exploration literature, adventure literature and nature writing. Another subgenre is the guide book, an early example of which was Thomas West's guide to the Lake District published in 1778. The genres can include activities such as exploration, survival, sailing, hiking, mountaineering, whitewater boating, geocaching or kayaking, or writing about nature and the environment. Travel literature is similar to outdoor literature but differs in that it does not always deal with the out-of-doors, but there is a considerable overlap between these genres, in particular with regard to long journeys. History Henry David Thoreau's Walden (1854) is an early and influential work. Although not entirely an outdoor work (he lived in a cabin close to civilization) he expressed the ideas of why people go out into the wilderness to camp, backpack and hike: to get away from the rush of modern society and simplify life. This was a new perspective for the time and thus Walden has had a lasting influence on most outdoor authors. Thoreau's careful observations and devastating conclusions have rippled into time, becoming stronger as the weaknesses Thoreau noted have become more pronounced […] Events that seem to be completely unrelated to his stay at Walden Pond have been influenced by it, including the national park system, the British labour movement, the creation of India, the civil rights movement, the hippie revolution, the environmental movement, and the wilderness movement. Today, Thoreau's words are quoted with feeling by liberals, socialists, anarchists, libertarians, and conservatives alike. Robert Louis Stevenson's Travels with a Donkey in the Cévennes (1879), about his travels in Cévennes (France), is among the first popular books to present hiking and camping as recreational activities, and tells of commissioning one of",
    "label": 0
  },
  {
    "text": "and conservatives alike. Robert Louis Stevenson's Travels with a Donkey in the Cévennes (1879), about his travels in Cévennes (France), is among the first popular books to present hiking and camping as recreational activities, and tells of commissioning one of the first sleeping bags. In the world of sailing Frank Cowper's Sailing Tours (1892–1896) and Joshua Slocum's Sailing Alone Around the World (1900) are classics of outdoor literature. In April 1895, Joshua Slocum set sail from Boston, Massachusetts and in Sailing Alone Around the World, he described his departure: I had resolved on a voyage around the world, and as the wind on the morning of April 24, 1895 was fair, at noon I weighed anchor, set sail, and filled away from Boston, where the Spray had been moored snugly all winter. […] A thrilling pulse beat high in me. My step was light on deck in the crisp air. I felt there could be no turning back, and that I was engaging in an adventure the meaning of which I thoroughly understood. More than three years later, on June 27, 1898, he returned to Newport, Rhode Island, having circumnavigated the world, a distance of more than 46,000 miles (74,000 km). The National Outdoor Book Award was established in 1997 as a US-based non-profit program which each year honours the best in outdoor writing and publishing. Outdoor classics 19th century John MacGregor (1866). A Thousand Miles in a Rob Roy Canoe. Considered the first documentation of recreational canoeing. Edward Whymper (1871). Scrambles Amongst the Alps in the Years 1860–1869. Mark Twain (1872). Roughing It. Part real part fiction, classic account of life in the American Old West. Frank Cowper (1892–1896). Sailing Tours. A classic of single-handed cruising. Walter Weston (1896). Mountaineering and Exploration in the Japanese Alps. 20th century John",
    "label": 0
  },
  {
    "text": "Twain (1872). Roughing It. Part real part fiction, classic account of life in the American Old West. Frank Cowper (1892–1896). Sailing Tours. A classic of single-handed cruising. Walter Weston (1896). Mountaineering and Exploration in the Japanese Alps. 20th century John Muir, (1911). My First Summer in the Sierra. Grey Owl (1935). Pilgrims of the Wild. About Grey Owl's life in the wilds of Canada. Gontran de Poncins (1939). Kabloona. French adventurer living with Eskimos in the late 1930s. Maurice Herzog (1951). Annapurna: Conquest of the First 8000-metre Peak. Probably the most influential mountaineering expedition book. Wallace Stegner (1954). Beyond the Hundredth Meridian: John Wesley Powell and the Second Opening of the West. Alfred Lansing (1959). Endurance: Shackleton's Incredible Voyage. John Hillaby, Journey to the Jade Sea (1964); Journey through Britain; Journey through Europe; Journey to the Gods (1991). Accounts of various long distance walks. Edward Abbey (1968) Desert Solitaire Colin Fletcher (1968) The Complete Walker Annie Dillard, (1974) Pilgrim at Tinker Creek Patrick Leigh Fermor, A Time of Gifts (1977); Between the Woods and the Water (1986); The Broken Road (2013). A trilogy describing a walk across Europe. Nan Shepherd, (1977). The Living Mountain. Jon Krakauer (1990s). Into the Wild, Into Thin Air. Joe Simpson, Touching the Void (1988). Mountain climbing in the Andes. 21st century Jim Perrin, Spirits of Place (1997); The Climbing Essays (2006); West: A Journey through the Landscapes of Loss (2010). A rock climber and travel writer. Bill McKibben (2005) Wandering Home. Also: The End of Nature (1989) Rory Stewart, The Places in Between (2006). A walk across Afghanistan in 2002, after the Russians had left. Cheryl Strayed, Wild: From Lost to Found on the Pacific Crest Trail (2013). Describes the grueling life of the long-distance hiker, the perils of the PCT, and its peculiar community",
    "label": 0
  },
  {
    "text": "Phoenician–Punic literature is literature written in Phoenician, the language of the ancient civilization of Phoenicia, or in the Punic language that developed from Phoenician and was used in Ancient Carthage. Its nature and extent are extremely uncertain because there is very little direct evidence. Phoenician and Punic text survives only in inscriptions, of which very few can be interpreted as literary; on coins; and via Greek and Latin sources, such as the fragments of Sanchuniathon's History and Mago's agricultural treatise, the Greek translation of the voyage of Hanno the Navigator, and a few lines in the Poenulus by Plautus. This limited evidence has led some scholars to argue that there was not a substantial literary tradition in Phoenician or Punic. However, Greek and Roman writings suggest that both Phoenicia and Carthage had extensive libraries and produced literature, including the Phoenician sources used by Greek and Latin writers like Philo of Byblos and Menander of Ephesus. History and sources The Jewish historian Flavius Josephus alludes to the Phoenician or Tyrian chronicles that he allegedly consulted to write his historical works. Herodotus also mentioned the existence of books from Byblos and a History of Tyre preserved in the temple of Hercules-Melqart in Tyre. In addition, it is possible to find some remnants of the influence exerted by certain writings of Ugarit in some biblical books, such as the Genesis or the Book of Ruth, that had traces of poetic compositions of religious or political themes – with a markedly propagandistic or philosophical undertone. Rufius Festus Avienius also alludes to old Punic records from which he would have drawn his reports on the voyage of Himilco. Greco-Roman sources mention a number of Punic books saved from the looting and burning of Carthage by the legions of Scipio Africanus in the spring of 146",
    "label": 0
  },
  {
    "text": "from which he would have drawn his reports on the voyage of Himilco. Greco-Roman sources mention a number of Punic books saved from the looting and burning of Carthage by the legions of Scipio Africanus in the spring of 146 BC. In his work Natural History, Pliny indicates that after the fall of Carthage, many of these books were handed over to the Numidian rulers and the Roman Senate ordered the translation into Latin of one text, Mago's agricultural treatise, establishing a commission under the leadership of Decimus Junius Piso. According to the Byzantine Encyclopedia called Suda, there was a historian of antiquity known as Charon of Carthage that wrote a collection of books: Lives of Illustrious Men, Lives of Illustrious Women, and Tyrants. Augustine of Hippo (who lived between the 3rd and 4th centuries AD) considered Punic as one of the main \"sapiential\" languages, along with Hebrew, Canaanite, Latin and Greek. On Punic literature, he wrote:Quae lingua si improbatur abs te, nega Punicis Libris, ut a viris doctissimus proditur, multa sapienter esse mandata memoriae (English: If you reject this language, you are denying what many scholars have acknowledged: many things have been wisely preserved from oblivion thanks to books written in Punic.)To Augustine, this literature was not only ancient but also contemporary. He mentioned abecedaria and psalms composed in Punic and that both Donatists and Neo-Punic Catholics wrote \"little books in Punic\" with \"testimonies of the sacred scriptures\". An important part of the Bible is thought to have been translated into Neo-Punic. Subjects Agricultural treatises This is one of the areas where the most information is available, since it is known that after the end of the Third Punic War, the Roman Senate decided to translate an encyclopedic treatise on agronomy written by Mago – considered by Columella as",
    "label": 0
  },
  {
    "text": "the areas where the most information is available, since it is known that after the end of the Third Punic War, the Roman Senate decided to translate an encyclopedic treatise on agronomy written by Mago – considered by Columella as the father of agronomy – into Latin. This treatise comprised 28 books, of which 66 fragments have been preserved. It includes topics such as viticulture, topography, veterinary medicine, beekeeping, and fruit arboriculture, as well as recommendations defending the idea that the properties should not be too large and that the owner should not be absent. Mago may not have been the only Carthaginian treatisist concerned with these topics, since Columella clearly indicates that there were several other writers focusing on the subject; however, he does not specify who they might have been or the depth of their work – with the exception of one Amilcar. Philosophical writings Philosophical works are likely to have been written even if there is little evidence, since it is known that in Carthage as well as in Gadir there were Platonic and Pythagorean schools, currents that seem to have been widely accepted in the colonial Phoenician sphere. We only know of works by Moderatus, of the Gaditan school, who wrote in Greek. A treatise on philosophy is attributed to Sanchuniathon – of which there is no record other than a simple mention. Religious writings The fragments of Sanchuniathon's work that have been preserved form the most extensive religious text on Phoenician mythology known to date. It is a kind of Theogony that includes passages on cosmogony, heroic tales, the life of the gods, and the use of rituals with snakes. There is also an allusion made by Plutarch in regards to a series of sacred scrolls rescued from Carthage and hidden underground, although the veracity",
    "label": 0
  },
  {
    "text": "cosmogony, heroic tales, the life of the gods, and the use of rituals with snakes. There is also an allusion made by Plutarch in regards to a series of sacred scrolls rescued from Carthage and hidden underground, although the veracity of this information has not been confirmed. Conversely, Phoenician religious literature is known to have had a profound influence on the biblical account of Job. Historical treatises In The Histories by Polybius, he clearly refers to Carthaginian historians, and Sallust claimed to have consulted the Punic books of the Numidian king Hiempsal. Sanchuniathon's historical work, considered the most extensive work produced in Phoenician, was translated into Greek in the 2nd century BC, although only a long fragment has been preserved – one that primarily covers religious themes. However, the authenticity of the texts attributed to Sanchuniathon has been questioned several times, without reaching a clear consensus. There are numerous allusions in Greek literature – until after 3rd century BC – to a Cosmogony written by Mochus of Sidon in the 14th century BC. The likely existence of biographies of Hannibal has also been noted. According to Polybius and Titus Livy, Hannibal had such deeds recorded in Phoenician and Greek in 205 BC, in the temple of Hera in Lacinia, and it is probable that he was simply continuing an ancient tradition by which Carthaginian generals used to write down their heroic deeds and give them to a temple to be preserved. Another example of this type of literature is the Agrigentum inscription, preserving a fragment of a narrative of the takeover of Agrigentum in 406 BC: Poetry Krahmalkov interprets a few Punic-language inscriptions as poetry representing remnants of a literary tradition that included rhymed rhetorical prose and iambic poetry. However, both Krahmalkov's text and translation have been harshly disputed, with",
    "label": 0
  },
  {
    "text": "takeover of Agrigentum in 406 BC: Poetry Krahmalkov interprets a few Punic-language inscriptions as poetry representing remnants of a literary tradition that included rhymed rhetorical prose and iambic poetry. However, both Krahmalkov's text and translation have been harshly disputed, with other scholars describing his analysis of the Iulius Nasif inscription as \"preposterous\" and \"misrepresentative\". Language and grammar A Latin manuscript, the Berne codex 123, suggests Phoenician had 12 parts of speech: noun, pronoun, verb, adjective, adverb, preposition, conjunction, interjection, article, impersonal mood, infinitive and the gerund. In addition, Eusebius of Caesarea attributes the authorship of a treatise titled On the Phoenician Alphabet to Sanchuniathon. Navigation and geographical treatises Although the Phoenicians were famous as navigators and explorers, the only two accounts to have survived up to this day are the stories of Hanno the Navigator and Himilco. Greek and Latin historiography seemed to be completely unaware of Hanno's voyage before the fall of the Punic capital. Himilcon's journey can be traced only to some comments made by Avienius, which he claims come from ancient Punic records he might have accessed. Another hypothesis is that King Juba II based his geographical knowledge of the Nile's origins on Punic books that he kept at his court – as recorded by Amianus Marcellinus – indicating that the river's headwaters were on a mountain in Mauritania. A similar thing applies to the voyages supposedly carried out by this monarch in the Canary archipelago, an expedition recorded by Pliny. Although it is clear from the way Pliny describes the islands that a real voyager reached this region, discussions are currently underway as to whether this expedition was carried out by Juba II or if he merely collected a series of data he found in the Carthaginian books he inherited from his ancestors. In turn, Marinus",
    "label": 0
  },
  {
    "text": "this region, discussions are currently underway as to whether this expedition was carried out by Juba II or if he merely collected a series of data he found in the Carthaginian books he inherited from his ancestors. In turn, Marinus of Tyre (born in 1 AD) was considered as the first geographer of his time worthy of the title of \"scientist\". Although his original work has disappeared, Claudius Ptolemy used it extensively while writing his Geographia. International and legislative treaties No direct information is available, but there is evidence that the international treaties Rome signed with Carthage were kept in the Capitol on bronze tablets, and it is to be presumed that the Punics preserved them as well. The treaty signed in 215 BC by Hannibal and Philip V of Macedon is known to have been drafted in Greek and Punic, and alluded to various Carthaginian divinities reminiscent of the treaty signed centuries before by Esarhaddon and the King of Tyre, a fact widely interpreted as a sign of state conservatism that could be explained only through the preservation of these documents over the centuries. Drama Poenulus (\"The Little Punic\"), a Latin comedy play by the playwright Plautus which has several Carthaginian characters, includes a few lines and a speech in Punic. Roman comedies were translations and adaptations of Greek New Comedy, and it is unclear whether the Punic text was similarly in the original Greek play (ὁ Καρχηδόνιος \"the Carthaginian\", possibly by Alexis of Thurii) or whether it was composed for the Poenulus, perhaps by a bilingual speaker working with Plautus. Alternately, Krahmalkov has suggested that there were full Punic translations of the same plays that formed the basis for Plautus' Poenulus and Aulularia, and that Plautus took the Punic lines from these plays. See also History of Carthage",
    "label": 0
  },
  {
    "text": "Poetics is the study or theory of poetry, specifically the study or theory of device, structure, form, type, and effect with regards to poetry, though usage of the term can also refer to literature broadly. Poetics is distinguished from hermeneutics by its focus on the synthesis of non-semantic elements in a text rather than its semantic interpretation. Most literary criticism combines poetics and hermeneutics in a single analysis; however, one or the other may predominate given the text and the aims of the one doing the reading. History of Poetics Western Poetics Generally speaking, poetics in the western tradition emerged out of Ancient Greece. Fragments of Homer and Hesiod represent the earliest Western treatments of poetic theory, followed later by the work of the lyricist Pindar. The term poetics derives from the Ancient Greek ποιητικός poietikos \"pertaining to poetry\"; also \"creative\" and \"productive\". It stems, not surprisingly, from the word for poetry, \"poiesis\" (ποίησις) meaning \"the activity in which a person brings something into being that did not exist before.\" Ποίησις itself derives from the Doric word \"poiéō\" (ποιέω) which translates, simply, as \"to make.\" In the Western world, the development and evolution of poetics featured three artistic movements concerned with poetical composition: (1) the formalist, (2) the objectivist, and (3) the Aristotelian. Plato's Republic The Republic by Plato represents the first major Western work to treat the theory of poetry. In Book III Plato defines poetry as a type of narrative which takes one of three forms: the \"simple,\" the \"imitative\" (mimetic), or any mix of the two. In Book X, Plato argues that poetry is too many degrees removed from the ideal form to be anything other than deceptive and, therefore, dangerous. Only capable of producing these ineffectual copies of copies, poets had no place in his utopic",
    "label": 0
  },
  {
    "text": "Book X, Plato argues that poetry is too many degrees removed from the ideal form to be anything other than deceptive and, therefore, dangerous. Only capable of producing these ineffectual copies of copies, poets had no place in his utopic city. Aristotle's Poetics Aristotle's Poetics is one of the first extant philosophical treatise to attempt a rigorous taxonomy of literature. The work was lost to the Western world for a long time. It was available in the Middle Ages and early Renaissance only through a Latin translation of an Arabic commentary written by Averroes and translated by Hermannus Alemannus in 1256. The accurate Greek-Latin translation made by William of Moerbeke in 1278 was virtually ignored. The Arabic translation departed widely in vocabulary from the original Poetics and it initiated a misinterpretation of Aristotelian thought that continued through the Middle Ages. The Poetics itemized the salient genres of ancient Greek drama into three categories (comedy, tragedy, and the satyr play) while drawing a larger-scale distinction between drama, lyric poetry, and the epic. Aristotle also critically revised Plato's interpretation of mimesis which Aristotle believed represented a natural human instinct for imitation, an instinct which could be found at the core of all poetry. Modern poetics developed in Renaissance Italy. The need to interpret ancient literary texts in the light of Christianity, to appraise and assess the narratives of Dante, Petrarch, and Boccaccio, contributed to the development of complex discourses on literary theory. Thanks first of all to Giovanni Boccaccio's Genealogia Deorum Gentilium (1360), the literate elite gained a rich understanding of metaphorical and figurative tropes. Giorgio Valla's 1498 Latin translation of Aristotle's text (the first to be published) was included with the 1508 Aldine printing of the Greek original as part of an anthology of Rhetores graeci. There followed an ever-expanding corpus",
    "label": 0
  },
  {
    "text": "and figurative tropes. Giorgio Valla's 1498 Latin translation of Aristotle's text (the first to be published) was included with the 1508 Aldine printing of the Greek original as part of an anthology of Rhetores graeci. There followed an ever-expanding corpus of texts on poetics in the later fifteenth century and throughout the sixteenth, a phenomenon that began in Italy and spread to Spain, England, and France. Among the most important Renaissance works on poetics are Marco Girolamo Vida's De Arte Poetica (1527) and Gian Giorgio Trissino's La Poetica (1529, expanded edition 1563). By the early decades of the sixteenth century, vernacular versions of Aristotle's Poetics appeared, culminating in Lodovico Castelvetro's Italian editions of 1570 and 1576. Luis de Góngora (1561–1627) and Baltasar Gracián (1601–58) brought a different kind of sophistication to poetic. Emanuele Tesauro wrote extensively in his Il Cannocchiale Aristotelico (The Aristotelian Spyglass, 1654), on figure ingeniose and figure metaforiche. During the Romantic era, poetics tended toward expressionism and emphasized the perceiving subject. Twentieth-century poetics returned to the Aristotelian paradigm, followed by trends toward meta-criticality, and the establishment of a contemporary theory of poetics. Eastern poetics developed lyric poetry, rather than the representational mimetic poetry of the Western world. See also Notes and references Further reading Olson, Charles (1950). Projective Verse. New York, NY: Poetry New York. Ciardi, John (1959). How Does a Poem Mean?. Cambridge, MA: The Riverside Press. Drew, Elizabeth (1933). Discovering Poetry. New York: W.W. Norton & Company. Harmon, William (2003). Classic Writings on Poetry. New York: Columbia University Press. Hashmi, Alamgir (2011). \"Eponymous Écriture and the Poetics of Reading a Transnational Epic\". Dublin Quarterly, 15. Hobsbaum, Philip (1996). Metre, Rhythm, and Verse Form. New York: Routledge. ISBN 0-415-12267-8. Kinzie, Mary (1999). A Poet's Guide to Poetry. Chicago: University of Chicago Press. ISBN 0-226-43739-6. Norman, Charles",
    "label": 0
  },
  {
    "text": "The sociology of literature is a subfield of the sociology of culture. It studies the social production of literature and its social implications. A notable example is Pierre Bourdieu's 1992 Les Règles de L'Art: Genèse et Structure du Champ Littéraire, translated by Susan Emanuel as Rules of Art: Genesis and Structure of the Literary Field (1996). Classical sociology None of the 'founding fathers' of sociology produced a detailed study of literature, but they did develop ideas that were subsequently applied to literature by others. Karl Marx's theory of ideology has been directed at literature by Pierre Macherey, Terry Eagleton and Fredric Jameson. Max Weber's theory of modernity as cultural rationalisation, which he applied to music, was later applied to all the arts, literature included, by Frankfurt School writers such as Theodor Adorno and Jürgen Habermas. Emile Durkheim's view of sociology as the study of externally defined social facts was redirected towards literature by Robert Escarpit. Bourdieu's work is clearly indebted to Marx, Weber and Durkheim. Lukács and the theory of the novel An important first step in the sociology of literature was taken by Georg Lukács's The Theory of the Novel, first published in German in 1916, in the Zeitschrift fur Aesthetik und Allgemeine Kunstwissenschaft. In 1920 it was republished in book form and this version strongly influenced the Frankfurt School. A second edition, published in 1962, was similarly influential on French structuralism. The Theory of the Novel argued that, whilst the classical epic poem had given form to a totality of life pregiven in reality by the social integration of classical civilisation, the modern novel had become 'the epic of an age in which the extensive totality of life is no longer directly given'. The novel form is therefore organised around the problematic hero in pursuit of problematic values",
    "label": 0
  },
  {
    "text": "of classical civilisation, the modern novel had become 'the epic of an age in which the extensive totality of life is no longer directly given'. The novel form is therefore organised around the problematic hero in pursuit of problematic values within a problematic world. Lukács's second distinctive contribution to the sociology of literature was The Historical Novel, written in German but first published in Russian in 1937, which appeared in English translation in 1962. Here, Lukács argued that the early 19th century historical novel's central achievement was to represent realistically the differences between pre-capitalist past and capitalist present. This was not a matter of individual talent, but of collective historical experience, because the French Revolution and the revolutionary and Napoleonic wars had made history for the first time a mass experience. He went on to argue that the success of the 1848 revolutions led to the decline of the historical novel into 'decorative monumentalization' and the 'making private of history'. The key figures in the historical novel were thus those of the early 19th century, especially Sir Walter Scott. Lukács was an important influence on Lucien Goldmann's Towards a Sociology of the Novel, Alan Swingewood's discussion of the sociology of the novel in Part 3 of Laurenson and Swingewood's The Sociology of Literature and Franco Moretti's Signs Taken for Wonders. The Frankfurt School Founded in 1923, the Institute for Social Research at the University of Frankfurt developed a distinctive kind of 'critical sociology' indebted to Marx, Weber and Freud. Leading Frankfurt School critics who worked on literature included Adorno, Walter Benjamin and Leo Löwenthal. Adorno's Notes to Literature, Benjamin's The Origin of German Tragic Drama and Löwentahl's Literature and the Image of Man were each influential studies in the sociology of literature. Löwenthal continued this work at the University of",
    "label": 0
  },
  {
    "text": "Benjamin and Leo Löwenthal. Adorno's Notes to Literature, Benjamin's The Origin of German Tragic Drama and Löwentahl's Literature and the Image of Man were each influential studies in the sociology of literature. Löwenthal continued this work at the University of California, Berkeley, during the 1950s. Adorno's Notes to Literature is a collection of essays, the most influential of which is probably 'On Lyric Poetry and Society'. It argued that poetic thought is a reaction against the commodification and reification of modern life, citing Goethe and Baudelaire as examples. Benjamin's The Origin of German Tragic Drama argued that the extreme 'sovereign violence' of the 16th and 17th century German 'Trauerspiel' (literally mourning play, less literally tragedy) playwrights expressed the historical realities of princely power far better than had classical tragedy. Habermas succeeded Adorno to the Chair of Sociology and Philosophy at Frankfurt. Habermas's first major work, Strukturwandel der Öffentlichkeit was published in German in 1962, and in English translation as The Structural Transformation of the Public Sphere in 1989. It attempted to explain the socio-historical emergence of middle-class public opinion in the seventeenth and eighteenth centuries. Developing a new kind of institutional sociology of literature, it argued that the public sphere had been organised around literary salons in France, learned and literary societies in Germany, and coffee houses in England. These institutions sustained the early novel, newspaper and periodical press. The sociology of the avant-garde Peter Bürger was Professor of French and Comparative Literature at the University of Bremen. His Theorie der Avantgarde was published in German in 1974 and in English translation in 1984. Like Habermas, Bürger was interested in the institutional sociology of literature and art. He postulated a historical typology of aesthetic social relations, measured along three main axes, the function of the artwork, its mode of production",
    "label": 0
  },
  {
    "text": "English translation in 1984. Like Habermas, Bürger was interested in the institutional sociology of literature and art. He postulated a historical typology of aesthetic social relations, measured along three main axes, the function of the artwork, its mode of production and its mode of reception. This gave him three main kinds of art, sacral, courtly and bourgeois. Bourgeois art, he argued, had as its function individual self-understanding and was produced and received individually. It became a celebration in form of the liberation of art from religion, the court and, eventually, even the bourgeoisie. Modernist art was thus an autonomous social 'institution', the preserve of an increasingly autonomous intellectual class. The 'historical avant-garde' of the inter-war years developed as a movement within and against modernism, he concluded, as an ultimately unsuccessful revolt against precisely this autonomy. Habermas adopts a very similar approach in his own account of the avant-garde. The sociology of the book trade Robert Escarpit held the position of Professor of Comparative Literature at the University of Bordeaux and founded the Centre for the Sociology of Literary Facts. His works included The Sociology of Literature, (French: Sociologie de la littérature) published in French in 1958 and in English translation in 1971, and The Book Revolution (French: La Révolution du livre), published in French in 1965 and in English in 1966. In Durkheimian fashion, Escarpit aimed to concern himself only with the externally defined \"social facts\" of literature, especially those registered in the book trade. His focus fell on the \"community of writers\", understood in aggregate as \"generations\" and \"teams\". He extended the definition of literature to include all \"non-functional\" writing and also insisted that literary success resulted from \"a convergence of intentions between author and reader\". Lewis Coser in the United States and Peter H. Mann in Britain carried",
    "label": 0
  },
  {
    "text": "He extended the definition of literature to include all \"non-functional\" writing and also insisted that literary success resulted from \"a convergence of intentions between author and reader\". Lewis Coser in the United States and Peter H. Mann in Britain carried out analogously empirical studies of the sociology of the book trade. Lucien Febvre and Henri-Jean Martin's L'Apparition du livre, first published in French 1958 and in English translation as The Coming of the Book in 1976, is - strictly speaking - a work of social history (Febvre was a leading figure in the Annales school of historiography). But it is deeply sociological in character - Annales history was determinedly social scientific - and provides a systematic account of the long-run development of the European book trade (it covers the period from 1450 to 1800). Genetic structuralism Lucien Goldmann was Director of Studies at the School for Advanced Studies in the Social Sciences in Paris and founding Director of the Centre for the Sociology of Literature at the Free University of Brussels. Like Escarpit, Goldmann was influenced by Durkheim: hence, his definition of the subject matter of sociology as the 'study of the facts of consciousness'. But he was also interested in developing a sociology of the text. The central task of the literary sociologist, he argued, was to bring out the objective meaning of the literary work by placing it in its historical context, studied as a whole. Goldmann defined the creating subject as transindividual, that is, as an instance of Durkheim's 'collective consciousness'. Following Marx and Lukács, however, Goldmann also assumed that group consciousness was normally class consciousnesses. The mediating agency between a social class and the work of literature then became the 'world vision', which binds the individual members of a social class together. Le Dieu caché, his",
    "label": 0
  },
  {
    "text": "also assumed that group consciousness was normally class consciousnesses. The mediating agency between a social class and the work of literature then became the 'world vision', which binds the individual members of a social class together. Le Dieu caché, his study of Blaise Pascal and Jean Racine, was published in French in 1955 and in English translation as The Hidden God in 1964. It identified 'structural homologies' between the Jansenist 'tragic vision', the textual structures of Pascal's Pensées and Racine's plays, and the social position of the seventeenth-century 'noblesse de robe'. Goldmann's structuralism was 'genetic' because it sought to trace the genesis of literary structures in extra-literary phenomena. In 1964 Goldmann published Pour une Sociologie du Roman translated by Alan Sheridan as Towards a Sociology of the Novel in 1974. Like Lukács, Goldmann sees the novel as revolving around the problematic hero's search for authentic values in a degraded society. But Goldmann also postulates a 'rigorous homology' between the literary form of the novel and the economic form of the commodity. The early novel, he argues, is concerned with individual biography and the problematic hero, but, as competitive capitalism evolves into monopoly capitalism, the problematic hero progressively disappears. The period between the First and Second World Wars witnesses a temporary experiment with the community as collective hero: Goldmann's example is André Malraux. But the main line of development is characterised by the effort to write the novel of 'the absence of subjects'. Here, Goldmann's example is the nouveau roman of Alain Robbe-Grillet and Nathalie Sarraute. Andrew Milner's John Milton and the English Revolution (1981) is essentially an application of Goldmann's genetic structuralism to the study of seventeenth-century English literature. Sociocriticism Goldmann's sociology of literature remains significant in itself and as a source of inspiration, both positive and negative, to the",
    "label": 0
  },
  {
    "text": "the English Revolution (1981) is essentially an application of Goldmann's genetic structuralism to the study of seventeenth-century English literature. Sociocriticism Goldmann's sociology of literature remains significant in itself and as a source of inspiration, both positive and negative, to the kind of 'sociocriticism' developed by Edmond Cros, Pierre Zima and their co-workers in France and Canada. Neo-Marxian ideology critique Marx used the term ideology to denote the inner connectedness of culture, including literature, and class. The philosopher Louis Althusser elaborated on this notion in the early 1970s, arguing that ideology functions so as to constitute biological individuals as social 'subjects' by representing their imaginary relation to their real conditions of existence. For Althusser himself art was not ideology. But his theory was applied to literature by Macherey in France, Eagleton in Britain and Jameson in the United States. The central novelty of Eagleton's Criticism and Ideology was its argument that literature could be understood as 'producing' ideology, in the sense of performing it. Jameson's The Political Unconscious argued that literary analysis can be focussed on three distinct levels, 'text', 'ideologeme' and 'ideology of form', each of which has its socio-historical corollary, in the equivalent 'semantic horizon' of political history, society and mode of production. The version of ideology Jameson applies to all three levels is essentially Althusserian. The novelty of his position, however, was to argue for a 'double hermeneutic' simultaneously concerned with ideology and utopia. Macherey, Eagleton and Jameson were literary critics by profession, but their applications of ideology-critique to literature are sociological in character, insofar as they seek to explain literary phenomena in extra-literary terms. Bourdieu Bourdieu was Professor of Sociology at the Collège de France and Director of the Centre de Sociologie Européenne. His first major contribution to the sociology of literature (and other arts) was La",
    "label": 0
  },
  {
    "text": "to explain literary phenomena in extra-literary terms. Bourdieu Bourdieu was Professor of Sociology at the Collège de France and Director of the Centre de Sociologie Européenne. His first major contribution to the sociology of literature (and other arts) was La Distinction, published in French in 1979 and in English translation in 1984. It is based on detailed sociological surveys and ethnographic observation of the social distribution of cultural preferences. Bourdieu identified three main zones of taste, 'legitimate', 'middle-brow' and 'popular', which he found to be dominant respectively in the educated sections of the dominant class, the middle classes and the working classes. He described legitimate taste as centred on an 'aesthetic disposition' to assert the primacy of form over function. The 'popular aesthetic', by contrast, is based on continuity between art and life and 'a deep-rooted demand for participation'. Hence, its hostility to representations of objects that in real life are either ugly or immoral. Artistic and social 'distinction' are inextricably interrelated, he concluded, because the 'pure gaze' implies a break with ordinary attitudes towards the world and, as such, is a 'social break'. The Rules of Art is more specifically focussed on literature, especially the significance of Gustave Flaubert for the making of modern French literature. Bourdieu postulated a model of 'the field of cultural production' as structured externally in relation to the 'field of power' and internally in relation to two 'principles of hierarchization', the heteronomous and the autonomous. The modern literary and artistic field is a site of contestation between the heteronomous principle, subordinating art to economy, and the autonomous, resisting such subordination. In Bourdieu's map of the French literary field in the late nineteenth century, the most autonomous genre, that is, the least economically profitable - poetry - is to the left, whilst the most heteronomous,",
    "label": 0
  },
  {
    "text": "and the autonomous, resisting such subordination. In Bourdieu's map of the French literary field in the late nineteenth century, the most autonomous genre, that is, the least economically profitable - poetry - is to the left, whilst the most heteronomous, the most economically profitable - drama - is to the right, with the novel located somewhere in between. Additionally, higher social status audiences govern the upper end of the field and lower status audiences the lower end. Flaubert's distinctive achievement in L'Éducation sentimentale was, in Bourdieu's account, to have understood and defined the rules of modern autonomous art. The rise of the novel One of the earliest English-language contributions to the sociology of literature is The Rise of the Novel (1957) by Ian Watt, Professor of English at Stanford University. For Watt, the novel's 'novelty' was its 'formal realism', the idea 'that the novel is a full and authentic report of human experience'. His paradigmatic instances are Daniel Defoe, Samuel Richardson and Henry Fielding. Watt argued that the novel's concern with realistically described relations between ordinary individuals, ran parallel to the more general development of philosophical realism, middle-class economic individualism and Puritan individualism. He also argued that the form addressed the interests and capacities of the new middle-class reading public and the new book trade evolving in response to them. As tradesmen themselves, Defoe and Richardson had only to 'consult their own standards' to know that their work would appeal to a large audience. Cultural materialism Raymond Williams was Professor of Drama at Cambridge University and one of the founders of contemporary cultural studies. He described his own distinctive approach as a 'cultural materialism', by which he meant a theory of culture 'as a (social and material) productive process' and of the arts 'as social uses of material means of",
    "label": 0
  },
  {
    "text": "of contemporary cultural studies. He described his own distinctive approach as a 'cultural materialism', by which he meant a theory of culture 'as a (social and material) productive process' and of the arts 'as social uses of material means of production'. This is clearly a sociological perspective, as distinct from a literary-critical perspective. Hence, its most general exposition in the United States is The Sociology of Culture and in Britain it is Culture, a 1981 title in Fontana's New Sociology series. Although Williams's interests ranged widely across the whole field of literary and cultural studies, his major work was concentrated on literature and drama. He was thus a sociologist of culture, specialising in the sociology of literature. In The Long Revolution (1961), Williams developed pioneering accounts of the sociology of the book trade, the sociology of authorship and the sociology of the novel. In The English Novel from Dickens to Lawrence (1970), he argued that the modern novel articulated a distinctively modern 'structure of feeling', the key problem of which was the 'knowable community'. In The Country and the City (1973) he developed a social history of English country-house poetry, aimed at demystifying the idealisations of rural life contained in the literature: 'It is what the poems are: not country life but social compliment; the familiar hyperboles of the aristocracy and its attendants'. His Marxism and Literature (1977) - simultaneously a critique of both Marxism and 'Literature' - is an extensive formal elaboration of Williams's own theoretical system. Alan Sinfield's Faultlines: Cultural Materialism and the Politics of Dissident Reading (1992) and Literature, Politics and Culture in Postwar Britain (1997) are both clearly indebted to Williams. So, too, is Andrew Milner's Literature, Culture and Society (2005). World-systems theory Franco Moretti was, by turn, Professor of English Literature at the University of",
    "label": 0
  },
  {
    "text": "and Literature, Politics and Culture in Postwar Britain (1997) are both clearly indebted to Williams. So, too, is Andrew Milner's Literature, Culture and Society (2005). World-systems theory Franco Moretti was, by turn, Professor of English Literature at the University of Salerno, of Comparative Literature at Verona University and of English and Comparative Literature at Stanford University. His first book, Signs Taken for Wonders (1983) was subtitled Essays in the Sociology of Literary Forms and was essentially qualitative in method. His later work, however, became progressively more quantitative. Applying Immanuel Wallerstein's world-systems theory to literature, Moretti argued, in Atlas of the European Novel (1998), that the nineteenth-century literary economy had comprised 'three Europes', with France and Britain at the core, most countries in the periphery and a variable semiperiphery located in between. Measured by the volume of translations in national bibliographies, he found that French novelists were more successful in the Catholic South and British in the Protestant North, but that the whole continent nonetheless read the leading figures from both. London and Paris 'rule the entire continent for over a century', he concluded, publishing half or more of all European novels. Moretti's theses prompted much subsequent controversy, collected together in Christopher Prendergast's edited collection Debating World Literature (2004). Moretti himself expanded on the argument in his Distant Reading (2013). Recent developments Building on earlier work in the production of culture, reception aesthetics and cultural capital, the sociology of literature has recently concentrated on readers' construction of meaning. New developments include studying the relationship between literature and group identities; concerning institutional and reader-response analysis; reintroducing the role of intentions of the author in literature; reconsidering the role of ethics and morality in literature and developing a clearer understanding of how literature is and is not like other media. The sociology of",
    "label": 0
  },
  {
    "text": "and reader-response analysis; reintroducing the role of intentions of the author in literature; reconsidering the role of ethics and morality in literature and developing a clearer understanding of how literature is and is not like other media. The sociology of literature has also recently taken an interest in the global inequality between First-World and Third-World authors, where the latter tend to be strongly dependent on the editorial decisions of publishers in Paris, London or New York and are often excluded from participation in the global literary market. The journal New Literary History devoted a special issue to new approaches to the sociology of literature in Spring 2010. Notes References Theodor W. Adorno, (1991) Notes to Literature, Vol. 1, trans. Shierry Weber Nicholson, ed. Rolf Tiedemann, New York: Columbia University Press. Theodor W. Adorno, (1992) Notes to Literature, Vol. 2, trans. Shierry Weber Nicholson, ed. Rolf Tiedemann, New York: Columbia University Press. Walter Benjamin, (1977) The Origin of German Tragic Drama, trans. John Osborne, London: New Left Books. Carlo Bordoni, (1974) Introduzione alla sociologia della letteratura, Pisa: Pacini. Pierre Bourdieu, (1984) Distinction: A Social Critique of the Judgement of Taste, trans. Richard Nice, London: Routledge and Kegan Paul. Pierre Bourdieu, (1996b) The Rules of Art: Genesis and Structure of the Literary Field, trans. Susan Emanuel, Cambridge: Polity Press. Elizabeth and Tom Burns eds, (1973) Sociology of Literature and Drama, Harmondsworth: Penguin. Peter Bürger, (1984) Theory of the Avant-Garde, trans. Michael Shaw, Minneapolis: University of Minnesota Press. Pascale Casanova, (2005) The World Republic of Letters, trans. M.B. Debevois, Cambridge, Mass.: Harvard University Press. Lewis A. Coser, Charles Kadushin and Walter W. Powell, (1985) Books: The Culture and Commerce of Publishing, Chicago: University of Chicago Press. Edmond Cros, (1988) Theory and Practice of Sociocriticism, trans. J. Schwartz, Minneapolis: University of Minnesota Press. Terry",
    "label": 0
  },
  {
    "text": "Press. Lewis A. Coser, Charles Kadushin and Walter W. Powell, (1985) Books: The Culture and Commerce of Publishing, Chicago: University of Chicago Press. Edmond Cros, (1988) Theory and Practice of Sociocriticism, trans. J. Schwartz, Minneapolis: University of Minnesota Press. Terry Eagleton, (1976) Criticism and Ideology, London: New Left Books. Robert Escarpit,(1966) The Book Revolution, London: George Harrap. Robert Escarpit, (1971) The Sociology of Literature, trans. E. Pick, London: Cass. Lucien Febvre and Henri-Jean Martin,(1976) The Coming of the Book: The Impact of Printing 1450–1800, trans. David Gerard, ed. Geoffrey Nowell-Smith and David Wootton, London: New Left Books. Lucien Goldmann, (1964) The Hidden God: A Study of Tragic Vision in the 'Pensées' of Pascal and the Tragedies of Racine, trans. Philip Thody, London: Routledge and Kegan Paul. Lucien Goldmann, (1975) Towards a Sociology of the Novel, trans. Alan Sheridan, London: Tavistock. Jürgen Habermas, (1989) The Structural Transformation of the Public Sphere: An Inquiry into a Category of Bourgeois Society, trans. T. Burger, Cambridge: Polity Press. John A. Hall, (1979), The Sociology of Literature, London: Longman. Fredric Jameson, (1981) The Political Unconscious: Narrative as a Socially Symbolic Act, London: Methuen. Diana T. Laurenson and Alan Swingewood, The Sociology of Literature, London: McGibbon and Kee. Leo Löwenthal, (1986) Literature and the Image of Man, New Jersey: Transaction Publishers. Leo Löwenthal, (2009) A margine. Teoria critica e sociologia della letteratura, a cura di C. Bordoni, Chieti: Solfanelli. Georg Lukács, (1962) The Historical Novel, trans. Hannah and Stanley Mitchell, London: Merlin Press. Georg Lukács, (1971) The Theory of the Novel: A Historico-Philosophical Essay on the Forms of Great Epic Literature, trans. A. Bostock, London: Merlin Press. Pierre Macherey, (1978) A Theory of Literary Production, trans. G. Wall, London: Routledge and Kegan Paul. Peter H. Mann, (1982) From Author to Reader: A Social Study of Books,",
    "label": 0
  },
  {
    "text": "Forms of Great Epic Literature, trans. A. Bostock, London: Merlin Press. Pierre Macherey, (1978) A Theory of Literary Production, trans. G. Wall, London: Routledge and Kegan Paul. Peter H. Mann, (1982) From Author to Reader: A Social Study of Books, London: Routledge and Kegan Paul. Andrew Milner, (1981) John Milton and the English Revolution: A Study in the Sociology of Literature, London: Macmillan. Andrew Milner, (2005) Literature, Culture and Society, second edition, London and New York: Routledge. Franco Moretti, (1988) Signs Taken For Wonders: Essays in the Sociology of Literary Forms, second edition, trans. Susan Fischer, David Forgacs and David Miller, London: Verso. Franco Moretti, (1998) Atlas of the European Novel 1800–1900, London: Verso. Franco Moretti, (2013) Distant Reading, London: Verso. Christopher Prendergast, ed., (2004) Debating World Literature, London: Verso. Jane Routh and Janet Wolff eds, (1977) The Sociology of Literature: Theoretical Approaches, Keele: Sociological Review Monographs. Alan Sinfield, (1992) Faultlines: Cultural Materialism and the Politics of Dissident Reading, Oxford: Oxford University Press. Alan Sinfield, (1997) Literature, Politics and Culture in Postwar Britain, second edition, London: Athlone Press. Socius, Ressources sur le littéraire et le social. Diana Spearman, (1966) The Novel and Society, London: Routledge and Kegan Paul. Ian Watt, (1963) The Rise of the Novel: Studies in Defoe, Richardson and Fielding, Harmondsworth: Penguin. Raymond Williams, (1961) The Long Revolution, London: Chatto and Windus. Raymond Williams, (1970) The English Novel from Dickens to Lawrence, London: Chatto and Windus. Raymond Williams, (1973) The Country and the City, New York: Oxford University Press. Raymond Williams, (1977) Marxism and Literature, Oxford: Oxford University Press. Raymond Williams, (1981) Culture, Glasgow: Fontana. Pierre V. Zima, (2000) Manuel de sociocritique, Paris et Montréal: l'Harmattan.",
    "label": 0
  },
  {
    "text": "Stylistics, a branch of applied linguistics, is the study and interpretation of texts of all types, but particularly literary texts, and spoken language with regard to their linguistic and tonal style, where style is the particular variety of language used by different individuals in different situations and settings. For example, the vernacular, or everyday language, may be used among casual friends, whereas more formal language, with respect to grammar, pronunciation or accent, and lexicon or choice of words, is often used in a cover letter and résumé and while speaking during a job interview. As a discipline, stylistics links literary criticism to linguistics. It does not function as an autonomous domain on its own, and it can be applied to an understanding of literature and journalism as well as linguistics. Sources of study in stylistics may range from canonical works of writing to popular texts, and from advertising copy to news, non-fiction, and popular culture, as well as to political and religious discourse. Indeed, as recent work in critical stylistics, multimodal stylistics and mediated stylistics has made clear, non-literary texts may be of just as much interest to stylisticians as literary ones. Literariness, in other words, is here conceived as 'a point on a cline rather than as an absolute'. Stylistics as a conceptual discipline may attempt to establish principles capable of explaining particular choices made by individuals and social groups in their use of language, such as in the literary production and reception of genre, the study of folk art, in the study of spoken dialects and registers, and can be applied to areas such as discourse analysis as well as literary criticism. Plain language has different features. Common stylistic features are using dialogue, regional accents and individual idioms (or idiolects). Stylistically, also sentence length prevalence and language register",
    "label": 0
  },
  {
    "text": "can be applied to areas such as discourse analysis as well as literary criticism. Plain language has different features. Common stylistic features are using dialogue, regional accents and individual idioms (or idiolects). Stylistically, also sentence length prevalence and language register use. Early twentieth century The analysis of literary style goes back to the study of classical rhetoric, though modern stylistics has its roots in Russian Formalism and the related Prague School of the early twentieth century. In 1909, Charles Bally proposed stylistics as a distinct academic discipline to complement Saussurean linguistics. For Bally, Saussure's linguistics by itself couldn't fully describe the language of personal expression. Bally's programme fits well with the aims of the Prague School. Taking forward the ideas of the Russian Formalists, the Prague School built on the concept of foregrounding, where it is assumed that poetic language is considered to stand apart from non-literary background language, by means of deviation (from the norms of everyday language) or parallelism. According to the Prague School, however, this background language isn't constant, and the relationship between poetic and everyday language is therefore always shifting. Late twentieth century Roman Jakobson had been an active member of the Russian Formalists and the Prague School, before emigrating to America in the 1940s. He brought together Russian Formalism and American New Criticism in his Closing Statement at a conference on stylistics at Indiana University in 1958. Published as Linguistics and Poetics in 1960, Jakobson's lecture is often credited with being the first coherent formulation of stylistics, and his argument was that the study of poetic language should be a sub-branch of linguistics. The poetic function was one of six general functions of language he described in the lecture. Michael Halliday is an important figure in the development of British stylistics. His 1971 study Linguistic",
    "label": 0
  },
  {
    "text": "poetic language should be a sub-branch of linguistics. The poetic function was one of six general functions of language he described in the lecture. Michael Halliday is an important figure in the development of British stylistics. His 1971 study Linguistic Function and Literary Style: An Inquiry into the Language of William Golding's The Inheritors is a key essay. One of Halliday's contributions has been the use of the term register to explain the connections between language and its context. For Halliday register is distinct from dialect. Dialect refers to the habitual language of a particular user in a specific geographical or social context. Register describes the choices made by the user, choices which depend on three variables: field (\"what the participants... are actually engaged in doing\", for instance, discussing a specific subject or topic), tenor (who is taking part in the exchange) and mode (the use to which the language is being put). Fowler comments that different fields produce different language, most obviously at the level of vocabulary (Fowler. 1996, 192) The linguist David Crystal points out that Halliday's 'tenor' stands as a roughly equivalent term for 'style', which is a more specific alternative used by linguists to avoid ambiguity (Crystal. 1985, 292). Halliday's third category, mode, is what he refers to as the symbolic organisation of the situation. Downes recognises two distinct aspects within the category of mode and suggests that not only does it describe the relation to the medium: written, spoken, and so on, but also describes the genre of the text (Downes. 1998, 316). Halliday refers to genre as pre-coded language, language that has not simply been used before, but that predetermines the selection of textual meanings. The linguist William Downes makes the point that the principal characteristic of register, no matter how peculiar or diverse,",
    "label": 0
  },
  {
    "text": "to genre as pre-coded language, language that has not simply been used before, but that predetermines the selection of textual meanings. The linguist William Downes makes the point that the principal characteristic of register, no matter how peculiar or diverse, is that it is obvious and immediately recognisable (Downes. 1998, 309). Literary stylistics In The Cambridge Encyclopedia of Language, Crystal observes that, in practice, most stylistic analysis has attempted to deal with the complex and 'valued' language within literature, i.e. 'literary stylistics'. He goes on to say that in such examination the scope is sometimes narrowed to concentrate on the more striking features of literary language, for instance, its 'deviant' and abnormal features, rather than the broader structures that are found in whole texts or discourses. For example, the compact language of poetry is more likely to reveal the secrets of its construction to the stylistician than is the language of plays and novels (Crystal. 1987, 71). Poetry As well as conventional styles of language there are the unconventional – the most obvious of which is poetry. In Practical Stylistics, HG Widdowson examines the traditional form of the epitaph, as found on headstones in a cemetery. For example: His memory is dear today As in the hour he passed away. (Ernest C. Draper 'Ern'. Died 4.1.38) (Widdowson. 1992, 6) Widdowson makes the point that such sentiments are usually not very interesting and suggests that they may even be dismissed as 'crude verbal carvings' and crude verbal disturbance (Widdowson, 3). Nevertheless, Widdowson recognises that they are a very real attempt to convey feelings of human loss and preserve affectionate recollections of a beloved friend or family member. However, what may be seen as poetic in this language is not so much in the formulaic phraseology but in where it appears. The",
    "label": 0
  },
  {
    "text": "convey feelings of human loss and preserve affectionate recollections of a beloved friend or family member. However, what may be seen as poetic in this language is not so much in the formulaic phraseology but in where it appears. The verse may be given undue reverence precisely because of the sombre situation in which it is placed. Widdowson suggests that, unlike words set in stone in a graveyard, poetry is unorthodox language that vibrates with inter-textual implications (Widdowson. 1992, 4). Two problems with a stylistic analysis of poetry are noted by PM Wetherill in Literary Text: An Examination of Critical Methods. The first is that there may be an over-preoccupation with one particular feature that may well minimise the significance of others that are equally important (Wetherill. 1974, 133). The second is that any attempt to see a text as simply a collection of stylistic elements will tend to ignore other ways whereby meaning is produced (Wetherill. 1974, 133). Implicature In 'Poetic Effects' from Literary Pragmatics, the linguist Adrian Pilkington analyses the idea of 'implicature', as instigated in the previous work of Dan Sperber and Deirdre Wilson. Implicature may be divided into two categories: 'strong' and 'weak' implicature, yet between the two extremes there are a variety of other alternatives. The strongest implicature is what is emphatically implied by the speaker or writer, while weaker implicatures are the wider possibilities of meaning that the hearer or reader may conclude. Pilkington's 'poetic effects', as he terms the concept, are those that achieve most relevance through a wide array of weak implicatures and not those meanings that are simply 'read in' by the hearer or reader. Yet the distinguishing instant at which weak implicatures and the hearer or reader's conjecture of meaning diverge remains highly subjective. As Pilkington says: 'there is no",
    "label": 0
  },
  {
    "text": "and not those meanings that are simply 'read in' by the hearer or reader. Yet the distinguishing instant at which weak implicatures and the hearer or reader's conjecture of meaning diverge remains highly subjective. As Pilkington says: 'there is no clear cut-off point between assumptions which the speaker certainly endorses and assumptions derived purely on the hearer's responsibility.' (Pilkington. 1991, 53) In addition, the stylistic qualities of poetry can be seen as an accompaniment to Pilkington's poetic effects in understanding a poem's meaning. Tense Widdowson points out that in Samuel Taylor Coleridge's poem \"The Rime of the Ancient Mariner\" (1798), the mystery of the Mariner's abrupt appearance is sustained by an idiosyncratic use of tense (Widdowson. 1992, 40). For instance, the Mariner 'holds' the wedding-guest with his 'skinny hand' in the present tense, but releases it in the past tense ('...his hands dropt he.'); only to hold him again, this time with his 'glittering eye', in the present (Widdowson. 1992, 41). The point of poetry Widdowson notices that when the content of poetry is summarised, it often refers to very general and unimpressive observations, such as 'nature is beautiful; love is great; life is lonely; time passes', and so on (Widdowson. 1992, 9). But to say: Like as the waves make towards the pebbled shore, So do our minutes hasten to their end ... William Shakespeare, '60'. Or, indeed: Love, all alike, no season knows nor clime, Nor hours, days months, which are the rags of time ... John Donne, 'The Sun Rising', Poems (1633) This language gives the reader a new perspective on familiar themes and allows us to look at them without the personal or social conditioning that we unconsciously associate with them (Widdowson. 1992, 9). So, although the reader may still use the same exhausted words and",
    "label": 0
  },
  {
    "text": "a new perspective on familiar themes and allows us to look at them without the personal or social conditioning that we unconsciously associate with them (Widdowson. 1992, 9). So, although the reader may still use the same exhausted words and vague terms like 'love', 'heart' and 'soul' to refer to human experience, to place these words in a new and refreshing context allows the poet the ability to represent humanity and communicate honestly. This, in part, is stylistics, and this, according to Widdowson, is the point of poetry (Widdowson. 1992, 76). See also Acrolect Aureation Basilect Classical language Code-switching Gender role in language Gianfranco Contini Internet linguistics Leo Spitzer Liturgical language Media stylistics Official language Philology Poetics and Linguistics Association Quantitative linguistics Standard language Style (sociolinguistics) Stylometry Notes References and related reading David Birch, ed. 1995. Context and Language: A Functional Linguistic Theory of Register (London, New York: Pinter) Richard Bradford. 1985. A Dictionary of Linguistics and Phonetics, 2nd edition (Oxford: Basil Blackwell) Richard Bradford. 1997. Stylistics (London and New York: Routledge) Richard Bradford. 1997. The Cambridge Encyclopedia of Language, 2nd edition (Cambridge: Cambridge University Press) Michael Burke. 2010. Literary Reading, Cognition and Emotion: An Exploration of the Oceanic Mind (London and New York: Routledge) David Crystal. 1998. Language Play (London: Penguin) William Downes. 1995. The Language of George Orwell (London: Macmillan Press) William Downes. 1998. Language and Society, 2nd edition (Cambridge: Cambridge University Press) Roger Fowler. 1996. Linguistic Criticism, 2nd edition (Oxford: Oxford University Press) Marcello Giovanelli & Chloe Harrison. Cognitive Grammar in Stylistics: A Practical Guide, 2nd edition. Bloomsbury Academic, 2024. MAK Halliday. 1964. Inside the Whale and Other Essays (London: Penguin Books) MAK Halliday. 1978. Language as Social Semiotic: The Social Interpretation of Language and Meaning (London: Edward Arnold) Zeki Hamawand. 2023. English Stylistics: A Cognitive Grammar",
    "label": 0
  },
  {
    "text": "Bloomsbury Academic, 2024. MAK Halliday. 1964. Inside the Whale and Other Essays (London: Penguin Books) MAK Halliday. 1978. Language as Social Semiotic: The Social Interpretation of Language and Meaning (London: Edward Arnold) Zeki Hamawand. 2023. English Stylistics: A Cognitive Grammar Approach. Palgrave Macmillan. Hernández-Campoy, Juan M. (2016). Sociolinguistic Styles. Wiley-Blackwell. ISBN 978-1-118-73764-4. Leslie Jeffries & Dan McIntyre. 2025. Stylistics, 2nd edn. Cambridge University Press. Brian Lamont. 2005. First Impressions (Edinburgh: Penbury Press) Geoffrey Leech and Michael H. Short. 1981. Style in Fiction: A Linguistic Introduction to English Fictional Prose (London: Longman) A McIntosh and P Simpson. 1964. The Linguistic Science and Language Teaching (London: Longman) George Orwell. 1949. Nineteen Eighty-Four (London: Heinemann) Adrian Pilkington. 1991. 'Poetic Effects', Literary Pragmatics, ed. Roger Sell (London: Routledge) ed. Thomas A. Sebeok. 1960. Style in Language (Cambridge, MA: MIT Press) Michael Toolan. 1998. Language in Literature: An Introduction to Stylistics (London: Hodder Arnold) Katie Wales. 2001. A Dictionary of Stylistics, 2nd edition, (Harlow: Longman) ed. Jean Jacques Weber. 1996. The Stylistics Reader: From Roman Jakobson to the Present (London: Arnold Hodder) PM Wetherill. 1974. Literary Text: An Examination of Critical Methods (Oxford: Basil Blackwell) H. G. Widdowson. 1992. Practical Stylistics (Oxford: Oxford University Press) External links A CC licensed introductory course to Stylistics from Lancaster University Checklist of American and British programs in stylistics and literary linguistics Stylistics – Theoretical issues of stylistics Stylistics from Scratch: My 'Take' on Stylistics and How to Go About a Stylistic Analysis Professor Mick Short The Poetics and Linguistics Association",
    "label": 0
  },
  {
    "text": "The Western canon is the embodiment of high-culture literature, music, philosophy, and works of art that are highly cherished across the Western world, such works having achieved the status of classics. Recent discussions upon the matter emphasise cultural diversity within the canon. The canons of music and visual arts have been broadened to encompass often overlooked periods, whilst recent media like cinema grapple with a precarious position. Criticism arises, with some viewing changes as prioritising activism over aesthetic values, often associated with critical theory, as well as postmodernism. Another critique highlights a narrow interpretation of the West, dominated by British and American culture, at least under contemporary circumstances, prompting demands for a more diversified canon amongst the hemisphere. There is no official list of works that a recognized panel of experts or scholars agreed upon that is \"the Western Canon,\" nor has there ever been such. A corpus of great works is an idea that has been discussed, negotiated, and criticized for the past century. Literary canon Classic book A classic is a book, or any other work of art, accepted as being exemplary or noteworthy. In the second-century Roman miscellany Attic Nights, Aulus Gellius refers to writers as \"classicus... scriptor, non proletarius\" (\"A distinguished, not a commonplace writer\"). Such classification were initiated with the Greeks' ranking their cultural works, with the word canon (ancient Greek κανών, kanṓn: \"measuring rod, standard\"). Similarly, early Christian Church Fathers declared as canon the authoritative texts of the New Testament, preserving them given the expense of vellum and papyrus and mechanical book reproduction. Thus, being included in a canon ensured a book's preservation as the best way to retain information about a civilization. In contemporary use, the Western canon defines the best of Western culture. In the ancient world, at the Alexandrian Library, scholars",
    "label": 0
  },
  {
    "text": "being included in a canon ensured a book's preservation as the best way to retain information about a civilization. In contemporary use, the Western canon defines the best of Western culture. In the ancient world, at the Alexandrian Library, scholars coined the Greek term Hoi enkrithentes [\"the admitted\", \"the included\"] to identify the writers in the canon. Although the term is often associated with the Western canon, it can be applied to works of literature, music and art, etc. from all traditions, such as the Chinese classics. With regard to books, what makes a book \"classic\" has concerned various authors, from Mark Twain to Italo Calvino, and questions such as \"Why Read the Classics?\", and \"What Is a Classic?\" have been considered by others, including T. S. Eliot, Charles Augustin Sainte-Beuve, Michael Dirda, and Ezra Pound. The terms \"classic book\" and Western canon are closely related concepts, but are not necessarily synonymous. A \"canon\" is a list of books considered to be \"essential\", and it can be published as a collection (such as Great Books of the Western World, Modern Library, Everyman's Library or Penguin Classics), presented as a list with an academic's imprimatur (such as Harold Bloom's), or be the official reading list of a university. In The Western Canon Bloom lists \"the major Western writers\" as Dante Alighieri, Geoffrey Chaucer, Miguel de Cervantes, Michel de Montaigne, William Shakespeare, Johann Wolfgang von Goethe, William Wordsworth, Charles Dickens, Leo Tolstoy, James Joyce and Marcel Proust. Great Books Program A university or college Great Books Program is a program inspired by the Great Books movement begun in the United States in the 1920s by John Erskine of Columbia University, which proposed to improve the higher education system by returning it to the western liberal arts tradition of broad cross-disciplinary learning. These",
    "label": 0
  },
  {
    "text": "the Great Books movement begun in the United States in the 1920s by John Erskine of Columbia University, which proposed to improve the higher education system by returning it to the western liberal arts tradition of broad cross-disciplinary learning. These academics and educators included Robert Hutchins, Mortimer Adler, Stringfellow Barr, Scott Buchanan, Jacques Barzun, and Alexander Meiklejohn. The view among them was that the emphasis on narrow specialization in American colleges had harmed the quality of higher education by failing to expose students to the important products of Western civilization and thought. The essential component of such programs is a high degree of engagement with primary texts, called the Great Books. The curricula of Great Books programs often follow a canon of texts considered more or less essential to a student's education, such as Plato's Republic, or Dante's Divine Comedy. Such programs often focus exclusively on Western culture. Their employment of primary texts dictates an interdisciplinary approach, as most of the Great Books do not fall neatly under the prerogative of a single contemporary academic discipline. Great Books programs often include designated discussion groups as well as lectures, and have small class sizes. In general students in such programs receive an abnormally high degree of attention from their professors, as part of the overall aim of fostering a community of learning. Over 100 institutions of higher learning, mostly in the United States, offer some version of a Great Books Program as an option for students. For much of the 20th century, the Modern Library provided a larger convenient list of the Western canon. The list numbered more than 300 items by the 1950s, by authors from Aristotle to Albert Camus, and has continued to grow. When in the 1990s the concept of the Western canon was vehemently condemned, just as",
    "label": 0
  },
  {
    "text": "the Western canon. The list numbered more than 300 items by the 1950s, by authors from Aristotle to Albert Camus, and has continued to grow. When in the 1990s the concept of the Western canon was vehemently condemned, just as earlier Modern Library lists had been criticized as \"too American,\" Modern Library responded by preparing new lists of \"100 Best Novels\" and \"100 Best Nonfiction\" compiled by famous writers, and later compiled lists nominated by book purchasers and readers. Debate Some intellectuals have championed a \"high conservative modernism\" that insists that universal truths exist, and have opposed approaches that deny the existence of universal truths. Yale University Professor of Humanities and famous literary critic Harold Bloom also argued strongly in favor of the canon, in his 1994 book The Western Canon: The Books and School of the Ages, and in general the canon remains as a represented idea in many institutions. Allan Bloom (no relation), in his highly influential The Closing of the American Mind: How Higher Education Has Failed Democracy and Impoverished the Souls of Today's Students (1987), argues that moral degradation results from ignorance of the great classics that shaped Western culture. Bloom further comments: \"But one thing is certain: wherever the Great Books make up a central part of the curriculum, the students are excited and satisfied.\" His book was widely cited by some intellectuals for its argument that the classics contained universal truths and timeless values which were being ignored by cultural relativists. Classicist Bernard Knox made direct reference to this topic when he delivered his 1992 Jefferson Lecture (the U.S. federal government's highest honor for achievement in the humanities). Knox used the intentionally \"provocative\" title \"The Oldest Dead White European Males\" as the title of his lecture and his subsequent book of the same name,",
    "label": 0
  },
  {
    "text": "1992 Jefferson Lecture (the U.S. federal government's highest honor for achievement in the humanities). Knox used the intentionally \"provocative\" title \"The Oldest Dead White European Males\" as the title of his lecture and his subsequent book of the same name, in both of which Knox defended the continuing relevance of classical culture to modern society. Defenders maintain that those who undermine the canon do so out of primarily political interests, and that such criticisms are misguided and/or disingenuous. As John Searle, Professor of Philosophy at the University of California, Berkeley, has written: There is a certain irony in this [i.e., politicized objections to the canon] in that earlier student generations, my own for example, found the critical tradition that runs from Socrates through the Federalist Papers, through the writings of Mill and Marx, down to the twentieth century, to be liberating from the stuffy conventions of traditional American politics and pieties. Precisely by inculcating a critical attitude, the \"canon\" served to demythologize the conventional pieties of the American bourgeoisie and provided the student with a perspective from which to critically analyze American culture and institutions. Ironically, the same tradition is now regarded as oppressive. The texts once served an unmasking function; now we are told that it is the texts which must be unmasked. One of the main objections to a canon of literature is the question of authority; who should have the power to determine what works are worth reading? Charles Altieri, of the University of California, Berkeley, states that canons are \"an institutional form for exposing people to a range of idealized attitudes.\" It is according to this notion that work may be removed from the canon over time to reflect the contextual relevance and thoughts of society. American historian Todd M. Compton argues that canons are always",
    "label": 0
  },
  {
    "text": "a range of idealized attitudes.\" It is according to this notion that work may be removed from the canon over time to reflect the contextual relevance and thoughts of society. American historian Todd M. Compton argues that canons are always communal in nature; that there are limited canons for, say a literature survey class, or an English department reading list, but there is no such thing as one absolute canon of literature. Instead, there are many conflicting canons. He regards Bloom's \"Western Canon\" as a personal canon only. The process of defining the boundaries of the canon is endless. The philosopher John Searle has said, \"In my experience there never was, in fact, a fixed 'canon'; there was rather a certain set of tentative judgments about what had importance and quality. Such judgments are always subject to revision, and in fact they were constantly being revised.\" One of the notable attempts at compiling an authoritative canon for literature in the English-speaking world was the Great Books of the Western World program. This program, developed in the middle third of the 20th century, grew out of the curriculum at the University of Chicago. University president Robert Maynard Hutchins and his collaborator Mortimer Adler developed a program that offered reading lists, books, and organizational strategies for reading clubs to the general public. An earlier attempt had been made in 1909 by Harvard University president Charles W. Eliot, with the Harvard Classics, a 51-volume anthology of classic works from world literature. Eliot's view was the same as that of Scottish philosopher and historian Thomas Carlyle: \"The true University of these days is a Collection of Books\". (\"The Hero as Man of Letters\", 1840) In the English-speaking world British renaissance poetry The canon of Renaissance English poetry of the 16th and early 17th century",
    "label": 0
  },
  {
    "text": "Thomas Carlyle: \"The true University of these days is a Collection of Books\". (\"The Hero as Man of Letters\", 1840) In the English-speaking world British renaissance poetry The canon of Renaissance English poetry of the 16th and early 17th century has always been in some form of flux and towards the end of the 20th century the established canon was criticised, especially by those who wished to expand it to include, for example, more women writers. However, the central figures of the British renaissance canon remain, Edmund Spenser, Sir Philip Sidney, Christopher Marlowe, William Shakespeare, Ben Jonson, and John Donne. Spenser, Donne, and Jonson were major influences on 17th-century poetry. However, poet John Dryden condemned aspects of the metaphysical poets in his criticism. In the 18th century Metaphysical poetry fell into further disrepute, while the interest in Elizabethan poetry was rekindled through the scholarship of Thomas Warton and others. However, the canon of Renaissance poetry was formed in the Victorian period with anthologies like Palgrave's Golden Treasury. In the twentieth century T. S. Eliot and Yvor Winters were two literary critics who were especially concerned with revising the canon of renaissance English literature. Eliot, for example, championed poet Sir John Davies in an article in The Times Literary Supplement in 1926. During the course of the 1920s, Eliot did much to establish the importance of the metaphysical school, both through his critical writing and by applying their method in his own work. However, by 1961 A. Alvarez was commenting that \"it may perhaps be a little late in the day to be writing about the Metaphysicals. The great vogue for Donne passed with the passing of the Anglo-American experimental movement in modern poetry.\" Two decades later, a hostile view was expressed that emphasis on their importance had been an attempt",
    "label": 0
  },
  {
    "text": "to be writing about the Metaphysicals. The great vogue for Donne passed with the passing of the Anglo-American experimental movement in modern poetry.\" Two decades later, a hostile view was expressed that emphasis on their importance had been an attempt by Eliot and his followers to impose a 'high Anglican and royalist literary history' on 17th-century English poetry. The American critic Yvor Winters suggested in 1939 an alternative canon of Elizabethan poetry, which would exclude the famous representatives of the Petrarchan school of poetry, represented by Sir Philip Sidney and Edmund Spenser. Winters claimed that the Native or Plain Style anti-Petrarchan movement had been undervalued and argued that George Gascoigne (1525–1577) \"deserves to be ranked [...] among the six or seven greatest lyric poets of the century, and perhaps higher\". Towards the end of the 20th century the established canon was increasingly disputed. Expansion of the literary canon in the 20th century In the twentieth century there was a general reassessment of the literary canon, including women's writing, post-colonial literatures, gay and lesbian literature, writing by racialized minorities, working people's writing, and the cultural productions of historically marginalized groups. This reassessment has resulted in a whole scale expansion of what is considered \"literature\", and genres hitherto not regarded as \"literary\", such as children's writing, journals, letters, travel writing, and many others are now the subjects of scholarly interest. The Western literary canon has also expanded to include the literature of Asia, Africa, the Middle East, and South America. Writers from Africa, Turkey, China, Egypt, Peru, and Colombia, Japan, etc., have received Nobel prizes since the late 1960s. Writers from Asia and Africa have also been nominated for, and also won, the Booker prize in recent years. Feminism and the literary canon Susan Hardy Aitken argues that the Western canon has",
    "label": 0
  },
  {
    "text": "received Nobel prizes since the late 1960s. Writers from Asia and Africa have also been nominated for, and also won, the Booker prize in recent years. Feminism and the literary canon Susan Hardy Aitken argues that the Western canon has maintained itself by excluding and marginalising women, whilst idealising the works of men. Where women's work is introduced it can be considered inappropriately rather than recognising the importance of their work; a work's greatness is judged against socially situated factors which exclude women, whilst being portrayed as an intellectual approach. The feminist movement produced both feminist fiction and non-fiction and created new interest in women's writing. It also prompted a general reevaluation of women's historical and academic contributions in response to the belief that women's lives and contributions have been underrepresented as areas of scholarly interest. However, in Britain and America at least women achieved major literary success from the late eighteenth century, and many major nineteenth-century British novelists were women, including Jane Austen, the Brontë family, Elizabeth Gaskell, and George Eliot. There were also three major female poets, Elizabeth Barrett Browning, Christina Rossetti and Emily Dickinson. In the twentieth century there were also many major female writers, including Katherine Mansfield, Dorothy Richardson, Virginia Woolf, Eudora Welty, and Marianne Moore. Notable female writers in France include Colette, Simone de Beauvoir, Marguerite Yourcenar, Nathalie Sarraute, Marguerite Duras and Françoise Sagan. Much of the early period of feminist literary scholarship was given over to the rediscovery and reclamation of texts written by women. Virago Press began to publish its large list of 19th and early 20th-century novels in 1975 and became one of the first commercial presses to join in the project of reclamation. African-American authors In the twentieth century, the Western literary canon started to include African writers not only from",
    "label": 0
  },
  {
    "text": "19th and early 20th-century novels in 1975 and became one of the first commercial presses to join in the project of reclamation. African-American authors In the twentieth century, the Western literary canon started to include African writers not only from African-American writers, but also from the wider African diaspora of writers in Britain, France, Latin America, and Africa. This correlated largely with the shift in social and political views during the civil rights movement in the United States. The first global recognition came in 1950 when Gwendolyn Brooks was the first African American to win a Pulitzer Prize for Literature. American Toni Morrison was the first African-American woman to win the Nobel Prize in Literature, in 1993. Some early African-American writers were inspired to defy ubiquitous racial prejudice by proving themselves equal to European American authors. As Henry Louis Gates Jr., has said, \"it is fair to describe the subtext of the history of black letters as this urge to refute the claim that because blacks had no written traditions they were bearers of an inferior culture.\" African-American writers were also attempting to subvert the literary and power traditions of the United States. Some scholars assert that writing has traditionally been seen as \"something defined by the dominant culture as a white male activity.\" This means that, in American society, literary acceptance has traditionally been intimately tied in with the very power dynamics which perpetrated such evils as racial discrimination. By borrowing from and incorporating the non-written oral traditions and folk life of the African diaspora, African-American literature broke \"the mystique of connection between literary authority and patriarchal power.\" In producing their own literature, African Americans were able to establish their own literary traditions devoid of the European intellectual filter. This view of African-American literature as a tool in the",
    "label": 0
  },
  {
    "text": "mystique of connection between literary authority and patriarchal power.\" In producing their own literature, African Americans were able to establish their own literary traditions devoid of the European intellectual filter. This view of African-American literature as a tool in the struggle for African-American political and cultural liberation has been stated for decades, most famously by W. E. B. Du Bois. Latin America Octavio Paz Lozano (1914–1998) was a Mexican poet and diplomat. For his body of work, he was awarded the 1981 Miguel de Cervantes Prize, the 1982 Neustadt International Prize for Literature, and the 1990 Nobel Prize in Literature. Gabriel García Márquez (1927–2014) was a Colombian novelist, short-story writer, screenwriter, and journalist. Considered one of the most significant authors of the 20th century and one of the best in the Spanish language, he was awarded the 1972 Neustadt International Prize for Literature and the 1982 Nobel Prize in Literature. García Márquez started as a journalist, and wrote many acclaimed non-fiction works and short stories, but is best known for his novels, such as One Hundred Years of Solitude (1967), The Autumn of the Patriarch (1975), and Love in the Time of Cholera (1985). His works have achieved significant critical acclaim and widespread commercial success, most notably for popularizing a literary style labeled as magic realism, which uses magical elements and events in otherwise ordinary and realistic situations. Some of his works are set in a fictional village called Macondo (the town mainly inspired by his birthplace Aracataca), and most of them explore the theme of solitude. On his death in April 2014, Juan Manuel Santos, the President of Colombia, described him as \"the greatest Colombian who ever lived.\" Mario Vargas Llosa, (1936-2025) is a Peruvian writer, politician, journalist, essayist, college professor, and recipient of the 2010 Nobel Prize in",
    "label": 0
  },
  {
    "text": "in April 2014, Juan Manuel Santos, the President of Colombia, described him as \"the greatest Colombian who ever lived.\" Mario Vargas Llosa, (1936-2025) is a Peruvian writer, politician, journalist, essayist, college professor, and recipient of the 2010 Nobel Prize in Literature. Vargas Llosa is one of Latin America's most significant novelists and essayists, and one of the leading writers of his generation. Some critics consider him to have had a larger international impact and worldwide audience than any other writer of the Latin American Boom. Upon announcing the 2010 Nobel Prize in Literature, the Swedish Academy said it had been given to Vargas Llosa \"for his cartography of structures of power and his trenchant images of the individual's resistance, revolt, and defeat\". Canon of philosophers Many philosophers today agree that Greek philosophy has influenced much of Western culture since its inception. Alfred North Whitehead once noted: \"The safest general characterization of the European philosophical tradition is that it consists of a series of footnotes to Plato.\" Clear, unbroken lines of influence lead from ancient Greek and Hellenistic philosophers to Early Islamic philosophy, the European Renaissance, and the Age of Enlightenment. Plato was a philosopher in Classical Greece and the founder of the Academy in Athens. He is widely considered the most pivotal figure in the development of philosophy, especially the Western tradition. Aristotle was an ancient Greek philosopher. His writings cover many subjects – including physics, biology, zoology, metaphysics, logic, ethics, aesthetics, rhetoric, linguistics, politics and government—and constitute the first comprehensive system of Western philosophy. Aristotle's views on physical science had a profound influence on medieval scholarship. Their influence extended from Late Antiquity into the Renaissance, and his views were not replaced systematically until the Enlightenment and theories such as classical mechanics. In metaphysics, Aristotelianism profoundly influenced Judeo-Islamic philosophical and",
    "label": 0
  },
  {
    "text": "had a profound influence on medieval scholarship. Their influence extended from Late Antiquity into the Renaissance, and his views were not replaced systematically until the Enlightenment and theories such as classical mechanics. In metaphysics, Aristotelianism profoundly influenced Judeo-Islamic philosophical and theological thought during the Middle Ages and continues to influence Christian theology, especially the Neoplatonism of the Early Church and the scholastic tradition of the Roman Catholic Church. Aristotle was well known among medieval Muslim intellectuals and revered as \"The First Teacher\" (Arabic: المعلم الأول). His ethics, though always influential, gained renewed interest with the modern advent of virtue ethics. Boethius' On the Consolation of Philosophy (Latin: De consolatione philosophiae) is often acclaimed as a central work from Late Antiquity, at the cusp of the early medieval period, that remained influential throughout the Middle Ages. Of Boethius, it has been said that \"[along] with Augustine and Aristotle, he is the fundamental philosophical and theological author in the Latin tradition\"; Edward Gibbon wrote of the Consolatione that it is \"a golden volume not unworthy of the leisure of Plato or Tully\", and Bertrand Russell wrote that, by merit of the same, Boethius \"would have been remarkable in any age; in the age in which he lived he is utterly amazing.\" The vast body of Christian philosophy is typically represented on reading lists mainly by Augustine of Hippo and Thomas Aquinas. The academic canon of early modern philosophy generally includes Descartes, Spinoza, Leibniz, Locke, Berkeley, Hume, and Kant. Renaissance philosophy Major philosophers of the Renaissance include Niccolò Machiavelli, Michel de Montaigne, Pico della Mirandola, Nicholas of Cusa and Giordano Bruno, Marsilio Ficino and Gemistos Plethon. Seventeenth-century philosophers The seventeenth century was important for philosophy, and the major figures were Francis Bacon, Thomas Hobbes, René Descartes, Blaise Pascal, Baruch Spinoza, John Locke and",
    "label": 0
  },
  {
    "text": "della Mirandola, Nicholas of Cusa and Giordano Bruno, Marsilio Ficino and Gemistos Plethon. Seventeenth-century philosophers The seventeenth century was important for philosophy, and the major figures were Francis Bacon, Thomas Hobbes, René Descartes, Blaise Pascal, Baruch Spinoza, John Locke and Gottfried Wilhelm Leibniz. Eighteenth-century philosophers Major philosophers of the eighteenth century include George Berkeley, Montesquieu, Voltaire, David Hume, Giambattista Vico, Jean-Jacques Rousseau, Denis Diderot, Immanuel Kant, Edmund Burke and Jeremy Bentham. Nineteenth-century philosophers Important nineteenth century philosophers include Georg Wilhelm Friedrich Hegel (1770–1831), Giuseppe Mazzini, Arthur Schopenhauer, Auguste Comte, Søren Kierkegaard, Karl Marx, Friedrich Engels and Friedrich Nietzsche. Twentieth-century philosophers Major twentieth century figures include Henri Bergson, Edmund Husserl, Bertrand Russell, Martin Heidegger, Ludwig Wittgenstein and Jean-Paul Sartre, Simone de Beauvoir, and Simone Weil, Michel Foucault, Pierre Bourdieu, Jacques Derrida and Jürgen Habermas. A porous distinction between analytic and continental approaches emerged during this period. Music Classical music forms the core of canon music and remains mostly unchanged to our days. It integrates a huge body of works starting from the 17th century and are reproduced on an ensemble of all acoustic musical instruments that were common in that century's Europe. The term \"classical music\" did not appear until the early 19th century, in an attempt to distinctly canonize the period from Johann Sebastian Bach to Ludwig van Beethoven as a golden age. In addition to Bach and Beethoven, the other major figures from this period were George Frideric Handel, Joseph Haydn and Wolfgang Amadeus Mozart. The earliest reference to \"classical music\" recorded by the Oxford English Dictionary is from about 1836. In classical music, during the nineteenth century a \"canon\" developed which focused on what was felt to be the most important works written since 1600, with a great concentration on the later part of this period, termed the",
    "label": 0
  },
  {
    "text": "about 1836. In classical music, during the nineteenth century a \"canon\" developed which focused on what was felt to be the most important works written since 1600, with a great concentration on the later part of this period, termed the Classical period, which is generally taken to begin around 1750. After Beethoven, the major nineteenth-century composers include Felix Mendelssohn, Franz Schubert, Robert Schumann, Frédéric Chopin, Hector Berlioz, Franz Liszt, Richard Wagner, Johannes Brahms, Antonín Dvořák, Anton Bruckner, Giuseppe Verdi, Giacomo Puccini, Camille Saint-Saëns, Gustav Mahler, and Pyotr Ilyich Tchaikovsky. In the 2000s, the standard concert repertoire of professional orchestras, chamber music groups, and choirs tends to focus on works by a relatively small number of mainly 18th- and 19th-century male composers. Many of the works deemed to be part of the musical canon are from genres regarded as the most serious, such as the symphony, concerto, string quartet, and opera. Folk music was already giving art music melodies, and from the late 19th century, in an atmosphere of increasing nationalism, folk music began to influence composers in formal and other ways, before being admitted to some sort of status in the canon itself. Since the early twentieth century non-Western music has begun to influence Western composers. In particular, direct homages to Javanese gamelan music are found in works for western instruments by Claude Debussy, Béla Bartók, Francis Poulenc, Olivier Messiaen, Pierre Boulez, Benjamin Britten, John Cage, Steve Reich, and Philip Glass. Debussy was immensely interested in non-Western music and its approaches to composition. Specifically, he was drawn to the Javanese gamelan, which he first heard at the 1889 Paris Exposition. He was not interested in directly quoting his non-Western influences, but instead allowed this non-Western aesthetic to generally influence his own musical work, for example, by frequently using quiet, unresolved",
    "label": 0
  },
  {
    "text": "gamelan, which he first heard at the 1889 Paris Exposition. He was not interested in directly quoting his non-Western influences, but instead allowed this non-Western aesthetic to generally influence his own musical work, for example, by frequently using quiet, unresolved dissonances, coupled with the damper pedal, to emulate the \"shimmering\" effect created by a gamelan ensemble. American composer Philip Glass was not only influenced by the eminent French composition teacher Nadia Boulanger, but also by the Indian musicians Ravi Shankar and Alla Rakha, His distinctive style arose from his work with Shankar and Rakha and their perception of rhythm in Indian music as being entirely additive. In the latter half of the 20th century the canon expanded to cover the so-called Early music of the pre-classical period, and Baroque music by composers other than Bach and George Frideric Handel, including Antonio Vivaldi, Claudio Monteverdi, Domenico Scarlatti, Alessandro Scarlatti, Henry Purcell, Georg Philipp Telemann, Jean-Baptiste Lully, Jean-Philippe Rameau, Marc-Antoine Charpentier, Arcangelo Corelli, François Couperin, Heinrich Schütz, and Dieterich Buxtehude. Earlier composers, such as Giovanni Pierluigi da Palestrina, Orlande de Lassus and William Byrd, have also received more attention in the last hundred years. The absence of women composers from the classical canon was brought to the forefront of musicological literature in the late twentieth and early twenty-first centuries. Even though many women composers have written music in the common practice period and beyond, their works remain extremely underrepresented in concert programs, music history curriculums, and music anthologies. In particular, musicologist Marcia J Citron has examined \"the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works.\" Since around 1980 the music of Hildegard von Bingen (1098–1179), a German Benedictine abbess, and Finnish composer Kaija Saariaho (born 1952) has begun to enter",
    "label": 0
  },
  {
    "text": "led to the exclusion of women composers from the received 'canon' of performed musical works.\" Since around 1980 the music of Hildegard von Bingen (1098–1179), a German Benedictine abbess, and Finnish composer Kaija Saariaho (born 1952) has begun to enter the canon. Saariaho's opera L'amour de loin has been staged in some of the world's major opera houses, including The English National Opera (2009) and in 2016 the Metropolitan Opera in New York. The classical ensemble canon very rarely integrates musical instruments that are not acoustic and of western origins, it stayed apart from the wide use of electric, electronic and digital instruments that are common in today's popular music. Visual arts The backbone of traditional Western art history are artworks commissioned by wealthy patrons for private or public enjoyment. Much of this was religious art, mostly Roman Catholic art. The classical art of Greece and Rome has, since the Renaissance, been the fount of the Western tradition. Giorgio Vasari (1511–1574) is the originator of the artistic canon and the originator of many of the concepts it embodies. His Lives of the Most Excellent Painters, Sculptors, and Architects covers only artists working in Italy, with a strong pro-Florentine prejudice, and has cast a long shadow over succeeding centuries. Northern European art has arguably never quite caught up to Italy in terms of prestige, and Vasari's placing of Giotto as the founding father of \"modern\" painting has largely been retained. In painting, the rather vague term of Old master covers painters up to about the time of Goya. This \"canon\" remains prominent, as indicated by the selection present in art history textbooks, as well as the prices obtained in the art trade. But there have been considerable swings in what is valued. In the 19th century the Baroque fell into great",
    "label": 0
  },
  {
    "text": "prominent, as indicated by the selection present in art history textbooks, as well as the prices obtained in the art trade. But there have been considerable swings in what is valued. In the 19th century the Baroque fell into great disfavour, but it was revived from around the 1920s, by which time the art of the 18th and 19th century was largely disregarded. The High Renaissance, which Vasari regarded as the greatest period, has always retained its prestige, including works by Leonardo da Vinci, Michelangelo, and Raphael, but the succeeding period of Mannerism has fallen in and out of favour. In the 19th century the beginnings of academic art history, led by German universities, led to much better understanding and appreciation of medieval art, and a more nuanced understanding of classical art, including the realization that many if not most treasured masterpieces of sculpture were late Roman copies rather than Greek originals. The European tradition of art was expanded to include Byzantine art and the new discoveries of archaeology, notably Etruscan art, Celtic art and Upper Paleolithic art. Since the 20th century there has been an effort to re-define the discipline to be more inclusive of art made by women; vernacular creativity, especially in printed media; and an expansion to include works in the Western tradition produced outside Europe. At the same time there has been a much greater appreciation of non-Western traditions, including their place with Western art in wider global or Eurasian traditions. The decorative arts have traditionally had a much lower critical status than fine art, although often highly valued by collectors, and still tend to be given little prominence in undergraduate studies or popular coverage on television and in print. Women and art English artist and sculptor Barbara Hepworth DBE (1903 – 1975), whose work exemplifies",
    "label": 0
  },
  {
    "text": "often highly valued by collectors, and still tend to be given little prominence in undergraduate studies or popular coverage on television and in print. Women and art English artist and sculptor Barbara Hepworth DBE (1903 – 1975), whose work exemplifies Modernism, and in particular modern sculpture, is one of the few female artists to achieve international prominence. Historical exclusion of women Women were discriminated against in terms of obtaining the training necessary to be an artist in the mainstream Western traditions. In addition, since the Renaissance the nude, more often than not female, has had a special position as subject matter. In her 1971 essay, \"Why Have There Been No Great Women Artists?\", Linda Nochlin analyzes what she sees as the embedded privilege in the predominantly male Western art world and argues that women's outsider status allowed them a unique viewpoint to not only critique women's position in art, but to additionally examine the discipline's underlying assumptions about gender and ability. Nochlin's essay develops the argument that both formal and social education restricted artistic development to men, preventing women (with rare exception) from honing their talents and gaining entry into the art world. In the 1970s, feminist art criticism continued this critique of the institutionalized sexism of art history, art museums, and galleries, and questioned which genres of art were deemed museum-worthy. This position is articulated by artist Judy Chicago: \"[I]t is crucial to understand that one of the ways in which the importance of male experience is conveyed is through the art objects that are exhibited and preserved in our museums. Whereas men experience presence in our art institutions, women experience primarily absence, except in images that do not necessarily reflect women's own sense of themselves.\" Sources containing canonical lists English literature Modern Library 100 Best Novels – English-language",
    "label": 0
  },
  {
    "text": "our museums. Whereas men experience presence in our art institutions, women experience primarily absence, except in images that do not necessarily reflect women's own sense of themselves.\" Sources containing canonical lists English literature Modern Library 100 Best Novels – English-language novels of the 20th century Library of America, classic American literature International literature Bibliothèque de la Pléiade Everyman's Library (Modern works) Great Books of the Western World História da Literatura Ocidental (in Portuguese) by Otto Maria Carpeaux Le Monde's 100 Books of the Century – books of the 20th century Modern Library Oxford World's Classics Penguin Classics John Cowper Powys: One Hundred Best Books (1916) Verso Books' Radical Thinkers ZEIT-Bibliothek der 100 Bücher – Die Zeit list of 100 books American and Canadian university reading lists Brigham Young University's Honors Program's Great Works List Bard College's Language & Thinking program, a series of seminars on great books taken on by all incoming freshmen St. John's College Great Books reading list (established by Scott Buchanan and Stringfellow Barr) Baylor University's Great Texts Reading List The Harvard Classics Contemporary anthologies of renaissance literature The preface to the Blackwell anthology of Renaissance Literature from 2003 acknowledges the importance of online access to literary texts on the selection of what to include, meaning that the selection can be made on basis of functionality rather than representativity\". This anthology has made its selection based on three principles. One is \"unabashedly canonical\", meaning that Sidney, Spenser, Marlowe, Shakespeare, and Jonson have been given the space prospective users would expect. A second principle is \"non-canonical\", giving female writers such as Anne Askew, Elizabeth Cary, Emilia Lanier, Martha Moulsworth, and Lady Mary Wroth a representative selection. It also includes texts that may not be representative of the qualitatively best efforts of Renaissance literature, but of the quantitatively most",
    "label": 0
  },
  {
    "text": "writers such as Anne Askew, Elizabeth Cary, Emilia Lanier, Martha Moulsworth, and Lady Mary Wroth a representative selection. It also includes texts that may not be representative of the qualitatively best efforts of Renaissance literature, but of the quantitatively most numerous texts, such as homilies and erotica. A third principle has been thematic, so that the anthology aims to include texts that shed light on issues of special interest to contemporary scholars. The Blackwell anthology is still firmly organised around authors, however. A different strategy has been observed by The Penguin Book of Renaissance Verse from 1992. Here the texts are organised according to topic, under the headings The Public World, Images of Love, Topographies, Friends, Patrons and the Good Life, Church, State and Belief, Elegy and Epitaph, Translation, Writer, Language and Public. It is arguable that such an approach is more suitable for the interested reader than for the student. While the two anthologies are not directly comparable, since the Blackwell anthology also includes prose and the Penguin anthology goes up to 1659, it is telling that while the larger Blackwell anthology contains work by 48 poets, seven of which are women, the Penguin anthology contains 374 poems by 109 poets, including 13 women and one poet each in Welsh, Siôn Phylip, and Irish, Eochaidh Ó Heóghusa. German literature Best German Novels of the Twentieth Century The Best German Novels of the Twentieth Century is a list of books compiled in 1999 by Literaturhaus München and Bertelsmann, in which 99 prominent German authors, literary critics, and scholars of German ranked the most significant German-language novels of the twentieth century. The group brought together 23 experts from each of the three categories. Each was allowed to name three books as having been the most important of the century. Cited by",
    "label": 0
  },
  {
    "text": "ranked the most significant German-language novels of the twentieth century. The group brought together 23 experts from each of the three categories. Each was allowed to name three books as having been the most important of the century. Cited by the group were five titles by both Franz Kafka and Arno Schmidt, four by Robert Walser, and three by Thomas Mann, Hermann Broch, Anna Seghers, and Joseph Roth. Der Kanon, edited by Marcel Reich-Ranicki, is a large anthology of exemplary works of German literature. French literature See key texts of French literature Le Monde's 100 Books of the Century Canon of Dutch Literature The Canon of Dutch Literature comprises a list of 1000 works of Dutch-language literature important to the cultural heritage of the Low Countries, and is published on the DBNL. Several of these works are lists themselves; such as early dictionaries, lists of songs, recipes, biographies, or encyclopedic compilations of information such as mathematical, scientific, medical, or plant reference books. Other items include early translations of literature from other countries, history books, first-hand diaries, and published correspondence. Notable original works can be found by author name. Scandinavia Danish Culture Canon The Danish Culture Canon consists of 108 works of cultural excellence in eight categories: architecture, visual arts, design and crafts, film, literature, music, performing arts, and children's culture. An initiative of Brian Mikkelsen in 2004, it was developed by a series of committees under the auspices of the Danish Ministry of Culture in 2006–2007 as \"a collection and presentation of the greatest, most important works of Denmark's cultural heritage.\" Each category contains 12 works, although music contains 12 works of score music and 12 of popular music, and the literature section's 12th item is an anthology of 24 works. Sweden Världsbiblioteket (The World Library) was a Swedish list",
    "label": 0
  },
  {
    "text": "heritage.\" Each category contains 12 works, although music contains 12 works of score music and 12 of popular music, and the literature section's 12th item is an anthology of 24 works. Sweden Världsbiblioteket (The World Library) was a Swedish list of the 100 best books in the world, created in 1991 by the Swedish literary magazine Tidningen Boken. The list was compiled through votes from members of the Svenska Akademien, Swedish Crime Writers' Academy, librarians, authors, and others. Approximately 30 of the books were Swedish. Norway Bokklubben World Library. Spain For the Spanish culture, specially for the Spanish literature, during the 19th and the first third of the 20th century similar lists were created trying to define the literary canon. This canon was established mainly through teaching programs, and literary critics like Pedro Estala, Antonio Gil y Zárate, Marcelino Menéndez Pelayo, Ramón Menéndez Pidal, or Juan Bautista Bergua. In the last decades, other important critics have been contributing to the topic, among them, Fernando Lázaro Carreter, José Manuel Blecua Perdices, Francisco Rico, and José Carlos Mainer. Other Spanish languages have also their own literary canons. A good introduction to the Catalan literary canon is La invenció de la tradició literària by Manel Ollé, from the Open University of Catalonia. Biblioteca de Autores Españoles, BAE (Manuel Rivadeneyra, Buenaventura Carlos Aribau, 1846–1888) Nueva Biblioteca de Autores Españoles (Marcelino Menéndez y Pelayo, ed. Bailly-Baillière, 1905–1918); the same author selected Las cien mejores poesías de la lengua castellana, Victoriano Suárez, 1908 Clásicos Castellanos (Ramón Menéndez Pidal, Centro de Estudios Históricos, eds. La Lectura, and Espasa Calpe, 1910–1935) Las mil mejores poesías de la lengua castellana (Juan Bautista Bergua) Mil libros (Luis Nueda, Antonio Espina, since 1940 —not limited to the books in Spanish—) Floresta de la lírica española (José Manuel Blecua Teijeiro, Antología Hispánica,",
    "label": 0
  },
  {
    "text": "Espasa Calpe, 1910–1935) Las mil mejores poesías de la lengua castellana (Juan Bautista Bergua) Mil libros (Luis Nueda, Antonio Espina, since 1940 —not limited to the books in Spanish—) Floresta de la lírica española (José Manuel Blecua Teijeiro, Antología Hispánica, Gredos, 1957) Centro Virtual Cervantes (Instituto Cervantes, online, since 1997) Biblioteca Clásica (Francisco Rico, Real Academia Española, Círculo de Lectores, 2011) Les millors obres de la literatura catalana (Joaquim Molas, Edicions 62, and La Caixa) See also References Further reading Hirsch, E. D.; Trefil, James; Kett, Joseph F. (1988). The dictionary of cultural literacy. Boston: Houghton Mifflin. ISBN 9780395437483. Guillory, John (1993). Cultural capital the problem of literary canon formation. Chicago: University of Chicago Press. ISBN 9780226310442. Knox, Bernard (1994). The oldest dead white European males and other reflections on the classics. New York: W.W. Norton. ISBN 9780393312331. Bloom, Harold (1995). The Western canon: the books and school of the ages. New York: Riverhead Books. ISBN 9781573225144. Owens, W. R. (1996). Shakespeare, Aphra Behn, and the Canon. New York: Routledge in association with the Open University. ISBN 9780415135757. Bloom, Harold (1998). Shakespeare: The Invention of the Human. New York: Riverhead Books. ISBN 9781573227513. Ross, Trevor (1998). The making of the English literary canon from the Middle Ages to the late eighteenth century. Montreal Que: McGill-Queen's University Press. ISBN 9780773520806. Kolbas, E. Dean (2001). Critical Theory and the Literary Canon, Boulder: Westview Press. ISBN 0813398134 Morrissey, Lee (2005). Debating the Canon: A Reader from Addison to Nafisii. New York: Palgrave Macmillan. ISBN 9781403968203. Brzyski, Anna, ed. (2007). Partisan Canons. Duke University Press. ISBN 9780822341062. Owens, W. R. (2009), \"The Canon and the curriculum\", in Gupta, Suman; Katsarska, Milena (eds.), English studies on this side: post-2007 reckonings, Plovdiv, Bulgaria: Plovdiv University Press, pp. 47–59, ISBN 9789544235680 Gorak, Jan (2013). The making of",
    "label": 0
  },
  {
    "text": "Philosophy (from Ancient Greek philosophía lit. 'love of wisdom') is a systematic study of general and fundamental questions concerning topics like existence, knowledge, mind, reason, language, and value. It is a rational and critical inquiry that reflects on its methods and assumptions. Historically, many of the individual sciences, such as physics and psychology, formed part of philosophy. However, they are considered separate academic disciplines in the modern sense of the term. Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Western philosophy originated in Ancient Greece and covers a wide area of philosophical subfields. A central topic in Arabic–Persian philosophy is the relation between reason and revelation. Indian philosophy combines the spiritual problem of how to reach enlightenment with the exploration of the nature of reality and the ways of arriving at knowledge. Chinese philosophy focuses principally on practical issues about right social conduct, government, and self-cultivation. Major branches of philosophy are epistemology, ethics, logic, and metaphysics. Epistemology studies what knowledge is and how to acquire it. Ethics investigates moral principles and what constitutes right conduct. Logic is the study of correct reasoning and explores how good arguments can be distinguished from bad ones. Metaphysics examines the most general features of reality, existence, objects, and properties. Other subfields are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, philosophy of mathematics, philosophy of history, and political philosophy. Within each branch, there are competing schools of philosophy that promote different principles, theories, or methods. Philosophers use a great variety of methods to arrive at philosophical knowledge. They include conceptual analysis, reliance on common sense and intuitions, use of thought experiments, analysis of ordinary language, description of experience, and critical questioning. Philosophy is related to many other fields, such as the natural and",
    "label": 0
  },
  {
    "text": "arrive at philosophical knowledge. They include conceptual analysis, reliance on common sense and intuitions, use of thought experiments, analysis of ordinary language, description of experience, and critical questioning. Philosophy is related to many other fields, such as the natural and social sciences, mathematics, business, law, and journalism. It provides an interdisciplinary perspective and studies the scope and fundamental concepts of these fields. It also investigates their methods and ethical implications. Etymology The word philosophy comes from the Ancient Greek words φίλος (philos) 'love' and σοφία (sophia) 'wisdom'. Some sources say that the term was coined by the pre-Socratic philosopher Pythagoras, but this is not certain. The word entered the English language primarily from Old French and Anglo-Norman starting around 1175 CE. The French philosophie is itself a borrowing from the Latin philosophia. The term philosophy acquired the meanings of \"advanced study of the speculative subjects (logic, ethics, physics, and metaphysics)\", \"deep wisdom consisting of love of truth and virtuous living\", \"profound learning as transmitted by the ancient writers\", and \"the study of the fundamental nature of knowledge, reality, and existence, and the basic limits of human understanding\". Before the modern age, the term philosophy was used in a wide sense. It included most forms of rational inquiry, such as the individual sciences, as its subdisciplines. For instance, natural philosophy was a major branch of philosophy. This branch of philosophy encompassed a wide range of fields, including disciplines like physics, chemistry, and biology. An example of this usage is the 1687 book Philosophiæ Naturalis Principia Mathematica by Isaac Newton. This book referred to natural philosophy in its title, but it is today considered a book of physics. The meaning of philosophy changed toward the end of the modern period when it acquired the more narrow meaning common today. In this new",
    "label": 0
  },
  {
    "text": "referred to natural philosophy in its title, but it is today considered a book of physics. The meaning of philosophy changed toward the end of the modern period when it acquired the more narrow meaning common today. In this new sense, the term is mainly associated with disciplines like metaphysics, epistemology, and ethics. Among other topics, it covers the rational study of reality, knowledge, and values. It is distinguished from other disciplines of rational inquiry such as the empirical sciences and mathematics. Conceptions of philosophy General conception The practice of philosophy is characterized by several general features: it is a form of rational inquiry, it aims to be systematic, and it tends to critically reflect on its own methods and presuppositions. It requires attentively thinking long and carefully about the provocative, vexing, and enduring problems central to the human condition. The philosophical pursuit of wisdom involves asking general and fundamental questions. It often does not result in straightforward answers but may help a person to better understand the topic, examine their life, dispel confusion, and overcome prejudices and self-deceptive ideas associated with common sense. For example, Socrates stated that \"the unexamined life is not worth living\" to highlight the role of philosophical inquiry in understanding one's own existence. And according to Bertrand Russell, \"the man who has no tincture of philosophy goes through life imprisoned in the prejudices derived from common sense, from the habitual beliefs of his age or his nation, and from convictions which have grown up in his mind without the cooperation or consent of his deliberate reason.\" Academic definitions Attempts to provide more precise definitions of philosophy are controversial and are studied in metaphilosophy. Some approaches argue that there is a set of essential features shared by all parts of philosophy. Others see only weaker family",
    "label": 0
  },
  {
    "text": "reason.\" Academic definitions Attempts to provide more precise definitions of philosophy are controversial and are studied in metaphilosophy. Some approaches argue that there is a set of essential features shared by all parts of philosophy. Others see only weaker family resemblances or contend that it is merely an empty blanket term. Precise definitions are often only accepted by theorists belonging to a certain philosophical movement and are revisionistic according to Søren Overgaard et al. in that many presumed parts of philosophy would not deserve the title \"philosophy\" if they were true. Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: \"What can I know?\"; \"What should I do?\"; \"What may I hope?\"; and \"What is the human being?\" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully",
    "label": 0
  },
  {
    "text": "long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as \"the midwife of the sciences\". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a \"rigorous science\" investigating essences. They practice a radical suspension of theoretical assumptions about reality to get back to the \"things themselves\", that is, as originally given in experience. They contend that this base-level of experience provides the foundation for higher-order theoretical knowledge, and that one needs to understand the former to understand the latter. An early approach found in ancient Greek and Roman philosophy is that philosophy is the spiritual practice of developing one's rational capacities. This practice is an expression of the philosopher's love of wisdom and has the aim of improving one's well-being by leading a reflective life. For example, the Stoics saw philosophy as an exercise to train the mind and thereby achieve eudaimonia and flourish in life. History As a discipline, the history of philosophy aims to provide a systematic and chronological exposition of philosophical",
    "label": 0
  },
  {
    "text": "life. For example, the Stoics saw philosophy as an exercise to train the mind and thereby achieve eudaimonia and flourish in life. History As a discipline, the history of philosophy aims to provide a systematic and chronological exposition of philosophical concepts and doctrines. Some theorists see it as a part of intellectual history, but it also investigates questions not covered by intellectual history such as whether the theories of past philosophers are true and have remained philosophically relevant. The history of philosophy is primarily concerned with theories based on rational inquiry and argumentation; some historians understand it in a looser sense that includes myths, religious teachings, and proverbial lore. Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Other philosophical traditions are Japanese philosophy, Latin American philosophy, and African philosophy. Western Western philosophy originated in Ancient Greece in the 6th century BCE with the pre-Socratics. They attempted to provide rational explanations of the cosmos as a whole. The philosophy following them was shaped by Socrates (469–399 BCE), Plato (427–347 BCE), and Aristotle (384–322 BCE). They expanded the range of topics to questions like how people should act, how to arrive at knowledge, and what the nature of reality and mind is. The later part of the ancient period was marked by the emergence of philosophical movements, for example, Epicureanism, Stoicism, Skepticism, and Neoplatonism. The medieval period started in the 5th century CE. Its focus was on religious topics and many thinkers used ancient philosophy to explain and further elaborate Christian doctrines. The Renaissance period started in the 14th century and saw a renewed interest in schools of ancient philosophy, in particular Platonism. Humanism also emerged in this period. The modern period started in the 17th century. One of its central concerns was how philosophical and",
    "label": 0
  },
  {
    "text": "in the 14th century and saw a renewed interest in schools of ancient philosophy, in particular Platonism. Humanism also emerged in this period. The modern period started in the 17th century. One of its central concerns was how philosophical and scientific knowledge are created. Specific importance was given to the role of reason and sensory experience. Many of these innovations were used in the Enlightenment movement to challenge traditional authorities. Several attempts to develop comprehensive systems of philosophy were made in the 19th century, for instance, by German idealism and Marxism. Influential developments in 20th-century philosophy were the emergence and application of formal logic, the focus on the role of language as well as pragmatism, and movements in continental philosophy like phenomenology, existentialism, and post-structuralism. The 20th century saw a rapid expansion of academic philosophy in terms of the number of philosophical publications and philosophers working at academic institutions. There was also a noticeable growth in the number of female philosophers, but they still remained underrepresented. Arabic–Persian Arabic–Persian philosophy arose in the early 9th century CE as a response to discussions in the Islamic theological tradition. Its classical period lasted until the 12th century CE and was strongly influenced by ancient Greek philosophers. It employed their ideas to elaborate and interpret the teachings of the Quran. Al-Kindi (801–873 CE) is usually regarded as the first philosopher of this tradition. He translated and interpreted many works of Aristotle and Neoplatonists in his attempt to show that there is a harmony between reason and faith. Avicenna (980–1037 CE) also followed this goal and developed a comprehensive philosophical system to provide a rational understanding of reality encompassing science, religion, and mysticism. Al-Ghazali (1058–1111 CE) was a strong critic of the idea that reason can arrive at a true understanding of reality and God.",
    "label": 0
  },
  {
    "text": "and developed a comprehensive philosophical system to provide a rational understanding of reality encompassing science, religion, and mysticism. Al-Ghazali (1058–1111 CE) was a strong critic of the idea that reason can arrive at a true understanding of reality and God. He formulated a detailed critique of philosophy and tried to assign philosophy a more limited place besides the teachings of the Quran and mystical insight. Following Al-Ghazali and the end of the classical period, the influence of philosophical inquiry waned. Mulla Sadra (1571–1636 CE) is often regarded as one of the most influential philosophers of the subsequent period. The increasing influence of Western thought and institutions in the 19th and 20th centuries gave rise to the intellectual movement of Islamic modernism, which aims to understand the relation between traditional Islamic beliefs and modernity. Indian One of the distinguishing features of Indian philosophy is that it integrates the exploration of the nature of reality, the ways of arriving at knowledge, and the spiritual question of how to reach enlightenment. It started around 900 BCE when the Vedas were written. They are the foundational scriptures of Hinduism and contemplate issues concerning the relation between the self and ultimate reality as well as the question of how souls are reborn based on their past actions. This period also saw the emergence of non-Vedic teachings, like Buddhism and Jainism. Buddhism was founded by Gautama Siddhartha (563–483 BCE), who challenged the Vedic idea of a permanent self and proposed a path to liberate oneself from suffering. Jainism was founded by Mahavira (599–527 BCE), who emphasized non-violence as well as respect toward all forms of life. The subsequent classical period started roughly 200 BCE and was characterized by the emergence of the six orthodox schools of Hinduism: Nyāyá, Vaiśeṣika, Sāṃkhya, Yoga, Mīmāṃsā, and Vedanta. The school",
    "label": 0
  },
  {
    "text": "non-violence as well as respect toward all forms of life. The subsequent classical period started roughly 200 BCE and was characterized by the emergence of the six orthodox schools of Hinduism: Nyāyá, Vaiśeṣika, Sāṃkhya, Yoga, Mīmāṃsā, and Vedanta. The school of Advaita Vedanta developed later in this period. It was systematized by Adi Shankara (c. 700–750 CE), who held that everything is one and that the impression of a universe consisting of many distinct entities is an illusion. A slightly different perspective was defended by Ramanuja (1017–1137 CE), who founded the school of Vishishtadvaita Vedanta and argued that individual entities are real as aspects or parts of the underlying unity. He also helped to popularize the Bhakti movement, which taught devotion toward the divine as a spiritual path and lasted until the 17th to 18th centuries CE. The modern period began roughly 1800 CE and was shaped by encounters with Western thought. Philosophers tried to formulate comprehensive systems to harmonize diverse philosophical and religious teachings. For example, Swami Vivekananda (1863–1902 CE) used the teachings of Advaita Vedanta to argue that all the different religions are valid paths toward the one divine. Chinese Chinese philosophy is particularly interested in practical questions associated with right social conduct, government, and self-cultivation. Many schools of thought emerged in the 6th century BCE in competing attempts to resolve the political turbulence of that period. The most prominent among them were Confucianism and Daoism. Confucianism was founded by Confucius (551–479 BCE). It focused on different forms of moral virtues and explored how they lead to harmony in society. Daoism was founded by Laozi (6th century BCE) and examined how humans can live in harmony with nature by following the Dao or the natural order of the universe. Other influential early schools of thought were Mohism, which",
    "label": 0
  },
  {
    "text": "in society. Daoism was founded by Laozi (6th century BCE) and examined how humans can live in harmony with nature by following the Dao or the natural order of the universe. Other influential early schools of thought were Mohism, which developed an early form of altruistic consequentialism, and Legalism, which emphasized the importance of a strong state and strict laws. Buddhism was introduced to China in the 1st century CE and diversified into new forms of Buddhism. Starting in the 3rd century CE, the school of Xuanxue emerged. It interpreted earlier Daoist works with a specific emphasis on metaphysical explanations. Neo-Confucianism developed in the 11th century CE. It systematized previous Confucian teachings and sought a metaphysical foundation of ethics. The modern period in Chinese philosophy began in the early 20th century and was shaped by the influence of and reactions to Western philosophy. The emergence of Chinese Marxism—which focused on class struggle, socialism, and communism—resulted in a significant transformation of the political landscape. Another development was the emergence of New Confucianism, which aims to modernize and rethink Confucian teachings to explore their compatibility with democratic ideals and modern science. Other traditions Traditional Japanese philosophy assimilated and synthesized ideas from different traditions, including the indigenous Shinto religion and Chinese and Indian thought in the forms of Confucianism and Buddhism, both of which entered Japan in the 6th and 7th centuries. Its practice is characterized by active interaction with reality rather than disengaged examination. Neo-Confucianism became an influential school of thought in the 16th century and the following Edo period and prompted a greater focus on language and the natural world. The Kyoto School emerged in the 20th century and integrated Eastern spirituality with Western philosophy in its exploration of concepts like absolute nothingness (zettai-mu), place (basho), and the self. Latin American",
    "label": 0
  },
  {
    "text": "a greater focus on language and the natural world. The Kyoto School emerged in the 20th century and integrated Eastern spirituality with Western philosophy in its exploration of concepts like absolute nothingness (zettai-mu), place (basho), and the self. Latin American philosophy in the pre-colonial period was practiced by indigenous civilizations and explored questions concerning the nature of reality and the role of humans. It has similarities to indigenous North American philosophy, which covered themes such as the interconnectedness of all things. Latin American philosophy during the colonial period, starting around 1550, was dominated by religious philosophy in the form of scholasticism. Influential topics in the post-colonial period were positivism, the philosophy of liberation, and the exploration of identity and culture. Early African philosophy was primarily conducted and transmitted orally. It focused on community, morality, and ancestral ideas, encompassing folklore, wise sayings, religious ideas, and philosophical concepts like Ubuntu. Systematic African philosophy emerged at the beginning of the 20th century. It discusses topics such as ethnophilosophy, négritude, pan-Africanism, Marxism, postcolonialism, the role of cultural identity, relativism, African epistemology, and the critique of Eurocentrism. Core branches Philosophical questions can be grouped into several branches. These groupings allow philosophers to focus on a set of similar topics and interact with other thinkers who are interested in the same questions. Epistemology, ethics, logic, and metaphysics are sometimes listed as the main branches. There are many other subfields besides them and the different divisions are neither exhaustive nor mutually exclusive. For example, political philosophy, ethics, and aesthetics are sometimes linked under the general heading of value theory as they investigate normative or evaluative aspects. Furthermore, philosophical inquiry sometimes overlaps with other disciplines in the natural and social sciences, religion, and mathematics. Epistemology Epistemology is the branch of philosophy that studies knowledge. It is also known",
    "label": 0
  },
  {
    "text": "value theory as they investigate normative or evaluative aspects. Furthermore, philosophical inquiry sometimes overlaps with other disciplines in the natural and social sciences, religion, and mathematics. Epistemology Epistemology is the branch of philosophy that studies knowledge. It is also known as theory of knowledge and aims to understand what knowledge is, how it arises, what its limits are, and what value it has. It further examines the nature of truth, belief, justification, and rationality. Some of the questions addressed by epistemologists include \"By what method(s) can one acquire knowledge?\"; \"How is truth established?\"; and \"Can we prove causal relations?\" Epistemology is primarily interested in declarative knowledge or knowledge of facts, like knowing that Princess Diana died in 1997. But it also investigates practical knowledge, such as knowing how to ride a bicycle, and knowledge by acquaintance, for example, knowing a celebrity personally. One area in epistemology is the analysis of knowledge. It assumes that declarative knowledge is a combination of different parts and attempts to identify what those parts are. An influential theory in this area claims that knowledge has three components: it is a belief that is justified and true. This theory is controversial and the difficulties associated with it are known as the Gettier problem. Alternative views state that knowledge requires additional components, like the absence of luck; different components, like the manifestation of cognitive virtues instead of justification; or they deny that knowledge can be analyzed in terms of other phenomena. Another area in epistemology asks how people acquire knowledge. Often-discussed sources of knowledge are perception, introspection, memory, inference, and testimony. According to empiricists, all knowledge is based on some form of experience. Rationalists reject this view and hold that some forms of knowledge, like innate knowledge, are not acquired through experience. The regress problem is a",
    "label": 0
  },
  {
    "text": "memory, inference, and testimony. According to empiricists, all knowledge is based on some form of experience. Rationalists reject this view and hold that some forms of knowledge, like innate knowledge, are not acquired through experience. The regress problem is a common issue in relation to the sources of knowledge and the justification they offer. It is based on the idea that beliefs require some kind of reason or evidence to be justified. The problem is that the source of justification may itself be in need of another source of justification. This leads to an infinite regress or circular reasoning. Foundationalists avoid this conclusion by arguing that some sources can provide justification without requiring justification themselves. Another solution is presented by coherentists, who state that a belief is justified if it coheres with other beliefs of the person. Many discussions in epistemology touch on the topic of philosophical skepticism, which raises doubts about some or all claims to knowledge. These doubts are often based on the idea that knowledge requires absolute certainty and that humans are unable to acquire it. Ethics Ethics, also known as moral philosophy, studies what constitutes right conduct. It is also concerned with the moral evaluation of character traits and institutions. It explores what the standards of morality are and how to live a good life. Philosophical ethics addresses such basic questions as \"Are moral obligations relative?\"; \"Which has priority: well-being or obligation?\"; and \"What gives life meaning?\" The main branches of ethics are meta-ethics, normative ethics, and applied ethics. Meta-ethics asks abstract questions about the nature and sources of morality. It analyzes the meaning of ethical concepts, like right action and obligation. It also investigates whether ethical theories can be true in an absolute sense and how to acquire knowledge of them. Normative ethics encompasses general",
    "label": 0
  },
  {
    "text": "and sources of morality. It analyzes the meaning of ethical concepts, like right action and obligation. It also investigates whether ethical theories can be true in an absolute sense and how to acquire knowledge of them. Normative ethics encompasses general theories of how to distinguish between right and wrong conduct. It helps guide moral decisions by examining what moral obligations and rights people have. Applied ethics studies the consequences of the general theories developed by normative ethics in specific situations, for example, in the workplace or for medical treatments. Within contemporary normative ethics, consequentialism, deontology, and virtue ethics are influential schools of thought. Consequentialists judge actions based on their consequences. One such view is utilitarianism, which argues that actions should increase overall happiness while minimizing suffering. Deontologists judge actions based on whether they follow moral duties, such as abstaining from lying or killing. According to them, what matters is that actions are in tune with those duties and not what consequences they have. Virtue theorists judge actions based on how the moral character of the agent is expressed. According to this view, actions should conform to what an ideally virtuous agent would do by manifesting virtues like generosity and honesty. Logic Logic is the study of correct reasoning. It aims to understand how to distinguish good from bad arguments. It is usually divided into formal and informal logic. Formal logic uses artificial languages with a precise symbolic representation to investigate arguments. In its search for exact criteria, it examines the structure of arguments to determine whether they are correct or incorrect. Informal logic uses non-formal criteria and standards to assess the correctness of arguments. It relies on additional factors such as content and context. Logic examines a variety of arguments. Deductive arguments are mainly studied by formal logic. An argument",
    "label": 0
  },
  {
    "text": "incorrect. Informal logic uses non-formal criteria and standards to assess the correctness of arguments. It relies on additional factors such as content and context. Logic examines a variety of arguments. Deductive arguments are mainly studied by formal logic. An argument is deductively valid if the truth of its premises ensures the truth of its conclusion. Deductively valid arguments follow a rule of inference, like modus ponens, which has the following logical form: \"p; if p then q; therefore q\". An example is the argument \"today is Sunday; if today is Sunday then I don't have to go to work today; therefore I don't have to go to work today\". The premises of non-deductive arguments also support their conclusion, although this support does not guarantee that the conclusion is true. One form is inductive reasoning. It starts from a set of individual cases and uses generalization to arrive at a universal law governing all cases. An example is the inference that \"all ravens are black\" based on observations of many individual black ravens. Another form is abductive reasoning. It starts from an observation and concludes that the best explanation of this observation must be true. This happens, for example, when a doctor diagnoses a disease based on the observed symptoms. Logic also investigates incorrect forms of reasoning. They are called fallacies and are divided into formal and informal fallacies based on whether the source of the error lies only in the form of the argument or also in its content and context. Metaphysics Metaphysics is the study of the most general features of reality, such as existence, objects and their properties, wholes and their parts, space and time, events, and causation. There are disagreements about the precise definition of the term and its meaning has changed throughout the ages. Metaphysicians attempt",
    "label": 0
  },
  {
    "text": "features of reality, such as existence, objects and their properties, wholes and their parts, space and time, events, and causation. There are disagreements about the precise definition of the term and its meaning has changed throughout the ages. Metaphysicians attempt to answer basic questions including \"Why is there something rather than nothing?\"; \"Of what does reality ultimately consist?\"; and \"Are humans free?\" Metaphysics is sometimes divided into general metaphysics and specific or special metaphysics. General metaphysics investigates being as such. It examines the features that all entities have in common. Specific metaphysics is interested in different kinds of being, the features they have, and how they differ from one another. An important area in metaphysics is ontology. Some theorists identify it with general metaphysics. Ontology investigates concepts like being, becoming, and reality. It studies the categories of being and asks what exists on the most fundamental level. Another subfield of metaphysics is philosophical cosmology. It is interested in the essence of the world as a whole. It asks questions including whether the universe has a beginning and an end and whether it was created by something else. A key topic in metaphysics concerns the question of whether reality only consists of physical things like matter and energy. Alternative suggestions are that mental entities (such as souls and experiences) and abstract entities (such as numbers) exist apart from physical things. Another topic in metaphysics concerns the problem of identity. One question is how much an entity can change while still remaining the same entity. According to one view, entities have essential and accidental features. They can change their accidental features but they cease to be the same entity if they lose an essential feature. A central distinction in metaphysics is between particulars and universals. Universals, like the color red, can exist",
    "label": 0
  },
  {
    "text": "and accidental features. They can change their accidental features but they cease to be the same entity if they lose an essential feature. A central distinction in metaphysics is between particulars and universals. Universals, like the color red, can exist at different locations at the same time. This is not the case for particulars including individual persons or specific objects. Other metaphysical questions are whether the past fully determines the present and what implications this would have for the existence of free will. Other major branches There are many other subfields of philosophy besides its core branches. Some of the most prominent are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, and political philosophy. Aesthetics in the philosophical sense is the field that studies the nature and appreciation of beauty and other aesthetic properties, like the sublime. Although it is often treated together with the philosophy of art, aesthetics is a broader category that encompasses other aspects of experience, such as natural beauty. In a more general sense, aesthetics is \"critical reflection on art, culture, and nature\". A key question in aesthetics is whether beauty is an objective feature of entities or a subjective aspect of experience. Aesthetic philosophers also investigate the nature of aesthetic experiences and judgments. Further topics include the essence of works of art and the processes involved in creating them. The philosophy of language studies the nature and function of language. It examines the concepts of meaning, reference, and truth. It aims to answer questions such as how words are related to things and how language affects human thought and understanding. It is closely related to the disciplines of logic and linguistics. The philosophy of language rose to particular prominence in the early 20th century in analytic philosophy due to the",
    "label": 0
  },
  {
    "text": "to things and how language affects human thought and understanding. It is closely related to the disciplines of logic and linguistics. The philosophy of language rose to particular prominence in the early 20th century in analytic philosophy due to the works of Frege and Russell. One of its central topics is to understand how sentences get their meaning. There are two broad theoretical camps: those emphasizing the formal truth conditions of sentences and those investigating circumstances that determine when it is suitable to use a sentence, the latter of which is associated with speech act theory. The philosophy of mind studies the nature of mental phenomena and how they are related to the physical world. It aims to understand different types of conscious and unconscious mental states, like beliefs, desires, intentions, feelings, sensations, and free will. An influential intuition in the philosophy of mind is that there is a distinction between the inner experience of objects and their existence in the external world. The mind-body problem is the problem of explaining how these two types of thing—mind and matter—are related. The main traditional responses are materialism, which assumes that matter is more fundamental; idealism, which assumes that mind is more fundamental; and dualism, which assumes that mind and matter are distinct types of entities. In contemporary philosophy, another common view is functionalism, which understands mental states in terms of the functional or causal roles they play. The mind-body problem is closely related to the hard problem of consciousness, which asks how the physical brain can produce qualitatively subjective experiences. The philosophy of religion investigates the basic concepts, assumptions, and arguments associated with religion. It critically reflects on what religion is, how to define the divine, and whether one or more gods exist. It also includes the discussion of worldviews that",
    "label": 0
  },
  {
    "text": "philosophy of religion investigates the basic concepts, assumptions, and arguments associated with religion. It critically reflects on what religion is, how to define the divine, and whether one or more gods exist. It also includes the discussion of worldviews that reject religious doctrines. Further questions addressed by the philosophy of religion are: \"How are we to interpret religious language, if not literally?\"; \"Is divine omniscience compatible with free will?\"; and, \"Are the great variety of world religions in some way compatible in spite of their apparently contradictory theological claims?\" It includes topics from nearly all branches of philosophy. It differs from theology since theological debates typically take place within one religious tradition, whereas debates in the philosophy of religion transcend any particular set of theological assumptions. The philosophy of science examines the fundamental concepts, assumptions, and problems associated with science. It reflects on what science is and how to distinguish it from pseudoscience. It investigates the methods employed by scientists, how their application can result in knowledge, and on what assumptions they are based. It also studies the purpose and implications of science. Some of its questions are \"What counts as an adequate explanation?\"; \"Is a scientific law anything more than a description of a regularity?\"; and \"Can some special sciences be explained entirely in the terms of a more general science?\" It is a vast field that is commonly divided into the philosophy of the natural sciences and the philosophy of the social sciences, with further subdivisions for each of the individual sciences under these headings. How these branches are related to one another is also a question in the philosophy of science. Many of its philosophical issues overlap with the fields of metaphysics or epistemology. Political philosophy is the philosophical inquiry into the fundamental principles and ideas governing",
    "label": 0
  },
  {
    "text": "are related to one another is also a question in the philosophy of science. Many of its philosophical issues overlap with the fields of metaphysics or epistemology. Political philosophy is the philosophical inquiry into the fundamental principles and ideas governing political systems and societies. It examines the basic concepts, assumptions, and arguments in the field of politics. It investigates the nature and purpose of government and compares its different forms. It further asks under what circumstances the use of political power is legitimate, rather than a form of simple violence. In this regard, it is concerned with the distribution of political power, social and material goods, and legal rights. Other topics are justice, liberty, equality, sovereignty, and nationalism. Political philosophy involves a general inquiry into normative matters and differs in this respect from political science, which aims to provide empirical descriptions of actually existing states. Political philosophy is often treated as a subfield of ethics. Influential schools of thought in political philosophy are liberalism, conservativism, socialism, and anarchism. Methods Methods of philosophy are ways of conducting philosophical inquiry. They include techniques for arriving at philosophical knowledge and justifying philosophical claims as well as principles used for choosing between competing theories. A great variety of methods have been employed throughout the history of philosophy. Many of them differ significantly from the methods used in the natural sciences in that they do not use experimental data obtained through measuring equipment. The choice of one's method usually has important implications both for how philosophical theories are constructed and for the arguments cited for or against them. This choice is often guided by epistemological considerations about what constitutes philosophical evidence. Methodological disagreements can cause conflicts among philosophical theories or about the answers to philosophical questions. The discovery of new methods has often had important",
    "label": 0
  },
  {
    "text": "or against them. This choice is often guided by epistemological considerations about what constitutes philosophical evidence. Methodological disagreements can cause conflicts among philosophical theories or about the answers to philosophical questions. The discovery of new methods has often had important consequences both for how philosophers conduct their research and for what claims they defend. Some philosophers engage in most of their theorizing using one particular method while others employ a wider range of methods based on which one fits the specific problem investigated best. Conceptual analysis is a common method in analytic philosophy. It aims to clarify the meaning of concepts by analyzing them into their component parts. Another method often employed in analytic philosophy is based on common sense. It starts with commonly accepted beliefs and tries to draw unexpected conclusions from them, which it often employs in a negative sense to criticize philosophical theories that are too far removed from how the average person sees the issue. It is similar to how ordinary language philosophy approaches philosophical questions by investigating how ordinary language is used. Various methods in philosophy give particular importance to intuitions, that is, non-inferential impressions about the correctness of specific claims or general principles. For example, they play an important role in thought experiments, which employ counterfactual thinking to evaluate the possible consequences of an imagined situation. These anticipated consequences can then be used to confirm or refute philosophical theories. The method of reflective equilibrium also employs intuitions. It seeks to form a coherent position on a certain issue by examining all the relevant beliefs and intuitions, some of which often have to be deemphasized or reformulated to arrive at a coherent perspective. Pragmatists stress the significance of concrete practical consequences for assessing whether a philosophical theory is true. According to the pragmatic maxim as",
    "label": 0
  },
  {
    "text": "and intuitions, some of which often have to be deemphasized or reformulated to arrive at a coherent perspective. Pragmatists stress the significance of concrete practical consequences for assessing whether a philosophical theory is true. According to the pragmatic maxim as formulated by Charles Sanders Peirce, the idea a person has of an object is nothing more than the totality of practical consequences they associate with this object. Pragmatists have also used this method to expose disagreements as merely verbal, that is, to show they make no genuine difference on the level of consequences. Phenomenologists seek knowledge of the realm of appearance and the structure of human experience. They insist upon the first-personal character of all experience and proceed by suspending theoretical judgments about the external world. This technique of phenomenological reduction is known as \"bracketing\" or epoché. The goal is to give an unbiased description of the appearance of things. Methodological naturalism places great emphasis on the empirical approach and the resulting theories found in the natural sciences. In this way, it contrasts with methodologies that give more weight to pure reasoning and introspection. Relation to other fields Philosophy is closely related to many other fields. It is sometimes understood as a meta-discipline that clarifies their nature and limits. It does this by critically examining their basic concepts, background assumptions, and methods. In this regard, it plays a key role in providing an interdisciplinary perspective. It bridges the gap between different disciplines by analyzing which concepts and problems they have in common. It shows how they overlap while also delimiting their scope. Historically, most of the individual sciences originated from philosophy. The influence of philosophy is felt in several fields that require difficult practical decisions. In medicine, philosophical considerations related to bioethics affect issues like whether an embryo is already",
    "label": 0
  },
  {
    "text": "their scope. Historically, most of the individual sciences originated from philosophy. The influence of philosophy is felt in several fields that require difficult practical decisions. In medicine, philosophical considerations related to bioethics affect issues like whether an embryo is already a person and under what conditions abortion is morally permissible. A closely related philosophical problem is how humans should treat other animals, for instance, whether it is acceptable to use non-human animals as food or for research experiments. In relation to business and professional life, philosophy has contributed by providing ethical frameworks. They contain guidelines on which business practices are morally acceptable and cover the issue of corporate social responsibility. Philosophical inquiry is relevant to many fields that are concerned with what to believe and how to arrive at evidence for one's beliefs. This is a key issue for the sciences, which have as one of their prime objectives the creation of scientific knowledge. Scientific knowledge is based on empirical evidence but it is often not clear whether empirical observations are neutral or already include theoretical assumptions. A closely connected problem is whether the available evidence is sufficient to decide between competing theories. Epistemological problems in relation to the law include what counts as evidence and how much evidence is required to find a person guilty of a crime. A related issue in journalism is how to ensure truth and objectivity when reporting on events. In the fields of theology and religion, there are many doctrines associated with the existence and nature of God as well as rules governing correct behavior. A key issue is whether a rational person should believe these doctrines, for example, whether revelation in the form of holy books and religious experiences of the divine are sufficient evidence for these beliefs. Philosophy in the form of",
    "label": 0
  },
  {
    "text": "behavior. A key issue is whether a rational person should believe these doctrines, for example, whether revelation in the form of holy books and religious experiences of the divine are sufficient evidence for these beliefs. Philosophy in the form of logic has been influential in the fields of mathematics and computer science. Further fields influenced by philosophy include psychology, sociology, linguistics, education, and the arts. The close relation between philosophy and other fields in the contemporary period is reflected in the fact that many philosophy graduates go on to work in related fields rather than in philosophy itself. In the field of politics, philosophy addresses issues such as how to assess whether a government policy is just. Philosophical ideas have prepared and shaped various political developments. For example, ideals formulated in Enlightenment philosophy laid the foundation for constitutional democracy and played a role in the American Revolution and the French Revolution. Marxist philosophy and its exposition of communism was one of the factors in the Russian Revolution and the Chinese Communist Revolution. In India, Mahatma Gandhi's philosophy of non-violence shaped the Indian independence movement. An example of the cultural and critical role of philosophy is found in its influence on the feminist movement through philosophers such as Mary Wollstonecraft, Simone de Beauvoir, and Judith Butler. It has shaped the understanding of key concepts in feminism, for instance, the meaning of gender, how it differs from biological sex, and what role it plays in the formation of personal identity. Philosophers have also investigated the concepts of justice and equality and their implications with respect to the prejudicial treatment of women in male-dominated societies. The idea that philosophy is useful for many aspects of life and society is sometimes rejected. According to one such view, philosophy is mainly undertaken for its own",
    "label": 0
  },
  {
    "text": "Philosophy is the study of general and fundamental problems concerning matters such as existence, knowledge, values, reason, mind, and language. It is distinguished from other ways of addressing fundamental questions (such as mysticism, myth) by being critical and generally systematic and by its reliance on rational argument. It involves logical analysis of language and clarification of the meaning of words and concepts. The word \"philosophy\" comes from the Greek philosophia (φιλοσοφία), which literally means \"love of wisdom\". Branches of philosophy The branches of philosophy and their sub-branches that are used in contemporary philosophy are as follows. Aesthetics Aesthetics is study of the nature of beauty, art, and taste, and the creation of personal kinds of truth. Philosophy of art Philosophy of color Philosophy of design Philosophy of film Philosophy of literature Epistemology Epistemology is the branch of philosophy that studies the source, nature and validity of knowledge. Social epistemology – inquiry into the social aspects of knowledge. Formal epistemology – the application of formal models to study knowledge. Metaepistemology – studying the foundations of epistemology itself. Ethics Ethics – study of value and morality. Applied ethics – philosophical examination, from a moral standpoint, of particular issues in private and public life that are matters of moral judgment. It is thus the attempts to use philosophical methods to identify the morally correct course of action in various fields of human life. Bioethics – analysis of controversial ethical issues emerging from advances in medicine. Environmental ethics – studies ethical issues concerning the non-human world. It exerts influence on a large range of disciplines including environmental law, environmental sociology, ecotheology, ecological economics, ecology and environmental geography. Medical ethics – studies ethical issues concerning medicine and medical research. Professional ethics – ethics to improve professionalism. Discourse ethics – discovery of ethical principles through the",
    "label": 0
  },
  {
    "text": "disciplines including environmental law, environmental sociology, ecotheology, ecological economics, ecology and environmental geography. Medical ethics – studies ethical issues concerning medicine and medical research. Professional ethics – ethics to improve professionalism. Discourse ethics – discovery of ethical principles through the study of language. Normative ethics – study of ethical theories that prescribe how people ought to act. Metaethics – branch of ethics that seeks to understand the nature of ethical properties, statements, attitudes, and judgments. Logic Logic – the systematic study of the form of valid inference and reasoning. Also regarded as the separate formal science. Classical logic Propositional logic First-order logic Second-order logic Higher-order logic Non-classical logic Description logic Digital logic Fuzzy logic Intuitionistic logic Many-valued logic Modal logic Alethic logic Deontic logic Doxastic logic Epistemic logic Temporal logic Paraconsistent logic Substructural logic Metalogic Philosophy of logic Metaphysics Metaphysics – concerned with explaining the fundamental nature of being and the world that encompasses it. Cosmology – the study of the nature and origins of the universe. Ontology – philosophical study of the nature of being, becoming, existence, or reality, as well as the basic categories of being and their relations. Meta-ontology – study of the ontological foundations of ontology itself. Philosophy of space and time – branch of philosophy concerned with the issues surrounding the ontology, epistemology, and character of space and time. Other Metaphilosophy Phenomenology Philosophy of action Philosophy of education Philosophy of environment Philosophy of history Philosophy of language Philosophy of law Philosophy of life Philosophy of mathematics Philosophy of mind Philosophy of religion Philosophy of love and sex Philosophy of science Philosophical anthropology Philosophy of archaeology Philosophy of biology Philosophy of chemistry Philosophy of computer science Philosophy of artificial intelligence Philosophy of geography Philosophy of medicine Philosophy of physics Interpretations of quantum mechanics Philosophy of social",
    "label": 0
  },
  {
    "text": "and sex Philosophy of science Philosophical anthropology Philosophy of archaeology Philosophy of biology Philosophy of chemistry Philosophy of computer science Philosophy of artificial intelligence Philosophy of geography Philosophy of medicine Philosophy of physics Interpretations of quantum mechanics Philosophy of social science Philosophy of economics Philosophy of psychology Philosophy of sports Philosophy of war Political philosophy Pragmatism Philosophic traditions by region Regional variations of philosophy. Africana philosophy Akan philosophy Ethiopian philosophy Ubuntu philosophy Eastern philosophy Chinese philosophy Indian philosophy Indonesian philosophy Japanese philosophy Korean philosophy Vietnamese philosophy Middle Eastern philosophy Iranian philosophy Pakistani philosophy Turkish philosophy Indigenous American philosophy Aztec philosophy Western philosophy American philosophy Australian philosophy British philosophy Canadian philosophy Czech philosophy Danish philosophy Dutch philosophy French philosophy Greek philosophy German philosophy Italian philosophy Maltese philosophy Polish philosophy Romanian philosophy Russian philosophy Scottish philosophy Slovene philosophy Spanish philosophy Yugoslav philosophy History of philosophy The history of philosophy in specific contexts of time and space. Timeline of philosophy 11th century in philosophy 12th century in philosophy 13th century in philosophy 14th century in philosophy 15th century in philosophy 16th century in philosophy 17th century in philosophy 18th century in philosophy 19th-century philosophy 20th-century philosophy Ancient and classical philosophy Philosophies during ancient history. Ancient Greek and Roman philosophy Pre-Socratic philosophy Milesian school - Thales, Anaximander, Anaximenes Xenophanes Pythagoreanism Heraclitus Eleatics - Parmenides, Zeno, and Melissus Pluralists - Empedocles and Anaxagoras Atomists - Leucippus, Democritus Sophists Classical Greek philosophy Socratic schools Megarian school Eretrian school Cynicism Cyrenaics Platonism Peripatetic school Hellenistic philosophy Academic skepticism Middle Academy New Academy Epicureanism Pyrrhonism Stoicism Ancient Roman philosophy Middle Platonism Neopythagoreanism Neoplatonism Classical Chinese philosophy Hundred Schools of Thought Confucianism Legalism Taoism Mohism School of Naturalists School of Names School of Diplomacy Agriculturalism Syncretism Yangism Classical Indian philosophy Orthodox schools Samkhya Yoga Nyaya Vaisheshika Mīmāṃsā Vedanta Heterodox",
    "label": 0
  },
  {
    "text": "philosophy Middle Platonism Neopythagoreanism Neoplatonism Classical Chinese philosophy Hundred Schools of Thought Confucianism Legalism Taoism Mohism School of Naturalists School of Names School of Diplomacy Agriculturalism Syncretism Yangism Classical Indian philosophy Orthodox schools Samkhya Yoga Nyaya Vaisheshika Mīmāṃsā Vedanta Heterodox schools Ajñana Jain philosophy Buddhist philosophy Ājīvika Charvaka Medieval and post-classical philosophy Philosophies during post-classical history. Christian philosophy Neoplatonism Christian Scholasticism Thomism Islamic philosophy Avicennism Averroism Illuminationism Jewish philosophy Judeo-Islamic philosophies Post-classical Chinese philosophy Neo-Confucianism Xuanxue Zen Modern and contemporary philosophy Philosophies during the modern era. Renaissance philosophy Renaissance humanism Renaissance Jewish philosophy Machiavellianism Neostoicism Ramism School of Salamanca Early modern philosophy Empiricism Rationalism Idealism Contemporary philosophy Analytic philosophy Logical positivism Logicism Continental philosophy Phenomenology Existentialism Deconstruction Structuralism Contemporary Asian philosophy Buddhist modernism New Confucianism Maoism Kyoto School Neo-Vedanta Contemporary Islamic philosophy Transcendent theosophy Traditionalist School Philosophical schools of thought Philosophical schools of thought not tied to particular historic contexts. Aesthetical movements Symbolism Romanticism Historicism Classicism Modernism Postmodernism Psychoanalytic theory Epistemological stances Ethical theories Consequentialism Deontology Virtue ethics Moral realism Moral relativism Error theory Non-cognitivism Ethical egoism Cultural relativism Evolutionary ethics Evolution of morality Face-to-face Logical systems Classical logic Intermediate logic Intuitionistic logic Minimal logic Relevant logic Affine logic Linear logic Ordered logic Dialetheism Metaphysical stances Political philosophies Anarchism Authoritarianism Conservatism Fascism Liberalism Libertarianism Social democracy Socialism Communism Philosophy of language theories and stances Philosophy of mind theories and stances Philosophy of religion stances Philosophy of science theories and stances Philosophical literature Reference works Encyclopedia of Philosophy – one of the major English encyclopedias of philosophy. The second edition, edited by Donald M. Borchert, was published in ten volumes in 2006 by Thomson Gale. Volumes 1–9 contain alphabetically ordered articles. Internet Encyclopedia of Philosophy – a free online encyclopedia on philosophical topics and philosophers founded by James Fieser in 1995.",
    "label": 0
  },
  {
    "text": "by Donald M. Borchert, was published in ten volumes in 2006 by Thomson Gale. Volumes 1–9 contain alphabetically ordered articles. Internet Encyclopedia of Philosophy – a free online encyclopedia on philosophical topics and philosophers founded by James Fieser in 1995. The current general editors are James Fieser (Professor of Philosophy at the University of Tennessee at Martin) and Bradley Dowden (Professor of Philosophy at California State University, Sacramento). The staff also includes numerous area editors as well as volunteers. Routledge Encyclopedia of Philosophy – encyclopedia of philosophy edited by Edward Craig that was first published by Routledge in 1998 (ISBN 978-0415073103). Originally published in both 10 volumes of print and as a CD-ROM, in 2002 it was made available online on a subscription basis. The online version is regularly updated with new articles and revisions to existing articles. It has 1,300 contributors providing over 2,000 scholarly articles. Stanford Encyclopedia of Philosophy – combines an online encyclopedia of philosophy with peer reviewed publication of original papers in philosophy, freely-accessible to internet users. Each entry is written and maintained by an expert in the field, including professors from many academic institutions worldwide. Chan, Wing-tsit (1963). A Source Book in Chinese Philosophy. Princeton University Press. ISBN 978-0-691-01964-2. {{cite book}}: ISBN / Date incompatibility (help) Huang, Siu-chi (1999). Essentials of Neo-Confucianism: Eight Major Philosophers of the Song and Ming Periods. Greenwood Publishing Group. ISBN 978-0-313-26449-8. The Cambridge Dictionary of Philosophy by Robert Audi Edwards, Paul, ed. (1967). The Encyclopedia of Philosophy. Macmillan & Free Press.; in 1996, a ninth supplemental volume appeared that updated the classic 1967 encyclopedia. International Directory of Philosophy and Philosophers. Charlottesville, Philosophy Documentation Center. Directory of American Philosophers. Charlottesville, Philosophy Documentation Center. Routledge History of Philosophy (10 vols.) edited by John Marenbon History of Philosophy (9 vols.) by Frederick Copleston",
    "label": 0
  },
  {
    "text": "the classic 1967 encyclopedia. International Directory of Philosophy and Philosophers. Charlottesville, Philosophy Documentation Center. Directory of American Philosophers. Charlottesville, Philosophy Documentation Center. Routledge History of Philosophy (10 vols.) edited by John Marenbon History of Philosophy (9 vols.) by Frederick Copleston A History of Western Philosophy (5 vols.) by W.T. Jones History of Italian Philosophy (2 vols.) by Eugenio Garin. Translated from Italian and Edited by Giorgio Pinton. Introduction by Leon Pompa. Encyclopaedia of Indian Philosophies (8 vols.), edited by Karl H. Potter et al. (first 6 volumes out of print) Indian Philosophy (2 vols.) by Sarvepalli Radhakrishnan A History of Indian Philosophy (5 vols.) by Surendranath Dasgupta History of Chinese Philosophy (2 vols.) by Fung Yu-lan, Derk Bodde Instructions for Practical Living and Other Neo-Confucian Writings by Wang Yang-ming by Chan, Wing-tsit Encyclopedia of Chinese Philosophy edited by Antonio S. Cua Encyclopedia of Eastern Philosophy and Religion by Ingrid Fischer-Schreiber, Franz-Karl Ehrhard, Kurt Friedrichs Companion Encyclopedia of Asian Philosophy by Brian Carr, Indira Mahalingam A Concise Dictionary of Indian Philosophy: Sanskrit Terms Defined in English by John A. Grimes History of Islamic Philosophy edited by Seyyed Hossein Nasr, Oliver Leaman History of Jewish Philosophy edited by Daniel H. Frank, Oliver Leaman A History of Russian Philosophy: From the Tenth to the Twentieth Centuries by Valerii Aleksandrovich Kuvakin Ayer, A.J. et al., Ed. (1994) A Dictionary of Philosophical Quotations. Blackwell Reference Oxford. Oxford, Basil Blackwell Ltd. Blackburn, S., Ed. (1996)The Oxford Dictionary of Philosophy. Oxford, Oxford University Press. Mautner, T., Ed. The Penguin Dictionary of Philosophy. London, Penguin Books. Runes, D., ed. (1942). The Dictionary of Philosophy. New York: The Philosophical Library, Inc. Archived from the original on 24 April 2014. Retrieved 27 December 2005. Angeles, P.A., Ed. (1992). The HarperCollins Dictionary of Philosophy. New York, Harper Perennial. Bunnin, Nicholas; Tsui-James,",
    "label": 0
  },
  {
    "text": "ed. (1942). The Dictionary of Philosophy. New York: The Philosophical Library, Inc. Archived from the original on 24 April 2014. Retrieved 27 December 2005. Angeles, P.A., Ed. (1992). The HarperCollins Dictionary of Philosophy. New York, Harper Perennial. Bunnin, Nicholas; Tsui-James, Eric, eds. (15 April 2008). The Blackwell Companion to Philosophy. John Wiley & Sons. ISBN 978-0-470-99787-1. Hoffman, Eric, Ed. (1997) Guidebook for Publishing Philosophy. Charlottesville, Philosophy Documentation Center. Popkin, R.H. (1999). The Columbia History of Western Philosophy. New York, Columbia University Press. Bullock, Alan, and Oliver Stallybrass, jt. eds. The Harper Dictionary of Modern Thought. New York: Harper & Row, 1977. xix, 684 p. N.B.: First published in England under the title, \"The Fontana Dictionary of Modern Thought\". ISBN 978-0-06-010578-5 Reese, W.L. Dictionary of Philosophy and Religion: Eastern and Western Thought. Atlantic Highlands, N.J.: Humanities Press, 1980. iv, 644 p. ISBN 978-0-391-00688-1 General introduction Topical introductions Historical introductions Lists List of important publications in philosophy List of philosophy awards Lists of philosophers Index of philosophy Index of philosophy of science articles Timeline of Western philosophers Timeline of Eastern philosophers Unsolved problems in philosophy See also Outline of ethics Outline of logic Outline of philosophy of artificial intelligence References External links Taxonomy of Philosophy – topic outline developed by David Chalmers as the category structure for the table of contents of the PhilPapers academic directory. PhilPapers – comprehensive directory of online philosophical articles and books. Dictionary of Philosophical Terms and Names Guide to Philosophy on the Internet Archived 2007-06-09 at the Wayback Machine The Internet Encyclopedia of Philosophy The Ism Book Introducing Philosophy Series. By Paul Newall (for beginners) Archived 2007-03-08 at the Wayback Machine Philosophical positions (philosophy, movement, school, theory, etc.) The Problems of Philosophy, by Bertrand Russell (links provided to full text) Stanford Encyclopedia of Philosophy",
    "label": 0
  },
  {
    "text": "Mathematics is a field of study that discovers and organizes methods, theories, and theorems that are developed and proven for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics). Mathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms. Mathematics uses pure reason to prove the properties of objects through proofs, which consist of a succession of applications of deductive rules to already established results. These results, called theorems, include previously proven theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration. Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications. Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was primarily divided into geometry and arithmetics (the manipulation of natural numbers and fractions) until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new fields. Since then, the interaction",
    "label": 0
  },
  {
    "text": "Euclid's Elements. Since its beginning, mathematics was primarily divided into geometry and arithmetics (the manipulation of natural numbers and fractions) until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics. Areas of mathematics Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes. Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics. During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics. The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the 17th century. At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics. The 2020 Mathematics Subject Classification contains",
    "label": 0
  },
  {
    "text": "mathematics until the 17th century. At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics. The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas. Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have \"geometry\" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations. Number theory Number theory began with the manipulation of numbers, that is, natural numbers ( N ) , {\\displaystyle (\\mathbb {N} ),} and later expanded to integers ( Z ) {\\displaystyle (\\mathbb {Z} )} and rational numbers ( Q ) . {\\displaystyle (\\mathbb {Q} ).} Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations. Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria. The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss. Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and",
    "label": 0
  },
  {
    "text": "mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra. Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort. Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), Diophantine analysis, and transcendence theory (problem oriented). Geometry Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields. A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements. The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space. Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a",
    "label": 0
  },
  {
    "text": "and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space. Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically. Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions. In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem. In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space. Today's subareas of geometry include: Projective geometry, introduced in the 16th century by Girard Desargues, extends Euclidean geometry by adding points at infinity at which parallel lines intersect. This simplifies many aspects of classical geometry by unifying the treatments for intersecting and parallel lines.",
    "label": 0
  },
  {
    "text": "include: Projective geometry, introduced in the 16th century by Girard Desargues, extends Euclidean geometry by adding points at infinity at which parallel lines intersect. This simplifies many aspects of classical geometry by unifying the treatments for intersecting and parallel lines. Affine geometry, the study of properties relative to parallelism and independent from the concept of length. Differential geometry, the study of curves, surfaces, and their generalizations, which are defined using differentiable functions. Manifold theory, the study of shapes that are not necessarily embedded in a larger space. Riemannian geometry, the study of distance properties in curved spaces. Algebraic geometry, the study of curves, surfaces, and their generalizations, which are defined using polynomials. Topology, the study of properties that are kept under continuous deformations. Algebraic topology, the use in topology of algebraic methods, mainly homological algebra. Discrete geometry, the study of finite configurations in geometry. Convex geometry, the study of convex sets, which takes its importance from its applications in optimization. Complex geometry, the geometry obtained by replacing real numbers with complex numbers. Algebra Algebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra. Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution. Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side. The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts' that he used for naming one of these methods in the title of his main treatise. Algebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers. Variables allow mathematicians to describe the operations that",
    "label": 0
  },
  {
    "text": "in the title of his main treatise. Algebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers. Variables allow mathematicians to describe the operations that have to be done on the numbers represented using mathematical formulas. Until the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid. The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether, and popularized by Van der Waerden's book Moderne Algebra. Some types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include: group theory field theory vector spaces, whose study is essentially the same as linear algebra ring theory commutative algebra, which is the study of commutative rings, includes the study of polynomials, and is a foundational part of algebraic geometry homological algebra Lie algebra and Lie group theory Boolean algebra, which is widely used for the study of the logical structure of computers The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category",
    "label": 0
  },
  {
    "text": "algebra Lie algebra and Lie group theory Boolean algebra, which is widely used for the study of the logical structure of computers The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory. The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology. Calculus and analysis Calculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz. It is fundamentally the study of the relationship between variables that depend continuously on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results. Presently, \"calculus\" refers mainly to the elementary part of this theory, and \"analysis\" is commonly used for advanced parts. Analysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include: Multivariable calculus Functional analysis, where variables represent varying functions Integration, measure theory and potential theory, all strongly related with probability theory on a continuum Ordinary differential equations Partial differential equations Numerical analysis, mainly devoted to the computation on computers of solutions of ordinary and partial differential equations that arise in many applications Discrete mathematics Discrete mathematics, broadly speaking, is the study of individual, countable mathematical objects. An example is the set of all integers. Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply. Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics. The four color theorem",
    "label": 0
  },
  {
    "text": "the set of all integers. Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply. Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics. The four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century. The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems. Discrete mathematics includes: Combinatorics, the art of enumerating mathematical objects that satisfy some given constraints. Originally, these objects were elements or subsets of a given set; this has been extended to various objects, which establishes a strong link between combinatorics and other parts of discrete mathematics. For example, discrete geometry includes counting configurations of geometric shapes. Graph theory and hypergraphs Coding theory, including error correcting codes and a part of cryptography Matroid theory Discrete geometry Discrete probability distributions Game theory (although continuous games are also studied, most common games, such as chess and poker are discrete) Discrete optimization, including combinatorial optimization, integer programming, constraint programming Mathematical logic and set theory The two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century. Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians. Before Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led",
    "label": 0
  },
  {
    "text": "infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory. In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour. This became the foundational crisis of mathematics. It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have. For example, in Peano arithmetic, the natural numbers are defined by \"zero is a number\", \"each number has a unique successor\", \"each number but zero has a unique predecessor\", and some rules of reasoning. This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910. The \"nature\" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even if many mathematicians have opinions on this nature, and use their opinion—sometimes called \"intuition\"—to guide their study and proofs. The approach allows considering \"logics\" (that is, sets of allowed deducing rules), theorems, proofs, etc. as mathematical objects, and to prove theorems about them. For example, Gödel's incompleteness theorems assert, roughly speaking that, in every consistent formal system that contains the natural numbers, there are theorems that are true (that is provable in a stronger system), but not provable inside the system. This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic (which explicitly lacks the",
    "label": 0
  },
  {
    "text": "in a stronger system), but not provable inside the system. This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic (which explicitly lacks the law of excluded middle). These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory. Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, formal verification, program analysis, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories. Statistics and other decision sciences The field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods such as, and especially, probability theory. Statisticians generate data with random sampling or randomized experiments. Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics. Computational mathematics Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity. Part of computational mathematics involves numerical analysis, which is the study of methods for",
    "label": 0
  },
  {
    "text": "operations research, control theory, and mathematical economics. Computational mathematics Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity. Part of computational mathematics involves numerical analysis, which is the study of methods for problems in analysis using functional analysis and approximation theory. Numerical analysis broadly includes the study of approximation and discretization, with special focus on rounding errors. Numerical analysis and, more broadly, scientific computing, also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation. History Etymology The word mathematics comes from the Ancient Greek word máthēma (μάθημα), meaning 'something learned, knowledge, mathematics', and the derived expression mathēmatikḗ tékhnē (μαθηματικὴ τέχνη), meaning 'mathematical science'. It entered the English language during the Late Middle English period through French and Latin. Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant \"learners\" rather than \"mathematicians\" in the modern sense. The Pythagoreans were likely the first to constrain the use of the word to just the study of arithmetic and geometry. By the time of Aristotle (384–322 BC) this meaning was fully established. In Latin and English, until around 1700, the term mathematics more commonly meant \"astrology\" (or sometimes \"astronomy\") rather than \"mathematics\"; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning \"astrologers\", is sometimes mistranslated as a condemnation of mathematicians. The apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly \"all things mathematical\", although it is plausible that English borrowed only the adjective",
    "label": 0
  },
  {
    "text": "The apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly \"all things mathematical\", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, inherited from Greek. In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math. Ancient In addition to recognizing how to count physical objects, prehistoric peoples may have also known how to count abstract quantities, like time—days, seasons, or years. Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time. In the 6th century BC, Greek mathematics began to emerge as a distinct discipline and some Ancient Greeks such as the Pythagoreans appeared to have considered it a subject in its own right. Around 300 BC, Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof. His book, Elements, is widely considered the most successful and",
    "label": 0
  },
  {
    "text": "Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof. His book, Elements, is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287 – c. 212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea, 2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD). The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series. Medieval and later During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarizmi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available",
    "label": 0
  },
  {
    "text": "the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarizmi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available in Europe. During the early modern period, mathematics began to develop at an accelerating pace in Western Europe, with innovations that revolutionized mathematics, such as the introduction of variables and symbolic notation by François Viète (1540–1603), the introduction of logarithms by John Napier in 1614, which greatly simplified numerical calculations, especially for astronomy and marine navigation, the introduction of coordinates by René Descartes (1596–1650) for reducing geometry to algebra, and the development of calculus by Isaac Newton (1643–1727) and Gottfried Leibniz (1646–1716). Leonhard Euler (1707–1783), the most notable mathematician of the 18th century, unified these innovations into a single corpus with a standardized terminology, and completed them with the discovery and the proof of numerous theorems. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved. Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, \"The number of papers and books included in the Mathematical Reviews (MR) database since 1940 (the first year of operation of MR) is now more than 1.9",
    "label": 0
  },
  {
    "text": "in the January 2006 issue of the Bulletin of the American Mathematical Society, \"The number of papers and books included in the Mathematical Reviews (MR) database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.\" Symbolic notation and terminology Mathematical notation is widely used in science and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way. This notation consists of symbols used for representing operations, unspecified numbers, relations and any other mathematical objects, and then assembling them into expressions and formulas. More precisely, numbers and other mathematical objects are represented by symbols called variables, which are generally Latin or Greek letters, and often include subscripts. Operation and relations are generally represented by specific symbols or glyphs, such as + (plus), × (multiplication), ∫ {\\textstyle \\int } (integral), = (equal), and < (less than). All these symbols are generally grouped according to specific rules to form expressions and formulas. Normally, expressions and formulas do not appear alone, but are included in sentences of the current language, where expressions play the role of noun phrases and formulas play the role of clauses. Mathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning,",
    "label": 0
  },
  {
    "text": "postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary. Numerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism. Other technical terms are words of the common language that are used in an accurate meaning that may differ slightly from their common meaning. For example, in mathematics, \"or\" means \"one, the other or both\", while, in common language, it is either ambiguous or means \"one or the other but not both\" (in mathematics, the latter is called \"exclusive or\"). Finally, many mathematical terms are common words that are used with a completely different meaning. This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, \"every free module is flat\" and \"a field is always a ring\". Relationship with sciences Mathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws. The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model. Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used. For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better",
    "label": 0
  },
  {
    "text": "by invalid mathematical concepts, imply the need to change the mathematical model used. For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model. There is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation. In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied \"durch planmässiges Tattonieren\" (through systematic experimentation). However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence. Pure and applied mathematics Until the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics. For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture, and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians. However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application",
    "label": 0
  },
  {
    "text": "scientists, and many scientists were also mathematicians. However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks. In the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics. This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred. The aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere. Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the \"pure theory\". An example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis. An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high. For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry. In the present day, the distinction between pure and",
    "label": 0
  },
  {
    "text": "an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry. In the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas. The Mathematics Subject Classification has a section for \"general applied mathematics\" but does not mention \"pure mathematics\". However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge. Unreasonable effectiveness The unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner. It is the fact that many mathematical theories (even the \"purest\") have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced. Examples of unexpected applications of mathematical theories can be found in many areas of mathematics. A notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem. A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It was almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses. In the 19th century, the internal development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th",
    "label": 0
  },
  {
    "text": "development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of special relativity is a non-Euclidean space of dimension four, and spacetime of general relativity is a (curved) manifold of dimension four. A striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon Ω − . {\\displaystyle \\Omega ^{-}.} In both cases, the equations of the theories had unexplained solutions, which led to conjecture of the existence of an unknown particle, and the search for these particles. In both cases, these particles were discovered a few years later by specific experiments. Specific sciences Physics Mathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly, and is also considered to be the motivation of major mathematical developments. Computing Computing is closely related to mathematics in several ways. Theoretical computer science is considered to be mathematical in nature. Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, and graph theory. In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer. Biology and chemistry Biology uses probability extensively in fields such as ecology or neurobiology. Most discussion of probability centers on the concept of evolutionary fitness. Ecology heavily uses modeling to simulate population dynamics, study ecosystems",
    "label": 0
  },
  {
    "text": "be partially proven by computer. Biology and chemistry Biology uses probability extensively in fields such as ecology or neurobiology. Most discussion of probability centers on the concept of evolutionary fitness. Ecology heavily uses modeling to simulate population dynamics, study ecosystems such as the predator-prey model, measure pollution diffusion, or to assess climate change. The dynamics of a population can be modeled by coupled differential equations, such as the Lotka–Volterra equations. Statistical hypothesis testing, is run on data from clinical trials to determine whether a new treatment works. Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions. Earth sciences Structural geology and climatology use probabilistic models to predict the risk of natural catastrophes. Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models. Social sciences Areas of mathematics used in the social sciences include probability/statistics and differential equations. These are used in linguistics, economics, sociology, and psychology. Often the fundamental postulate of mathematical economics is that of the rational individual actor – Homo economicus (lit. 'economic man'). In this model, the individual seeks to maximize their self-interest, and always makes optimal choices using perfect information. This atomistic view of economics allows it to relatively easily mathematize its thinking, because individual calculations are transposed into mathematical calculations. Such mathematical modeling allows one to probe economic mechanisms. Some reject or criticise the concept of Homo economicus. Economists note that real people have limited information, make poor choices, and care about fairness and altruism, not just personal gain. Without mathematical modeling, it is hard to go beyond statistical observations or untestable speculation. Mathematical modeling allows economists to create structured frameworks to test hypotheses and analyze complex interactions. Models provide clarity and precision, enabling the translation of theoretical concepts into quantifiable predictions",
    "label": 0
  },
  {
    "text": "it is hard to go beyond statistical observations or untestable speculation. Mathematical modeling allows economists to create structured frameworks to test hypotheses and analyze complex interactions. Models provide clarity and precision, enabling the translation of theoretical concepts into quantifiable predictions that can be tested against real-world data. At the start of the 20th century, there was a development to express historical movements in formulas. In 1922, Nikolai Kondratiev discerned the ~50-year-long Kondratiev cycle, which explains phases of economic growth or crisis. Towards the end of the 19th century, mathematicians extended their analysis into geopolitics. Peter Turchin developed cliodynamics in the 1990s. Mathematization of the social sciences is not without risk. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences. The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models, can be subject to controversy. Philosophy Reality The connection between mathematics and material reality has led to philosophical debates since at least the time of Pythagoras. The ancient philosopher Plato argued that abstractions that reflect material reality have themselves a reality that exists outside space and time. As a result, the philosophical view that mathematical objects somehow exist on their own in abstraction is often referred to as Platonism. Independently of their possible philosophical opinions, modern mathematicians may be generally considered as Platonists, since they think of and talk of their objects of study as real objects. Armand Borel summarized this view of mathematics reality as follows, and provided quotations of G. H. Hardy, Charles Hermite, Henri Poincaré and Albert Einstein that support his views. Something becomes objective (as opposed to",
    "label": 0
  },
  {
    "text": "objects of study as real objects. Armand Borel summarized this view of mathematics reality as follows, and provided quotations of G. H. Hardy, Charles Hermite, Henri Poincaré and Albert Einstein that support his views. Something becomes objective (as opposed to \"subjective\") as soon as we are convinced that it exists in the minds of others in the same form as it does in ours and that we can think about it and discuss it together. Because the language of mathematics is so precise, it is ideally suited to defining concepts for which such a consensus exists. In my opinion, that is sufficient to provide us with a feeling of an objective existence, of a reality of mathematics ... Nevertheless, Platonism and the concurrent views on abstraction do not explain the unreasonable effectiveness of mathematics (as Platonism assumes mathematics exists independently, but does not explain why it matches reality). Proposed definitions There is no general consensus about the definition of mathematics or its epistemological status—that is, its place inside knowledge. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, \"mathematics is what mathematicians do\". A common approach is to define mathematics by its object of study. Aristotle defined mathematics as \"the science of quantity\" and this definition prevailed until the 18th century. However, Aristotle also noted a focus on quantity alone may not distinguish mathematics from sciences like physics; in his view, abstraction and studying quantity as a property \"separable in thought\" from real instances set mathematics apart. In the 19th century, when mathematicians began to address topics—such as infinite sets—which have no clear-cut relation to physical reality, a variety of new definitions were given. With",
    "label": 0
  },
  {
    "text": "as a property \"separable in thought\" from real instances set mathematics apart. In the 19th century, when mathematicians began to address topics—such as infinite sets—which have no clear-cut relation to physical reality, a variety of new definitions were given. With the large number of new areas of mathematics that have appeared since the beginning of the 20th century, defining mathematics by its object of study has become increasingly difficult. For example, in lieu of a definition, Saunders Mac Lane in Mathematics, form and function summarizes the basics of several areas of mathematics, emphasizing their inter-connectedness, and observes: the development of Mathematics provides a tightly connected network of formal rules, concepts, and systems. Nodes of this network are closely bound to procedures useful in human activities and to questions arising in science. The transition from activities to the formal Mathematical systems is guided by a variety of general insights and ideas. Another approach for defining mathematics is to use its methods. For example, an area of study is often qualified as mathematics as soon as one can prove theorems—assertions whose validity relies on a proof, that is, a purely logical deduction. Rigor Mathematical reasoning requires rigor. This means that the definitions must be absolutely unambiguous and the proofs must be reducible to a succession of applications of inference rules, without any use of empirical evidence and intuition. Rigorous reasoning is not specific to mathematics, but, in mathematics, the standard of rigor is much higher than elsewhere. Despite mathematics' concision, rigorous proofs can require hundreds of pages to express, such as the 255-page Feit–Thompson theorem. The emergence of computer-assisted proofs has allowed proof lengths to further expand. The result of this trend is a philosophy of the quasi-empiricist proof that can not be considered infallible, but has a probability attached to it.",
    "label": 0
  },
  {
    "text": "255-page Feit–Thompson theorem. The emergence of computer-assisted proofs has allowed proof lengths to further expand. The result of this trend is a philosophy of the quasi-empiricist proof that can not be considered infallible, but has a probability attached to it. The concept of rigor in mathematics dates back to ancient Greece, where their society encouraged logical, deductive reasoning. However, this rigorous approach would tend to discourage exploration of new approaches, such as irrational numbers and concepts of infinity. The method of demonstrating rigorous proof was enhanced in the sixteenth century through the use of symbolic notation. In the 18th century, social transition led to mathematicians earning their keep through teaching, which led to more careful thinking about the underlying concepts of mathematics. This produced more rigorous approaches, while transitioning from geometric methods to algebraic and then arithmetic proofs. At the end of the 19th century, it appeared that the definitions of the basic concepts of mathematics were not accurate enough for avoiding paradoxes (non-Euclidean geometries and Weierstrass function) and contradictions (Russell's paradox). This was solved by the inclusion of axioms with the apodictic inference rules of mathematical theories; the re-introduction of axiomatic method pioneered by the ancient Greeks. It results that \"rigor\" is no more a relevant concept in mathematics, as a proof is either correct or erroneous, and a \"rigorous proof\" is simply a pleonasm. Where a special concept of rigor comes into play is in the socialized aspects of a proof, wherein it may be demonstrably refuted by other mathematicians. After a proof has been accepted for many years or even decades, it can then be considered as reliable. Nevertheless, the concept of \"rigor\" may remain useful for teaching to beginners what is a mathematical proof. Training and practice Education Mathematics has a remarkable ability to cross cultural",
    "label": 0
  },
  {
    "text": "years or even decades, it can then be considered as reliable. Nevertheless, the concept of \"rigor\" may remain useful for teaching to beginners what is a mathematical proof. Training and practice Education Mathematics has a remarkable ability to cross cultural boundaries and time periods. As a human activity, the practice of mathematics has a social side, which includes education, careers, recognition, popularization, and so on. In education, mathematics is a core part of the curriculum and forms an important element of the STEM academic disciplines. Prominent careers for professional mathematicians include mathematics teacher or professor, statistician, actuary, financial analyst, economist, accountant, commodity trader, or computer consultant. Archaeological evidence shows that instruction in mathematics occurred as early as the second millennium BCE in ancient Babylonia. Comparable evidence has been unearthed for scribal mathematics training in the ancient Near East and then for the Greco-Roman world starting around 300 BCE. The oldest known mathematics textbook is the Rhind papyrus, dated from c. 1650 BCE in Egypt. Due to a scarcity of books, mathematical teachings in ancient India were communicated using memorized oral tradition since the Vedic period (c. 1500 – c. 500 BCE). In Imperial China during the Tang dynasty (618–907 CE), a mathematics curriculum was adopted for the civil service exam to join the state bureaucracy. Following the Dark Ages, mathematics education in Europe was provided by religious schools as part of the Quadrivium. Formal instruction in pedagogy began with Jesuit schools in the 16th and 17th century. Most mathematical curricula remained at a basic and practical level until the nineteenth century, when it began to flourish in France and Germany. The oldest journal addressing instruction in mathematics was L'Enseignement Mathématique, which began publication in 1899. The Western advancements in science and technology led to the establishment of centralized education systems",
    "label": 0
  },
  {
    "text": "century, when it began to flourish in France and Germany. The oldest journal addressing instruction in mathematics was L'Enseignement Mathématique, which began publication in 1899. The Western advancements in science and technology led to the establishment of centralized education systems in many nation-states, with mathematics as a core component—initially for its military applications. While the content of courses varies, in the present day nearly all countries teach mathematics to students for significant amounts of time. During school, mathematical capabilities and positive expectations have a strong association with career interest in the field. Extrinsic factors such as feedback motivation by teachers, parents, and peer groups can influence the level of interest in mathematics. Some students studying mathematics may develop an apprehension or fear about their performance in the subject. This is known as mathematical anxiety, and is considered the most prominent of the disorders impacting academic performance. Mathematical anxiety can develop due to various factors such as parental and teacher attitudes, social stereotypes, and personal traits. Help to counteract the anxiety can come from changes in instructional approaches, by interactions with parents and teachers, and by tailored treatments for the individual. Psychology (aesthetic, creativity and intuition) The validity of a mathematical theorem relies only on the rigor of its proof, which could theoretically be done automatically by a computer program. This does not mean that there is no place for creativity in a mathematical work. On the contrary, many important mathematical results (theorems) are solutions of problems that other mathematicians failed to solve, and the invention of a way for solving them may be a fundamental way of the solving process. An extreme example is Apery's theorem: Roger Apery provided only the ideas for a proof, and the formal proof was given only several months later by three other mathematicians. Creativity",
    "label": 0
  },
  {
    "text": "them may be a fundamental way of the solving process. An extreme example is Apery's theorem: Roger Apery provided only the ideas for a proof, and the formal proof was given only several months later by three other mathematicians. Creativity and rigor are not the only psychological aspects of the activity of mathematicians. Some mathematicians can see their activity as a game, more specifically as solving puzzles. This aspect of mathematical activity is emphasized in recreational mathematics. Mathematicians can find an aesthetic value to mathematics. Like beauty, it is hard to define, it is commonly related to elegance, which involves qualities like simplicity, symmetry, completeness, and generality. G. H. Hardy in A Mathematician's Apology expressed the belief that the aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He also identified other criteria such as significance, unexpectedness, and inevitability, which contribute to mathematical aesthetics. Paul Erdős expressed this sentiment more ironically by speaking of \"The Book\", a supposed divine collection of the most beautiful proofs. The 1998 book Proofs from THE BOOK, inspired by Erdős, is a collection of particularly succinct and revelatory mathematical arguments. Some examples of particularly elegant results included are Euclid's proof that there are infinitely many prime numbers and the fast Fourier transform for harmonic analysis. Some feel that to consider mathematics a science is to downplay its artistry and history in the seven traditional liberal arts. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematical results are created (as in art) or discovered (as in science). The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions. Cultural impact Artistic expression Notes that sound well together to a Western ear are sounds whose fundamental frequencies of vibration",
    "label": 0
  },
  {
    "text": "(as in science). The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions. Cultural impact Artistic expression Notes that sound well together to a Western ear are sounds whose fundamental frequencies of vibration are in simple ratios. For example, an octave doubles the frequency and a perfect fifth multiplies it by 3 2 {\\textstyle {\\frac {3}{2}}} . Humans, as well as some other animals, find symmetric patterns to be more beautiful. Mathematically, the symmetries of an object form a group known as the symmetry group. For example, the group underlying mirror symmetry is the cyclic group of two elements, Z / 2 Z {\\displaystyle \\mathbb {Z} /2\\mathbb {Z} } . A Rorschach test is a figure invariant by this symmetry, as are butterfly and animal bodies more generally (at least on the surface). Waves on the sea surface possess translation symmetry: moving one's viewpoint by the distance between wave crests does not change one's view of the sea. Fractals possess self-similarity. Popularization Popular mathematics is the act of presenting mathematics without technical terms. Presenting mathematics may be hard since the general public suffers from mathematical anxiety and mathematical objects are highly abstract. However, popular mathematics writing can overcome this by using applications or cultural links. Despite this, mathematics is rarely the topic of popularization in printed or televised media. Awards and prize problems The most prestigious award in mathematics is the Fields Medal, established by Canadian John Charles Fields in 1936 and awarded every four years (except around World War II) to up to four individuals. It is considered the mathematical equivalent of the Nobel Prize. Other prestigious mathematics awards include: The Abel Prize, instituted in 2002 and first awarded in 2003 The Chern Medal for lifetime achievement, introduced in 2009 and first",
    "label": 0
  },
  {
    "text": "Over the millennia, many ways to construct magic squares have been discovered. These methods can be classified as general methods and special methods, in the sense that general methods allow us to construct more than a single magic square of a given order, whereas special methods allow us to construct just one magic square of a given order. Special methods are specific algorithms whereas general methods may require some trial-and-error. Special methods are the most simple ways to construct magic squares. They follow certain algorithms which generate regular patterns of numbers in a square. The correctness of these special methods can be proved using one of the general methods given in later sections. After a magic square has been constructed using a special method, the transformations described in the previous section can be applied to yield further magic squares. Special methods are usually referred to using the name of the author(s) (if known) who described the method, for e.g. De la Loubere's method, Starchey's method, Bachet's method, etc. Magic squares are believed to exist for all orders, except for order 2. Magic squares can be classified according to their order as odd, doubly even (n divisible by four), and singly even (n even, but not divisible by four). This classification is based on the fact that entirely different techniques need to be employed to construct these different species of squares. Odd and doubly even magic squares are easy to generate; the construction of singly even magic squares is more difficult but several methods exist, including John Horton Conway's LUX method for magic squares and the Strachey method for magic squares. Special methods of construction A method for constructing a magic square of order 3 In the 19th century, Édouard Lucas devised the general formula for order 3 magic squares. Consider",
    "label": 0
  },
  {
    "text": "for magic squares and the Strachey method for magic squares. Special methods of construction A method for constructing a magic square of order 3 In the 19th century, Édouard Lucas devised the general formula for order 3 magic squares. Consider the following table made up of positive integers a, b and c: These nine numbers will be distinct positive integers forming a magic square with the magic constant 3c so long as 0 < a < b < c − a and b ≠ 2a. Moreover, every 3×3 magic square of distinct positive integers is of this form. In 1997 Lee Sallows discovered that leaving aside rotations and reflections, then every distinct parallelogram drawn on the Argand diagram defines a unique 3×3 magic square, and vice versa, a result that had never previously been noted. A method for constructing a magic square of odd order A method for constructing magic squares of odd order was published by the French diplomat de la Loubère in his book, A new historical relation of the kingdom of Siam (Du Royaume de Siam, 1693), in the chapter entitled The problem of the magical square according to the Indians. The method operates as follows: The method prescribes starting in the central column of the first row with the number 1. After that, the fundamental movement for filling the squares is diagonally up and right, one step at a time. If a square is filled with a multiple of the order n, one moves vertically down one square instead, then continues as before. When an \"up and to the right\" move would leave the square, it is wrapped around to the last row or first column, respectively. Starting from other squares rather than the central column of the first row is possible, but then only the",
    "label": 0
  },
  {
    "text": "and to the right\" move would leave the square, it is wrapped around to the last row or first column, respectively. Starting from other squares rather than the central column of the first row is possible, but then only the row and column sums will be identical and result in a magic sum, whereas the diagonal sums will differ. The result will thus be a semimagic square and not a true magic square. Moving in directions other than north east can also result in magic squares. A method of constructing a magic square of doubly even order Doubly even means that n is an even multiple of an even integer; or 4p (e.g. 4, 8, 12), where p is an integer. Generic pattern All the numbers are written in order from left to right across each row in turn, starting from the top left hand corner. Numbers are then either retained in the same place or interchanged with their diametrically opposite numbers in a certain regular pattern. In the magic square of order four, the numbers in the four central squares and one square at each corner are retained in the same place and the others are interchanged with their diametrically opposite numbers. A construction of a magic square of order 4 Starting from top left, go left to right through each row of the square, counting each cell from 1 to 16 and filling the cells along the diagonals with its corresponding number. Once the bottom right cell is reached, continue by going right to left, starting from the bottom right of the table through each row, and fill in the non-diagonal cells counting up from 1 to 16 with its corresponding number. As shown below: An extension of the above example for Orders 8 and 12 First generate",
    "label": 0
  },
  {
    "text": "bottom right of the table through each row, and fill in the non-diagonal cells counting up from 1 to 16 with its corresponding number. As shown below: An extension of the above example for Orders 8 and 12 First generate a pattern table, where a '1' indicates selecting from the square where the numbers are written in order 1 to n2 (left-to-right, top-to-bottom), and a '0' indicates selecting from the square where the numbers are written in reverse order n2 to 1. For M = 4, the pattern table is as shown below (third matrix from left). With the unaltered cells (cells with '1') shaded, a criss-cross pattern is obtained. The patterns are a) there are equal number of '1's and '0's in each row and column; b) each row and each column are \"palindromic\"; c) the left- and right-halves are mirror images; and d) the top- and bottom-halves are mirror images (c and d imply b). The pattern table can be denoted using hexadecimals as (9, 6, 6, 9) for simplicity (1-nibble per row, 4 rows). The simplest method of generating the required pattern for higher ordered doubly even squares is to copy the generic pattern for the fourth-order square in each four-by-four sub-squares. For M = 8, possible choices for the pattern are (99, 66, 66, 99, 99, 66, 66, 99); (3C, 3C, C3, C3, C3, C3, 3C, 3C); (A5, 5A, A5, 5A, 5A, A5, 5A, A5) (2-nibbles per row, 8 rows). For M = 12, the pattern table (E07, E07, E07, 1F8, 1F8, 1F8, 1F8, 1F8, 1F8, E07, E07, E07) yields a magic square (3-nibbles per row, 12 rows.) It is possible to count the number of choices one has based on the pattern table, taking rotational symmetries into account. Method of superposition The earliest discovery of",
    "label": 0
  },
  {
    "text": "E07, E07, E07) yields a magic square (3-nibbles per row, 12 rows.) It is possible to count the number of choices one has based on the pattern table, taking rotational symmetries into account. Method of superposition The earliest discovery of the superposition method was made by the Indian mathematician Narayana in the 14th century. The same method was later re-discovered and studied in early 18th century Europe by de la Loubere, Poignard, de La Hire, and Sauveur; and the method is usually referred to as de la Hire's method. Although Euler's work on magic square was unoriginal, he famously conjectured the impossibility of constructing the evenly odd ordered mutually orthogonal Graeco-Latin squares. This conjecture was disproved in the mid 20th century. For clarity of exposition, two important variations of this method can be distinguished. Euler's method This method consists in constructing two preliminary squares, which when added together gives the magic square. As a running example, a 3×3 magic square is considered. Each number of the 3×3 natural square by a pair of numbers can be labeled as where every pair of Greek and Latin alphabets, e.g. αa, are meant to be added together, i.e. {{{1}}}. Here, {{{1}}}. The numbers 0, 3, and 6 are referred to as the root numbers while the numbers 1, 2, and 3 are referred to as the primary numbers. An important general constraint here is a Greek letter is paired with a Latin letter only once. Thus, the original square can now be split into two simpler squares: The lettered squares are referred to as Greek square or Latin square if they are filled with Greek or Latin letters, respectively. A magic square can be constructed by ensuring that the Greek and Latin squares are magic squares too. The converse of this statement is",
    "label": 0
  },
  {
    "text": "as Greek square or Latin square if they are filled with Greek or Latin letters, respectively. A magic square can be constructed by ensuring that the Greek and Latin squares are magic squares too. The converse of this statement is also often, but not always (e.g. bordered magic squares), true: A magic square can be decomposed into a Greek and a Latin square, which are themselves magic squares. Thus the method is useful for both synthesis as well as analysis of a magic square. Lastly, by examining the pattern in which the numbers are laid out in the finished square, it is often possible to come up with a faster algorithm to construct higher order squares that replicate the given pattern, without the necessity of creating the preliminary Greek and Latin squares. During the construction of the 3×3 magic square, the Greek and Latin squares with just three unique terms are much easier to deal with than the original square with nine different terms. The row sum and the column sum of the Greek square will be the same, α + β + γ, if each letter appears exactly once in a given column or a row. This can be achieved by cyclic permutation of α, β, and γ. Satisfaction of these two conditions ensures that the resulting square is a semi-magic square; and such Greek and Latin squares are said to be mutually orthogonal to each other. For a given order n, there are at most n − 1 squares in a set of mutually orthogonal squares, not counting the variations due to permutation of the symbols. This upper bound is exact when n is a prime number. In order to construct a magic square, we should also ensure that the diagonals sum to magic constant. For this, we",
    "label": 0
  },
  {
    "text": "the variations due to permutation of the symbols. This upper bound is exact when n is a prime number. In order to construct a magic square, we should also ensure that the diagonals sum to magic constant. For this, we have a third condition: either all the letters should appear exactly once in both the diagonals; or in case of odd ordered squares, one of the diagonals should consist entirely of the middle term, while the other diagonal should have all the letters exactly once. The mutually orthogonal Greek and Latin squares that satisfy the first part of the third condition (that all letters appear in both the diagonals) are said to be mutually orthogonal doubly diagonal Graeco-Latin squares. Odd squares: For the 3×3 odd square, since α, β, and γ are in arithmetic progression, their sum is equal to the product of the square's order and the middle term, i.e. α + β + γ = 3 β. Thus, the diagonal sums will be equal if we have βs in the main diagonal and α, β, γ in the skew diagonal. Similarly, for the Latin square. The resulting Greek and Latin squares and their combination will be as below. The Latin square is just a 90 degree anti-clockwise rotation of the Greek square (or equivalently, flipping about the vertical axis) with the corresponding letters interchanged. Substituting the values of the Greek and Latin letters will give the 3×3 magic square. For the odd squares, this method explains why the Siamese method (method of De la Loubere) and its variants work. This basic method can be used to construct odd ordered magic squares of higher orders. To summarise: For odd ordered squares, to construct Greek square, place the middle term along the main diagonal, and place the rest of the",
    "label": 0
  },
  {
    "text": "work. This basic method can be used to construct odd ordered magic squares of higher orders. To summarise: For odd ordered squares, to construct Greek square, place the middle term along the main diagonal, and place the rest of the terms along the skew diagonal. The remaining empty cells are filled by diagonal moves. The Latin square can be constructed by rotating or flipping the Greek square, and replacing the corresponding alphabets. The magic square is obtained by adding the Greek and Latin squares. A peculiarity of the construction method given above for the odd magic squares is that the middle number (n2 + 1)/2 will always appear at the center cell of the magic square. Since there are (n − 1)! ways to arrange the skew diagonal terms, we can obtain (n − 1)! Greek squares this way; same with the Latin squares. Also, since each Greek square can be paired with (n − 1)! Latin squares, and since for each of Greek square the middle term may be arbitrarily placed in the main diagonal or the skew diagonal (and correspondingly along the skew diagonal or the main diagonal for the Latin squares), we can construct a total of 2 × (n − 1)! × (n − 1)! magic squares using this method. For n = 3, 5, and 7, this will give 8, 1152, and 1,036,800 different magic squares, respectively. Dividing by 8 to neglect equivalent squares due to rotation and reflections, we obtain 1, 144, and 129,600 essentially different magic squares, respectively. As another example, the construction of 5×5 magic square is given. Numbers are directly written in place of alphabets. The numbered squares are referred to as primary square or root square if they are filled with primary numbers or root numbers, respectively. The numbers are",
    "label": 0
  },
  {
    "text": "of 5×5 magic square is given. Numbers are directly written in place of alphabets. The numbered squares are referred to as primary square or root square if they are filled with primary numbers or root numbers, respectively. The numbers are placed about the skew diagonal in the root square such that the middle column of the resulting root square has 0, 5, 10, 15, 20 (from bottom to top). The primary square is obtained by rotating the root square counter-clockwise by 90 degrees, and replacing the numbers. The resulting square is an associative magic square, in which every pair of numbers symmetrically opposite to the center sum up to the same value, 26. For e.g., 16+10, 3+23, 6+20, etc. In the finished square, 1 is placed at center cell of bottom row, and successive numbers are placed via elongated knight's move (two cells right, two cells down), or equivalently, bishop's move (two cells diagonally down right). When a collision occurs, the break move is to move one cell up. All the odd numbers occur inside the central diamond formed by 1, 5, 25 and 21, while the even numbers are placed at the corners. The occurrence of the even numbers can be deduced by copying the square to the adjacent sides. The even numbers from four adjacent squares will form a cross. A variation of the above example, where the skew diagonal sequence is taken in different order, is given below. The resulting magic square is the flipped version of the famous Agrippa's Mars magic square. It is an associative magic square and is the same as that produced by Moschopoulos's method. Here the resulting square starts with 1 placed in the cell which is to the right of the centre cell, and proceeds as De la Loubere's method, with",
    "label": 0
  },
  {
    "text": "magic square and is the same as that produced by Moschopoulos's method. Here the resulting square starts with 1 placed in the cell which is to the right of the centre cell, and proceeds as De la Loubere's method, with downwards-right move. When a collision occurs, the break move is to shift two cells to the right. In the previous examples, for the Greek square, the second row can be obtained from the first row by circularly shifting it to the right by one cell. Similarly, the third row is a circularly shifted version of the second row by one cell to the right; and so on. Likewise, the rows of the Latin square is circularly shifted to the left by one cell. The row shifts for the Greek and Latin squares are in mutually opposite direction. It is possible to circularly shift the rows by more than one cell to create the Greek and Latin square. For odd ordered squares, whose order is not divisible by three, we can create the Greek squares by shifting a row by two places to the left or to the right to form the next row. The Latin square is made by flipping the Greek square along the main diagonal and interchanging the corresponding letters. This gives us a Latin square whose rows are created by shifting the row in the direction opposite to that of the Greek square. A Greek square and a Latin square should be paired such that their row shifts are in mutually opposite direction. The magic square is obtained by adding the Greek and Latin squares. When the order also happens to be a prime number, this method always creates pandiagonal magic square. This essentially re-creates the knight's move. All the letters will appear in both the diagonals,",
    "label": 0
  },
  {
    "text": "obtained by adding the Greek and Latin squares. When the order also happens to be a prime number, this method always creates pandiagonal magic square. This essentially re-creates the knight's move. All the letters will appear in both the diagonals, ensuring correct diagonal sum. Since there are n! permutations of the Greek letters by which we can create the first row of the Greek square, there are thus n! Greek squares that can be created by shifting the first row in one direction. Likewise, there are n! such Latin squares created by shifting the first row in the opposite direction. Since a Greek square can be combined with any Latin square with opposite row shifts, there are n! × n! such combinations. Lastly, since the Greek square can be created by shifting the rows either to the left or to the right, there are a total of 2 × n! × n! magic squares that can be formed by this method. For n = 5 and 7, since they are prime numbers, this method creates 28,800 and 50,803,200 pandiagonal magic squares. Dividing by 8 to neglect equivalent squares due to rotation and reflections, we obtain 3,600 and 6,350,400 equivalent squares. Further dividing by n2 to neglect equivalent panmagic squares due to cyclic shifting of rows or columns, we obtain 144 and 129,600 essentially different panmagic squares. For order 5 squares, these are the only panmagic square there are. The condition that the square's order not be divisible by 3 means that we cannot construct squares of orders 9, 15, 21, 27, and so on, by this method. In the example below, the square has been constructed such that 1 is at the center cell. In the finished square, the numbers can be continuously enumerated by the knight's move (two cells",
    "label": 0
  },
  {
    "text": "27, and so on, by this method. In the example below, the square has been constructed such that 1 is at the center cell. In the finished square, the numbers can be continuously enumerated by the knight's move (two cells up, one cell right). When collision occurs, the break move is to move one cell up, one cell left. The resulting square is a pandiagonal magic square. This square also has a further diabolical property that any five cells in quincunx pattern formed by any odd sub-square, including wrap around, sum to the magic constant, 65. For e.g., 13+7+1+20+24, 23+1+9+15+17, 13+21+10+19+2 etc. Also the four corners of any 5×5 square and the central cell, as well as the middle cells of each side together with the central cell, including wrap around, give the magic sum: 13+10+19+22+1 and 20+24+12+8+1. Lastly the four rhomboids that form elongated crosses also give the magic sum: 23+1+9+24+8, 15+1+17+20+12, 14+1+18+13+19, 7+1+25+22+10. Such squares with 1 at the center cell are also called God's magic squares in Islamic amulet design, where the center cell is either left blank or filled with God's name. We can also combine the Greek and Latin squares constructed by different methods. In the example below, the primary square is made using knight's move. We have re-created the magic square obtained by De la Loubere's method. As before, we can form 8 × (n − 1)! × n! magic squares by this combination. For n = 5 and 7, this will create 23,040 and 29,030,400 magic squares. After dividing by 8 in order to neglect equivalent squares due to rotation and reflection, we get 2,880 and 3,628,800 squares. For order 5 squares, these three methods give a complete census of the number of magic squares that can be constructed by the method of",
    "label": 0
  },
  {
    "text": "to neglect equivalent squares due to rotation and reflection, we get 2,880 and 3,628,800 squares. For order 5 squares, these three methods give a complete census of the number of magic squares that can be constructed by the method of superposition. Neglecting the rotation and reflections, the total number of magic squares of order 5 produced by the superposition method is 144 + 3,600 + 2,880 = 6,624. Even squares: We can also construct even ordered squares in this fashion. Since there is no middle term among the Greek and Latin alphabets for even ordered squares, in addition to the first two constraint, for the diagonal sums to yield the magic constant, all the letters in the alphabet should appear in the main diagonal and in the skew diagonal. An example of a 4×4 square is given below. For the given diagonal and skew diagonal in the Greek square, the rest of the cells can be filled using the condition that each letter appear only once in a row and a column. Using these two Graeco-Latin squares, we can construct 2 × 4! × 4! = 1,152 magic squares. Dividing by 8 to eliminate equivalent squares due to rotation and reflections, we get 144 essentially different magic squares of order 4. These are the only magic squares constructible by the Euler method, since there are only two mutually orthogonal doubly diagonal Graeco-Latin squares of order 4. Similarly, an 8×8 magic square can be constructed as below. Here the order of appearance of the numbers is not important; however the quadrants imitate the layout pattern of the 4×4 Graeco-Latin squares. Euler's method has given rise to the study of Graeco-Latin squares. Euler's method for constructing magic squares is valid for any order except 2 and 6. Variations: Magic squares constructed from",
    "label": 0
  },
  {
    "text": "imitate the layout pattern of the 4×4 Graeco-Latin squares. Euler's method has given rise to the study of Graeco-Latin squares. Euler's method for constructing magic squares is valid for any order except 2 and 6. Variations: Magic squares constructed from mutually orthogonal doubly diagonal Graeco-Latin squares are interesting in themselves since the magic property emerges from the relative position of the alphabets in the square, and not due to any arithmetic property of the value assigned to them. This means that we can assign any value to the alphabets of such squares and still obtain a magic square. This is the basis for constructing squares that display some information (e.g. birthdays, years, etc.) in the square and for creating \"reversible squares\". For example, we can display the number π ≈ 3.141592 at the bottom row of a 4×4 magic square using the Graeco-Latin square given above by assigning (α, β, γ, δ) = (10, 0, 90, 15) and (a, b, c, d) = (0, 2, 3, 4). We will obtain the following non-normal magic square with the magic sum 124: Narayana-De la Hire's method for even orders Narayana-De la Hire's method for odd square is the same as that of Euler's. However, for even squares, we drop the second requirement that each Greek and Latin letter appear only once in a given row or column. This allows us to take advantage of the fact that the sum of an arithmetic progression with an even number of terms is equal to the sum of two opposite symmetric terms multiplied by half the total number of terms. Thus, when constructing the Greek or Latin squares, for even ordered squares, a letter can appear n/2 times in a column but only once in a row, or vice versa. As a running example, if",
    "label": 0
  },
  {
    "text": "the total number of terms. Thus, when constructing the Greek or Latin squares, for even ordered squares, a letter can appear n/2 times in a column but only once in a row, or vice versa. As a running example, if we take a 4×4 square, where the Greek and Latin terms have the values (α, β, γ, δ) = (0, 4, 8, 12) and (a, b, c, d) = (1, 2, 3, 4), respectively, then we have α + β + γ + δ = 2 (α + δ) = 2 (β + γ). Similarly, a + b + c + d = 2 (a + d) = 2 (b + c). This means that the complementary pair α and δ (or β and γ) can appear twice in a column (or a row) and still give the desired magic sum. Thus, we can construct: For even ordered squares, the Greek magic square is made by first placing the Greek alphabets along the main diagonal in some order. The skew diagonal is then filled in the same order or by picking the terms that are complementary to the terms in the main diagonal. Finally, the remaining cells are filled column wise. Given a column, we use the complementary terms in the diagonal cells intersected by that column, making sure that they appear only once in a given row but n/2 times in the given column. The Latin square is obtained by flipping or rotating the Greek square and interchanging the corresponding alphabets. The final magic square is obtained by adding the Greek and Latin squares. In the example given below, the main diagonal (from top left to bottom right) is filled with sequence ordered as α, β, γ, δ, while the skew diagonal (from bottom left to top right) filled",
    "label": 0
  },
  {
    "text": "the Greek and Latin squares. In the example given below, the main diagonal (from top left to bottom right) is filled with sequence ordered as α, β, γ, δ, while the skew diagonal (from bottom left to top right) filled in the same order. The remaining cells are then filled column wise such that the complementary letters appears only once within a row, but twice within a column. In the first column, since α appears on the 1st and 4th row, the remaining cells are filled with its complementary term δ. Similarly, the empty cells in the 2nd column are filled with γ; in 3rd column β; and 4th column α. Each Greek letter appears only once along the rows, but twice along the columns. As such, the row sums are α + β + γ + δ while the column sums are either 2 (α + δ) or 2 (β + γ). Likewise for the Latin square, which is obtained by flipping the Greek square along the main diagonal and interchanging the corresponding letters. The above example explains why the \"criss-cross\" method for doubly even magic square works. Another possible 4×4 magic square, which is also pan-diagonal as well as most-perfect, is constructed below using the same rule. However, the diagonal sequence is chosen such that all four letters α, β, γ, δ appear inside the central 2×2 sub-square. Remaining cells are filled column wise such that each letter appears only once within a row. In the 1st column, the empty cells need to be filled with one of the letters selected from the complementary pair α and δ. Given the 1st column, the entry in the 2nd row can only be δ since α is already there in the 2nd row; while, in the 3rd row the entry",
    "label": 0
  },
  {
    "text": "the letters selected from the complementary pair α and δ. Given the 1st column, the entry in the 2nd row can only be δ since α is already there in the 2nd row; while, in the 3rd row the entry can only be α since δ is already present in the 3rd row. We proceed similarly until all cells are filled. The Latin square given below has been obtained by flipping the Greek square along the main diagonal and replacing the Greek alphabets with corresponding Latin alphabets. We can use this approach to construct singly even magic squares as well. However, we have to be more careful in this case since the criteria of pairing the Greek and Latin alphabets uniquely is not automatically satisfied. Violation of this condition leads to some missing numbers in the final square, while duplicating others. Thus, here is an important proviso: For singly even squares, in the Greek square, check the cells of the columns which is vertically paired to its complement. In such a case, the corresponding cell of the Latin square must contain the same letter as its horizontally paired cell. Below is a construction of a 6×6 magic square, where the numbers are directly given, rather than the alphabets. The second square is constructed by flipping the first square along the main diagonal. Here in the first column of the root square the 3rd cell is paired with its complement in the 4th cells. Thus, in the primary square, the numbers in the 1st and 6th cell of the 3rd row are same. Likewise, with other columns and rows. In this example the flipped version of the root square satisfies this proviso. As another example of a 6×6 magic square constructed this way is given below. Here the diagonal entries are",
    "label": 0
  },
  {
    "text": "are same. Likewise, with other columns and rows. In this example the flipped version of the root square satisfies this proviso. As another example of a 6×6 magic square constructed this way is given below. Here the diagonal entries are arranged differently. The primary square is constructed by flipping the root square about the main diagonal. In the second square the proviso for singly even square is not satisfied, leading to a non-normal magic square (third square) where the numbers 3, 13, 24, and 34 are duplicated while missing the numbers 4, 18, 19, and 33. The last condition is a bit arbitrary and may not always need to be invoked, as in this example, where in the root square each cell is vertically paired with its complement: As one more example, we have generated an 8×8 magic square. Unlike the criss-cross pattern of the earlier section for evenly even square, here we have a checkered pattern for the altered and unaltered cells. Also, in each quadrant the odd and even numbers appear in alternating columns. Variations: A number of variations of the basic idea are possible: a complementary pair can appear n/2 times or less in a column. That is, a column of a Greek square can be constructed using more than one complementary pair. This method allows us to imbue the magic square with far richer properties. The idea can also be extended to the diagonals too. An example of an 8×8 magic square is given below. In the finished square each of four quadrants are pan-magic squares as well, each quadrant with same magic constant 130. Method of borders Bordering method for order 3 In this method, the objective is to wrap a border around a smaller magic square which serves as a core. Consider the 3×3",
    "label": 0
  },
  {
    "text": "as well, each quadrant with same magic constant 130. Method of borders Bordering method for order 3 In this method, the objective is to wrap a border around a smaller magic square which serves as a core. Consider the 3×3 square for example. Subtracting the middle number 5 from each number 1, 2, ..., 9, we obtain 0, ±1, ±2, ±3, and ±4, which we will, for lack of better words, following S. Harry White, refer to as bone numbers. The magic constant of a magic square, which we will refer to as the skeleton square, made by these bone numbers will be zero since adding all the rows of a magic square will give nM = Σ k = 0; thus M = 0. It is not difficult to argue that the middle number should be placed at the center cell: let x be the number placed in the middle cell, then the sum of the middle column, middle row, and the two diagonals give Σ k + 3 x = 4 M. SinceΣ k = 3 M, we have x = M / 3. Here M = 0, so x = 0. Putting the middle number 0 in the center cell, we want to construct a border such that the resulting square is magic. Let the border be given by: Since the sum of each row, column, and diagonals must be a constant (which is zero), we have a + a∗ = 0, b + b∗ = 0, u + u∗ = 0, v + v∗ = 0. Now, if we have chosen a, b, u, and v, then we have a∗ = −a, b∗ = −b, u∗ = −u, and v∗ = −v. This means that if we assign a given number to a variable, say a",
    "label": 0
  },
  {
    "text": "Now, if we have chosen a, b, u, and v, then we have a∗ = −a, b∗ = −b, u∗ = −u, and v∗ = −v. This means that if we assign a given number to a variable, say a = 1, then its complement will be assigned to a∗, i.e. a∗ = −1. Thus out of eight unknown variables, it is sufficient to specify the value of only four variables. We will consider a, b, u, and v as independent variables, while a∗, b∗, u∗, and v∗ as dependent variables. This allows us to consider a bone number ±x as a single number regardless of sign because (1) its assignment to a given variable, say a, will automatically imply that the same number of opposite sign will be shared with its complement a∗, and (2) two independent variables, say a and b, cannot be assigned the same bone number. But how should we choose a, b, u, and v? We have the sum of the top row and the sum of the right column as u + a + v = 0, v + b + u∗ = 0. Since 0 is an even number, there are only two ways that the sum of three integers will yield an even number: 1) if all three were even, or 2) if two were odd and one was even. Since in our choice of numbers we only have two even non-zero number (±2 and ±4), the first statement is false. Hence, it must be the case that the second statement is true: that two of the numbers are odd and one even. The only way that both the above two equations can satisfy this parity condition simultaneously, and still be consistent with the set of numbers we have, is when u",
    "label": 0
  },
  {
    "text": "true: that two of the numbers are odd and one even. The only way that both the above two equations can satisfy this parity condition simultaneously, and still be consistent with the set of numbers we have, is when u and v are odd. For on the contrary, if we had assumed u and a to be odd and v to be even in the first equation, then u∗ = −u will be odd in the second equation, making b odd as well, in order to satisfy the parity condition. But this requires three odd numbers (u, a, and b), contradicting the fact that we only have two odd numbers (±1 and ±3) which we can use. This proves that the odd bone numbers occupy the corners cells. When converted to normal numbers by adding 5, this implies that the corners of a 3×3 magic square are all occupied by even numbers. Thus, taking u = 1 and v = 3, we have a = −4 and b = −2. Hence, the finished skeleton square will be as in the left. Adding 5 to each number, we get the finished magic square. Similar argument can be used to construct larger squares. Since there does not exist a 2×2 magic square around which we can wrap a border to construct a 4×4 magic square, the next smallest order for which we can construct bordered square is the order 5. Bordering method for order 5 Consider the fifth-order square. For this, we have a 3×3 magic core, around which we will wrap a magic border. The bone numbers to be used will be ±5, ±6, ±7, ±8, ±9, ±10, ±11, and ±12. Disregarding the signs, we have 8 bone numbers, 4 of which are even and 4 of which are odd. In",
    "label": 0
  },
  {
    "text": "a magic border. The bone numbers to be used will be ±5, ±6, ±7, ±8, ±9, ±10, ±11, and ±12. Disregarding the signs, we have 8 bone numbers, 4 of which are even and 4 of which are odd. In general, for a square of any order n, there will be 4(n − 1) border cells, which are to be filled using 2(n − 1) bone numbers. Let the magic border be given as As before, we should place a bone number and its complement opposite to each other, so that the magic sum will be zero. It is sufficient to determine the numbers u, v, a, b, c, d, e, f to describe the magic border. As before, we have the two constraint equations for the top row and right column: u + a + b + c + v = 0 v + d + e + f + u* = 0. Multiple solutions are possible. The standard procedure is to first try to determine the corner cells, after which we will try to determine the rest of the border. There are 28 ways of choosing two numbers from the set of 8 bone numbers for the corner cells u and v. However, not all pairs are admissible. Among the 28 pairs, 16 pairs are made of an even and an odd number, 6 pairs have both as even numbers, while 6 pairs have them both as odd numbers. We can prove that the corner cells u and v cannot have an even and an odd number. This is because if this were so, then the sums u + v and v + u∗ will be odd, and since 0 is an even number, the sums a + b + c and d + e + f should",
    "label": 0
  },
  {
    "text": "is because if this were so, then the sums u + v and v + u∗ will be odd, and since 0 is an even number, the sums a + b + c and d + e + f should be odd as well. The only way that the sum of three integers will result in an odd number is when 1) two of them are even and one is odd, or 2) when all three are odd. Since the corner cells are assumed to be odd and even, neither of these two statements are compatible with the fact that we only have 3 even and 3 odd bone numbers at our disposal. This proves that u and v cannot have different parity. This eliminates 16 possibilities. Using similar type reasoning we can also draw some conclusions about the sets {a, b, c} and {d, e, f}. If u and v are both even, then both the sets should have two odd numbers and one even number. If u and v are both odd, then one of the sets should have three even numbers while the other set should have one even number and two odd numbers. As a running example, consider the case when both u and v are even. The 6 possible pairs are: (6, 8), (6, 10), (6, 12), (8, 10), (8, 12), and (10, 12). Since the sums u + v and v + u∗ are even, the sums a + b + c and d + e + f should be even as well. The only way that the sum of three integers will result in an even number is when 1) two of them are odd and one is even, or 2) when all three are even. The fact that the two corner cells",
    "label": 0
  },
  {
    "text": "only way that the sum of three integers will result in an even number is when 1) two of them are odd and one is even, or 2) when all three are even. The fact that the two corner cells are even means that we have only 2 even numbers at our disposal. Thus, the second statement is not compatible with this fact. Hence, it must be the case that the first statement is true: two of the three numbers should be odd, while one be even. Now let a, b, d, e be odd numbers while c and f be even numbers. Given the odd bone numbers at our disposal: ±5, ±7, ±9, and ±11, their differences range from D = {±2, ±4, ±6} while their sums range from S = {±12, ±14, ±16, ±18, ±20}. It is also useful to have a table of their sum and differences for later reference. Now, given the corner cells (u, v), we can check its admissibility by checking if the sums u + v + c and v + u∗ + f fall within the set D or S. The admissibility of the corner numbers is a necessary but not a sufficient condition for the solution to exist. For example, if we consider the pair (u, v) = (8, 12), then u + v = 20 and v + u* = 6; and we will have ±6 and ±10 even bone numbers at our disposal. Taking c = ±6, we have the sum u + v + c to be 26 and 14, depending on the sign of ±6 taken, both of which do not fall within the sets D or S. Likewise, taking c = ±10, we have the sum u + v + c to be 30 and 10,",
    "label": 0
  },
  {
    "text": "and 14, depending on the sign of ±6 taken, both of which do not fall within the sets D or S. Likewise, taking c = ±10, we have the sum u + v + c to be 30 and 10, both of which again do not fall within the sets D or S. Thus, the pair (8, 12) is not admissible. By similar process of reasoning, we can also rule out the pair (6, 12). As another example, if we consider the pair (u, v) = (10, 12), then u + v = 22 and v + u∗ = 2; and we will have ± 6 and ± 8 even bone numbers at our disposal. Taking c = ±6, we have the sum u + v + c to be 28 and 16. While 28 does not fall within the sets D or S, 16 falls in set S. By inspection, we find that if (a, b) = (−7, − 9), then a + b = −16; and it will satisfy the first constraint equation. Also, taking f = ± 8, we have the sum v + u∗ + f to be 10 and -6. While 10 does not fall within the sets D or S, −6 falls in set D. Since −7 and −9 have already been assigned to a and b, clearly (d, e) = (-5, 11) so that d + e = 6; and it will satisfy the second constraint equation. Likewise, taking c = ±8, we have the sum u + v + c to be 30 and 14. While 30 does not fall within the sets D or S, 14 falls in set S. By inspection, we find that if (a, b) = (−5, −9), then a + b = −14. Also, taking f =",
    "label": 0
  },
  {
    "text": "30 and 14. While 30 does not fall within the sets D or S, 14 falls in set S. By inspection, we find that if (a, b) = (−5, −9), then a + b = −14. Also, taking f = ± 6, we have the sum v + u∗ + f to be 8 and -4. While 8 does not fall within the sets D or S, −4 falls in set D. Clearly, (d, e) = (−7, 11) so that d + e = 4, and the second constraint equation will be satisfied. Hence the corner pair (u, v) = (10, 12) is admissible; and it admits two solutions: ⁠ ( a , b , c , d , e , f ) = ( − 7 , − 9 , − 6 , − 5 , 11 , − 8 ) {\\displaystyle (a,b,c,d,e,f)=(-7,-9,-6,-5,11,-8)} ⁠ and ⁠ ( a , b , c , d , e , f ) = ( − 5 , − 9 , − 8 , − 7 , 11 , − 6 ) {\\displaystyle (a,b,c,d,e,f)=(-5,-9,-8,-7,11,-6)} ⁠. The finished skeleton squares are given below. The magic square is obtained by adding 13 to each cells. Using similar process of reasoning, we can construct the following table for the values of u, v, a, b, c, d, e, f expressed as bone numbers as given below. There are only 6 possible choices for the corner cells, which leads to 10 possible border solutions. Given this group of 10 borders, we can construct 10×8×(3!)2 = 2880 essentially different bordered magic squares. Here the bone numbers ±5, ..., ±12 were consecutive. More bordered squares can be constructed if the numbers are not consecutive. If non-consecutive bone numbers were also used, then there are a total of 605 magic",
    "label": 0
  },
  {
    "text": "different bordered magic squares. Here the bone numbers ±5, ..., ±12 were consecutive. More bordered squares can be constructed if the numbers are not consecutive. If non-consecutive bone numbers were also used, then there are a total of 605 magic borders. Thus, the total number of order 5 essentially different bordered magic squares (with consecutive and non-consecutive numbers) is 174,240. See history. The number of fifth-order magic squares constructible via the bordering method is about 26 times larger than via the superposition method. Continuous enumeration methods Exhaustive enumeration of all the borders of a magic square of a given order, as done previously, is very tedious. As such a structured solution is often desirable, which allows us to construct a border for a square of any order. Below we give three algorithms for constructing border for odd, doubly even, and singly even squares. These continuous enumeration algorithms were discovered in 10th century by Arab scholars; and their earliest surviving exposition comes from the two treatises by al-Buzjani and al-Antaki, although they themselves were not the discoverers. Since then many more such algorithms have been discovered. Odd-ordered squares: The following is the algorithm given by al-Buzjani to construct a border for odd squares. A peculiarity of this method is that for order n square, the two adjacent corners are numbers ⁠ n − 1 {\\displaystyle n-1} ⁠ and ⁠ n + 1 {\\displaystyle n+1} ⁠. Starting from the cell above the lower left corner, we put the numbers alternately in left column and bottom row until we arrive at the middle cell. The next number is written in the middle cell of the bottom row just reached, after which we fill the cell in the upper left corner, then the middle cell of the right column, then the upper right corner.",
    "label": 0
  },
  {
    "text": "cell. The next number is written in the middle cell of the bottom row just reached, after which we fill the cell in the upper left corner, then the middle cell of the right column, then the upper right corner. After this, starting from the cell above middle cell of the right column already filled, we resume the alternate placement of the numbers in the right column and the top row. Once half of the border cells are filled, the other half are filled by numbers complementary to opposite cells. The subsequent inner borders is filled in the same manner, until the square of order 3 is filled. Below is an example for 9th-order square. Doubly even order: The following is the method given by al-Antaki. Consider an empty border of order n = 4k with k ≥ 3. The peculiarity of this algorithm is that the adjacent corner cells are occupied by numbers n and ⁠ n − 1 {\\displaystyle n-1} ⁠. Starting at the upper left corner cell, we put the successive numbers by groups of four, the first one next to the corner, the second and the third on the bottom, and the fourth at the top, and so on until there remains in the top row (excluding the corners) six empty cells. We then write the next two numbers above and the next four below. We then fill the upper corners, first left then right. We place the next number below the upper right corner in the right column, the next number on the other side in the left column. We then resume placing groups of four consecutive numbers in the two columns as before. Once half of the border cells are filled, the other half are filled by numbers complementary to opposite cells. The example",
    "label": 0
  },
  {
    "text": "in the left column. We then resume placing groups of four consecutive numbers in the two columns as before. Once half of the border cells are filled, the other half are filled by numbers complementary to opposite cells. The example below gives the border for order 16 square. For order 8 square, we just begin directly with the six cells. Singly even order: For singly even order, we have the algorithm given by al-Antaki. Here the corner cells are occupied by n and n − 1. Below is an example of 10th-order square. Start by placing 1 at the bottom row next to the left corner cell, then place 2 in the top row. After this, place 3 at the bottom row and turn around the border in anti-clockwise direction placing the next numbers, until n − 2 is reached on the right column. The next two numbers are placed in the upper corners (n − 1 in upper left corner and n in upper right corner). Then, the next two numbers are placed on the left column, then we resume the cyclic placement of the numbers until half of all the border cells are filled. Once half of the border cells are filled, the other half are filled by numbers complementary to opposite cells. Method of composition For squares of order m × n where m, n > 2 This is a method reminiscent of the Kronecker product of two matrices, that builds an nm × nm magic square from an n × n magic square and an m × m magic square. The \"product\" of two magic squares creates a magic square of higher order than the two multiplicands. Let the two magic squares be of orders m and n. The final square will be of order m",
    "label": 0
  },
  {
    "text": "× m magic square. The \"product\" of two magic squares creates a magic square of higher order than the two multiplicands. Let the two magic squares be of orders m and n. The final square will be of order m × n. Divide the square of order m × n into m × m sub-squares, such that there are a total of n2 such sub-squares. In the square of order n, reduce by 1 the value of all the numbers. Multiply these reduced values by m2, and place the results in the corresponding sub-squares of the m × n whole square. The squares of order m are added n2 times to the sub-squares of the final square. The peculiarity of this construction method is that each magic subsquare will have different magic sums. The square made of such magic sums from each magic subsquare will again be a magic square. The smallest composite magic square of order 9, composed of two order 3 squares is given below. Since each of the 3×3 sub-squares can be independently rotated and reflected into 8 different squares, from this single 9×9 composite square we can derive 89 = 134,217,728 essentially different 9×9 composite squares. Plenty more composite magic squares can also be derived if we select non-consecutive numbers in the magic sub-squares, like in Yang Hui's version of the 9×9 composite magic square. The next smallest composite magic squares of order 12, composed of magic squares of order 3 and 4 are given below. For the base squares, there is only one essentially different 3rd order square, while there 880 essentially different 4th-order squares that we can choose from. Each pairing can produce two different composite squares. Since each magic sub-squares in each composite square can be expressed in 8 different forms due to",
    "label": 0
  },
  {
    "text": "3rd order square, while there 880 essentially different 4th-order squares that we can choose from. Each pairing can produce two different composite squares. Since each magic sub-squares in each composite square can be expressed in 8 different forms due to rotations and reflections, there can be 1×880×89 + 880×1×816 ≈ 2.476×1017 essentially different 12×12 composite magic squares created this way, with consecutive numbers in each sub-square. In general, if there are cm and cn essentially different magic squares of order m and n, then we can form cm × cn × ( 8m2 + 8n2) composite squares of order mn, provided m ≠ n. If m = n, then we can form (cm)2 × 8m2 composite squares of order m2. For squares of doubly even order When the squares are of doubly even order, we can construct a composite magic square in a manner more elegant than the above process, in the sense that every magic subsquare will have the same magic constant. Let n be the order of the main square and m the order of the equal subsquares. The subsquares are filled one by one, in any order, with a continuous sequence of m2/2 smaller numbers (i.e. numbers less than or equal to n2/2) together with their complements to n2 + 1. Each subsquare as a whole will yield the same magic sum. The advantage of this type of composite square is that each subsquare is filled in the same way and their arrangement is arbitrary. Thus, the knowledge of a single construction of even order will suffice to fill the whole square. Furthermore, if the subsquares are filled in the natural sequence, then the resulting square will be pandiagonal. The magic sum of the subsquares is related to the magic sum of the whole square by M",
    "label": 0
  },
  {
    "text": "to fill the whole square. Furthermore, if the subsquares are filled in the natural sequence, then the resulting square will be pandiagonal. The magic sum of the subsquares is related to the magic sum of the whole square by M m = M n k {\\displaystyle M_{m}={\\frac {M_{n}}{k}}} where n = km. In the examples below, we have divided the order 12 square into nine subsquares of order 4 filled each with eight smaller numbers and, in the corresponding bishop's cells (two cells diagonally across, including wrap arounds, in the 4×4 subsquare), their complements to n2 + 1 = 145. Each subsquare is pandiagonal with magic constant 290; while the whole square on the left is also pandiagonal with magic constant 870. In another example below, we have divided the order 12 square into four order 6 squares. Each of the order 6 squares are filled with eighteen small numbers and their complements using bordering technique given by al-Antaki. If we remove the shaded borders of the order 6 subsquares and form an order 8 square, then this order 8 square is again a magic square. In its full generality, we can take any m2/2 smaller numbers together with their complements to n2 + 1 to fill the subsquares, not necessarily in continuous sequence. Medjig method for squares of even order 2n, where n > 2 In this method a magic square is \"multiplied\" with a Medjig square to create a larger magic square. The namesake of this method derives from mathematical game called Medjig created by Willem Barink in 2006, although the method itself is much older. An early instance of a magic square constructed using this method occurs in Yang Hui's text for order 6 magic square. The LUX method to construct singly even magic squares is a",
    "label": 0
  },
  {
    "text": "2006, although the method itself is much older. An early instance of a magic square constructed using this method occurs in Yang Hui's text for order 6 magic square. The LUX method to construct singly even magic squares is a special case of the Medjig method, where only 3 out of 24 patterns are used to construct the Medjig square. The pieces of the Medjig puzzle are 2×2 squares on which the numbers 0, 1, 2 and 3 are placed. There are three basic patterns by which the numbers 0, 1, 2 and 3 can be placed in a 2×2 square, where 0 is at the top left corner: Each pattern can be reflected and rotated to obtain 8 equivalent patterns, giving us a total of 3×8 = 24 patterns. The aim of the puzzle is to take n2 Medjig pieces and arrange them in an n × n Medjig square in such a way that each row, column, along with the two long diagonals, formed by the Medjig square sums to 3n, the magic constant of the Medjig square. An n × n Medjig square can create a 2n × 2n magic square where n > 2. Given an n×n Medjig square and an n×n magic square base, a magic square of order 2n×2n can be constructed as follows: Each cell of an n×n magic square is associated with a corresponding 2×2 subsquare of the Medjig square Fill each 2×2 subsquares of the Medjig square with the four numbers from 1 to 4n2 that equal the original number modulo n2, i.e. x+n2y where x is the corresponding number from the magic square and y is a number from 0 to 3 in the 2×2 subsquares. Assuming that we have an initial magic square base, the challenge lies in constructing",
    "label": 0
  },
  {
    "text": "n2, i.e. x+n2y where x is the corresponding number from the magic square and y is a number from 0 to 3 in the 2×2 subsquares. Assuming that we have an initial magic square base, the challenge lies in constructing a Medjig square. For reference, the sums of each Medjig piece along the rows, columns and diagonals, denoted in italics, are: Doubly even squares: The smallest even ordered Medjig square is of order 2 with magic constant 6. While it is possible to construct a 2×2 Medjig square, we cannot construct a 4×4 magic square from it since 2×2 magic squares required to \"multiply\" it does not exist. Nevertheless, it is worth constructing these 2×2 Medjig squares. The magic constant 6 can be partitioned into two parts in three ways as 6 = 5 + 1 = 4 + 2 = 3 + 3. There exist 96 such 2×2 Medjig squares. In the examples below, each 2×2 Medjig square is made by combining different orientations of a single Medjig piece. We can use the 2×2 Medjig squares to construct larger even ordered Medjig squares. One possible approach is to simply combine the 2×2 Medjig squares together. Another possibility is to wrap a smaller Medjig square core with a Medjig border. The pieces of a 2×2 Medjig square can form the corner pieces of the border. Yet another possibility is to append a row and a column to an odd ordered Medjig square. An example of an 8×8 magic square is constructed below by combining four copies of the left most 2×2 Medjig square given above: The next example is constructed by bordering a 2×2 Medjig square core. Singly even squares: Medjig square of order 1 does not exist. As such, the smallest odd ordered Medjig square is of order 3,",
    "label": 0
  },
  {
    "text": "2×2 Medjig square given above: The next example is constructed by bordering a 2×2 Medjig square core. Singly even squares: Medjig square of order 1 does not exist. As such, the smallest odd ordered Medjig square is of order 3, with magic constant 9. There are only 7 ways of partitioning the integer 9, our magic constant, into three parts. If these three parts correspond to three of the Medjig pieces in a row, column or diagonal, then the relevant partitions for us are: 9 = 1 + 3 + 5 = 1 + 4 + 4 = 2 + 3 + 4 = 2 + 2 + 5 = 3 + 3 + 3. A 3×3 Medjig square can be constructed with some trial-and-error, as in the left most square below. Another approach is to add a row and a column to a 2×2 Medjig square. In the middle square below, a left column and bottom row has been added, creating an L-shaped Medjig border, to a 2×2 Medjig square given previously. The right most square below is essentially same as the middle square, except that the row and column has been added in the middle to form a cross while the pieces of 2×2 Medjig square are placed at the corners. Once a 3×3 Medjig square has been constructed, it can be converted into a 6×6 magic square. For example, using the left most 3×3 Medjig square given above: There are 1,740,800 such 3×3 Medjig squares. An easy approach to construct higher order odd Medjig square is by wrapping a smaller odd ordered Medjig square with a Medjig border, just as with even ordered Medjig squares. Another approach is to append a row and a column to an even ordered Medjig square. Approaches such as the LUX method",
    "label": 0
  },
  {
    "text": "Medicine is the science and practice of caring for patients, managing the diagnosis, prognosis, prevention, treatment and palliation of their injury or disease, and promoting their health. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others. Medicine has been practiced since prehistoric times, and for most of this time it was an art (an area of creativity and skill), frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). For example, while stitching technique for sutures is an art learned through practice, knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science. Prescientific forms of medicine, now known as traditional medicine or folk medicine, remain commonly used in the absence of scientific medicine and are thus called alternative medicine. Alternative treatments outside of scientific medicine with ethical, safety and efficacy concerns are termed quackery. Etymology Medicine (UK: , US: ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease. The word \"medicine\" is derived from Latin medicus, meaning \"a physician\". The word \"physic\" itself, from which \"physician\" derives, was the old word",
    "label": 0
  },
  {
    "text": ", US: ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease. The word \"medicine\" is derived from Latin medicus, meaning \"a physician\". The word \"physic\" itself, from which \"physician\" derives, was the old word for what is now called a medicine, and also the field of medicine. Clinical practice Medical availability and clinical practice vary across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed and widespread in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine which has limited evidence and efficacy and no required formal training for practitioners. In the developed world, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm. In modern clinical practice, physicians and mid-level practitioners such as physician assistants personally assess patients to diagnose, prognose, treat, and prevent disease using clinical judgment. An initial medical encounter with a patient typically begins with a review of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g., stethoscope, tongue depressor) are typically used. After examining for signs and interviewing for symptoms, the doctor may order medical tests (e.g., blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust within the context of the doctor-patient relationship. The medical encounter is then documented in the medical record, which",
    "label": 0
  },
  {
    "text": "the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust within the context of the doctor-patient relationship. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-up encounters may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks, depending on the complexity of the issue. The components of the medical interview and encounter are: Chief complaint (CC): the reason for the current medical visit. These are the symptoms. They are in the patient's own words and are recorded along with the duration of each one. Also called chief concern or presenting complaint. Current activity: occupation, hobbies, what the patient actually does. Family history (FH): listing of diseases in the family that may impact the patient. A family tree is sometimes used. History of present illness (HPI): the chronological order of events of symptoms and further clarification of each symptom. Distinguishable from history of previous illness, often called past medical history (PMH). Medical history comprises HPI and PMH. Medications (Rx): what drugs the patient takes including prescribed, over-the-counter, and home remedies, as well as alternative and herbal medicines or remedies. Allergies are also recorded. Past medical history (PMH/PMHx): concurrent medical problems, past hospitalizations and operations, injuries, past infectious diseases or vaccinations, history of known allergies. Review of systems (ROS) or systems inquiry: a set of additional questions to ask, which may be missed on HPI: a general enquiry (have you noticed any weight loss, change in sleep quality, fevers, lumps and bumps? etc.), followed by questions on the body's main organ systems (heart, lungs, digestive tract, urinary tract, etc.). Social history",
    "label": 0
  },
  {
    "text": "be missed on HPI: a general enquiry (have you noticed any weight loss, change in sleep quality, fevers, lumps and bumps? etc.), followed by questions on the body's main organ systems (heart, lungs, digestive tract, urinary tract, etc.). Social history (SH): birthplace, residences, marital history, social and economic status, habits (including diet, medications, tobacco, alcohol). The physical examination is the examination of the patient for medical signs of disease that are objective and observable, in contrast to symptoms that are volunteered by the patient and are not necessarily objectively observable. The healthcare provider uses sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order, although auscultation occurs prior to percussion and palpation for abdominal assessments. The clinical examination involves the study of: Abdomen and rectum Cardiovascular (heart and blood vessels) General appearance of the patient and specific indicators of disease (nutritional status, presence of jaundice, pallor or clubbing) Genitalia (and pregnancy if the patient is or could be pregnant) Head, eye, ear, nose, and throat (HEENT) Musculoskeletal (including spine and extremities) Neurological (consciousness, awareness, brain, vision, cranial nerves, spinal cord and peripheral nerves) Psychiatric (orientation, mental state, mood, evidence of abnormal perception or thought). Respiratory (large airways and lungs) Skin Vital signs including height, weight, body temperature, blood pressure, pulse, respiration rate, and hemoglobin oxygen saturation It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above. The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. A follow-up may be advised. Depending upon the health insurance plan and the managed",
    "label": 0
  },
  {
    "text": "everything listed above. The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. A follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of \"utilization review\", such as prior authorization of tests, may place barriers on accessing expensive services. The medical decision-making (MDM) process includes the analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem. On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, lab or imaging results, or specialist consultations. Institutions Contemporary medicine is, in general, conducted within health care systems. Legal, credentialing, and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have a significant impact on the way medical care is provided. From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals, and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system or compulsory private or cooperative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices, state-owned hospitals and clinics, or charities, most",
    "label": 0
  },
  {
    "text": "health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices, state-owned hospitals and clinics, or charities, most commonly a combination of all three. Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those who can afford to pay for it, have self-insured it (either directly or as part of an employment contract), or may be covered by care financed directly by the government or tribe. Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice of patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for its lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other. The health professionals who provide care in medicine comprise multiple professions, such as medics, nurses, physiotherapists, and psychologists. These professions will have their own ethical standards, professional education, and bodies. The medical profession has been conceptualized from a sociological perspective. Delivery Provision of medical care is classified into primary, secondary, and tertiary care categories. Primary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care. These occur in physician offices, medical practices, clinics, nursing homes, schools, patients' homes, and in other places that are typically geographically close to where patients live, work or study. About 90% of medical visits",
    "label": 0
  },
  {
    "text": "patient seeking medical treatment or care. These occur in physician offices, medical practices, clinics, nursing homes, schools, patients' homes, and in other places that are typically geographically close to where patients live, work or study. About 90% of medical visits can be satisfactorily and effectively dealt with by primary care provider(s). Primary care visits might include treatment of minor, acute or chronic illnesses, preventive care, and health education. Primary care is directed to the health of entire populations and thus providers care for patients of all ages and sexes. Secondary care medical services are provided by medical specialists in their offices, practices or clinics, or at local community hospitals, to patients referred by the primary care provider who first diagnosed or treated the patient. 'Referrals' are made of those patients who required the particular expertise of, or specific procedures performed by, specialists. Secondary care services include both ambulatory care and inpatient services, emergency departments, some intensive care medicine, some surgeries and related services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, and others depending on the health services systems within which the care is being delivered. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting. Tertiary care medical services are provided by specialist teams of providers in larger, more specialised hospitals or regional medical centers, which are equipped with diagnostic and treatment facilities not typically available at local (often smaller) hospitals. This allows for the treatment and care of patients with more complex or urgent or serious medical conditions, which in turn may require more expertise (including multi-disciplinary teams) and resources (facilities, staff, bed days) to effectively treat. Tertiary care may include that provided at burn treatment or trauma centers, advanced neonatology unit",
    "label": 0
  },
  {
    "text": "or urgent or serious medical conditions, which in turn may require more expertise (including multi-disciplinary teams) and resources (facilities, staff, bed days) to effectively treat. Tertiary care may include that provided at burn treatment or trauma centers, advanced neonatology unit services, organ transplants, high-risk pregnancy and child delivery, radiation oncology, and very many other forms of specialist and intensive care. Modern medical care also depends on the keeping and use of information, including about a particular patient—still kept in many health care settings on paper 'medical records', but increasingly nowadays by electronic means. In low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that \"user fees\" be removed in these areas to ensure access; however, even with removal of patient fee obligations, significant costs and barriers remain for the poor and the sick in accessing sufficient care. Separation of prescribing and dispensing is a practice in medicine and pharmacy in which the physician who provides a medical prescription is different from the pharmacist who provides the prescription drug to the patient. In the Western world there are centuries of tradition and practice differentiating pharmacists from physicians, and two quite separate professions developed. In many Asian countries, on the other hand, it is traditional for physicians to also deliver drugs directly to patients, at least in some cases. This model is also being used increasingly in the west: especially for simply-treated conditions (eg, those needing general antibiotics), in remote locations, with vulnerable communities of patients, and in small or integrated medical facilities. Branches Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech",
    "label": 0
  },
  {
    "text": "Branches Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, medical physicists, surgeons, surgeon's assistant, surgical technologist. The scope and sciences underpinning human medicine overlap many other fields. A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments. Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in. The main branches of medicine are: Basic sciences of medicine; this is what every physician is educated in, and some return to in biomedical research. Interdisciplinary fields, where different medical specialties are mixed to function in certain occasions. Medical specialties Basic sciences Anatomy is the study of the physical structure of organisms. In contrast to macroscopic or gross anatomy, cytology and histology are concerned with microscopic structures. Biochemistry is the study of the chemistry taking place in living organisms, especially the structure and function of their chemical components. Biomechanics is the study of the structure and function of biological systems by means of the methods of Mechanics. Biophysics is an interdisciplinary science that uses the methods of physics and physical chemistry to study biological systems. Biostatistics is the application of statistics to biological fields in the broadest sense. A knowledge of biostatistics is essential in the planning, evaluation, and interpretation of medical research. It is also fundamental to epidemiology",
    "label": 0
  },
  {
    "text": "physical chemistry to study biological systems. Biostatistics is the application of statistics to biological fields in the broadest sense. A knowledge of biostatistics is essential in the planning, evaluation, and interpretation of medical research. It is also fundamental to epidemiology and evidence-based medicine. Cytology is the microscopic study of individual cells. Embryology is the study of the early development of organisms. Endocrinology is the study of hormones and their effect throughout the body of animals. Epidemiology is the study of the demographics of disease processes, and includes, but is not limited to, the study of epidemics. Genetics is the study of genes, and their role in biological inheritance. Gynecology is the study of female reproductive system. Histology is the study of the structures of biological tissues by light microscopy, electron microscopy and immunohistochemistry. Immunology is the study of the immune system, which includes the innate and adaptive immune system in humans, for example. Lifestyle medicine is the study of the chronic conditions, and how to prevent, treat and reverse them. Medical physics is the study of the applications of physics principles in medicine. Microbiology is the study of microorganisms, including protozoa, bacteria, fungi, and viruses. Molecular biology is the study of molecular underpinnings of the process of replication, transcription and translation of the genetic material. Neuroscience includes those disciplines of science that are related to the study of the nervous system. A main focus of neuroscience is the biology and physiology of the human brain and spinal cord. Some related clinical specialties include neurology, neurosurgery and psychiatry. Nutrition science (theoretical focus) and dietetics (practical focus) is the study of the relationship of food and drink to health and disease, especially in determining an optimal diet. Medical nutrition therapy is done by dietitians and is prescribed for diabetes, cardiovascular diseases, weight",
    "label": 0
  },
  {
    "text": "focus) and dietetics (practical focus) is the study of the relationship of food and drink to health and disease, especially in determining an optimal diet. Medical nutrition therapy is done by dietitians and is prescribed for diabetes, cardiovascular diseases, weight and eating disorders, allergies, malnutrition, and neoplastic diseases. Pathology as a science is the study of disease – the causes, course, progression and resolution thereof. Pharmacology is the study of drugs and their actions. Photobiology is the study of the interactions between non-ionizing radiation and living organisms. Physiology is the study of the normal functioning of the body and the underlying regulatory mechanisms. Radiobiology is the study of the interactions between ionizing radiation and living organisms. Toxicology is the study of hazardous effects of drugs and poisons. Specialties In the broadest meaning of \"medicine\", there are many different specialties. In the UK, most specialities have their own body or college, which has its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term \"Royal\". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination. Within medical circles, specialities usually fit into one of two broad categories: \"Medicine\" and \"Surgery\". \"Medicine\" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. \"Surgery\" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in",
    "label": 0
  },
  {
    "text": "by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. \"Surgery\" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA). Surgical specialty Surgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se. Surgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus",
    "label": 0
  },
  {
    "text": "of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming. Surgical subspecialties include those a physician may specialize in after undergoing general surgery residency training as well as several surgical fields with separate residency training. Surgical subspecialties that one may pursue following general surgery residency training: Bariatric surgery Cardiovascular surgery – may also be pursued through a separate cardiovascular surgery residency track Colorectal surgery Endocrine surgery General surgery Hand surgery Hepatico-Pancreatico-Biliary Surgery Minimally invasive surgery Pediatric surgery Plastic surgery – may also be pursued through a separate plastic surgery residency track Surgical critical care Surgical oncology Transplant surgery Trauma surgery Vascular surgery – may also be pursued through a separate vascular surgery residency track Other surgical specialties within medicine with their own individual residency training: Dermatology Neurosurgery Ophthalmology Oral and maxillofacial surgery Orthopedic surgery Otorhinolaryngology Podiatric surgery – do not undergo medical school training, but rather separate training in podiatry school Urology Internal medicine specialty Internal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases. According to some sources, an emphasis on internal structures is implied. In North America, specialists in internal medicine are commonly called \"internists\". Elsewhere, especially in Commonwealth nations, such specialists are often called physicians. These terms, internist or physician (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities. Because their patients are often seriously ill or require complex investigations, internists do",
    "label": 0
  },
  {
    "text": "terms, internist or physician (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities. Because their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such general physicians would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys. In the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as specialist physicians (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care. There are many subspecialities (or subdisciplines) of internal medicine: Training in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on medical education for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the US. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average. Diagnostic specialties Clinical laboratory sciences are the clinical diagnostic services that apply laboratory techniques to diagnosis and management of patients. In the United States, these services are",
    "label": 0
  },
  {
    "text": "required by law to work less than 48 hours per week on average. Diagnostic specialties Clinical laboratory sciences are the clinical diagnostic services that apply laboratory techniques to diagnosis and management of patients. In the United States, these services are supervised by a pathologist. The personnel that work in these medical laboratory departments are technically trained staff who do not hold medical degrees, but who usually hold an undergraduate medical technology degree, who actually perform the tests, assays, and procedures needed for providing the specific services. Subspecialties include transfusion medicine, cellular pathology, clinical chemistry, hematology, clinical microbiology and clinical immunology. Clinical neurophysiology is concerned with testing the physiology or function of the central and peripheral aspects of the nervous system. These kinds of tests can be divided into recordings of: (1) spontaneous or continuously running electrical activity, or (2) stimulus evoked responses. Subspecialties include electroencephalography, electromyography, evoked potential, nerve conduction study and polysomnography. Sometimes these tests are performed by techs without a medical degree, but the interpretation of these tests is done by a medical professional. Diagnostic radiology is concerned with imaging of the body, e.g. by x-rays, x-ray computed tomography, ultrasonography, and nuclear magnetic resonance tomography. Interventional radiologists can access areas in the body under imaging for an intervention or diagnostic sampling. Nuclear medicine is concerned with studying human organ systems by administering radiolabelled substances (radiopharmaceuticals) to the body, which can then be imaged outside the body by a gamma camera or a PET scanner. Each radiopharmaceutical consists of two parts: a tracer that is specific for the function under study (e.g., neurotransmitter pathway, metabolic pathway, blood flow, or other), and a radionuclide (usually either a gamma-emitter or a positron emitter). There is a degree of overlap between nuclear medicine and radiology, as evidenced by the emergence of combined",
    "label": 0
  },
  {
    "text": "under study (e.g., neurotransmitter pathway, metabolic pathway, blood flow, or other), and a radionuclide (usually either a gamma-emitter or a positron emitter). There is a degree of overlap between nuclear medicine and radiology, as evidenced by the emergence of combined devices such as the PET/CT scanner. Pathology as a medical specialty is the branch of medicine that deals with the study of diseases and the morphologic, physiologic changes produced by them. As a diagnostic specialty, pathology can be considered the basis of modern scientific medical knowledge and plays a large role in evidence-based medicine. Many modern molecular tests such as flow cytometry, polymerase chain reaction (PCR), immunohistochemistry, cytogenetics, gene rearrangements studies and fluorescent in situ hybridization (FISH) fall within the territory of pathology. Other major specialties The following are some major medical specialties that do not directly fit into any of the above-mentioned groups: Anesthesiology (also known as anaesthetics): concerned with the perioperative management of the surgical patient. The anesthesiologist's role during surgery is to prevent derangement in the vital organs' (i.e. brain, heart, kidneys) functions and postoperative pain. Outside of the operating room, the anesthesiology physician also serves the same function in the labor and delivery ward, and some are specialized in critical medicine. Emergency medicine is concerned with the diagnosis and treatment of acute or life-threatening conditions, including trauma, surgical, medical, pediatric, and psychiatric emergencies. Family medicine, family practice, general practice or primary care is, in many countries, the first port-of-call for patients with non-emergency medical problems. Family physicians often provide services across a broad range of settings including office based practices, emergency department coverage, inpatient care, and nursing home care. Medical genetics is concerned with the diagnosis and management of hereditary disorders. Neurology is concerned with diseases of the nervous system. In the UK, neurology is a",
    "label": 0
  },
  {
    "text": "including office based practices, emergency department coverage, inpatient care, and nursing home care. Medical genetics is concerned with the diagnosis and management of hereditary disorders. Neurology is concerned with diseases of the nervous system. In the UK, neurology is a subspecialty of general medicine. Obstetrics and gynecology (often abbreviated as OB/GYN (American English) or Obs & Gynae (British English)) are concerned respectively with childbirth and the female reproductive and associated organs. Reproductive medicine and fertility medicine are generally practiced by gynecological specialists. Pediatrics (AE) or paediatrics (BE) is devoted to the care of infants, children, and adolescents. Like internal medicine, there are many pediatric subspecialties for specific age ranges, organ systems, disease classes, and sites of care delivery. Pharmaceutical medicine is the medical scientific discipline concerned with the discovery, development, evaluation, registration, monitoring and medical aspects of marketing of medicines for the benefit of patients and public health. Physical medicine and rehabilitation (or physiatry) is concerned with functional improvement after injury, illness, or congenital disorders. Podiatric medicine is the study of, diagnosis, and medical and surgical treatment of disorders of the foot, ankle, lower limb, hip and lower back. Preventive medicine is the branch of medicine concerned with preventing disease. Community health or public health is an aspect of health services concerned with threats to the overall health of a community based on population health analysis. Psychiatry is the branch of medicine concerned with the bio-psycho-social study of the etiology, diagnosis, treatment and prevention of cognitive, perceptual, emotional and behavioral disorders. Related fields include psychotherapy and clinical psychology. Interdisciplinary fields Some interdisciplinary sub-specialties of medicine include: Addiction medicine deals with the treatment of addiction. Aerospace medicine deals with medical problems related to flying and space travel. Biomedical Engineering is a field dealing with the application of engineering principles to medical",
    "label": 0
  },
  {
    "text": "Some interdisciplinary sub-specialties of medicine include: Addiction medicine deals with the treatment of addiction. Aerospace medicine deals with medical problems related to flying and space travel. Biomedical Engineering is a field dealing with the application of engineering principles to medical practice. Clinical pharmacology is concerned with how systems of therapeutics interact with patients. Conservation medicine studies the relationship between human and non-human animal health, and environmental conditions. Also known as ecological medicine, environmental medicine, or medical geology. Disaster medicine deals with medical aspects of emergency preparedness, disaster mitigation and management. Diving medicine (or hyperbaric medicine) is the prevention and treatment of diving-related problems. Evolutionary medicine is a perspective on medicine derived through applying evolutionary theory. Forensic medicine deals with medical questions in legal context, such as determination of the time and cause of death, type of weapon used to inflict trauma, reconstruction of the facial features using remains of deceased (skull) thus aiding identification. Gender-based medicine studies the biological and physiological differences between the human sexes and how that affects differences in disease. Health informatics is a relatively recent field that deal with the application of computers and information technology to medicine. Hospice and Palliative Medicine is a relatively modern branch of clinical medicine that deals with pain and symptom relief and emotional support in patients with terminal illnesses including cancer and heart failure. Hospital medicine is the general medical care of hospitalized patients. Physicians whose primary professional focus is hospital medicine are called hospitalists in the United States and Canada. The term Most Responsible Physician (MRP) or attending physician is also used interchangeably to describe this role. Laser medicine involves the use of lasers in the diagnostics or treatment of various conditions. Many other health science fields, e.g. dietetics Medical ethics deals with ethical and moral principles that apply",
    "label": 0
  },
  {
    "text": "is also used interchangeably to describe this role. Laser medicine involves the use of lasers in the diagnostics or treatment of various conditions. Many other health science fields, e.g. dietetics Medical ethics deals with ethical and moral principles that apply values and judgments to the practice of medicine. Medical humanities includes the humanities (literature, philosophy, ethics, history and religion), social science (anthropology, cultural studies, psychology, sociology), and the arts (literature, theater, film, and visual arts) and their application to medical education and practice. Nosokinetics is the science/subject of measuring and modelling the process of care in health and social care systems. Nosology is the classification of diseases for various purposes. Occupational medicine is the provision of health advice to organizations and individuals to ensure that the highest standards of health and safety at work can be achieved and maintained. Pain management (also called pain medicine, or algiatry) is the medical discipline concerned with the relief of pain. Pharmacogenomics is a form of individualized medicine. Podiatric medicine is the study of, diagnosis, and medical treatment of disorders of the foot, ankle, lower limb, hip and lower back. Sexual medicine is concerned with diagnosing, assessing and treating all disorders related to sexuality. Sports medicine deals with the treatment and prevention and rehabilitation of sports/exercise injuries such as muscle spasms, muscle tears, injuries to ligaments (ligament tears or ruptures) and their repair in athletes, amateur and professional. Therapeutics is the field, more commonly referenced in earlier periods of history, of the various remedies that can be used to treat disease and promote health. Travel medicine or emporiatrics deals with health problems of international travelers or travelers across highly different environments. Tropical medicine deals with the prevention and treatment of tropical diseases. It is studied separately in temperate climates where those diseases are quite",
    "label": 0
  },
  {
    "text": "Travel medicine or emporiatrics deals with health problems of international travelers or travelers across highly different environments. Tropical medicine deals with the prevention and treatment of tropical diseases. It is studied separately in temperate climates where those diseases are quite unfamiliar to medical practitioners and their local clinical needs. Urgent care focuses on delivery of unscheduled, walk-in care outside of the hospital emergency department for injuries and illnesses that are not severe enough to require care in an emergency department. In some jurisdictions this function is combined with the emergency department. Veterinary medicine; veterinarians apply similar techniques as physicians to the care of non-human animals. Wilderness medicine entails the practice of medicine in the wild, where conventional medical facilities may not be available. Education and legal controls Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university. Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs. A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/ Archived 4 October 2018 at the Wayback Machine. In most countries, it is a legal requirement for",
    "label": 0
  },
  {
    "text": "online programs. A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/ Archived 4 October 2018 at the Wayback Machine. In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in \"evidence based\", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health. In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification. The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC. Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions. Medical ethics Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are: autonomy – the patient has the right",
    "label": 0
  },
  {
    "text": "discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are: autonomy – the patient has the right to refuse or choose their treatment. (Latin: Voluntas aegroti suprema lex.) beneficence – a practitioner should act in the best interest of the patient. (Latin: Salus aegroti suprema lex.) justice – concerns the distribution of scarce health resources, and the decision of who gets what treatment (fairness and equality). non-maleficence – \"first, do no harm\" (Latin: primum non-nocere). respect for persons – the patient (and the person treating the patient) have the right to be treated with dignity. truthfulness and honesty – the concept of informed consent has increased in importance since the historical events of the Doctors' Trial of the Nuremberg trials, Tuskegee syphilis experiment, and others. Values such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era. History Ancient world Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known",
    "label": 0
  },
  {
    "text": "not emphasized to a large extent before the HIV era. History Ancient world Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues. The earliest known medical texts in the world were found in the ancient Syrian city of Ebla and date back to 2500 BCE. Other early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (Alternative medicine) predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine. In Egypt, Imhotep (3rd millennium BCE) is the first physician in history known by name. The oldest Egyptian medical text is the Kahun Gynaecological Papyrus from around 2000 BCE, which describes gynaecological diseases. The Edwin Smith Papyrus dating back to 1600 BCE is an early work on surgery, while the Ebers Papyrus dating back to 1500 BCE is akin to a textbook on medicine. In China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang dynasty, based on seeds for herbalism and tools presumed to have been used for surgery. The Huangdi Neijing, the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century. In India, the oldest known surgical text, the Sushruta Samhita written by the surgeon Sushruta, described",
    "label": 0
  },
  {
    "text": "Huangdi Neijing, the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century. In India, the oldest known surgical text, the Sushruta Samhita written by the surgeon Sushruta, described numerous surgical operations, including the earliest forms of plastic surgery as well as methods of sterilization for surgical instruments. The earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found. In Greece, the ancient Greek physician Hippocrates, the \"father of modern medicine\", laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, \"exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence\". The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire. Most of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew. Middle Ages The concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire. Although the concept",
    "label": 0
  },
  {
    "text": "concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire. Although the concept of uroscopy was known to Galen, he did not see the importance of using it to localize the disease. It was under the Byzantines with physicians such of Theophilus Protospatharius that they realized the potential in uroscopy to determine disease in a time when no microscope or stethoscope existed. That practice eventually spread to the rest of Europe. After 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the \"father of medicine\". He wrote The Canon of Medicine which became a standard medical text at many medieval European universities, considered one of the most famous books in the history of medicine. Others include Abulcasis, Avenzoar, Ibn al-Nafis, and Averroes. Persian physician Rhazes was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. Some volumes of Rhazes's work Al-Mansuri, namely \"On Surgery\" and \"A General Book on Therapy\", became part of the medical curriculum in European universities. Additionally, he has been described as a doctor's doctor, the father of pediatrics, and a pioneer of ophthalmology. For example, he was the first to recognize the reaction of the eye's pupil to light. The Persian Bimaristan hospitals were an early example of public hospitals. In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey",
    "label": 0
  },
  {
    "text": "recognize the reaction of the eye's pupil to light. The Persian Bimaristan hospitals were an early example of public hospitals. In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: \"It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal\". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in medieval Europe. However, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East. In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey. The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the \"traditional authority\" approach to science and medicine. This was the notion",
    "label": 0
  },
  {
    "text": "William Harvey. The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the \"traditional authority\" approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia. Andreas Vesalius was the author of De humani corporis fabrica, an important book on human anatomy. Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology. Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the \"Manuscript of Paris\" in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a \"father of physiology\" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called \"the father of modern dentistry\". Modern Veterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals. Modern scientific biomedical research (where results are testable and reproducible)",
    "label": 0
  },
  {
    "text": "from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals. Modern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek \"four humours\" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of variolation originated in ancient China), Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900. The post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience. From New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet. Others that did significant work include William Williams Keen, William Coley, James D. Watson (United States); Salvador Luria (Italy); Alexandre Yersin (Switzerland); Kitasato Shibasaburō (Japan); Jean-Martin Charcot, Claude Bernard, Paul Broca (France); Adolfo Lutz (Brazil); Nikolai Korotkov (Russia); Sir William Osler (Canada); and Harvey Cushing (United States). As science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only plant products were used as medicine, but also animal (including human) body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, vinca alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur. The",
    "label": 0
  },
  {
    "text": "(including human) body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, vinca alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur. The first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes. Pharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics and human evolution is having increasingly significant influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology, evolution, and genetics are influencing medical technology, practice and decision-making. Evidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect. Quality, efficiency, and access Evidence-based medicine, prevention of medical error (and other \"iatrogenesis\"), and avoidance of unnecessary health care are a priority in modern medical systems. These topics generate significant political and public policy attention, particularly in the United States where healthcare is regarded",
    "label": 0
  },
  {
    "text": "Evidence-based medicine, prevention of medical error (and other \"iatrogenesis\"), and avoidance of unnecessary health care are a priority in modern medical systems. These topics generate significant political and public policy attention, particularly in the United States where healthcare is regarded as excessively costly but population health metrics lag similar nations. Health spending varies by country, which results in differences in access to care and access to medicines. Health care rationing varies by country. Most developed countries provide health care to all citizens, with a few exceptions such as the United States where restrictions on health insurance coverage may limit affordability. Telemedicine Telemedicine (also Telehealth) refers to preventive, promotive, and curative care delivery, including remote clinical services, such as diagnosis, monitoring, administration and provider education. The main categories of telehealth: Telenursing is experiencing significant growth globally due to factors such as the need to reduce healthcare costs, an increasing aging and chronically ill population, and expanded healthcare coverage to distant, rural, small, or sparsely populated regions. Telenursing can help address nurse shortages, reduce travel time and distances, and minimize hospital admissions. Telepalliative care is a remote approach to optimising quality of life and relieving suffering for people with serious, complex and often fatal illnesses. The World Health Organization (WHO) recommends integrating palliative care as early as possible for any chronic and fatal illness. Telepalliative care typically utilizes telecommunication technologies like video conferencing, messaging for follow-ups, and digital symptom assessments through questionnaires that generate alerts for healthcare professionals. Telepharmacy involves delivering pharmaceutical care via telecommunications to patients in locations where direct contact with a pharmacist may not be possible or difficult. Telepsychiatry (telemental health) uses telecommunications technology to provide remote psychiatric care for individuals with mental health conditions. Telepsychology is the use of communication technology for the remote administration of psychological tests",
    "label": 0
  },
  {
    "text": "with a pharmacist may not be possible or difficult. Telepsychiatry (telemental health) uses telecommunications technology to provide remote psychiatric care for individuals with mental health conditions. Telepsychology is the use of communication technology for the remote administration of psychological tests and psychotherapy. Teleneurotherapy utilizes computers and communications technology to deliver neurotherapy remotely. Research indicates that systematic physical stimuli from standard electronic devices, such as tablets with headphones, may treat injured nervous systems online by modulating neuronal plasticity. Evidence suggests that teleneurotherapy could enhance neurological treatment if it incorporates the therapeutic effect of a systematic abiotic impact of physical forces with key parameters of mother-fetus interaction. Recent research has shown therapeutic effects when implementing the APIN method in the online treatment of patients with various neurological conditions. Telenutrition refers to the use of video conferencing or telephony to provide online consultations by nutritionists or dieticians. Telerehabilitation (or e-rehabilitation) is the delivery of rehabilitation services over telecommunication networks and the Internet. Most services fall into two categories: clinical assessment (evaluating the patient's functional abilities in their environment) and clinical therapy. See also Notes References",
    "label": 0
  },
  {
    "text": "The following outline is provided as an overview of and topical guide to medicine: Medicine – science of healing. It encompasses a variety of health care practices evolved to maintain health by the prevention and treatment of illness. Aims Cure Health Homeostasis Medical ethics Prevention of illness Palliation Branches of medicine Anesthesiology – practice of medicine dedicated to the relief of pain and total care of the surgical patient before, during and after surgery. Alternative medicine; is any healing practice, \"that does not fall within the realm of conventional medicine. Cardiology – branch of medicine that deals with disorders of the heart and the blood vessels. Critical care medicine – focuses on life support and the intensive care of the seriously ill. Dentistry – branch of medicine that deals with treatment of diseases in the oral cavity Dermatology – branch of medicine that deals with the skin, hair, and nails. Emergency medicine – focuses on care provided in the emergency department Endocrinology – branch of medicine that deals with disorders of the endocrine system. Epidemiology – study of cause and prevalence of diseases and programs to contain them First aid – assistance given to any person experiencing a sudden illness or injury, with care provided to preserve life, prevent the condition from worsening, and/or promote recovery. It includes initial intervention in a serious condition prior to professional medical help being available, such as performing CPR while awaiting an ambulance, as well as the complete treatment of minor conditions, such as applying a plaster to a cut. Gastroenterology – branch of medicine that deals with the study and care of the digestive system. General practice (often called family medicine) is a branch of medicine that specializes in primary care. Geriatrics – branch of medicine that deals with the general health and",
    "label": 0
  },
  {
    "text": "medicine that deals with the study and care of the digestive system. General practice (often called family medicine) is a branch of medicine that specializes in primary care. Geriatrics – branch of medicine that deals with the general health and well-being of the elderly. Gynaecology – diagnosis and treatment of the female reproductive system Hematology – branch of medicine that deals with the blood and the circulatory system. Hepatology – branch of medicine that deals with the liver, gallbladder and the biliary system. Infectious disease (Outline of concepts) – branch of medicine that deals with the diagnosis and management of infectious disease, especially for complex cases and immunocompromised patients. Internal medicine – involved with adult diseases Neurology – branch of medicine that deals with the brain and the nervous system. Nephrology – branch of medicine which deals with the kidneys. Obstetrics – care of women during and after pregnancy Occupational medicine – branch of medicine concerned with the maintenance of health in the workplace Oncology – branch of medicine that studies the types of cancer. Ophthalmology – branch of medicine that deals with the eyes. Optometry – branch of medicine that involves examining the eyes and applicable visual systems for defects or abnormalities as well as the medical diagnosis and management of eye disease. Orthopaedics – branch of medicine that deals with conditions involving the musculoskeletal system. Otorhinolaryngology – branch of medicine that deals with the ears, nose and throat. Pathology – study of causes and pathogenesis of diseases. Pediatrics – branch of medicine that deals with the general health and well-being of children and in some countries like the U.S. young adults. Physiatry – comprising physical medicine and rehabilitation Preventive medicine – measures taken for disease prevention, as opposed to disease treatment. Psychiatry – branch of medicine that deals",
    "label": 0
  },
  {
    "text": "and well-being of children and in some countries like the U.S. young adults. Physiatry – comprising physical medicine and rehabilitation Preventive medicine – measures taken for disease prevention, as opposed to disease treatment. Psychiatry – branch of medicine that deals with the study, diagnosis, treatment, and prevention of mental disorders. Pulmonology – branch of medicine that deals with the respiratory system. Radiology – branch of medicine that employs medical imaging to diagnose and treat disease. Sports medicine – branch of medicine that deals with physical fitness and the treatment and prevention of injuries related to sports and exercise. Rheumatology – branch of medicine that deals with the diagnosis and treatment of rheumatic diseases. Surgery – branch of medicine that uses operative techniques to investigate or treat both disease and injury, or to help improve bodily function or appearance. Urology – branch of medicine that deals with the urinary system of both sexes and the male reproductive system History of medicine History of medicine Prehistoric medicine Greco-Roman medicine Medicine in the medieval Islamic world Medieval medicine of Western Europe Ancient Egyptian medicine Babylonian medicine Ancient Iranian medicine Alternative medicine Homeopathy Herbalism Siddha medicine Ayurveda Traditional Chinese medicine Jewish medicine Medical biology Medical biology Fields of medical biology Anatomy – study of the physical structure of organisms. In contrast to macroscopic or gross anatomy, cytology and histology are concerned with microscopic structures. List of anatomical topics List of bones of the human skeleton List of homologues of the human reproductive system List of human anatomical features List of human anatomical parts named after people List of human blood components List of human hormones List of human nerves List of muscles of the human body List of regions in the human brain Biochemistry – study of the chemistry taking place in living organisms,",
    "label": 0
  },
  {
    "text": "after people List of human blood components List of human hormones List of human nerves List of muscles of the human body List of regions in the human brain Biochemistry – study of the chemistry taking place in living organisms, especially the structure and function of their chemical components. Bioinformatics Biological engineering Biophysics Biostatistics – application of statistics to biological fields in the broadest sense. A knowledge of biostatistics is essential in the planning, evaluation, and interpretation of medical research. It is also fundamental to epidemiology and evidence-based medicine. Biotechnology Nanobiotechnology Cell biology – microscopic study of individual cells. Embryology – study of the early development of organisms. Gene therapy Genetics – study of genes, and their role in biological inheritance. Cytogenetics Histology – study of the structures of biological tissues by light microscopy, electron microscopy and immunohistochemistry. Immunology – study of the immune system, which includes the innate and adaptive immune system in humans, for example. Laboratory medical biology Microbiology – study of microorganisms, including protozoa, bacteria, fungi, and viruses. Molecular biology Neuroscience (outline) – includes those disciplines of science that are related to the study of the nervous system. A main focus of neuroscience is the biology and physiology of the human brain and spinal cord. Parasitology Pathology – study of disease, including the causes, course, progression and resolution thereof. Physiology – study of the normal functioning of the body and the underlying regulatory mechanisms. Systems biology Virology Toxicology – study of hazardous effects of drugs and poisons. and many others (typically, life sciences that pertain to medicine) Illness (diseases and disorders) Disease Disability List of cancer types List of childhood diseases List of diseases caused by insects List of eponymous diseases List of fictional diseases List of food-borne illness outbreaks in the United States List of genetic",
    "label": 0
  },
  {
    "text": "Illness (diseases and disorders) Disease Disability List of cancer types List of childhood diseases List of diseases caused by insects List of eponymous diseases List of fictional diseases List of food-borne illness outbreaks in the United States List of genetic disorders List of human parasitic diseases List of illnesses related to poor nutrition List of infectious diseases List of infectious diseases causing flu-like syndrome List of latent human viral infections List of mental illnesses List of neurological disorders List of notifiable diseases List of parasites (human) List of skin-related conditions List of systemic diseases with ocular manifestations Medical practice Practice of medicine Physical examination Diagnosis Surgery Medication Drugs Drugs Drug Pharmaceutical drug/ Medication Recreational drug List of anaesthetic drugs List of antibiotics List of antiviral drugs List of bestselling drugs List of drugs affected by grapefruit List of drugs banned from the Olympics List of controlled drugs in the United Kingdom List of medical inhalants List of monoclonal antibodies List of psychedelic drugs List of psychiatric medications List of psychiatric medications by condition treated List of schedules of controlled substances (USA) List of Schedule I drugs List of Schedule II drugs List of Schedule III drugs List of Schedule IV drugs List of Schedule V drugs List of withdrawn drugs Medical equipment Medical equipment MRI Computed axial tomography Medical labs Blood test Medical facilities Clinic Walk in clinic Hospice List of hospice programs Hospital Private hospital Public hospital List of hospitals in the United States List of burn centers in the United States List of Veterans Affairs medical facilities Medical education Medical education – education related to the practice of being a medical practitioner; either the initial training to become a physician, additional training thereafter, and fellowship. Medical school List of medical schools Internship Residency Fellowship Medical research Medical research",
    "label": 0
  },
  {
    "text": "education Medical education – education related to the practice of being a medical practitioner; either the initial training to become a physician, additional training thereafter, and fellowship. Medical school List of medical schools Internship Residency Fellowship Medical research Medical research Clinical research (outline) Medical jargon Medical terminology List of medical roots, suffixes and prefixes Medical abbreviations and acronyms Acronyms in healthcare List of medical abbreviations: Overview List of medical abbreviations: Latin abbreviations List of abbreviations for diseases and disorders List of abbreviations for medical organisations and personnel List of abbreviations used in medical prescriptions List of abbreviations used in health informatics List of optometric abbreviations Medical glossaries Glossary of alternative medicine Glossary of anatomical terminology, definitions and abbreviations Glossary of clinical research Glossary of communication disorders Glossary of diabetes Glossary of medical terms related to communications disorders Glossary of medicine Glossary of psychiatry Medical organizations List of medical organisations List of LGBT medical organizations List of pharmacy associations Government agencies Centers for Disease Control and Prevention (US) Food and Drug Administration (US) National Academy of Medicine (US) National Institutes of Health (US) Medical publications List of important publications in medicine List of medical journals List of defunct medical journals List of medical and health informatics journals Persons influential in medicine List of physicians Medical scholars The earliest known physician, Hesyre. The first recorded female physician, Peseshet. Borsippa, a Babylonian who wrote the Diagnostic Handbook. The Iranian chemist, Rhazes. Avicenna, the philosopher and physician. Greco-Roman medical scholars: Hippocrates, commonly considered the father of modern medicine. Galen, known for his ambitious surgeries. Andreas Vesalius Oribasius, a Byzantine who compiled medical knowledge. Abu al-Qasim, an Islamic physician known as the father of modern surgery. Medieval European medical scholars: Theodoric Borgognoni, one of the most significant surgeons of the medieval period, responsible for introducing",
    "label": 0
  },
  {
    "text": "Andreas Vesalius Oribasius, a Byzantine who compiled medical knowledge. Abu al-Qasim, an Islamic physician known as the father of modern surgery. Medieval European medical scholars: Theodoric Borgognoni, one of the most significant surgeons of the medieval period, responsible for introducing and promoting important surgical advances including basic antiseptic practice and the use of anaesthetics. Guy de Chauliac, considered to be one of the earliest fathers of modern surgery, after the great Islamic surgeon, Abu al-Qasim. Realdo Colombo, anatomist and surgeon who contributed to understanding of lesser circulation. Michael Servetus, considered to be the first European to discover the pulmonary circulation of the blood. Ambroise Paré suggested using ligatures instead of cauterisation and tested the bezoar stone. William Harvey describes blood circulation. John Hunter, surgeon. Amato Lusitano described venous valves and guessed their function. Garcia de Orta first to describe Cholera and other tropical diseases and herbal treatments Percivall Pott, surgeon. Sir Thomas Browne physician and medical neologist. Thomas Sydenham physician and so-called \"English Hippocrates.\" Kuan Huang, who studied abroad and brought his techniques back to homeland china. Ignaz Semmelweis, who studied and decreased the incidence of childbed fever. Louis Pasteur and Robert Koch founded bacteriology. Alexander Fleming, whose accidental discovery of penicillin advanced the field of antibiotics. Pioneers in medicine Wilhelm Röntgen discovered x-rays, earning the first Nobel Prize in Physics in 1901, \"in recognition of the extraordinary services he has rendered by the discovery of the remarkable rays (or x-rays),\" and invented radiography. Christiaan Barnard performed the first heart transplant Ian Donald pioneered the use of the ultrasound scan, which led to its use as a diagnostic tool. Sir Godfrey Hounsfield invented the computed tomography (CT) scanner, sharing the 1979 Nobel Prize in Physiology or Medicine with Allan M. Cormack, \"for the development of computer assisted tomography.\" Sir Peter",
    "label": 0
  },
  {
    "text": "which led to its use as a diagnostic tool. Sir Godfrey Hounsfield invented the computed tomography (CT) scanner, sharing the 1979 Nobel Prize in Physiology or Medicine with Allan M. Cormack, \"for the development of computer assisted tomography.\" Sir Peter Mansfield invented the MRI scanner, sharing the 2003 Nobel Prize in Physiology or Medicine with Paul Lauterbur for their \"discoveries concerning magnetic resonance imaging.\" Robert Jarvik, inventor of the artificial heart. Anthony Atala, creator of the first lab-grown organ, an artificial urinary bladder. General concepts in medicine Epidemiology – study of the demographics of disease processes, and includes, but is not limited to, the study of epidemics. Nutrition – study of the relationship of food and drink to health and disease, especially in determining an optimal diet. Medical nutrition therapy is done by dietitians and is prescribed for diabetes, cardiovascular diseases, weight and eating disorders, allergies, malnutrition, and neoplastic diseases. Pharmacology – study of drugs and their actions. Psychology – an academic and applied discipline that involves the scientific study of mental functions and behaviors. Outline of nutrition List of macronutrients List of micronutrients Outline of emergency medicine List of emergency medicine courses List of surgical procedures List of eye surgical procedures List of disabilities List of disability-related terms with negative connotations List of medical emergencies List of eponymous fractures List of AIDS-related topics List of clinically important bacteria List of distinct cell types in the adult human body List of eponymous medical signs List of life extension-related topics List of medical inhalants List of medical symptoms List of oncology-related terms List of oral health and dental topics List of pharmaceutical companies List of psychotherapies List of vaccine topics Outline of autism Outline of exercise Outline of obstetrics (pregnancy and childbirth) Outline of psychology Pharmacology, for list of medicinal substances",
    "label": 0
  },
  {
    "text": "Alternative medicine is a term often used to describe medical practices where are untested or untestable. Complementary medicine (CM), complementary and alternative medicine (CAM), integrated medicine or integrative medicine (IM), functional medicine, and holistic medicine are among many rebrandings of the same phenomenon. Terms for alternative medicine The terms alternative medicine, complementary medicine, integrative medicine, holistic medicine, natural medicine, unorthodox medicine, fringe medicine, unconventional medicine, and new age medicine are used interchangeably as having the same meaning and are almost synonymous in most contexts. There is concern that a lack of stabilized terminology for these practices may give the appearance of effectiveness. Loose terminology may also be used to suggest that a dichotomy exists when it does not, e.g., the use of the expressions \"Western medicine\" and \"Eastern medicine\" to suggest that the difference is a cultural difference between the Asiatic east and the European west, rather than that the difference is between evidence-based medicine and other forms of treatment. Some scholars adopt this \"Western\" and \"Eastern\" language. For example, in a study done on musculoskeletal pain acupuncture treatment, researchers use the term \"Western Acupuncture\", which is defined as the acupuncture practices that are evidence based. This term also removes the cultural connotations that are used in acupuncture such as \"qi\", or \"primordial energy\". Complementary or integrative medicine Complementary medicine (CM) or integrative medicine (IM) is when alternative medicine is used together with mainstream medical treatment, in a belief that it improves the effect of treatments. For example, acupuncture (piercing the body with needles to influence the flow of a supernatural energy) might be believed to increase the effectiveness or \"complement\" science-based medicine when used at the same time. Instead, significant drug interactions caused by alternative therapies may make treatments less effective, notably in cancer therapy. Integrative medicine has been",
    "label": 0
  },
  {
    "text": "supernatural energy) might be believed to increase the effectiveness or \"complement\" science-based medicine when used at the same time. Instead, significant drug interactions caused by alternative therapies may make treatments less effective, notably in cancer therapy. Integrative medicine has been described as an attempt to bring pseudoscience into academic science-based medicine. Due to its many names, the field has been criticized by writer Rose Shapiro for what she describes as intense rebranding of what are essentially the same practices. CAM is an abbreviation of the phrase complementary and alternative medicine. The 2019 World Health Organization (WHO) Global Report on Traditional and Complementary Medicine states that the terms complementary and alternative medicine \"refer to a broad set of health care practices that are not part of that country's own traditional or conventional medicine and are not fully integrated into the dominant health care system. They are used interchangeably with traditional medicine in some countries.\" Other terms Traditional medicine comprises medical aspects of traditional knowledge that developed over generations within the folk beliefs of various societies, including indigenous peoples, before the era of modern medicine. The 2019 WHO study defines traditional medicine as \"the sum total of the knowledge, skill and practices based on the theories, beliefs and experiences indigenous to different cultures, whether explicable or not, used in the maintenance of health as well as in the prevention, diagnosis, improvement or treatment of physical and mental illness.\" Holistic medicine is another rebranding of alternative medicine. In this case, the words balance and holism are often used alongside complementary or integrative, claiming to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. The \"whole\" person idea is referring to the 'analysis of physical, nutritional, environmental, emotional, spiritual and lifestyle elements' as defined by the American Holistic Health",
    "label": 0
  },
  {
    "text": "to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. The \"whole\" person idea is referring to the 'analysis of physical, nutritional, environmental, emotional, spiritual and lifestyle elements' as defined by the American Holistic Health Association. When specific ailments are diagnosed, or health is being analyzed, things like mental health are factors that are also taken into consideration alongside physical health. According to a random national survey in America of US osteopathic physicians, less than a third of the respondents agreed with the fact that holism is a concept in osteopathic medicine that is distinct from allopathic medicine. This study highlights that osteopathic physicians view that practitioners in the allopathic field also practice holism in their respective practices. Functional medicine is a marketing term for alternative medicine created by Jeffrey Bland, who founded The Institute for Functional Medicine (IFM). Definitions of alternative medicine Alternative medicine is defined loosely as a set of products, practices, and theories that are believed or perceived by their users to have the healing effects of medicine, but whose effectiveness has not been established using scientific methods, or whose theory and practice is not part of biomedicine, or whose theories or practices are directly contradicted by scientific evidence or scientific principles used in biomedicine. \"Biomedicine\" or \"medicine\" is that part of medical science that applies principles of biology, physiology, molecular biology, biophysics, and other natural sciences to clinical practice, using scientific methods to establish the effectiveness of that practice. Unlike medicine, an alternative product or practice does not originate from using scientific methods, but may instead be based on hearsay, religion, tradition, superstition, belief in supernatural energies, pseudoscience, errors in reasoning, propaganda, fraud, or other unscientific sources. Challenges in defining alternative medicine Terminology has shifted over time, reflecting the preferred branding",
    "label": 0
  },
  {
    "text": "scientific methods, but may instead be based on hearsay, religion, tradition, superstition, belief in supernatural energies, pseudoscience, errors in reasoning, propaganda, fraud, or other unscientific sources. Challenges in defining alternative medicine Terminology has shifted over time, reflecting the preferred branding of practitioners. For example, the United States National Institutes of Health department studying alternative medicine, currently named the National Center for Complementary and Integrative Health (NCCIH), was established as the Office of Alternative Medicine (OAM) and was renamed the National Center for Complementary and Alternative Medicine (NCCAM) before obtaining its current name. Therapies are often framed as \"natural\" or \"holistic\", in apparent opposition to conventional medicine which is \"artificial\" and \"narrow in scope\", statements which are intentionally misleading. Prominent members of the science and biomedical science community say that it is not meaningful to define an alternative medicine that is separate from a conventional medicine, because the expressions \"conventional medicine\", \"alternative medicine\", \"complementary medicine\", \"integrative medicine\", and \"holistic medicine\" do not refer to any medicine at all. Others say that alternative medicine cannot be precisely defined because of the diversity of theories and practices it includes, and because the boundaries between alternative and conventional medicine overlap, are porous, and change. The systems and practices it refers to are diffuse, and its boundaries poorly defined. Healthcare practices categorized as alternative may differ in their historical origin, theoretical basis, diagnostic technique, therapeutic practice and in their relationship to the medical mainstream. Some alternative therapies, such as traditional Chinese medicine (TCM) and Ayurveda, have antique origins in East or South Asia and are entirely alternative medical systems; others, such as homeopathy and chiropractic, have origins in Europe or the United States and emerged in the eighteenth and nineteenth centuries. Some, such as osteopathy and chiropractic, employ manipulative physical methods of treatment; others, such",
    "label": 0
  },
  {
    "text": "entirely alternative medical systems; others, such as homeopathy and chiropractic, have origins in Europe or the United States and emerged in the eighteenth and nineteenth centuries. Some, such as osteopathy and chiropractic, employ manipulative physical methods of treatment; others, such as meditation and prayer, are based on mind-body interventions. Under a definition of alternative medicine as \"non-mainstream\", treatments considered alternative in one location may be considered conventional in another. Critics say the expression is deceptive because it implies there is an effective alternative to science-based medicine, and that complementary is deceptive because it implies that the treatment increases the effectiveness of (complements) science-based medicine, while alternative medicines that have been tested nearly always have no measurable positive effect compared to a placebo. It has been said that \"there is really no such thing as alternative medicine, just medicine that works and medicine that doesn't\", and that the very idea of \"alternative\" treatments is paradoxical because any treatment proven to work is by definition \"medicine.\" Different types of definitions Some definitions seek to specify alternative medicine in terms of its social and political marginality to mainstream healthcare. This can refer to the lack of support that alternative therapies receive from medical scientists regarding access to research funding, sympathetic coverage in the medical press, or inclusion in the standard medical curriculum. In 1993, the British Medical Association (BMA) stated that it referred to \"...those forms of treatment which are not widely used by the conventional healthcare professions, and the skills of which are not taught as part of the undergraduate curriculum of conventional medical and paramedical healthcare courses\". In a US context, a definition coined in 1993 by the Harvard-based physician David M. Eisenberg described alternative medicine as \"interventions neither taught widely in medical schools nor generally available in US hospitals\". In",
    "label": 0
  },
  {
    "text": "conventional medical and paramedical healthcare courses\". In a US context, a definition coined in 1993 by the Harvard-based physician David M. Eisenberg described alternative medicine as \"interventions neither taught widely in medical schools nor generally available in US hospitals\". In a definition published in 2000 by the World Health Organization (WHO), CAM was defined as a broad set of health care practices that are not part of that country's own tradition and are not integrated into the dominant health care system. A widely used descriptive definition devised by the US NCCIH calls it \"a group of diverse medical and health care systems, practices, and products that are not generally considered part of conventional medicine\". However, these descriptive definitions are inadequate in the present-day when some conventional doctors offer alternative medical treatments and introductory courses or modules can be offered as part of standard undergraduate medical training; alternative medicine is taught in more than half of US medical schools and US health insurers are increasingly willing to provide reimbursement for alternative therapies. In 1999, 7.7% of US hospitals reported using some form of alternative therapy; this proportion had risen to 37.7% by 2008. A 15-year systematic review published in 2022 on the global acceptance and use of CAM among medical specialists found the overall acceptance of CAM at 52% and the overall use at 45%. An expert panel at a conference hosted in 1995 by the US Office for Alternative Medicine (OAM), devised a theoretical definition of alternative medicine as \"a broad domain of healing resources ... other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period\". This definition has been widely adopted, and has been cited by the UK Department of Health, attributed as the definition used by the",
    "label": 0
  },
  {
    "text": "to the politically dominant health system of a particular society or culture in a given historical period\". This definition has been widely adopted, and has been cited by the UK Department of Health, attributed as the definition used by the Cochrane Collaboration, and, with some modification, was preferred in the 2005 consensus report of the US Institute of Medicine. This definition, an expansion of Eisenberg's 1993 formulation, is silent regarding questions of the medical effectiveness of alternative therapies. Its proponents hold that it thus avoids relativism about differing forms of medical knowledge and, while it is an essentially political definition, this should not imply that the dominance of mainstream medicine is solely due to political forces. According to this definition, alternative and mainstream medicine can only be differentiated with reference to what is \"intrinsic to the politically dominant health system of a particular society of culture\". However, there is neither a reliable method to distinguish between cultures and subcultures, nor to attribute them as dominant or subordinate, nor any accepted criteria to determine the dominance of a cultural entity. If the culture of a politically dominant healthcare system is held to be equivalent to the perspectives of those charged with the medical management of leading healthcare institutions and programs, the definition fails to recognize the potential for division either within such an elite or between a healthcare elite and the wider population. Evidence-based definitions distinguish alternative medicine based on its provision of therapies that are unproven, unvalidated, or ineffective and support of theories with no recognized scientific basis. These definitions characterize practices as constituting alternative medicine when, used independently or in place of evidence-based medicine, they are put forward as having the healing effects of medicine, but are not based on evidence gathered with the scientific method. Exemplifying this perspective,",
    "label": 0
  },
  {
    "text": "characterize practices as constituting alternative medicine when, used independently or in place of evidence-based medicine, they are put forward as having the healing effects of medicine, but are not based on evidence gathered with the scientific method. Exemplifying this perspective, a 1998 editorial co-authored by Marcia Angell, a former editor of The New England Journal of Medicine, argued that: It is time for the scientific community to stop giving alternative medicine a free ride. There cannot be two kinds of medicine – conventional and alternative. There is only medicine that has been adequately tested and medicine that has not, medicine that works and medicine that may or may not work. Once a treatment has been tested rigorously, it no longer matters whether it was considered alternative at the outset. If it is found to be reasonably safe and effective, it will be accepted. But assertions, speculation, and testimonials do not substitute for evidence. Alternative treatments should be subjected to scientific testing no less rigorous than that required for conventional treatments. This line of division has been subject to criticism on the grounds that not all forms of standard medical practice have adequately demonstrated evidence of benefit, and that most conventional therapies, if proven to be ineffective, would not later be classified as alternative. Another definition is that alternative medicine refers to a diverse range of related and unrelated products, practices, and theories ranging from biologically plausible practices and products and practices with some evidence, to practices and theories that are directly contradicted by basic science or clear evidence, and products that have been conclusively proven to be ineffective or even toxic and harmful. Proponents of an evidence-base for medicine such as the Cochrane Collaboration take a position that all systematic reviews of treatments, whether \"mainstream\" or \"alternative\", ought to be",
    "label": 0
  },
  {
    "text": "that have been conclusively proven to be ineffective or even toxic and harmful. Proponents of an evidence-base for medicine such as the Cochrane Collaboration take a position that all systematic reviews of treatments, whether \"mainstream\" or \"alternative\", ought to be held to the current standards of scientific method. In 2011, the Cochrane Collaboration proposed that indicators of a therapy's level of acceptance include government licensing of practitioners, coverage by health insurance, statements of approval by government agencies, and recommendation as part of a practice guideline; and that if something is currently a standard, accepted therapy, then it is not likely to be widely considered as alternative. Australia The public information website maintained by the National Health and Medical Research Council (NHMRC) of the Commonwealth of Australia uses the acronym \"CAM\" for a wide range of health care practices, therapies, procedures and devices not within the domain of conventional medicine. In the Australian context this is stated to include acupuncture; aromatherapy; chiropractic; homeopathy; massage; meditation and relaxation therapies; naturopathy; osteopathy; reflexology, traditional Chinese medicine; and the use of vitamin supplements. Denmark The Danish National Board of Health's Council for Alternative Medicine (Sundhedsstyrelsens Råd for Alternativ Behandling (SRAB)), an independent institution under the National Board of Health (Danish: Sundhedsstyrelsen), uses the term \"alternative medicine\" for: Treatments performed by therapists that are not authorized healthcare professionals. Treatments performed by authorized healthcare professionals, but those based on methods otherwise used mainly outside the healthcare system. People without a healthcare authorisation are [also] allowed to perform the treatments. Other terms Allopathic medicine Allopathic medicine or allopathy is a pejorative term used by proponents of alternative medicine to refer to modern scientific systems of medicine, such as the use of pharmacologically active agents or physical interventions to treat or suppress symptoms or pathophysiologic processes of diseases",
    "label": 0
  },
  {
    "text": "allopathy is a pejorative term used by proponents of alternative medicine to refer to modern scientific systems of medicine, such as the use of pharmacologically active agents or physical interventions to treat or suppress symptoms or pathophysiologic processes of diseases or conditions. The expression was coined in 1810 by the creator of homeopathy, Samuel Hahnemann (1755–1843). Among homeopaths and other alternative medicine advocates, the expression \"allopathic medicine\" is still used to refer to \"the broad category of medical practice that is sometimes called Western medicine, biomedicine, evidence-based medicine, or modern medicine.\" Use of the term remains common among homeopaths and has spread to other alternative medicine practices. The meaning implied by the label has never been accepted by conventional medicine and is still considered pejorative by some. William Jarvis, an expert on alternative medicine and public health, states that \"although many modern therapies can be construed to conform to an allopathic rationale (e.g., using a laxative to relieve constipation), standard medicine has never paid allegiance to an allopathic principle\" and that the label \"allopath\" was \"considered highly derisive by regular medicine.\" Many modern science-based medical treatments (antibiotics, vaccines, and chemotherapeutics, for example) do not fit Samuel Hahnemann's definition of allopathy, as they seek to prevent illness, or remove the cause of an illness by acting on the cause of disease. See also List of forms of alternative medicine Therapeutic nihilism Notes References Bibliography External links",
    "label": 0
  },
  {
    "text": "An anti-asthmatic agent, also known as an anti-asthma drug, refers to a drug that can aid in airway smooth muscle dilation to allow normal breathing during an asthma attack or reduce inflammation on the airway to decrease airway resistance for asthmatic patients, or both. The goal of asthmatic agents is to reduce asthma exacerbation frequencies and related hospital visits. Anti-asthmatic agents as rescue medications for acute asthma attacks include short-acting β2-adrenergic receptor agonists (SABA), short-acting muscarinic antagonists (SAMA), systemic glucocorticoids, and magnesium sulfate. Anti-asthmatic agents as maintenance medications for asthmatic symptom control include long-acting β2-adrenergic receptor agonists (LABA), inhaled glucocorticoids, long-acting muscarinic antagonists (LAMA), methylxanthines/phosphodiesterase inhibitors, leukotriene receptor antagonists, mast cell stabilizers, and certain types of monoclonal antibodies. Global Initiative of Asthma (GINA) is the official guideline on the usage of anti-asthmatic agents. The GINA guideline outlines the class, dosage, and administration of anti-asthmatic agents prescription depending on the severity of asthma symptoms and nature. Rescue medications Inhaled short-acting β2-adrenergic agonists Inhaled short-acting β2-adrenergic agonists, such as terbutaline and salbutamol, are the first-line drugs indicated for asthma exacerbation for all patients to provide rapid bronchodilating effects. Short-acting β2-adrenergic agonists can be delivered by different devices, for example, nebulizers and metered-dose inhalers. β2-adrenergic agonists can trigger the activation of Gs protein-coupled β2-adrenergic receptors on the airway smooth muscle cells in the lungs. The β2-adrenergic receptors activation allows the adenylyl cyclase within the airway smooth muscle cells to catalyse the conversion of ATP to cAMP. cAMP as a second messenger further activates protein kinase A and decreases the intracellular calcium level, causing subsequent smooth muscle relaxation. Common side effects of inhaled β2-adrenergic agonists include tremors, palpitations and headache. The incidence and severity of side effects depend on the dose and route of administration of the β2-adrenergic agonists. Inhaled short-acting muscarinic antagonists Inhaled",
    "label": 0
  },
  {
    "text": "subsequent smooth muscle relaxation. Common side effects of inhaled β2-adrenergic agonists include tremors, palpitations and headache. The incidence and severity of side effects depend on the dose and route of administration of the β2-adrenergic agonists. Inhaled short-acting muscarinic antagonists Inhaled short-acting muscarinic antagonists, such as oxitropium and ipratropium, can be used as an adjunct therapy with short-acting β2-adrenergic agonists in moderate to severe asthma exacerbations to achieve bronchodilation. Short-acting muscarinic antagonists are usually discontinued upon hospital admission due to a lack of benefits among hospitalized patients. Muscarinic antagonists can compete with acetylcholine for muscarinic receptors and provide an antagonistic effect on muscarinic receptors, causing inhibition of cholinergic bronchomotor tone and hence bronchodilation. Inhaled muscarinic antagonists commonly cause dry mouth, throat irritation and dizziness. Systemic glucocorticoids Systemic glucocorticoids, such as oral prednisolone and intravenous hydrocortisone, are indicated for moderate to severe asthma exacerbation to reduce airway inflammation. It is important for patients with refractory asthma exacerbation who are already on intensive bronchodilator therapy as airflow resistance in the airway is likely to be caused by mucus accumulation and inflammation on the airway. Systemic administration of glucocorticoids can reduce airway mucus production. It can also suppress inflammatory responses by inhibiting the synthesis and release of inflammatory mediators and lowering the infiltration and activity of inflammatory cells. Additionally, glucocorticoids can increase the amount of β2-adrenergic receptors and their sensitivity towards β2-adrenergic agonists on the airway smooth muscles. The use of systemic glucocorticoids may cause depressed immunity, osteoporosis and Cushing's syndrome. The side effects of glucocorticoids depend on the dose and duration of treatment. Magnesium Sulfate Magnesium sulfate is indicated for severe or life-threatening asthma exacerbation to achieve bronchodilation. Intravenous magnesium sulfate can reduce calcium ions influx into smooth muscle cells on the airway, causing airway muscle relaxation. It is possible for intravenous magnesium",
    "label": 0
  },
  {
    "text": "Magnesium Sulfate Magnesium sulfate is indicated for severe or life-threatening asthma exacerbation to achieve bronchodilation. Intravenous magnesium sulfate can reduce calcium ions influx into smooth muscle cells on the airway, causing airway muscle relaxation. It is possible for intravenous magnesium sulfate to cause hypermagnesemia, resulting in muscle weakness. Intravenous magnesium sulfate is contraindicated in patients with renal insufficiency. Maintenance medications Long-acting β2-adrenergic agonists Long-acting β2-adrenergic agonists, for example, vilanterol, indacaterol, olodaterol, formoterol and salmeterol, are commonly used together with inhaled corticosteroid in maintenance treatment. Inhaled glucocorticoids Inhaled corticosteroids are commonly used together with long-acting β2-adrenergic agonists in maintenance therapy as corticosteroids can increase the amount of airway bronchial β2-receptors and their sensitivity towards β2-selective agents. The use of inhaled corticosteroid may commonly cause dysphonia and overgrowth of oropharyngeal candidiasis. The risk of overgrowth of oropharyngeal candidiasis can be reduced by rinsing the mouth with water after use. Long-acting muscarinic antagonists Long-acting muscarinic antagonists, including tiotropium, aclidinium and umeclidinium, are indicated for severe asthma in maintenance treatment. Muscarinic antagonists can reduce cholinergic bronchomotor tone, resulting in airway muscle relexation and bronchodilation. Muscarinic antagonists commonly cause dry mouth, throat irritation and dizziness. Methylxanthine / Phosphodiesterase Inhibitors Methylxanthines, including theophylline, aminophylline and dyphylline, are a class of drugs that can achieve bronchodilation and reduce bronchospasm for symptomatic control of asthma. Methylxanthines act as a competitive inhibitor of phosphodiesterase, inhibiting phosphodiesterase degradation action of cyclic 3′,5′-adenosine monophosphate (cAMP). This resulted accumulation of cAMP relaxes smooth muscles, leading to dilation of airways. Methylxanthines activate histone deacetylases, promoting the deacetylation of histone and subsequent DNA folding. This inhibits the synthesis of pro-inflammatory factors that induce asthma attacks and exacerbations, achieving anti-inflammatory effects. For asthma maintenance therapy, methylxanthines are taken orally. Therapeutic drug monitoring is required for patients on methylxanthines as the therapeutic range is narrow. Methylxanthines",
    "label": 0
  },
  {
    "text": "This inhibits the synthesis of pro-inflammatory factors that induce asthma attacks and exacerbations, achieving anti-inflammatory effects. For asthma maintenance therapy, methylxanthines are taken orally. Therapeutic drug monitoring is required for patients on methylxanthines as the therapeutic range is narrow. Methylxanthines are not routinely used owing to their adverse effect profiles and the risk of toxicity. Adverse effects of Methylxanthines include nervousness, insomnia, irritability, anxiety, gastrointestinal disturbance (nausea, vomiting), tremor, palpitation and increased urine output. Leukotriene Receptor Antagonists Leukotriene receptor antagonists, including montelukast and zafirlukast, inhibit pro-inflammatory leukotrienes bindings to LTC4 and LTD4 receptors. This blocks the downstream inflammatory pathways that lead to bronchospasm and smooth muscle contractions in asthmatic patients. Leukotriene receptor antagonists are taken orally. Common adverse effects of leukotriene receptor antagonists include headache, abdominal pain and diarrhoea. Mast cell stabilizers Mast Cell Stabilizers, including sodium cromoglycate, nedocromil sodium, amlexanox, pemirolast potassium, repirinast and tranilast are drugs that inhibit the degranulation and activation of mast cells upon contact with antigen. This prevents the subsequent release of pro-inflammatory mediators such as histamines and leukotrienes. Mast cell stabilizers are given as prophylactic treatment to prevent exacerbation of asthmatic symptoms. For asthma maintenance therapy, mast cell stabilizers are taken by inhalation. Common adverse effects of Mast cell stabilizers include mouth dryness, cough, throat irritation, nasal congestion and bronchospasm. Monoclonal Antibodies Monoclonal Antibodies that aid in asthma symptomatic control include omalizumab, mepolizumab, reslizumab, benralizumab, dupilumab and tezepelumab. Omalizumab binds to free human immunoglobulin (IgE) to reduce IgE level in circulation. This reduces the subsequent binding of IgE to the IgE receptors on inflammatory cells, including mast cells, basophils and dendritic cells. The release of inflammatory mediators is then prevented. Mepolizumab and reslizumab inhibit Interleukin (IL)-5 binding with IL-5 receptors on the surface of eosinophils, inhibiting subsequent inflammatory responses. Benralizumab blocks the IL-5 receptors",
    "label": 0
  },
  {
    "text": "cells, including mast cells, basophils and dendritic cells. The release of inflammatory mediators is then prevented. Mepolizumab and reslizumab inhibit Interleukin (IL)-5 binding with IL-5 receptors on the surface of eosinophils, inhibiting subsequent inflammatory responses. Benralizumab blocks the IL-5 receptors on basophils, preventing binding of IL-5 with IL-5 receptors on basophils, inhibiting subsequent inflammatory responses. Dupilumab blocks IL-4 receptors, inhibiting subsequent inflammatory activities of IL-4 and IL-13. Tezepelumab binds to thymic stromal lymphopoietin (TSLP), which is an inflammatory cytokine in the airway epithelial cells involved in asthma exacerbations, inhibiting subsequent inflammatory responses. Monoclonal antibodies for the treatment of asthmatic symptoms are given by subcutaneous injections. Common adverse effects include local site reactions, joint pain, back pain, headache and sore throat. Treatment steps GINA guideline According to the Global Initiative of Asthma (GINA), the guideline for anti-asthmatic treatment is divided into 5 levels according to asthma severity. For newly diagnosed asthma patients, the 5 levels derived from the severity of asthma depend on the occurrence of symptoms and their frequencies. These symptoms include bronchoconstriction, shortness of breath and wheezing that exacerbates after physical activities. Frequent coughing, chest tightness and breathing difficulties are also signs of asthma worsening. These symptoms can interfere with a patient's daily living and affect quality of life. These 5 levels are indicators of what drug treatments should be administered. The guideline is as follows: Step 1-2: Symptoms less than 4–5 days a week Low-dose inhaled corticosteroids and formoterol combination therapy when required Step 3: Symptoms most days, or waking with asthma once a week or more Low-dose inhaled corticosteroids and formoterol maintenance therapy Step 4: Daily symptoms, or waking with asthma once a week or more, and low lung function Medium dose inhaled corticosteroids and formoterol maintenance therapy Short-course oral corticosteroids when required in severely uncontrolled asthma",
    "label": 0
  },
  {
    "text": "Breastmilk medicine refers to the non-nutritional usage of human breast milk (HBM) as a medicine or therapy to cure diseases. Breastmilk is perceived as an important food that provides essential nutrition to infants. It also provides protection in terms of immunity by direct transfer of antibodies from mothers to infants. The immunity developed via this mean protects infants from diseases such as respiratory diseases, middle ear infections, and gastrointestinal diseases. HBM can also produce lifelong positive therapeutic effects on a number of chronic diseases, including diabetes mellitus, obesity, hyperlipidemia, hypertension, cardiovascular diseases, autoimmunity, and asthma. Therapeutic use of breastmilk has long been a part of natural pharmacopeia, and ethnomedicine. The effectiveness of HBM and fresh colostrum as a treatment for inflammatory disorders such as rhinitis, skin infection, soring nipples, and conjunctivitis has been reported by public health nurses. Currently, many breastmilk components have shown therapeutic benefits in preclinical studies and are being evaluated by clinical studies. Anti-inflammatory effects HBM can be used to treat inflammations. Breastfeeding has an anti-inflammatory effect that is conveyed by its chemical components' interaction with body cells. The major chemical component that produces the anti-inflammatory effect in both colostrum and transitional milk are glycoprotein and lactoferrin. Lactoferrin has multiple actions including lymph-stimulatory, anti-inflammatory, anti-bacterial, anti-viral, and anti-fungal effects. The anti-inflammatory effects of lactoferrin are attributed to its iron-binding properties, inhibition of inflammation-causing molecules including interleukin-1β (IL-1β) and tumor necrosis factor-alpha (TNF-α), stimulation of the activity and maturation of lymphocytes as well as preservation of an antioxidant environment. Besides, lactoferrin protects infants against bacterial and fungal infections in combination with other peptides present in HBM. Respiratory viral infection in infants Lactoferrin in HBM can also inhibit the invasion and proliferation of respiratory syncytial virus (RSV), which is a virus commonly found in the human respiratory tract and",
    "label": 0
  },
  {
    "text": "in combination with other peptides present in HBM. Respiratory viral infection in infants Lactoferrin in HBM can also inhibit the invasion and proliferation of respiratory syncytial virus (RSV), which is a virus commonly found in the human respiratory tract and causes mild cold-like symptoms. Lactoferrin can interact directly with the F glycoprotein which is a protein on the surface of the virus that is responsible for presenting the virus to body cells and causing infections. Adenovirus is another group of viruses that targets the mucosal membrane of the human respiratory tract. It usually causes mild to severe infection with symptoms like the common cold or flu. Lactoferrin can prevent infection of adenovirus since it can interfere with the primary receptors of the virus. HBM regurgitation into the nose after breastfeeding is a way to eliminate these mucosal bacteria and protect infants against recurring nose infections in breastfed infants in the long term. Skin Problems: atopic eczema and diaper dermatitis Atopic eczema is an inflammatory disorder that occurs in the outermost skin layer called the epidermis. This skin disorder affects 50% of infants in the first year after birth. Infants suffering from atopic eczema are characterized by intense itching, redness, and crusting in their skin. Skin thickening may result in chronic or sub-acute patients due to scratching and fissuring over time. One of the commonly used medications for atopic eczema is non-prescription cream containing an anti-inflammatory agent 1% hydrocortisone. On the other hand, applying HBM on the skin as ointments is therapeutically beneficial to infants with mild to moderate atopic eczema. It is evidenced that, compared to 1% hydrocortisone, HBM has similar effectiveness as 1% hydrocortisone to relieve infants' inflaming skin conditions. Diaper dermatitis is another prevalent infant dermatological disorder. Common clinical features of diaper dermatitis include inflamed, itchy, tender skin",
    "label": 0
  },
  {
    "text": "It is evidenced that, compared to 1% hydrocortisone, HBM has similar effectiveness as 1% hydrocortisone to relieve infants' inflaming skin conditions. Diaper dermatitis is another prevalent infant dermatological disorder. Common clinical features of diaper dermatitis include inflamed, itchy, tender skin and soreness in the diaper area. Study results have shown that human breastmilk is highly effective in healing diaper rash. There is much evidence supporting the anti-inflammatory effect of HBM. The immunological components in HBM help strengthen the baby's immune system. These immunological components include antimicrobial proteins that can inhibit or kill a wide range of pathogens whose invasion may lead to an inflammatory response. This antimicrobial effect could be achieved by indirectly creating an unfavorable environment for the growth of pathogens by modifying commensal flora, pH, or level of bacterial substrates. The antimicrobial effect is also brought by an antibody immunoglobulin A (IgA) which is the prenominal immunoglobulin present in HBM that can protect infants from a variety of skin infections. Nipple problems: sore nipples The painful nipple is a common difficulty confronted by mothers who decided to carry out breastfeeding. Topical application of expressed breastmilk has long been a non-pharmacological intervention to reduce nipple pain. According to the research outcome of many studies, topical application of HBM can help reduce the perception of nipple pain in a treatment course of 4 to 5 days. It is also stated that HBM is more effective in lowering pain perception than Lanolin. However, another study indicates that Lanolin produces lower pain levels in mothers with nipple pain than HBM. This study stated that lanolin shows a better therapeutic effect for healing rates, nipple trauma, and nipple pain. Although lanolin may be more efficacious than HBM to cure nipple problems, HBM is not proven to be ineffective to treat nipple pain. Considering",
    "label": 0
  },
  {
    "text": "stated that lanolin shows a better therapeutic effect for healing rates, nipple trauma, and nipple pain. Although lanolin may be more efficacious than HBM to cure nipple problems, HBM is not proven to be ineffective to treat nipple pain. Considering HBM is more easily available than Lanolin, it is still useful for treating nipple problems in a practical sense. Eye Problems Traditional uses The topical application of breastmilk as a treatment for an infectious disease called conjunctivitis has been present since ancient times in different nations such as ancient Egypt, Rome, Greece, etc. HBM was also recommended by the Greek physician Galen as a remedy for conjunctivitis. Physicians in early modern England recommended human milk for conditions ranging from mild symptoms such as soreness to even blindness. Healers in that era even believed that a mixture of HBM with other components could restore eyesight. Scientific findings Evidence from clinical research has shown that applying HBM can prevent people from getting conjunctivitis infections. Gonorrhea is a sexually transmitted infection. Besides sex-borne transmission, it can also be transmitted to babies during childbirth. This infectious disease can cause neonatal conjunctivitis, which could lead to blindness, if untreated. Hospitals in the United States are required to apply antibiotics to the eyes of new-born within one hour of childbirth to prevent the development of conjunctivitis. This is because certain bacteria in HBM are found to be effective against gonorrhea bacteria and may serve as a convenient and readily available substitute for antibiotics in places where antibiotics are not widely available. Breastmilk in umbilical cord care After labor in childbirth, the umbilical cord is clamped and cut, and part of it stays in contact with the infant. The remaining part of the umbilical cord dries out and eventually separates after 5–15 days. In taking care of",
    "label": 0
  },
  {
    "text": "After labor in childbirth, the umbilical cord is clamped and cut, and part of it stays in contact with the infant. The remaining part of the umbilical cord dries out and eventually separates after 5–15 days. In taking care of the umbilical cord, the dry care method involving soap and water is recommended by the WHO and many national health organizations. For the use of HBM in umbilical cord care, clinical studies found that topical application of breast milk will lead to a shorter time of cord separation than other methods including ethanol and dry cord care. Anti tumoricidal and anti-bacterial effects Human alpha-lactalbumin is a natural protein component of HBM. It can be extracted by chromatography from breast milk. It complexes with oleic acid to form a protein called the \"human alpha-lactalbumin made lethal to tumor cells\" (HAMLET). The HAMLET complex induces apoptosis in lung carcinoma cells. In in vitro and animal model studies, HAMLET has shown preventative and therapeutic effects in reducing and controlling tumor growth. The physiological effects of HAMLET may explain the proposal that breastfeeding has protective effects for mothers and children against cancer, as shown by the association length of breastfeeding and childhood cancer incidence. The HAMLET has also been found to have anti-bacterial effects through the inhibition of enzymes in glycolysis. Role in society Researchers' interest in HBM is led by the discovery of a number of chemical components in HBM. These components include growth factors, cytokines, and a heterogeneous population of cells which are stem cells, probiotic bacteria, and the HAMLET complex. By considering the easy accessibility of HBM and high prevalence of infant inflammation disorders, breastmilk may be a cheap and convenient ways to relieve inflammatory symptoms. The prophylactic antibiotic use of human milk may be important in areas where mothers and",
    "label": 0
  },
  {
    "text": "considering the easy accessibility of HBM and high prevalence of infant inflammation disorders, breastmilk may be a cheap and convenient ways to relieve inflammatory symptoms. The prophylactic antibiotic use of human milk may be important in areas where mothers and infants do not have easy access to medicine, such as people living in developing countries. Under these circumstances, practice of HBM therapy as medicine will be a determining factor in infant recovery and survival. General limitations Breastfeeding difficulties Breastfeeding may not be feasible and easy for some mothers due to psychological or physiological reasons. For instance, breastfeeding self-efficacy, the mother's confidence in her breastfeeding abilities, is positively associated with exclusive breastfeeding while postpartum depression makes it more difficult to breastfeed. Mothers who have undergone breast surgeries such as mastectomy may have reduced capabilities of HBM production. Suitability of breastmilk For some individuals, HBM may not be suitable for use, as it may transmit of viruses and other pathogens to infants. For instance, cytomegalovirus, HIV, and bacterial infections from the mother may be transmitted through HBM, causing complications for infants. Evaluation of medical effectiveness of breastmilk There is difficulty in the generalization of study results in evidence-based practice due to inconsistencies in the clinical study findings on breastfeeding medicine. HBM compositions are diverse among different individuals, or the same individual at various times. It is influenced by factors such as maternal diet and changes at various times after pregnancy. For instance, protein composition in HBM is higher in the earlier stages of lactation. Gradually, the mother produces more mature milk, which is whiter in color, compared to the yellowish colostrum. These changes may affect the effectiveness of HBM in medical use. References",
    "label": 0
  },
  {
    "text": "Confocal endoscopy, or confocal laser endomicroscopy (CLE), is a modern imaging technique that allows the examination of real-time microscopic and histological features inside the body. In the word \"endomicroscopy\", endo- means \"within\" and -skopein means \"to view or observe\". CLE, also known as \"optical biopsy\", can analyse histology and cytology features of a tissue which otherwise is only possible by tissue biopsy. Similar to confocal microscopy, the laser in CLE filtered by the pinhole excites the fluorescent dye through a beam splitter and objective lens. The fluorescent emission then follows similar paths into the detector. A pinhole is used to select emissions from the desired focal plane. Two categories of CLE exist, namely probe-based (pCLE) and the less common endoscopy-based endoscopy (eCLE). CLE can be intubated to study the gastrointestinal (GI) tract and accessory digestive organs with a fluorescent dye. A variety of diseases, including inflammatory bowel disease (IBD) and Barrett's oesophagus, can be diagnosed by the magnified and in-depth view in combination with traditional endoscopy. Significance CLE can identify the lesions with a small depths of view under the tissue, in contrast to the surface level in conventional endoscopy. It also allows clinicians to discriminate benign or malignant lesions through real-time histological diagnosis by revealing the properties of the lamina at a cellular level. An example is Whipple's disease. Conventional endoscopy presents a whitish-patterned duodenal mucosa. CLE, in comparison, generates two images –– the superficial images show capillary leak in duodenal mucosa while the deep images show cells of duodenal mucosa, including goblet cells and foamy macrophages in lamina propria. Compared to histological examination of the same duodenal site after periodic acid-Schiff staining, CLE identifies similar patterns of goblet cells and foamy macrophages. Types Two types of CLE have been invented, namely probe-based (pCLE) and endoscope-based CLE (eCLE). Probe-based",
    "label": 0
  },
  {
    "text": "propria. Compared to histological examination of the same duodenal site after periodic acid-Schiff staining, CLE identifies similar patterns of goblet cells and foamy macrophages. Types Two types of CLE have been invented, namely probe-based (pCLE) and endoscope-based CLE (eCLE). Probe-based CLE pCLE, developed by Mauna Kea Technologies, is a fibre bundle transit through the 2.8 mm working channel (the hollow hole) of the standard endoscope into the GI tract. With a fixed plane of imaging, each fibre acts as a pinhole to filter unwanted noise. The frame rate lies between 9 and 12 images/second. Endoscope-based CLE eCLE, developed by Pentax, is a confocal microscopy fixed at the end of the endoscopic tube. The integrated machine of eCLE is larger than the pCLE in diameter, making GI tract endoscopic intubation more difficult. eCLE has ceased commercially due to the camera's inflexibility. Medical uses Oesophagus CLE is effective in detecting premalignant, including Barrett's oesophagus, and malignant (cancerous) lesions in the upper GI tract. The modifications of mucosa shown in histopathology as an index of malignancy can be identified under CLE, such as high-grade dysplasia. CLE can also be implemented to refer to the treatment of Barrett's oesophagus by measuring the lateral extent of neoplasia. The Miami classification is the most popular system in oesophageal CLE diagnosis. Stomach and duodenum Similar to that of the oesophagus, CLE is able to detect early gastric cancer, as well as premalignant conditions, such as gastritis and intestinal metaplasia. CLE can detect and distinguish the stomach pit patterns to identify the disease in accordance with the Miami classification, which was refined in 2016 to include both pit patterns and the architecture of blood vessels. The refined classification allows clinicians to differentiate between neoplastic and non-neoplastic lesions. The presence of Helicobacter pylori can also be identified using CLE",
    "label": 0
  },
  {
    "text": "classification, which was refined in 2016 to include both pit patterns and the architecture of blood vessels. The refined classification allows clinicians to differentiate between neoplastic and non-neoplastic lesions. The presence of Helicobacter pylori can also be identified using CLE by viewing the morphological changes in tissues. Lower Gastrointestinal Tract CLE reveals \"soccer ball-like pattern\" of narrower capillaries in malignant lymphomas; distorted architecture and fluorescein leakage from lumen in colonic adenocarcinoma; and blunt-shaped villi and crypts and increased intraepithelial lymphocytes in coeliac disease. CLE can be utilized to identify adenoma and neoplasia in colorectal polyps and lesions. The Miami classification provides guidelines for clinicians to differentiate neoplastic and non-neoplastic lesions. Inflammatory Bowel Disease (IBD) CLE can be used for the identification of IBD and its subtypes (Crohn's disease and ulcerative colitis) based on the observation on morphological characteristics, such as architectural distortion, lowered crypt density, crypt irregularity and an abnormally high density of epithelial gaps. The prediction of IBD progression on non-inflamed epithelium is achievable, too, making way for a novel \"treat-to-target\" therapeutic approach. Pancreas Incorporating an EUS, CLE can accurately diagnose pancreatic cystic lesions, including mucinous and non-mucinous lesions. Special needles are used to collect fluid and cyst wall tissues for testing. Pancreatic ductal adenocarcinoma (PDAC) can also be viewed by CLE. Observing cystic lesions and PDAC, clinicians can identify early chronic pancreatitis and determine the malignancies of lesions. Biliary duct Biliary stricture can be viewed by CLE. The Miami and Paris classifications can be adapted to differentiate cancerous and inflammatory causes. Others The discrimination of inflammation and malignant tumor in lung and the urinary system may be done by using CLE and this is currently under research. Some usages such as oral and other head-and-neck cancer diagnosis have been proposed. Molecular imaging Antibodies of molecular targets are used",
    "label": 0
  },
  {
    "text": "tumor in lung and the urinary system may be done by using CLE and this is currently under research. Some usages such as oral and other head-and-neck cancer diagnosis have been proposed. Molecular imaging Antibodies of molecular targets are used to diagnose GI diseases by histology. CLE captures the fluorescence produced by specific antibodies binding to vascular endothelial growth factor (VEGF). Comparing the significant difference in fluorescent strength, clinicians can differentiate normal and neoplastic tissue. Molecular imaging with antibodies may be applied to CLE as a diagnostic benchmark due to high correlation with ex vivo microscopy. The molecular imaging technique can be used in a similar manner in the examination of head and neck cancer using CLE, though the diagnostic targets may be different from those in the gastrointestinal tract. Mechanism Basic mechanism The laser emitted by CLE through a pinhole is reflected by the beam splitter or a dichroic mirror and focused by an objective lens. The fluorescent dye in targeted tissue is excited and emits a specific wavelength. The emission from the focal plane of the tissue then is collected by the objective lens and the beam splitter. The laser is eventually filtered by a pinhole to reduce out-of-focus noise to enter the detector or photomultiplier tube. By including only light reflected from the same focal plane and excluding any other reflective scatter, the system captures a small, highly detailed section of the tissue. This illumination and detection process is repeated across the whole tissue sample to build a complete image. Limiting the focal plane in this way allows for optical sectioning which reduces the haze observed in standard light microscopy. Topical dyes Cresyl violet and acriflavine can be used as topical dyes. Cresyl violet is a common stain in histology used for light microscopy sections, especially brain",
    "label": 0
  },
  {
    "text": "allows for optical sectioning which reduces the haze observed in standard light microscopy. Topical dyes Cresyl violet and acriflavine can be used as topical dyes. Cresyl violet is a common stain in histology used for light microscopy sections, especially brain sections. In CLE, it can enhance the viewing of the cytoplasm, yet it limits tissue penetration and does not show anything about vasculature. Acriflavine is an antiseptic and dye. In CLE, it can stain the nuclei of GI surface epithelial cells. It is however subjected to cytotoxic and mutagenic properties, in addition to common side effects of irritation. Intravenous dyes Fluorescein is the most popular IV dye for CLE. Fluorescein is an FDA-cleared dye that is used in ophthalmology clinics in routine as it appears green under cobalt blue light. It is commonly applied topically to identify corneal diseases with slit lamp microscopes including corneal abrasion, ulcers, and infections; or intravenously to identify retinal diseases with angiography including macular degeneration and diabetic retinopathy. In CLE, it is usually administered intravenously immediately before the intubation of an endoscopic tube. The fluorescence is reported to be the most prominent from a few seconds to 8 minutes. Fluorescein is slowly eliminated; thus the fluorescence slowly decays up to a minimal detectable level after 1 hour, giving a time window for clinicians to investigate. Recognition and optical flow algorithm CLE's narrow field of vision makes it difficult for clinicians to identify the location and path of the probe, making it challenging to correspond the image obtained and the lesion location and direction. Research has proposed a crypt recognition algorithm, which predicts the pixel displacement by the moving angle and distance. By restoring the exploration path of CLE, clinicians can locate the sites of interest and improve diagnostic efficiency. Resolution The resulting image resolution is",
    "label": 0
  },
  {
    "text": "proposed a crypt recognition algorithm, which predicts the pixel displacement by the moving angle and distance. By restoring the exploration path of CLE, clinicians can locate the sites of interest and improve diagnostic efficiency. Resolution The resulting image resolution is determined by the numerical aperture (NA) of the objective lens, the refraction index of the surrounding media (η), the wavelength of light (λ), and the pinhole size. A shorter wavelength allows smaller features to be distinguished and a higher NA allows the system to collect more light and resolve finer details. Similarly, smaller pinholes can block scattered light, although if too small can reduce signal intensity. The refractive index of the media surrounding the tissue also impacts the final image quality by influencing how laser’s light is focused. Ultimately lateral and axial resolution can be defined by: Rlateral = 0.4λ / NA Raxial = 1.4λη / (NA)2 Image quality assessment Research has proposed a new assessment method for filtering images yielded from CLE. As CLE often encounters image distortions, the degradation of image quality and loss of image information, eventually increase the difficulty of accurate diagnosis. A new image quality assessment (IQA) utilising Weber's Law and local descriptors assesses the quality and filters images with low diagnostic value. Limitations The variety of pathology conditions identifiable by CLE is limited. The histological diagnosis is limited to cancerous lesions and inflammation where the number of specific diseases identifiable is not numerous. Moreover, it requires specific training to operate CLE and correctly interpret CLE images, which are rarely used skills by experts in endoscopy. Owing to the narrow field of view, the applications of CLE might be restricted. Computer-aided diagnosis with AI technology may be beneficial in diagnosing CLE images. Fluorescein is the only safe dye approved while cresyl violet and acriflavine are",
    "label": 0
  },
  {
    "text": "endoscopy. Owing to the narrow field of view, the applications of CLE might be restricted. Computer-aided diagnosis with AI technology may be beneficial in diagnosing CLE images. Fluorescein is the only safe dye approved while cresyl violet and acriflavine are commonly used agents. The lack of choice of contrast agents may limit the application of CLE. For instance, patients allergic to fluorescein should never undergo CLE that involves the use of this intravenous dye. The optical system consists of complex microscopic optical instruments, which are difficult to manufacture and assemble. Therefore, the tool is expensive. CLE is mostly used in combination with other techniques instead of replacing conventional endoscopy with biopsy. CLE can only serve as a complementary to the traditional biopsy. By sharing the same working channel, conventional biopsy and CLE can be done alternatively by single intubation. Adverse effects CLE is generally considered a safe imaging technique due to the low-power lasers and minimally invasive probes. The laser used in non-ionizing, blue visible light that would not cause tissue damage. Additionally, procedures only involve brief exposure times making this technique well tolerated and safe for repeated use. The allergic properties of fluorescein, the common intravenous fluorescent dye for CLE, is the major culprit for the mild adverse events. Intubation CLE, similar to other diagnostic endoscopic techniques, may give rise to pancreatitis when used to examine the pancreas. The likelihood for pancreatitis is especially high in needle-based CLE. The incidence can be minimized by shortening the inspection time and avoiding excessive needle movement within the pancreatic cyst wall. Fluorescein Mild side effects, which are rare, include Nausea Vomiting with mild epigastric pain Rash at injection site Transient hypotension without shock Diffuse erythema These effects are manageable unless patients have prior experiences of them. Cases of anaphylaxis are reported by",
    "label": 0
  },
  {
    "text": "Mild side effects, which are rare, include Nausea Vomiting with mild epigastric pain Rash at injection site Transient hypotension without shock Diffuse erythema These effects are manageable unless patients have prior experiences of them. Cases of anaphylaxis are reported by ophthalmological uses of fluorescein. Prophylactic use of antihistamines can reduce the chances of allergic reactions or skin prick tests can identify the risk of allergic reactions. Acriflavine Acriflavine, another contrast agent for CLE, is potentially carcinogenic to humans due to its known mutagenic ability. The dye is therefore not approved by the FDA. Regulations CLE is currently regulated both in the US and Europe to ensure its safety and clinical effectiveness. In the United States, the Food and Drug Administration (FDA) has cleared two main types of CLE systems, one of them being the Cellvizio series by Mauna Kea Technologies through the 510(k) process. These include both probe-based and endoscope-based systems, intended for imaging internal microstructures of tissues accessed during standard endoscopic procedures. FDA approval covers various mini probes for specific anatomical indications, and recent updates have extended approval to new models and the use of additional contrast agents. Additionally, in Europe, CLE devices (including the Cellvizio system and related miniprobes) are regulated as Class IIa medical devices and must bear CE marking to be distributed and used clinically. History CLE is a modern, in vivo adaptation of confocal microscopy, the microscopic technique invented by Marvin Minsky in 1957. Since 2004, CLE has been used for observing histopathological changes in gastrointestinal tissues. See also Confocal microscopy Endoscopy Contrast agent References",
    "label": 0
  },
  {
    "text": "Diabetes self-management refers to the ongoing process in which individuals with diabetes actively participate in managing their condition through lifestyle choices, medication adherence, blood glucose monitoring, and education, aimed at maintaining optimal blood sugar levels and preventing complications. Diabetes is a chronic disease which affected over 537 million adults worldwide in 2021; it is predicted to reach 643 million people by 2030. It is a global health burden and improving the health outcomes for people with diabetes is critical to reducing the economic and human burden of diabetes. Self-management is the cornerstone for successful health outcomes in diabetes patients as there is a positive association between self-management behaviour and care outcomes. Self-management stresses the importance of the role of an individual and their responsibility in developing skilled behaviours to manage one's own illness. Self-management activities Healthcare organisations are increasingly focusing on providing diabetes self-management education and support programs to enable diabetes patients to effective self-management. Diabetes patients face daily challenges due to the impact of their decisions on their health outcomes. Diabetes self-management helps diabetes patients to make better decisions and change their behaviour to achieve better outcomes. Diabetes self-management activities mainly consist of seven self-care behaviours. They are healthy eating, monitoring indicators of diabetes, physical activity, taking medication, healthy coping, and problem-solving. Foot care Diabetes patients with peripheral neuropathy and peripheral artery disease are at risk of developing foot ulcers and infection. Poor knowledge about self-care increases the risk of amputation. Adoption of suitable preventative measures and early treatment of diabetic foot problems are important components of diabetes foot care. Good knowledge and practice regarding diabetes foot care can reduce the risk of foot complications and amputation. Regular examination of the foot is one of the fundamental steps to modifying the foot risk factors thereby reducing the risk of",
    "label": 0
  },
  {
    "text": "care. Good knowledge and practice regarding diabetes foot care can reduce the risk of foot complications and amputation. Regular examination of the foot is one of the fundamental steps to modifying the foot risk factors thereby reducing the risk of ulceration and amputation. Footwear tailored to the specific pathology of the patient can enable conservative management of the foot including debridement of the callus. Appropriate footwear can reduce abnormal pressure, reduce the rate of formation callus and ulcers and protect the foot from external trauma. Blood glucose monitoring Regular monitoring of blood glucose and optimal glucose control is a major part of diabetes self-management. Diabetes patients need to be capable of testing blood sugar at home at the recommended frequency. Frequent self- monitoring of blood glucose and record keeping is key to identifying the possibility of hypoglycemia. Diabetes patients should be able to know how to respond when blood sugar levels are too high or too low. Medication Effective medication is the cornerstone of the proper treatment of diseases. Many patients fail to take the medication as prescribed and many patients prematurely discontinue their medication. Poor medication adherence in patients with diabetes is a costly public health challenge in many healthcare systems. Non-adherence to medication leads to poor treatment outcomes and the progression of diseases and complications. The medication adherence of type 1 and type 2 diabetes patients assessed using self-report, pill counts, electronic monitoring devices and medication possession ratio found that the medication adherence rates ranged from 31% to 87%. The medication adherence of diabetes patients is also measured by persistence which is defined by the proportion of patients who remained in treatment for a predetermined period and the mean number of days till discontinuation of treatment. The persistence rates ranged from 16% to 63% at 12 months and",
    "label": 0
  },
  {
    "text": "by persistence which is defined by the proportion of patients who remained in treatment for a predetermined period and the mean number of days till discontinuation of treatment. The persistence rates ranged from 16% to 63% at 12 months and ranged from 29% to 70%. Physical activity Physical activity has a favourable influence on the health and well-being of diabetes patients as it achieves physiological changes, including improved overall glycemic control, liver insulin sensitivity, muscle glucose uptake and utilisation and overcomes the metabolic abnormalities related to type 2 diabetes. Diabetes patients can undertake light to moderate physical activity. The type of physical activity that can be performed by diabetes patients needs to be determined after consultations with health care providers. The physical activities recommended for diabetes patients include brisk walking, recreational games and leisure time activities. The most benefit of physical activity happens in the early progression of the disease. Healthy eating habits A healthy diet is one component of the management of diabetes. Dietary self-care behaviours include eating a low-saturated fat diet, making choices based on the glycemic index of food, and controlling the amount of carbohydrates in food. Sticking to the eating plan and following the diet plan when eating from a restaurant or when feeling stressed is a major challenge for diabetes patients. Recommended practices and specific behaviours Understanding the levels (recommended practices) and patterns (specific behaviours) of primary daily diabetes self-management from a heterogeneous sample population is essential to devise suitable interventions to enhance the daily diabetes management of diabetes patients. Many of the diabetes self-management data involve only a small and highly selected sample, which does not represent the minorities and disadvantaged communities. The study conducted on a heterogeneous sample of population including minorities found that self-management levels increase with age. The same study found",
    "label": 0
  },
  {
    "text": "data involve only a small and highly selected sample, which does not represent the minorities and disadvantaged communities. The study conducted on a heterogeneous sample of population including minorities found that self-management levels increase with age. The same study found that retired individuals and homemakers have better self-management than employed individuals. Barriers to effective self-management Diabetes patients need to actively self-manage their diseases in everyday life for good diabetes outcomes. However, there are certain barriers to the effective day-to-day management of the disease. This section identified the main barriers to the effective self-management of diabetes. Financial constraints Financial constraints or poverty is a barrier to effective self-management as it prevents access to food, healthcare, medication and information. The most significant impact of lack of financial resources is on the food consumption pattern, resulting in a vicious cycle of high carbohydrate consumption and hyperglycaemia. Diabetes patients with limited financial resources often report that they find it difficult to purchase adequate food and it becomes impossible to buy different food for the family. Financial difficulties cause diabetes care to become a problem of least importance as they have more pressing needs such as feeding the family and repaying loans. When it comes to buying medicines for diabetes management, people from poor financial backgrounds choose food over medicine. To save costs, people from poor economic backgrounds alter the prescribed dosage of medication and medication is often taken with diabetes complications or the development of co-morbid illness. Norms and belief system The attitude towards self-care behaviour is influenced by the local belief systems and social norms. Patients who attribute diabetes control to god are less likely to self-manage and control their sugar intake. A study found that subjective norms attributed to 49% of the variance in the intent to perform diabetes-related self-management. An individual's",
    "label": 0
  },
  {
    "text": "social norms. Patients who attribute diabetes control to god are less likely to self-manage and control their sugar intake. A study found that subjective norms attributed to 49% of the variance in the intent to perform diabetes-related self-management. An individual's and their family's beliefs about diabetes influence how they make sense of their disease and make efforts to manage their illness. For example, individuals who are not adhering to the dietary intake shared the view that their decision to not follow the required dietary pattern is because they believe that their family, friends and peers would not approve of their diet. Inadequate family support and cultural beliefs prevent diabetes patients from adhering to a diet with low-saturated fatty acids. In Subsaharan Africa, diabetes patients face social stigma from family and community members from diabetes and diabetes-related self-management requirements which prevent diabetes-related self-care. A study found that when there is diabetes that runs in the family, it becomes a family affair and participants normalise and downplay the seriousness of the disease. Gender-based family roles prevent adhering to the medication. In a study, woman responsible for house duties were found to have inadequate time to visit health facilities resulting in their ignoring their health care needs such as diabetes management. Low knowledge Diabetes knowledge has a significant influence on the self-care and glycemic control of a diabetes patient. The lower knowledge about diabetes can affect diabetes management. Studies have found that patient's lack of knowledge and poor self-care practice is increasing the severity of diabetes every year. The level of education is a factor that has a positive correlation with self-care knowledge. Stigma Family support is highly beneficial for effective self-care. Diabetes-related stigma leads to a lack of family support and poor diabetes-related self-management behaviours. Inadequate support Diabetes patients expressed dissatisfaction with",
    "label": 0
  },
  {
    "text": "a factor that has a positive correlation with self-care knowledge. Stigma Family support is highly beneficial for effective self-care. Diabetes-related stigma leads to a lack of family support and poor diabetes-related self-management behaviours. Inadequate support Diabetes patients expressed dissatisfaction with the attitude of healthcare professionals as they directly wrote prescription and directions without a proper conversation with the patients. This prevents patients from asking lingering questions about their health conditions and management. Inadequate coordination between the health care providers and diabetes patients is a major barrier to properly implementing the care guidelines. Lack of collaboration and coordination leads to information conflict affecting the quality of self-management. The diabetes patients who were identified to develop healthy diabetes management habits had a supportive patient-provider relationship. People from economically disadvantaged backgrounds can have limited access to care which is one reason for inadequate coordination between healthcare providers and patients. Role of healthcare professionals Healthcare professionals play a crucial role in diabetes self-management. Diabetes patients rely on health care professionals to obtain information and support in developing an individualised self-management plan. Self-management goals and recommendations given by health care professionals were identified to have a significant positive impact on the eating habits and physical activity of diabetes patients. Lack of proper emphasis on self-management may suggest to diabetes patients that self-management is either less important or even unimportant for diabetes management. References",
    "label": 0
  },
  {
    "text": "Dorsal pancreatic agenesis is a congenital anomaly characterised by the absence of the duct of Santorini, tail and body of the pancreas. It is regarded as asymptomatic and the most common clinical manifestation is non-specific abdominal pain. While the cause is unclear, its mechanism is thought to be impaired dorsal buds or mutation of genes that regulate organogenesis during embryogenesis. Early diagnostic methods are laparotomy and autopsy. Endoscopic retrograde cholangiopancreatography (ERCP) and computed tomography scan (CT scan) are combined for diagnosis of this agenesis in recent years. No specific medications are needed for relieving symptoms, but pancreatic enzymes capsule and insulin are prescribed. Scholars have studied several transcription factors and proteins that can manipulate pancreatic growth and related to dorsal pancreatic agenesis. This malformation is associated with other pancreatic disorders including pancreatitis and pancreatic tumors. Also, patients with this dysgenesis are usually accompanied with pancreatic exocrine dysfunction such as diabetes mellitus. The prevalence and geographical pattern are unknown. First reported in 1911 by Heiberg in an autopsy. Signs and symptoms The agenesis of the dorsal pancreas is asymptomatic in most cases due to the functional reserves of exocrine and endocrine pancreas. Thereby it is often diagnosed incidentally by autopsy, surgery, endoscopy, or imaging technologies during evaluation of other diseases. Some common indicators reported by patients include abdominal pain, weight loss, and jaundice. Abdominal pain could be associated with acute or chronic pancreatitis, pancreatic tumor, diabetic autonomic neuropathy; and weight loss can also be due to diabetes mellitus in some patients. The localisation of abdominal pain is at epigastrium and aggregates after meals. The two possible reasons for abdominal pain include the underdevelopment of the papillary muscle of the sphincter of Oddi or the autonomic neuropathy due to diabetes mellitus. Dorsal pancreatic agenesis is associated with diabetes mellitus because the β",
    "label": 0
  },
  {
    "text": "after meals. The two possible reasons for abdominal pain include the underdevelopment of the papillary muscle of the sphincter of Oddi or the autonomic neuropathy due to diabetes mellitus. Dorsal pancreatic agenesis is associated with diabetes mellitus because the β cells in the islets of Langerhans are present in the pancreatic body and tail, which are both absent in agenesis of the dorsal pancreas. Patients with dorsal pancreatic agenesis may result in a defect in the metabolism of hepatic glycogen, which may be related to the reduced β cell mass. Associated conditions Pancreatitis Pancreatitis is a common disorder associated with dorsal pancreatic agenesis. However, it is unclear whether the high occurrence of pancreatitis is due to repetitive imaging procedures or whether it is a comorbidity of agenesis. Diabetes mellitus Diabetes mellitus is an endocrine disease that is due to insufficient amount of insulin produced by the pancreas, and it is another common disease linked with dorsal pancreatic agenesis. Insulin is produced by the β cells of the islet of Langerhans in the dorsal pancreas. In patients with agenesis of the dorsal pancreas, the amount of β cells are reduced thus leading to a high possibility of causing diabetes mellitus. Pancreatic tumors Tumors can also be associated with dorsal pancreatic agenesis. Reported pancreatic tumors include solid papillary, solid pseudopapillary tumors, adenocarcinomas, and intraductal papillary mucinous neoplasms (IPMN). Organ malfunction Organ malfunction is also associated with dorsal pancreatic agenesis. Disorders such as polycystic kidney disease, Kartagener syndrome, multiple splenic deformities, congenital choledochal cysts, and biliary atresia have been reported. Cause There is no suggested cause for the dorsal pancreatic agenesis, however there are several hypotheses for the underlying mechanism. As the dorsal pancreatic bud is the ancestor for the majority of pancreas, one possible explanation to this rare anomaly is the dorsal",
    "label": 0
  },
  {
    "text": "no suggested cause for the dorsal pancreatic agenesis, however there are several hypotheses for the underlying mechanism. As the dorsal pancreatic bud is the ancestor for the majority of pancreas, one possible explanation to this rare anomaly is the dorsal mesentery ischemia which induces the dysgenesis of pancreas. In spite of the unknown molecular mechanism, some scholars have speculated the mode of inheritance of this agenesis, which is autosomal-dominant or X-linked disease. Genetics Scientists have demonstrated a wide spectrum of genes that can manipulate and regulate the embryonic pancreatic development by in vivo animal study. The formation of dorsal and ventral buds is reliant on the interaction of transcription factors, for instance the Pax4, Pax6 and Ptf1a. By expressing sonic hedgehog (Shh) or Indian hedgehog (Ihh), the growth of both dorsal and ventral buds from the foregut endoderm is repressed to a large extent, inhibition of Shh or Ihh signaling pathway favor the pancreatic development. The Homeodomain protein HB9 (Hlxb9) is critical for the formation of the dorsal bud, however it is not significant to the ventral bud. Another homeodomain protein, PDX1 (also known as Ipf-1 insulin promoter factor 1) is the initiator of buds expression, genetic mutation on gene Ipf-1 can lead to pancreatic agenesis. Also, both heterozygous and homozygous variant of PDX1 can cause dysgenesis of pancreas and neonatal diabetes. CDH2 (CD352) gene encoded Neural-cadherin (N-cadherin) is identified to have an active role in recruiting dorsal pancreatic mesenchyme for pancreatic morphogenesis, depletion of N-cadherin in mice can lead to the apoptosis of dorsal pancreatic mesenchyme cells. Moreover, research suggests that retinoid acid is significant for the development of pancreas as the deficiency of retinoid acid can induce dorsal pancreatic agenesis in mice. Embryonic development of pancreas The pancreas is responsible for secreting various enzymes for most digestion. It",
    "label": 0
  },
  {
    "text": "suggests that retinoid acid is significant for the development of pancreas as the deficiency of retinoid acid can induce dorsal pancreatic agenesis in mice. Embryonic development of pancreas The pancreas is responsible for secreting various enzymes for most digestion. It is a retroperitoneal organ posterior to the great curvature of the stomach. Its anterior surface is covered by the parietal peritoneum while the posterior surface contacts the aorta as well as other viscera on the left posterior body wall. Its 15 cm entity is divided into three parts, a globular head attached to the right side of duodenum, a long body, and a blunt head. In embryonic development, the pancreas is formed by the convergence of the two pancreatic buds (dorsal and ventral) during the sixth and seventh week of gestation, but the development of these two buds is initiated at about week 4. During the gestation phase which takes place at week 4, the dorsal bud develops from the dorsal mesentery, and form the body and tail of the pancreas. Meanwhile, the ventral bud arises inferior to form the head of the pancreas. Two weeks later, the ventral bud will rotate dorsally and fuse with the dorsal bud to form the pancreas. The fusion of the duct happened at the same time, to form the duct of Wirsung and the duct of Santorini. Any deviation from the normal pancreatic embryogenesis process can potentially leads to congenital malformation of the pancreas. Among all anomalies, Pancreatic Divisum is the most prevalent one, followed by Pancreatic Heterotopia. In comparison, a complete agenesis of dorsal pancreas is rare and often associated with infant mortality. The agenesis of dorsal pancreas can be classified into two classes based on the morphological features: complete and partial agenesis. For the former one, the accessory pancreatic duct (also",
    "label": 0
  },
  {
    "text": "of dorsal pancreas is rare and often associated with infant mortality. The agenesis of dorsal pancreas can be classified into two classes based on the morphological features: complete and partial agenesis. For the former one, the accessory pancreatic duct (also known as duct of Santorini) as well as the body and tail of the pancreas are missing. For partial agenesis, the tail of the pancreas is still present. Diagnosis Dorsal pancreatic agenesis with advancement in modern day technology, is currently diagnosed with the following imaging technologies, and sometimes a combination of these methods. Endoscopic retrograde cholangiopancreatography (ERCP). An invasive, operator-dependent gold standard for diagnosis. It presents the pancreatic duct; the absence or presence of the dorsal ductal system and the minor papilla can be used to indicate dorsal pancreatic agenesis. Magnetic resonance cholangiopancreatography (MRCP). A non-invasive, accurate type of MRI to depict the major and accessory pancreatic ducts. Magnetic resonance imaging (MRI). A non-invasive, non-radiation exposed method that can be used to identify the morphological structure of the pancreas. Computed Tomography (CT). A non-invasive method to diagnose dorsal pancreatic agenesis by an absence of pancreatic tissue ventral to the splenic vein. Stomach/intestine sign. Can be used to distinguish dorsal pancreatic agenesis from distal pancreatic lipomatosis. Transabdominal Ultrasound (US). The diagnostic ability depends on pancreatic bowel gas or the patient's body habitus. Agenesis of dorsal pancreas has to be differentiated with pancreatic fat infiltration, chronic pancreatitis and atrophy of pancreatic body and tail. The absence of pancreatic body and tail is the usual manifestation of dorsal pancreatic agenesis, the density and morphological features of the pancreatic head should be either normal or slightly enlarged due to compensation of the absent body and tail. Stomach/intestine sign can combine with CT for better diagnosis. Since dorsal pancreatic agenesis can be at times complicated",
    "label": 0
  },
  {
    "text": "morphological features of the pancreatic head should be either normal or slightly enlarged due to compensation of the absent body and tail. Stomach/intestine sign can combine with CT for better diagnosis. Since dorsal pancreatic agenesis can be at times complicated with pancreatic tumors, CT is a visualisation tool that can be used to exclude both pancreatic tumors and abdominal tumors, which allows for improved observation. Management Vast majority of dorsal pancreatic agenesis cases are diagnosed incidentally and asymptomatic, hence there is no special medication for dorsal pancreatic agenesis. Medication 1. Pancreatic enteric-coated capsules. A proposed study suggested dorsal pancreatic agenesis patients show Exocrine Pancreatic Insufficiency (EPI) related symptoms. Oral administration of pancreatic enzymes, including lipase, protease and amylase is useful for relieving EPI related symptoms. In addition, pay attention to drugs that may interact with pancreatic enzymes such as Acarbose and Miglitol. 2. Exogenous insulin. Patients with dorsal pancreatic agenesis have lost the tail of pancreas which contain the Islet of Langerhan for insulin production. Therefore, exogenous insulin can be used to treat the accompanied Diabetes Mellitus. History Dorsal pancreatic agenesis is an extremely rare disease, there are only about 100 cases reported so far and no association discovered between a specific geographical location and disease prevalence. First reported in 1911 by Heiberg in an autopsy. The patient was diagnosed with diabetes mellitus and associated with pulmonary tuberculosis, the observed morphology of pancreatic head was described as \"large and thick\". Two years later, the second case was reported by Ghon and Roman (1913). References",
    "label": 0
  },
  {
    "text": "Drone Emergency Medical Services (DEMS) involve the use of highly autonomous Beyond Visual Line of Sight (BVLOS) drones to deliver critical medical supplies, such as Automated External Defibrillators (AEDs), life-saving medications, and remote diagnostic equipment, directly to emergency situations. This innovative approach is gaining traction globally, as it significantly reduces response times, thereby improving patient outcomes in time-sensitive scenarios like cardiac arrests and other emergencies where every second counts. The evolution of drone technology in EMS has been fueled by advancements in unmanned aerial systems (UAS) and the growing recognition of their capabilities in addressing logistical challenges, particularly in remote or underserved areas. Initial trials in the early 2010s laid the groundwork for delivering medical supplies, while subsequent pilot programs have focused on specialized applications, including rapid delivery of emergency medical equipment and live video feeds to support first responders before their arrival at the scene. Despite the promising benefits of drone-enhanced EMS, some challenges remain, including public acceptance, regulatory hurdles, and the technological complexity of integrating these systems into existing emergency response frameworks. As healthcare organizations seek innovative solutions to improve emergency medical responses, addressing these challenges will be crucial. History The concept of using drones in emergency medical services (EMS) has evolved significantly over the past decade, driven by advancements in drone technology and the growing recognition of their potential in healthcare. Initially, drones were primarily used for military operations and recreational activities. However, their versatility has led to a broader range of use cases, including EMS. Early developments The use of drones for healthcare logistics can be traced back to early trials in the 2010s, which focused on delivering medical supplies to remote areas, aiming to address logistical challenges such as extreme weather, natural disasters, and limited transport infrastructure. These early implementations demonstrated the potential for drones",
    "label": 0
  },
  {
    "text": "traced back to early trials in the 2010s, which focused on delivering medical supplies to remote areas, aiming to address logistical challenges such as extreme weather, natural disasters, and limited transport infrastructure. These early implementations demonstrated the potential for drones to significantly improve medical delivery efficiency, particularly in emergencies where time is critical. Expansion into Emergency medicine As drone technology matured, EMS began to explore more specialized applications. By the mid-2010s, pilot programs were developed to test drones for the delivery of critical medical supplies, such as Automated External Defibrillators (AEDs), particularly for out-of-hospital cardiac arrest cases. These programs often involved collaboration between healthcare providers, technology developers, and regulatory agencies to ensure safety and operational compliance. Future directions Today, drones are increasingly being integrated into EMS, with applications ranging from rapid assessment of emergency situations to delivering medical supplies like AEDs and medical supplies and remote diagnostic equipment. The potential of drone technology continues to expand as regulatory frameworks evolve, and ongoing research promises further innovations in the field. Technology The technology behind drone emergency medical services (DEMS) leverages advancements in unmanned aerial systems (UAS), enabling the rapid and autonomous delivery of medical supplies to incident sites before regular units arrive on scene. These drones operate Beyond Visual Line of Sight (BVLOS), allowing them to respond faster, cover greater distances, often equipped with advanced navigation systems, sensors, and real-time data transmission capabilities. Adaptations for medical deliveries The drones used in DEMS are often modified to meet healthcare-specific requirements. Delivery solutions are designed to carry a flexible payload adaptable to local conditions and needs. These adaptations ensure that sensitive medical materials can be delivered safely and effectively in an emergency. Operational challenges Despite their effectiveness, drones face several operational challenges. Factors such as weather conditions, battery life, and payload capacity can",
    "label": 0
  },
  {
    "text": "conditions and needs. These adaptations ensure that sensitive medical materials can be delivered safely and effectively in an emergency. Operational challenges Despite their effectiveness, drones face several operational challenges. Factors such as weather conditions, battery life, and payload capacity can limit the performance of drones in emergency medical applications. However, ongoing advancements in drone technology aim to overcome these limitations. Some of these innovations include more efficient batteries, better obstacle detection systems, and improved communication networks that allow for more reliable operation in adverse conditions. Real-time data transmission In addition to delivering physical medical supplies, drones used in DEMS are often equipped with high-definition cameras and sensors, providing live video feeds and biometric data to emergency responders. This allows healthcare professionals to assess the situation remotely and guide on-site interventions before physical responders arrive. Applications Medical supply deliveries Drones are being used in the rapid transportation of medical supplies in emergencies. For example, healthcare drones have been adapted to carry critical items such as defibrillators (AEDs), pharmaceuticals, anti bleeding equipment and remote diagnostic equipment which can make a significant difference in emergency response scenarios. Emergency response and disaster relief In emergency medical situations, such as out-of-hospital cardiac arrests, drones have been deployed to deliver AEDs quickly to the scene. Studies show that AED-equipped drones can reduce response times compared to traditional ground transportation. This reduction in time is crucial for improving patient outcomes, particularly in cardiac arrest cases where immediate defibrillation can save lives. Drones have also been used in disaster relief, where they can rapidly deliver medical supplies to hard-to-reach areas following natural disasters or large-scale emergencies. These drones can fly over blocked roads and other obstacles, ensuring that medical help reaches those in need as quickly as possible. Remote patient monitoring and telemedicine Drones are also being used",
    "label": 0
  },
  {
    "text": "hard-to-reach areas following natural disasters or large-scale emergencies. These drones can fly over blocked roads and other obstacles, ensuring that medical help reaches those in need as quickly as possible. Remote patient monitoring and telemedicine Drones are also being used to support telemedicine and remote patient monitoring. By transmitting real-time video feeds and biometric data from delivered devices, enables healthcare professionals to assess patients' conditions remotely and provide real-time consultation and diagnosis. This is particularly useful in areas with limited healthcare resources, allowing for more timely interventions and support. AED-drone trials Several trials have evaluated the effectiveness of drones delivering AEDs in real-life cardiac arrest situations. A Swedish study in 2020-2023 reported that drones arrived before ambulances in 57% of cases, and AEDs were attached by bystanders in 35% of these cases. These drones achieved a 92% success rate in delivering AEDs within 9 meters of the target. The real-life case study published in the New England Journal of Medicine showed that a drone-delivered AED was successfully used to defibrillate a cardiac arrest patient before emergency medical services arrived. See also Unmanned Aerial Vehicle Emergency Medical Services References",
    "label": 0
  },
  {
    "text": "Follicular drug delivery is a mechanism that enables the transport of therapeutic agents through the hair follicles present on the skin. This approach leverages the use of nanoparticles, which are widely employed in the broader field of drug delivery, to specifically target and penetrate these follicular pathways. By utilizing follicular delivery, drugs can be delivered in a more targeted and localized manner to treat conditions including acne, alopecia, fungal infections, and skin cancer. This article will explore the anatomy of the hair follicle, various drug carriers and delivery vehicles utilized, relevant in vitro and in vivo models, current clinical applications, and the existing challenges and future directions within this field. Background Targeted drug delivery is a therapeutic approach designed to concentrate medications at specific sites within the body, enhancing drug concentration at the intended site while minimizing exposure to non-target areas. This approach helps improve treatment outcomes and minimize side effects. Within the various targeted drug delivery approaches, follicular drug delivery specifically targets hair follicles, with the aim of delivering drugs to treat skin-associated infections and conditions. Key considerations in experimental design for follicular drug delivery include hair follicle anatomy and circulatory proximity, specific target sites for drug delivery, models for follicular delivery, quantitative assessment methods, and formulation design. The target region within the hair follicle must be clearly defined for the formulation and delivery approach. Hair Follicle Anatomy Hair is separated into two distinct regions. One region is the hair shaft, which protrudes from the skin's surface. The hair shaft consists of three layers. The inner layer is the medulla. This layer is surrounded by the cortex. The cuticle is a single-cell layer that shields the cortex and medulla from the surrounding skin. The second region is the hair follicle, which rises from the surface of the epidermis and",
    "label": 0
  },
  {
    "text": "medulla. This layer is surrounded by the cortex. The cuticle is a single-cell layer that shields the cortex and medulla from the surrounding skin. The second region is the hair follicle, which rises from the surface of the epidermis and has a structural unit called the pilosebaceous unit. The pilosebaceous unit is made up of the hair follicle, its associated sebaceous gland, and the arrector pili muscle. The hair follicle consists of the inner root sheath and outer root sheath. The inner root sheath protects the growing hair, while the outer root sheath is continuous with the epidermal layer, increasing the surface area for absorption beneath the skin's surface. The arrector pili muscle is a small band of smooth muscle fibers that attaches to the follicle. The sebaceous gland secretes a lipid-rich sebum onto hair follicles which protects the hair and provides a hydrophobic protective barrier over the skin. This lipid-rich environment may provide a lipophilic pathway for potential drug delivery. Drug Carriers Nanoparticles, including nanocrystals, micelles, lipid, polymeric, and silica nanoparticles, and exosomes are all drug carriers used in follicular drug delivery. When each of these drug carriers are applied to the surface of the skin, they can travel down the hair shaft, bypassing the outer layer of the skin. This section will outline the structure and function of each nanoparticle, and how it lends itself specifically towards follicular drug delivery. Nanocrystals Nanocrystals are nanosized, inorganic materials that are within thermodynamically stable colloidal solutions. They have a crystalline structure and are between 1 and 1000 μm in diameter. Unlike other nanoparticles, there is no carrier material, meaning that nanocrystals only consist of the drug being delivered. When the drug is dispersed into liquid media, it undergoes surface degradation, with the dispersed particles needing to be stabilized. Nanocrystals are much",
    "label": 0
  },
  {
    "text": "other nanoparticles, there is no carrier material, meaning that nanocrystals only consist of the drug being delivered. When the drug is dispersed into liquid media, it undergoes surface degradation, with the dispersed particles needing to be stabilized. Nanocrystals are much more drug-saturated, and their dissolution process is significantly faster than that of when the drug is in its normal state, increasing the bioavailability of the drug during delivery. Nanocrystals interface well with follicular drug delivery because of their small size, their reliance on surface degradation during drug delivery, the establishment of a concentration gradient along the length of the hair follicle, and the high drug saturation within the nanocrystal. Once on the skin, the nanocrystals travel down the hair shaft of the follicle, bypassing the outer layer of the skin due to the concentration gradient, and then form localized depots within the hair follicle, allowing for sustained drug release and therapeutic effects. Once past the outer layer of the skin, the drug can more easily diffuse through the skin, as this layer is more permeable. Polymeric Micelles Micelles are spherical molecules formed when lipids are placed in a hydrophilic solvent. Lipids are small molecules consisting of a hydrophilic head and at least one hydrophobic tail. When enough lipids are placed into a hydrophilic substance, a concentration known as the Critical Micelle Concentration, they aggregate into spherical molecules, with their hydrophilic heads on the exterior and their hydrophobic tails on the interior. In drug delivery, polymeric micelles are preferred to classic micelles due to their lower critical micelle concentration and higher stability upon dilution. Polymeric micelles rely on the spontaneous formation of spherical molecules from amphiphilic block copolymers, rather than lipids, when placed in a hydrophilic substance. The exterior of these spherical molecules consists of a hydrophilic shell, with the core",
    "label": 0
  },
  {
    "text": "stability upon dilution. Polymeric micelles rely on the spontaneous formation of spherical molecules from amphiphilic block copolymers, rather than lipids, when placed in a hydrophilic substance. The exterior of these spherical molecules consists of a hydrophilic shell, with the core being hydrophobic. In drug delivery, the hydrophobic drug of interest is present in the hydrophilic substance when the lipids are added, ensuring that when the micelles form, the hydrophobic drug becomes trapped within the core of the micelle. These copolymers have a customizable size and makeup, allowing for more flexibility in loading capacity, rate of degradation, and therapeutic potential. Polymeric micelles are small enough to access tissues with leaky vasculature, can improve the solubility of poorly water-soluble drugs, and can remain in the blood for a longer period of time, allowing for accumulation of the drug at the desired location. Polymeric micelles are considered effective in follicular drug delivery due to their small size, high stability, and ability to make water-insoluble drugs soluble. When polymeric micelles make contact with the skin, they can travel down the hair shaft, bypassing the outer layer of the skin, with smaller micelles permeating further into the hair follicle. Lipid Nanoparticles Lipid nanoparticles are spherical molecules with a diameter between 10 and 500 nm made up of a lipid bilayer with a hydrophilic core and exterior, and a hydrophobic intermediate layer. The hydrophilic core can contain a hydrophilic drug payload, protecting the drug from degradation in the body, increasing solubility and efficacy, and altering distribution of the drug to the desired sites. The hydrophobic intermediate layer can also contain a hydrophobic drug, increasing its solubility and bioavailability within the body. Lipid nanoparticles enter the hair follicle via the hair shaft and travel down the length of the follicle, where they sit and deliver the drug",
    "label": 0
  },
  {
    "text": "layer can also contain a hydrophobic drug, increasing its solubility and bioavailability within the body. Lipid nanoparticles enter the hair follicle via the hair shaft and travel down the length of the follicle, where they sit and deliver the drug over a sustained period of time. The lipid nanoparticles remain in the follicle until slow processes like hair growth, shedding and sebum flow remove the follicles from the pores. Polymeric Nanoparticles Polymeric nanoparticles are nanoscale-sized delivery systems composed of drug, polymer, and surfactants. These carriers can be made from hydrophilic natural polymers, like proteins, or from hydrophobic synthetic polymers. Their versatility allows them to be administered through various dosage forms, including topical applications. Polymeric nanoparticles have high encapsulation efficiency and physical stability, which can be optimized based on the method of preparation and the physicochemical properties of the constituent materials. The polymeric coating protects the active drug from premature degradation, improves steric stability, and minimizes irritation upon application. Additionally, these nanoparticles can be mucoadhesive, biodegradable, and provide skin permeation enhancement, improving both therapeutic efficacy and safety. In general, polymeric nanoparticles exhibit a natural tendency to accumulate within the hair follicle cavities. The targeting efficiency of these systems can be enhanced by modulating their surface properties, including particle size, superficial charge, and aggregation state. These particles can also be functionalized with an antibody or a substance that promotes the targeting of the particle and facilitates site-specific drug delivery. Silica Nanoparticles Silicon dioxide nanoparticles are inorganic nanoparticles made up of an alternating mixture of silicon and oxygen atoms, with dimensions between 400 and 700 nm. These nanoparticles are hydrophilic, biocompatible, and have low-cost synthesis and dispersive properties. Silica nanoparticles have a porous structure which produces cavities that can store and release a variety of substances and provide a high specific surface area,",
    "label": 0
  },
  {
    "text": "and 700 nm. These nanoparticles are hydrophilic, biocompatible, and have low-cost synthesis and dispersive properties. Silica nanoparticles have a porous structure which produces cavities that can store and release a variety of substances and provide a high specific surface area, stable structure, and easy modification. For follicular drug delivery, the particle surface chemistry influences the effectiveness of the silica nanoparticles. Thiolated silica nanoparticles have demonstrated strong binding affinity to hair shafts and the outer skin stratum corneum. Polyethylene glycol (PEG)-modified silica nanoparticles have demonstrated improved diffusivity and increased follicular penetration. Larger PEGylated particles were observed to penetrate the follicular structure more deeply compared to smaller sizes. Exosome Therapy Exosomes, a class of extracellular vehicles (EVs), are nanoscale vesicles with diameters ranging from 30 to 150 nm. These vesicles are secreted by a wide variety of cell types and are characterized by a phospholipid bilayer structure. As mediators of intercellular communication, exosomes play a role in processes including homeostasis, cellular differentiation, and organogenesis. Exosomes encapsulate molecules within the vesicle, providing protection against degradation and allowing for potential targeted delivery to recipient cells. Their ability to transfer bioactive molecules has made them a subject of increasing interest in therapeutic research. Exosomes have been observed to affect hair follicle cell function, including proliferation, differentiation, and survival. Notably, exosomes are capable of carrying hydrophobic Wnt proteins on their surface over a long distance. These proteins induce the activation of β-catenin, a key signaling pathway in the regulation of hair regeneration and morphogenesis. Exosomes that are derived from mesenchymal stem cells (MSCs) have been used in experimental studies related to hair growth and regeneration. These MSC-derived exosomes offer properties such as lower immunogenicity and greater scalability. Delivery Vehicles Conventional follicular drug delivery relies on the use of topical delivery vehicles, such as gels and microneedles,",
    "label": 0
  },
  {
    "text": "in experimental studies related to hair growth and regeneration. These MSC-derived exosomes offer properties such as lower immunogenicity and greater scalability. Delivery Vehicles Conventional follicular drug delivery relies on the use of topical delivery vehicles, such as gels and microneedles, to deliver nanoparticles containing the necessary drugs to hair follicles. Gels Gels are semi-solid materials that range from being soft and weak to hard and tough. Gels consist of a cross-linked, primarily polymer network suspended in a liquid medium, and the properties of a gel depend heavily on the interactions between these two components. The bilayer composition and structure allows for a more effective and less toxic administration of drugs compared to previous methods. External topical gels are typically used for follicular drug delivery. These gels are spread or sprayed over the affected area of the skin. Rather than penetrating the skin, topical gels sit on the skin for a longer period of time, allowing for drug absorption. The drug carriers are embedded into the gel matrix, from where they can disperse into the hair follicles. This vehicle interfaces well with the drug carriers previously mentioned, as it allows for the carriers to remain on the skin, eventually traveling into the hair follicles, driven by a concentration gradient. Microneedles Microneedles are three-dimensional mechanical, needle-like structures that make small penetrations into the outer layer of the skin barrier, allowing for drug delivery beyond the outer skin layer and into the pilosebaceous unit. There are five types of microneedles: solid, coated, hollow, dissolvable, and hydrogel-forming, with each type having its own utility. Solid microneedles puncture the skin and are removed, at which point a patch of drug is placed on the area in question; coated microneedles have the drug coated on the needles themselves, so when the skin is punctured, the drug",
    "label": 0
  },
  {
    "text": "Solid microneedles puncture the skin and are removed, at which point a patch of drug is placed on the area in question; coated microneedles have the drug coated on the needles themselves, so when the skin is punctured, the drug is delivered into the area, with removal of the microneedle ending delivery. Hollow microneedles have a channel through which drug can flow; dissolvable microneedles degrade as the drug is released, with the base of the needle removed at the completion of administration. Hydrogel-forming microneedles absorb fluid within the tissue and swell, then the drugs within the hydrogel structure are released along a concentration gradient into the dermal layer. This vehicle interfaces well with the drug carriers discussed in the previous section because it provides a more direct route for the carriers to travel into the hair follicles. In Vitro and In Vivo Models Follicular drug delivery research utilizes multiple in vitro and in vivo models, each offering unique advantages for studying efficacy and safety. In Vitro Models Various in vitro models are employed to investigate follicular drug delivery, including excised skin models, reconstructed human skin models, and isolated hair follicle studies. Excised skin models typically rely on human cadaver skin or skin obtained from surgical procedures as the gold standard, but porcine skin, specifically from the ear, is also widely accepted due to its similarities in hair follicle density and structure to human skin. The preparation of excised skin for in vitro studies typically involves the removal of subcutaneous tissue, and storage conditions are carefully controlled to preserve skin integrity. Reconstructed human skin models with functional hair follicles is an area of ongoing research aiming to create more physiologically accurate in vitro systems using epidermal equivalents and full-thickness skin models. These models rely on the application of 3D printing to",
    "label": 0
  },
  {
    "text": "Reconstructed human skin models with functional hair follicles is an area of ongoing research aiming to create more physiologically accurate in vitro systems using epidermal equivalents and full-thickness skin models. These models rely on the application of 3D printing to model the human skin. While reconstructed human skin models do have better standardization and reduced inter-individual variability compared to excised skin models, they still face limitations including potential impairment of barrier properties and a lack of complete skin appendages. Isolated hair follicle studies isolate and culture individual hair follicles to study drug penetration and metabolism within the follicle in a more controlled manner. This approach is more focused on investigating intrafollicular drug behavior, eliminating the influence of the surrounding skin to evaluate the effects of active compounds on hair growth and follicle biology. In vitro testing methodologies for follicular delivery include Franz diffusion cell assays, tape stripping techniques, confocal and fluorescence microscopy, and flow-through cell systems. Franz diffusion cell assays provide a quantitative measure of drug permeation and retention in excised skin, allowing for the comparison of different formulations and penetration enhancers. Tape stripping techniques sequentially remove the stratum corneum layers using adhesive tapes to assess the depth and distribution of topically applied substances. Confocal microscopy uses lasers to obtain high-resolution, non-invasive imaging of drug distribution in different skin layers and within follicular structures, and Fluorescence microscopy tags drugs with a fluorescent marker to visualize their penetration pathways and distribution within hair follicles and surrounding skin tissue. Flow-through cell systems use a continuous flow of receptor fluid, to create a more dynamic assessment of drug release and permeation through follicular pathways over extended periods. In Vivo Models In vivo models provide insights into follicular drug delivery in a natural physiological environment and undergo normal cyclic activity. Rodent models are widely",
    "label": 0
  },
  {
    "text": "more dynamic assessment of drug release and permeation through follicular pathways over extended periods. In Vivo Models In vivo models provide insights into follicular drug delivery in a natural physiological environment and undergo normal cyclic activity. Rodent models are widely used due to their ease of handling, short hair cycles, and availability of genetically modified strains. Although these models are suitable for studying normal hair cycling, their anatomical differences limit the direct translatability of findings to humans. Large animal models offer closer approximation to human skin in terms of follicular structure and depth. They also allow for real-time monitoring of intrafollicular processes. Xenograft models are used for investigating androgen action and other mechanisms related to follicular delivery. In these models, human skin grafts are transplanted onto immunodeficient mice. These grafts retain the structural characteristics of human follicles and have contributed to the understanding of alopecia areata and androgenetic alopecia. Clinical Applications Heightened interest in hair follicles stem from several dermatological abnormalities that relate to the follicles, including acne, androgenetic alopecia, and some skin cancers. Acne Acne, or acne vulgaris, is a chronic skin condition in which oil and dead skin cells clog the hair follicles, leading to pimples, blackheads, and whiteheads. Because acne occurs within the hair follicles, follicular drug delivery is a popular area of research in combating this condition for its potential for localized delivery, sustained release, and enhanced penetration. Acne can be treated either systemically or topically, and while current systemic treatments are more effective, they lead to side effects including gastrointestinal irritation, liver toxicity, and fetal abnormalities that are otherwise avoided with topical treatment. Topical delivery of nanoparticles through hair follicles offers an alternative approach, avoiding the side effects of systemic delivery thanks to the localization of the drug to the hair follicles. Alopecia Alopecia, hair",
    "label": 0
  },
  {
    "text": "abnormalities that are otherwise avoided with topical treatment. Topical delivery of nanoparticles through hair follicles offers an alternative approach, avoiding the side effects of systemic delivery thanks to the localization of the drug to the hair follicles. Alopecia Alopecia, hair loss on the scalp or other parts of the body, can occur in various forms with a range of severity and differing causes. Each type of alopecia affects the hair follicle, disrupting its growth and function. Traditional treatment options for alopecia, such as topical and oral medications, have limitations including systemic side effects and poor penetration. Follicular drug delivery is being explored to overcome these challenges by enabling localized, sustained, and biologically relevant drug release. Androgenic alopecia is a type of alopecia characterized by pattern hair loss. This condition is driven by elevated androgen levels, which contribute to the gradual miniaturization of hair follicles during the growth phase, resulting in hair thinning and balding. Due to the adverse systemic effects of widespread hormonal modulation that occur from oral antiandrogens to treat this condition, localized drug delivery to the pilosebaceous unit is being explored. Research is investigating incorporation of antiandrogenic drugs within nanostructured lipid carriers. These lipid-based nanoparticles can localize within the hair follicle reservoir, enabling controlled drug release and reducing systemic absorption. Fungal Infections Superficial mycoses, or fungal infections of the skin or hair shaft, often localize in the stratum corneum, hair shafts, and follicular openings. Follicular drug delivery can target dermatophytosis fungal infections that penetrate in or through hair follicles. Topical therapy provides site-specific delivery, reduces systemic toxicity, increases drug retention rate, and enhances bioavailability and efficacy. Common antifungals have been encapsulated in polymeric micelles, which enhance skin penetration and protect the drug from degradation Skin Cancers Skin cancer is a type of cancer that arises from the skin,",
    "label": 0
  },
  {
    "text": "drug retention rate, and enhances bioavailability and efficacy. Common antifungals have been encapsulated in polymeric micelles, which enhance skin penetration and protect the drug from degradation Skin Cancers Skin cancer is a type of cancer that arises from the skin, and is caused by abnormal development and proliferation of cells. Traditional treatment options for skin cancers include surgical excision, radiation therapy, topical agents, and systemic chemotherapy. While these approaches can be effective, they face limitations due to systemic toxicity, off-target effects, and challenges in treating deeper or metastatic tumors. Follicular drug delivery offers several potential advantages for skin cancer treatment. It allows for localized delivery of anticancer drugs directly to the tumor site, potentially increasing therapeutic efficacy while reducing systemic side effects associated with conventional chemotherapy. The hair follicle can also act as a reservoir, enabling sustained drug release and prolonged therapeutic effects, which can be particularly beneficial for maintaining cytotoxic concentrations at the tumor site over extended periods. Compared to traditional topical formulations that often struggle to penetrate beyond the superficial layers of the skin, follicular delivery can achieve enhanced penetration depth, crucial for reaching tumor cells located in deeper skin layers or within the follicle itself. Furthermore, by delivering drugs topically, follicular drug delivery can avoid the first-pass metabolism that occurs with oral drug administration, potentially increasing the bioavailability of certain anticancer agents at the target site. Challenges There are several challenges that follicular drug delivery has yet to overcome in its path to clinical usage, some of which are due to the incomplete understanding of how drugs can overcome the barriers to enter and transport through the pilosebaceous unit. Oftentimes, sebum or dead skin blocks the hair shaft, preventing drugs from entering the follicle. Once inside the hair follicle, drugs still must face the presence of sebum,",
    "label": 0
  },
  {
    "text": "can overcome the barriers to enter and transport through the pilosebaceous unit. Oftentimes, sebum or dead skin blocks the hair shaft, preventing drugs from entering the follicle. Once inside the hair follicle, drugs still must face the presence of sebum, which acts as a barrier to delivery of both hydrophilic and hydrophobic drugs. Hydrophilic drugs are blocked from transport through the follicle, while hydrophobic drugs become encapsulated by sebum, unable to reach target cells. Additionally, the changes in the properties of hair follicles during the anagen, catagen, and telogen growth phases lead to disparate levels of drug penetration. Because of these barriers, creation of a vehicular formulation that can spread readily on the skin's surface and preferentially enter the follicular openings has proven difficult. More generally, the field of follicular drug delivery as a whole has struggled to translate preclinical animal model data into human clinical data due to anatomical differences present in the animal model. Lastly, concerns surrounding the potential toxicity of long-term use of nanoparticles still need to be investigated as follicular drug delivery relies on sustained release to increase therapeutic effect. Future development Emerging research into improving follicular drug delivery involves creating systems that can provide controlled release in response to specific stimuli present within the follicular environment, such as changes in pH, temperature, or the presence of specific enzymes. Nanocarriers are also being designed with tailored properties, including modifying their surface charge and particle size to improve penetration into the hair follicle, and increase the precision of targeting specific areas within the hair follicle. One study found that stem cell membrane-coated nanoparticles had improved biocompatibility, enhanced targeting of specific cell types within the hair follicle, and sustained release of the encapsulated drug. Overall, follicular drug delivery is working towards more sophisticated and bio-inspired drug delivery systems",
    "label": 0
  },
  {
    "text": "People with intersex variations, also called disorders of sex development, have hormonal, genetic, or anatomical differences unexpected of an endosex (non-intersex) male or female. These can include, but are not limited to reproductive organs with a mix of male and female structures, underdeveloped reproductive organs, and rare sex chromosomes or chromosomes unexpected of one's sex. The healthcare needs of intersex people differ depending on which variations they have. Some intersex variations can cause a lack of sex hormones, infertility, or an increased risk of cancer. Intersex people have a higher risk of experiencing mental health issues like depression and PTSD compared to the general population. Healthcare for intersex people can include treatments for one's mental, cognitive, physical, and sexual health. This can include hormone replacement, peer support, medical assistance for conceiving children, and other treatments depending on the needs of the individual. Intersex conditions are diagnosed prenatally (before birth), at birth, or later in life via genetic and hormone testing as well as medical imaging. Intersex healthcare often comes with additional challenges compared to healthcare for endosex people. This is due to stigma and medical abuse towards intersex people, lack of intersex inclusion in medical research, and lack access to supportive and specialized care. Intersex healthcare has historically focused on making intersex people fit into physical and social norms for one's sex. This includes concealing information from patients and medically unnecessary surgeries. Intersex organizations advocate to end these practices and make further changes to respect and include intersex people. The medicalization of intersex variations and the use of the term 'disorders of sex development' are disputed as well. Presentation and diagnosis Variations of sex characteristics, also called intersex or disorders in sex development, refer to people with innate genetic, hormonal, or physical sex characteristics outside of medical norms for males",
    "label": 0
  },
  {
    "text": "'disorders of sex development' are disputed as well. Presentation and diagnosis Variations of sex characteristics, also called intersex or disorders in sex development, refer to people with innate genetic, hormonal, or physical sex characteristics outside of medical norms for males or females. It refers to a wide spectrum of variations to genitals, hormones, chromosomes, and/or sex characteristics.\" Intersex conditions can result in a combination of male and female structures, such as having both a uterus and testes; atypical genital appearance, such as a closed vagina; or missing or underdeveloped reproductive organs, such as a vaginal opening with no womb. Other characteristics include the presence of micropenis, hypospadias, urethra opening into the vagina, partly fused labia, large clitoris, electrolyte imbalance, undescended testes, and masses in the labia or groin for females. Some intersex conditions may result in genitals expected of one's sex, but differing chromosomes, such as having only a single X chromosome (Turner syndrome) or XXY chromosomes (Klinefelter syndrome). Up to 1.7% of the general population is estimated to be intersex. The cause of one's intersex variation is often unknown. Some intersex conditions can be inherited from a child's parents; for example, congenital adrenal hyperplasia (CAH) can be inherited if both parents have a copy of the affected gene. The same intersex condition can have different presentations or severity. Androgen insensitivity syndrome (AIS) has 3 forms: complete androgen insensitivity syndrome (CAIS), partial androgen insensitivity syndrome (PAIS), and mild androgen insensitivity syndrome (MAIS). CAIS makes the body unable to respond to androgens, while PAIS and MAIS lead to the body having partial sensitivity to androgens. Those with CAIS present with female genitals but no womb or ovaries and those with MAIS have male genitals but are often infertile. Those with PAIS can present with male genitals, female genitals, or ambiguous genitals.",
    "label": 0
  },
  {
    "text": "partial sensitivity to androgens. Those with CAIS present with female genitals but no womb or ovaries and those with MAIS have male genitals but are often infertile. Those with PAIS can present with male genitals, female genitals, or ambiguous genitals. CAH has different variants as well. 90 percent of cases take the form of 21-hydroxylase deficiency, also known as CAH 1, but there are more uncommon variations such as 11β-hydroxylase deficiency and 17α-hydroxylase deficiency depending on which gene is affected. CAH 1 can have either a classical or nonclassical presentation. Classical CAH takes the form of salt-wasting or simple virilization. Three quarters of classical CAH cases involve salt imbalance causing symptoms such as vomiting, dehydration, and low blood pressure. One quarter of classical CAH cases include simple virilization (ambiguous or male external genitals with female reproductive organs). Nonclassical CAH results in typical female genitals, but presents symptoms later in life including early puberty, irregular periods, infertility, hirsutism and hyperandrogenism (excessive male hormones). Mayer-Rokitansky-Küster-Hauser syndrome (MRKH) has two types which can produce different presentations. Type 1 only impacts the reproductive organs causing a lack of a uterus and cervix as well as a shallow vagina. Symptoms for type 1 MRKH are noticed at adolescence or adulthood, often due to the lack of a period. Type 2 MRKH produces symptoms in other parts of the body rather than only the reproductive organs. People born with type 2 MRKH may have heart defects, hearing loss, atypical vertebrae, abnormally formed kidneys, or only one kidney. These congenital anomalies are evaluated via MRI or transabdominal ultrasonography. Intersex variations are diagnosed using physical exams, genetic testing, hormonal testing, and medical imaging or laparoscopy. Getting a diagnosis by evaluating the individual's DNA molecules is preferred for future medical predictions, but is not always possible. Intersex variations may",
    "label": 0
  },
  {
    "text": "ultrasonography. Intersex variations are diagnosed using physical exams, genetic testing, hormonal testing, and medical imaging or laparoscopy. Getting a diagnosis by evaluating the individual's DNA molecules is preferred for future medical predictions, but is not always possible. Intersex variations may become apparent at birth, during puberty, or while trying to have children. Intersex variations may cause a delay in puberty, absence of puberty, or bodily changes unexpected for one's perceived sex. Genetic counsellors can help an intersex person or their family understand their diagnosis and its genetic impacts on health. Some intersex variations can be detected before birth. Screening cell-free DNA (DNA fragments found in bodily fluids like plasma) can detect if a fetus has Turner syndrome or Klinefelter syndrome. Ultrasounds may show ambiguous genitalia or genitals that do not match what is expected based on the sex chromosomes found in the cell-free fetal DNA screening. For those using in vitro fertilization, genetic testing before an embryo is implanted can detect intersex conditions such as congenital adrenal hyperplasia and androgen insensitivity syndrome. 21-hydroxylase deficiency can be treated during pregnancy using a medication called dexamethasone to prevent ambiguous genitalia. Sometimes an intersex condition may never be diagnosed. Most infants with intersex conditions are born with either male or female genitals and assigned a binary sex. Intersex people with ambiguous genitals are uncommon (1 in 2000 births). Medical characterization and nomenclature The term 'disorders of sex development', often abbreviated as DSD, is contentious as some organizations and intersex advocates believe that being intersex should not be pathologized. For instance, Planned Parenthood describes being intersex as a \"naturally occurring variation\" rather than a medical problem. Professor Elizabeth Reis supports changing disorders of sex development to divergence of sex development, writing that \"using divergence, intersex people would not be labeled as being in a",
    "label": 0
  },
  {
    "text": "being intersex as a \"naturally occurring variation\" rather than a medical problem. Professor Elizabeth Reis supports changing disorders of sex development to divergence of sex development, writing that \"using divergence, intersex people would not be labeled as being in a physical state absolutely in need of repair.\" DSD is sometimes used to abbreviate differences of sex development rather than disorders of sex development. Some parents of intersex children prefer the term 'disorders of sex development' because the term intersex evokes ideas of sexuality or that their child is a third gender rather than a boy or girl. Some people with intersex variations prefer using DSD instead of intersex because they see their variation as a medical condition rather than an identity. 60 percent of Australian intersex people prefer the term 'intersex' or a related term such as 'intersex variation', and 3 percent prefer the term 'disorder of sex development' outside of seeking medical care. The term 'intersex' has been criticized by clinicians and parents who believe the term only applies to those with ambiguous genitals or whose chromosomes and anatomy do not align. Disagreement remains over which variations should be considered intersex. Variations of sex characteristics, sometimes abbreviated as VSC, is an alternative term used to avoid the stigma associated with DSD. It is supported by some advocates to express a spectrum of sex variations and include people who prefer not to identify as intersex. Healthcare needs Physical health Bone health is a common concern for intersex people. For instance, people with complete androgen insensitivity syndrome (CAIS) have low bone density as a result of \"decreased circulating estrogen and skeletal resistance to androgen action.\" Some intersex people no longer produce their own sex hormones due to receiving a gonadectomy (surgical removal of the gonads). This necessitates the use of long",
    "label": 0
  },
  {
    "text": "density as a result of \"decreased circulating estrogen and skeletal resistance to androgen action.\" Some intersex people no longer produce their own sex hormones due to receiving a gonadectomy (surgical removal of the gonads). This necessitates the use of long term hormone therapy to maintain bone health. People with Turner syndrome, gonadal dysgenesis, Klinefelter syndrome and congenital adrenal hyperplasia also face issues with bone health, particularly osteopenia and osteoporosis. In Klinefelter syndrome and gonadal dysgenesis, this is caused by insufficient sex hormones. In CAH, it is due to excessive adrenal androgen as well as lifelong treatment with glucocorticoid, a type of steroid. Bone health can be maintained through monitoring bone mineral density using dual-energy x-ray absorptiometry and providing hormone replacement therapy to prevent osteoporosis. Hormone replacement serves additional purposes other than just maintaining bone health. In those with CAH, steroids called mineralocorticoids help regulate salt and water in the body to prevent salt-wasting (an imbalance of salt in the body). In some intersex people, oestrogen or testosterone is needed to induce puberty. This is needed to help the body mature and develop secondary sex characteristics, which allows intersex people to develop alongside their peers, and prevent delayed intellectual, social, and emotional development. Some intersex variations are associated with an increased risk of cancer. Men with Klinefelter syndrome, especially those with mosaicism, are at a higher risk of dying from breast cancer than endosex men. Those with Swyer syndrome are at an increased risk of developing cancer, specifically germ cell tumors, if atypical gonads are not removed. Gonadoblastomas are precancerous lesions that predominantly form in intersex people with gonadal dysgenesis and a Y chromosome. This includes conditions such as Swyer syndrome or Turner syndrome with Y chromosome mosaicism. These cancer risks are generally addressed through genetic screening and a gonadectomy, if",
    "label": 0
  },
  {
    "text": "that predominantly form in intersex people with gonadal dysgenesis and a Y chromosome. This includes conditions such as Swyer syndrome or Turner syndrome with Y chromosome mosaicism. These cancer risks are generally addressed through genetic screening and a gonadectomy, if necessary. The undescended testes of those with androgen insensitivity syndrome also pose a cancer risk. Those with complete androgen insensitivity syndrome have a 3.6 percent chance of developing a malignant tumor by age 25 and a 33 percent chance by age 50, though malignancy before adulthood is rare. Partial androgen insensitivity syndrome poses a higher cancer risk if undescended testes are present. The risk for germ cell tumors from undescended testes is 15 to 50 percent, but the risk for testes surgically moved to the scrotum is unknown. Orchiectomy, removal of the testes, is a preventative treatment option for these cancer risks. For those with partial androgen insensitivity syndrome who are raised male, a procedure to put the testes into the scrotum called an orchiopexy is done to lower the risk of malignancy. Some intersex variations are associated with metabolic and cardiovascular conditions. Those with XY DSD are more likely to be born with heart defects. Intersex variations in sex chromosomes are associated with an increased risk of both type one and type two diabetes. Turner syndrome, one such chromosomal intersex variation, is also associated with hypertension. 23 to 50 percent of those with Turner syndrome are born with congenital heart abnormalities. Fetuses with confirmed or suspected Turner syndrome should receive a fetal echocardiogram. Children born with Turner syndrome and accompanying heart abnormalities are recommended to receive care from a pediatric cardiologist. Various intersex conditions coincide with kidney abnormalities. People with androgen insensitivity syndrome may have atypical kidneys or upper urinary tracts. Hypospadias may also be related to urinary tract",
    "label": 0
  },
  {
    "text": "accompanying heart abnormalities are recommended to receive care from a pediatric cardiologist. Various intersex conditions coincide with kidney abnormalities. People with androgen insensitivity syndrome may have atypical kidneys or upper urinary tracts. Hypospadias may also be related to urinary tract anomalies, though existing research is conflicting. People with Herlyn-Werner-Wunderlich syndrome can be born with one kidney. Urinary tract ultrasounds can be used to check for kidney abnormalities in those with Herlyn-Werner-Wunderlich syndrome. Other types of anomalies may be present at birth. For example, most people born with campomelic dysplasia have micrognathism, a small lower jaw, and laryngomalacia, floppy cartilage in the larynx that obstructs one's airway. Cleft palates are also present in most people born with the condition. Cleft palates, severe laryngomalacia and micrognathia are treatable through surgery. Sexual health Fertility varies depending on the intersex variations one has. For instance, people with Turner syndrome can become pregnant with donor eggs or, more rarely, become pregnant unassisted; meanwhile, men with 46 XX are always infertile. Fertility counseling can be provided by a clinician to address fertility complications caused by the patient's intersex variation and what treatment options are available. Procedures such as testicular sperm extraction, in vitro fertilization, or receiving a uterus transplant can help some intersex people have children. People with MRKH syndrome, a rare intersex condition causing an absent or underdeveloped vagina and uterus, can produce children with a surrogate and in-vitro fertilization of their own eggs since they usually have fully-formed ovaries. Since 2015, uterus transplants have allowed women with MRKH to give birth without a surrogate. Intracytoplasmic sperm injection, a type of in vitro fertilization in which sperm is directly injected into an egg cell, is another tool for assisted reproduction. It can help people with 5-alpha-reductase type 2 deficiency or Klinefelter syndrome to have children.",
    "label": 0
  },
  {
    "text": "Intracytoplasmic sperm injection, a type of in vitro fertilization in which sperm is directly injected into an egg cell, is another tool for assisted reproduction. It can help people with 5-alpha-reductase type 2 deficiency or Klinefelter syndrome to have children. It can also be used in some cases of ovotesticular DSD if the father is able to produce sperm and has not had his testes removed. In-vitro fertilization with donor eggs allows people with a uterus but no ovaries to give birth, such as in some cases of Swyer syndrome. Females with Turner syndrome lose their eggs at a fast rate compared to endosex females; before they are born, the majority of their egg cells are apoptopic, or in the process of dying. Most people with Turner syndrome experience early menopause. In some cases, cryopreservation is used to maintain reproductive options. For those with Turner syndrome who experience puberty and periods without medical intervention, primarily those with 45X/46XX mosaicism, eggs may be cryopreserved. In 2022, the first successful live birth using this method on a mother with mosaic Turner syndrome was documented. Some intersex variations may result in increased risks of complications during pregnancy. Classical CAH is associated with higher risk of gestational diabetes. Those with classic CAH are at a higher risk of needing to deliver via cesarean section if they have previously undergone vaginoplasty. Those with CAH caused by 11-beta-hydroxylase deficiency often experience hypertension; it is advised that they receive care from cardiology specialists during pregnancy. Sixty percent of women with Turner syndrome who become pregnant via egg donation experience complications such as pre-eclampsia, preterm birth, and intrauterine growth restriction. There is also a risk of death for the mother due to root aortic dilation, or the ballooning of the large artery sending blood from the heart. This",
    "label": 0
  },
  {
    "text": "donation experience complications such as pre-eclampsia, preterm birth, and intrauterine growth restriction. There is also a risk of death for the mother due to root aortic dilation, or the ballooning of the large artery sending blood from the heart. This can result in aortic dissection, a splitting of the aorta's walls that causes blood to leak. Doctors recommend that women with Turner syndrome be informed of this risk, undergo cardiac review prior to pregnancy, and receive care from a multidisciplinary team while pregnant. Some intersex people are more likely to experience sexual dissatisfaction or difficulty in sexual functioning. For example, 39.3 percent of men with Klinefelter syndrome and 37.1 to 44.1 percent of women with XY DSD are unsatisfied to very unsatisfied with their sex life, compared to 20.8 percent of the general population. This is not universal across all intersex conditions, as those with Turner syndrome and CAH report sex life satisfaction closer to that of the general population. Some intersex conditions are associated with genital dissatisfaction, including concerns about penile length, vaginal length, and clitoral size. Intersex people who have undergone genital surgery may experience reduced sexual sensation and functioning as a result. As of 2020, young intersex people often experience sexual anxiety, and a quarter of intersex adults have not been in a romantic or sexual relationship. Research suggests intersex patients should be offered psychological support or sex therapy if they express anxiety about sexual function or fear of intimacy. A sexologist can be included in a multidisciplinary team to help address sexual health concerns. Hormone replacement therapy can be used to maintain sexual functioning for some intersex conditions such as ovotesticular DSD. For those with CAIS who have had a gonadectomy, androgen substitution can prove better for sexual well-being and ability to orgasm than oestrogen. Mental",
    "label": 0
  },
  {
    "text": "therapy can be used to maintain sexual functioning for some intersex conditions such as ovotesticular DSD. For those with CAIS who have had a gonadectomy, androgen substitution can prove better for sexual well-being and ability to orgasm than oestrogen. Mental health Intersex people are at a high risk of developing mental health disorders. A review of 18 studies found that intersex people were more likely to have depressive and anxiety disorders. A survey of intersex adults in the U.S. found that 61.1% of respondents reported having depressive disorders compared to 19% of the general adult population. Post-traumatic stress disorder was also reported by 40.9% of those surveyed. Surgeries on intersex people as infants can be a cause of mental trauma. Scarring, infertility, and other health issues caused by such surgeries can result in shame, gender dysphoria, sexual dysfunction and feeling betrayal or devalued. Stressful diagnosis procedures and medical treatments can feel like a challenge to one's gender identity, contributing to psychological distress and stigmatization. Intersex students are often the target of abuse at school, especially in bathrooms and changing rooms. Students whose appearance do not fit expected gender norms are at even greater risk of mistreatment. A survey in Mexico found that 26% of intersex respondents reported being bullied as children compared to about 15 percent of endosex respondents. Intersex students are often told to keep medical treatments secret by family members or clinicians, leading to further isolation and untreated trauma. Intersex people are at an increased risk of suicide; in a European study, 6.8% of intersex people age 16 or older reporting having attempted suicide compared to 1.8% of the general population sampled. Intersex people are more likely to experience suicidal thoughts as well. In a Mexican survey, 16% of intersex respondents reported experiencing suicidal ideation compared to 10%",
    "label": 0
  },
  {
    "text": "or older reporting having attempted suicide compared to 1.8% of the general population sampled. Intersex people are more likely to experience suicidal thoughts as well. In a Mexican survey, 16% of intersex respondents reported experiencing suicidal ideation compared to 10% of endosex women and 7% of endosex men. Intersex people can face issues that worsen their mental health such as stigma and discrimination, bullying, family rejection, tension with partners about fertility, etc. It is not uncommon for intersex people born with ambiguous sex characteristics to experience gender dysphoria. A 2021 meta-analysis found that 15% of intersex people born with ambiguous sex characteristics experience gender identity disorder or gender dysphoria in adolescence or adulthood. Those assigned female at birth with 5-alpha-reductase deficiency or 17-hydroxysteroid dehydrogenase deficiency have a high prevalence of gender dysphoria (53 percent for both conditions). People assigned female at birth with CAH, CAIS, and complete gonadal dysgenesis have low prevalence of gender dysphoria. Those with intersex variations should have a psychiatrist or psychologist working with a multidisciplinary team. Mental health professionals can aid intersex people when making choices about hormone treatments, gender assignment, gender reassignment, and surgeries. Psychotherapy can also be used to support self-acceptance. Psychological support for parents and family members helps reduce harm to an intersex child. This support helps by educating family against misconceptions about intersex variations, as well as encouraging parents to share age-appropriate information with their child. This aids in preventing secrecy about the child's intersex status. Findings suggest peer support for intersex people and their parents can positively contribute to their well-being, though robust evidence is lacking. Peer support groups supervised by a facilitator serve to mitigate social isolation, provide emotional validation, and help members of the group process medical information. Accessing peer support can prove difficult due to lack of referrals.",
    "label": 0
  },
  {
    "text": "though robust evidence is lacking. Peer support groups supervised by a facilitator serve to mitigate social isolation, provide emotional validation, and help members of the group process medical information. Accessing peer support can prove difficult due to lack of referrals. Cognitive health and neurodevelopment Some intersex variations can impact cognitive functioning or carry higher rates of neurodevelopmental conditions like autism or ADHD. Rates of autism symptoms are higher in intersex people. A survey of European intersex adults found a 9.1 percent prevalence, compared to 1 percent in the general adult population. This percentage differs depending on the particular intersex variation, with those with Klinefelter syndrome having the highest autism symptom prevalence of the variations measured. Intersex people have no significant difference in intelligence quotient compared to endosex people. Other cognitive and neurodevelopmental differences depend on the particular variation. Females with Turner syndrome are more likely to be diagnosed with attention deficit disorder, can struggle with social competence in both childhood and adulthood, and can have visuospatial deficits that negatively impact math ability. Those with Klinefelter syndrome are likely to experience language issues. 70 to 80 percent of males with Klinefelter syndrome experience language difficulties at an early age. Impairments in verbal fluency and naming viewed stimuli have been found in adolescents and adults. Reading difficulties are similarly common in children and adults with Klinefelter syndrome. A psychologist can be consulted to assess learning difficulties if they are present. Academic accommodations, tutoring, or other forms of support may be equipped to help those with cognitive impairments. Psychiatric interventions and other supports may be needed throughout one's life to help with difficulties in executive and cognitive functioning. Mental health issues may be the main cause of executive function issues, as a study comparing intersex and endosex people found that there was no",
    "label": 0
  },
  {
    "text": "may be needed throughout one's life to help with difficulties in executive and cognitive functioning. Mental health issues may be the main cause of executive function issues, as a study comparing intersex and endosex people found that there was no significant difference in executive function when adjusting for mental health. History Intersex variations have been documented since antiquity. Intersex people were included in art and ancient myths such as the creation of an intersex person by the Sumerian gods Enki and Nimnak or Hermaphroditus in ancient Greece. The first surgery on ambiguous genitalia was recorded by historian Diodorus Siculus in the first century BCE. The patient, Callo, was believed to be female prior to having a fistula lanced, exposing testes and a penis. In the 600s CE, surgeon Paulus Aegineta described intersex conditions and hypospadias. This influenced later surgeons like Albucasis in the 11th century to write about surgeries for intersex genital variations such as clitoris removal. Ottoman surgeon Şerafeddin Sabuncuoğlu included an illustrated chapter about surgical treatments for intersex genital variations in his 15th century manuscript. In the 1800s, medical professionals in Great Britain, France, and the United States began searching for definitive markers to determine what was thought to be the intersex person's true sex. These determinants were used to inform the patient of their true sex with the expectation that the patient would subsequently change to fit social norms. The term 'intersexuality' was created in 1915 by German biologist Richard Goldschmidt. He came up with the term while studying sphynx moths with sex characteristics between male and female. The term was adopted for pediatric use. In the 1930s, surgeries intended to correct the genitals of intersex people were developed and became a standard treatment. At the time, surgeon Lennox Ross Broster described that corrective surgery would cause",
    "label": 0
  },
  {
    "text": "female. The term was adopted for pediatric use. In the 1930s, surgeries intended to correct the genitals of intersex people were developed and became a standard treatment. At the time, surgeon Lennox Ross Broster described that corrective surgery would cause patients to \"lose their acquired male characters, and revert to their normal feminine ones...[and] return to normal sexuality psychologically.\" Broster worked with psychiatrist Clifford Allen towards understanding the psychological impacts of an intersex person's biology; both believed that psychological abnormalities would arise from atypical anatomy. In 1940, Allen argued for the separation of intersex people from endosex people desiring sex reassignment, providing surgical procedures only to intersex people while directing transgender people towards psychological services. In the 1950s, medical advances emerged to evaluate a fetus' sex and birth defects such as ultrasound and amniocentesis. As technologies advanced, clinicians became able to detect intersex conditions such as congenital adrenal hyperplasia and Turner syndrome before the birth of the child. The same decade, John Money theorized that children were gender neutral until the age of two, and gender could be assigned based on how a child was raised. Money and his colleagues, Joan and John Hampton, put forward several protocols for the management of intersex people that became greatly influential in the global north. For instance, he coined the term gender role initially as a diagnostic criteria to determine if someone with ambiguous genitalia was a male or female. Predictions of an infant's future gender role would inform what treatments the child received. Money's work promoted surgeries for intersex people, young children in particular. Clinicians often hid that a child was intersex both from patients and their parents. Money's gender categorization and surgical approach was informed by norms at the time such as the belief in male-female dichotomy and a homosexual-heterosexual dichotomy.",
    "label": 0
  },
  {
    "text": "in particular. Clinicians often hid that a child was intersex both from patients and their parents. Money's gender categorization and surgical approach was informed by norms at the time such as the belief in male-female dichotomy and a homosexual-heterosexual dichotomy. Correcting intersex bodies reinforced these norms by aiming to produce heterosexual males or females. His approach was also perpetuated by the pathologization of intersex people; it fit intersexuality into the medical process of diagnosis, analysis, and treatment. In other words, intersex conditions would be diagnosed and normalizing treatments were determined by analyzing which gender role the person fit most. This made his ideas agreeable to surgeons and psychologists, who could view their work as helping treat abnormal children. In 1966, Money advised sex reassignment for a baby boy after a botched circumcision. The child would be raised as a girl until he learned the truth about his birth sex at age 15 and lived as a male after. When Money began publishing his findings in 1972, he portrayed it as a case of successful sex reassignment supporting his idea that gender could be changed at an early age. This case, which became known as the Joan/John case, would be followed in medical literature and mainstream press for decades. Money's publications on this case perpetuated the practice of performing normalizing surgeries on intersex children by portraying a child's gender identity as changeable through upbringing. The first intersex support groups formed in the 1980s, starting with Canadian support groups for Turner syndrome in 1981. The later half of the decade saw a rise in advocacy against normalizing surgeries from patients and the start of objections from professionals. Intersex activists continued to challenge the use of corrective genital surgery and the stigma around intersex variations the following decade; in 1993, activist Bo Laurent,",
    "label": 0
  },
  {
    "text": "rise in advocacy against normalizing surgeries from patients and the start of objections from professionals. Intersex activists continued to challenge the use of corrective genital surgery and the stigma around intersex variations the following decade; in 1993, activist Bo Laurent, also known as Cheryl Chase, formed the Intersex Society of North America. In the early 1990s, feminist scholars began criticizing Johns Hopkins Hospital's medical policy for using gender norms to make predictions about the gender of intersex infants, as well as medically altering their bodies to fit those norms. In 1995, a paper advocating for withholding medical information from patients with androgen insensitivity syndrome was awarded second place in a medical ethics essay contest. The author received a cash prize from the Canadian Medical Association. In 1996, the first modern intersex public demonstration took place in Boston outside of the American Academy of Pediatrics' annual meeting. In 2005, the European Society for Paediatric Endocrinology and the Wilkins Pediatric Endocrine Society held a conference with 50 international experts and two intersex participants decide which term should be used to refer to people with intersex variations. The term disorders of sex development, abbreviated as DSD, was chosen. In a 2006 consensus statement, the adoption of the term DSD was purposed. Since then, the term DSD has replaced intersexuality in pediatric use. In 2011, the first International Intersex Forum took place in Brussels, Belgium. The forum was designed for intersex activists, advocates and organizations to raise awareness towards intersex human rights issues and make political demands such as ending normalizing medical procedures and selective abortion of intersex fetuses. In 2013, the third International Intersex Forum created the Malta Declaration. This broadly outlines what the intersex community wants to achieve globally. Africa, Asia, Europe, Latin America and the Caribbean, Oceania, and North America each",
    "label": 0
  },
  {
    "text": "selective abortion of intersex fetuses. In 2013, the third International Intersex Forum created the Malta Declaration. This broadly outlines what the intersex community wants to achieve globally. Africa, Asia, Europe, Latin America and the Caribbean, Oceania, and North America each have regional additions to the declaration. Barriers to care Medical trauma Many intersex people have gone through negative medical experiences including childhood genital surgery, having their medical history hidden from them, pathologization of intersex variations, and genital examinations and photography. Genital surgery on young children, excessive medical exams and nonconsentual genital photography have been described as medicalized rape. According to the NNID Foundation, an intersex organization in the Netherlands, \"These actions [of medicalized rape and sexual violence] include: grooming behaviour, provoking sexual arousal without consent sometimes even on small children, construction of vagina's [sic] on children and teens that require insertion of penis-shaped objects [vaginal dilation], repeated observing and examining genitals far beyond any level needed to provide care, but rather out of personal interest, and producing photographs and video's [sic] of intersex genitals without consent and spreading these images without restrictions.\" These experiences negatively impact the well-being of intersex people. Vaginal dilation and surgeries early in life cause physical and psychological trauma. Nonconsensual surgeries can contribute to healthcare avoidance and medical distrust among intersex people, causing them to delay receiving preventative or emergency care. Some intersex people also have information about their diagnosis withheld from them, potentially leading to them learning about their condition in an unsupportive setting rather than from an (ideally) sensitive disclosure from a doctor. Lack of quality data Data is often collected from populations using either sex or gender, but not both. This results in problems when collecting data from intersex people. Uncertainty in how to answer can result in data being miscollected. Assumptions underlying",
    "label": 0
  },
  {
    "text": "of quality data Data is often collected from populations using either sex or gender, but not both. This results in problems when collecting data from intersex people. Uncertainty in how to answer can result in data being miscollected. Assumptions underlying questions about sex, such as expecting a respondent's sex characteristics not to vary from the sex they answered with, can lead to the misuse of data. These issues in data collection negatively impact the healthcare of intersex people as some tests and medical treatments are affected by one's sex. In some cases, data on intersex people may not be collected at all. Data systems supported or led by the Center for Disease Control and Prevention between 2015 and 2018 collected no data on intersex conditions. There is a lack of research into the general health, mental health, and cancer risks of intersex people. Intersex people are often not studied in the field of toxicology, making risk assessment for intersex patients difficult. Anesthesiology also under-represents intersex people. In 2016, none of the major anesthesiology journals reported if any of their participants were intersex. There is lack of research in fertility preservation for intersex people due to the larger focus on creating a normative sex presentation. Inaccessibility Intersex people have better outcomes when receiving specialized care. When intersex people reach adulthood, they can experience difficulty finding specialized care for their variations; this is in part due to a lack of specialists for adults with significant training about intersex conditions. A small European study found that 28 percent of adult intersex participants had difficulty accessing specialist care. A study in Bangladesh found that intersex people were often turned away from government healthcare facilities. Less than half of intersex people succeeded in accessing care from government healthcare facilities, while over 80 percent managed to",
    "label": 0
  },
  {
    "text": "difficulty accessing specialist care. A study in Bangladesh found that intersex people were often turned away from government healthcare facilities. Less than half of intersex people succeeded in accessing care from government healthcare facilities, while over 80 percent managed to access care from private ones. Some of the barriers to care included unfriendly interactions with both physicians and nonclinical staff, excessive public attention due to their intersex status, public fear of interacting with intersex people, and limited treatment for patients who are neither male nor female. Intersex people desiring psychological support may not be offered such services. Another study found that the majority of intersex patients had not been offered psychological counseling, and 27 percent reported not being offered counseling while having an unmet need for it. Intersex children can have difficulty finding support at school as well. There are no established standards for educational psychologists or counselors supporting intersex children, and peers are typically undereducated about intersex people. Intersex people and their families frequently experience a lack of psychological support. About 40 percent of parents of intersex children in Germany requested psychological support, half of whom received it. When psychologists are included within multidisciplinary care teams for intersex patients, they often feel their role is marginalized by their peers. Family members of intersex children may feel that being referred to a psychologist implies they are not performing their role in the family well. Intersex people and their caretakers may also have negative perceptions about therapy, finding it taboo or unpleasant. Intersex people experience issues accessing sexual health and affirming doctors. A survey from the United Kingdom's Government Equalities Office found that 11 percent of intersex participants reported that it was difficult to access sexual healthcare compared to 5 percent of endosex LGBT participants. In the UK, 6 percent of",
    "label": 0
  },
  {
    "text": "doctors. A survey from the United Kingdom's Government Equalities Office found that 11 percent of intersex participants reported that it was difficult to access sexual healthcare compared to 5 percent of endosex LGBT participants. In the UK, 6 percent of intersex people reported that their general practitioner was unsupportive compared to 2 percent of endosex LGBT people. It is difficult for elderly intersex people to find providers capable of meeting their needs. They may fear living in retirement communities due to potential intolerance from other residents. Elderly intersex people can feel concerned about home care as well; they may fear caretaker abuse due to their intersex variation or surgical changes to their body. Cost and availability in developing countries Healthcare for intersex people in developing countries can be especially difficult to access due to cost or low availability. For instance, Vietnam only has one pediatric endocrinology department in the country, at the National Hospital in Hanoi. Due to the lack of government funding towards healthcare, the cost of basic medications like estrogen tablets can be a major financial burden. In Bangladesh, the lack of treatment from government healthcare facilities can pose issues for intersex people if they cannot afford private care or do not have a private health facility near them. Cost can also influence what type of procedures are performed. In Zimbabwe, genetic testing to determine a child's sex is not offered in public hospitals, and testing can cost up to 1200 U.S dollars from private hospitals. This makes it cost prohibitive for many families. Since normalizing surgeries on intersex infants are offered for free in public hospitals, this can lead parents to opt for these procedures to assign a child's sex instead of testing. Lack of expertise and resources can also hinder diagnosis of intersex conditions. Radiography and",
    "label": 0
  },
  {
    "text": "intersex infants are offered for free in public hospitals, this can lead parents to opt for these procedures to assign a child's sex instead of testing. Lack of expertise and resources can also hinder diagnosis of intersex conditions. Radiography and ultrasound diagnosis can be unreliable depending on the age of the patient and the experience level of the operator. This can be an issue in developing countries, where skilled operators are scarce. Doctors in developing countries may lack access to continuing medical education, causing them to lack the knowledge to diagnose uncommon intersex conditions. Cultural conflicts In India, parents often argue with physicians when an intersex child is assigned female. Due to women often being economically dependent on men, there is a fear that an intersex woman will not be able to get a husband, especially if her intersex variation makes her infertile. This could cause her to financially depend on her family for life or become impoverished. In developing countries, an intersex woman may also have difficulty finding work if infertility causes her to remain unmarried due to the overarching social stigma towards unmarried women. Oldest sons are also expected to financially provide for their parents. Together, cultural preferences for sons and the importance of marriage for women can cause families with intersex children to put off seeking medical care. It can also contribute to healthcare inequality with care being sought out more for intersex boys than intersex girls. Fears about marriage and financial security also impact sex assignment in Egypt and Saudi Arabia. Due to this, over 60 percent of intersex children are raised male in Egypt. Preference for assigning intersex children as male has also been observed in Thailand, Malaysia, and Turkey. Advocacy Intersex and human rights organizations advocate against medically unnecessary genital surgeries on young children.",
    "label": 0
  },
  {
    "text": "percent of intersex children are raised male in Egypt. Preference for assigning intersex children as male has also been observed in Thailand, Malaysia, and Turkey. Advocacy Intersex and human rights organizations advocate against medically unnecessary genital surgeries on young children. Deferral of procedures is promoted whenever possible so patients are old enough to provide consent. Clinician advocates have worked with professional organizations to write position statements in support of deferring non-essential genital surgery until a child is older. Most discourse criticizing nonconsensual normalizing surgeries, also called intersex genital mutilation, comes from Western countries. However, Eastern organizations such as Intersex Asia and Intersex South Africa oppose intersex genital mutilation as well. Advocates promote patient-centered care rather than procedures to ease parents. Intersex organizations and clinician advocates both promote mental health support of intersex people as a key part of intersex healthcare. Other changes supported by intersex advocates include ensuring intersex patients have access to medical records, acquiring government reparations for those who have received medically unnecessary surgery or demeaning treatment, and holding doctors who have given such treatment legally accountable. Intersex activists also advocate against sex-selective abortion of intersex fetuses. Prenatal testing can detect chromosomal intersex variations, leading some parents to feel pressured to abort the fetus due to a lack of information about these conditions. NNID views the use of medical testing for intersex prevention as \"a form of eugenics\". The Malta Declaration from the Third Intersex International Forum includes \"To put an end to preimplantation genetic diagnosis, pre-natal screening and treatment, and selective abortion of intersex foetuses\" on its list of demands. Ensuring intersex people are respected in medical settings is another priority for intersex rights. Lambda Legal and InterACT support hospitals adding language to their patients' bill of rights to explicitly state the rights to nondiscrimination, privacy, and",
    "label": 0
  },
  {
    "text": "LAMA2 muscular dystrophy (LAMA2-MD) is a genetically determined muscle disease caused by pathogenic mutations in the LAMA2 gene. It is a subtype of a larger group of genetic muscle diseases known collectively as congenital muscular dystrophies. The clinical presentation of LAMA2-MD varies according to the age at presentation. The severe forms present at birth and are known as early onset LAMA2 congenital muscular dystrophy type 1A or MDC1A. The mild forms are known as late onset LAMA2 muscular dystrophy or late onset LAMA2-MD. The nomenclature LGMDR23 can be used interchangeably with late onset LAMA2-MD. Suggestive clinical features include, muscular hyperlaxity or hypotonia, growth retardation progressive spine and joint contractures, and cardiac and respiratory failure. For consensus, generally, the term congenital muscular dystrophy refers to a diverse group of childhood onset muscle diseases -usually occurring the first two years of life- and mostly inherited through an autosomal recessive mode. Congenital muscular dystrophies have known phenotype-genotype profiles and produce muscle degenerative pathology. Symptoms and signs There are two types of LAMA2 muscular dystrophy (LAMA2-MD). The first type is the congenital type known as early onset LAMA2 congenital muscular dystrophy type 1A or MDC1A. It presents at birth and has a relatively severe clinical presentation. Characteristically it manifests in muscle weakness, hyperlaxity or hypotonia, respiratory difficulties and developmental delay. The second type is the late onset LAMA2 muscular dystrophy or late onset LAMA2-MD. The age of presentation of late onset LAMA2-MD ranges from early childhood to adulthood. It usually has a mild clinical presentation in the form of progressive spine and joint contractures, and cardiac and respiratory failure. Musculoskeletal manifestations Musculoskeletal manifestations Delayed development of motor milestones and loss of ambulatory capacity are usually more severe in the congenital type 1A or MDC1A. Skeletal muscle weakness is a characteristic feature. It is more",
    "label": 0
  },
  {
    "text": "and cardiac and respiratory failure. Musculoskeletal manifestations Musculoskeletal manifestations Delayed development of motor milestones and loss of ambulatory capacity are usually more severe in the congenital type 1A or MDC1A. Skeletal muscle weakness is a characteristic feature. It is more evident in the proximal muscles of the extremities. Facial and neck weakness have also been reported. Scoliosis is a side curvature or abnormal deviation of the spine with an element of rotation. Scoliosis is usually rigid and progressive. It may be accompanied by lordosis. The clinical orthopedic features of congenital type 1A (MDC1A) in terms of type, distribution, laterality, deformity progression, and chronological order of muscle and joint involvement have shown a fairly characteristic pattern. This is important to the differential diagnosis of LAMA2-MD and other subtypes of congenital muscular dystrophies. LAMA2-MD, especially MDC1A, usually manifests in progressive contractures of large joints such as knees, ankles, elbows, and hips. Contractures tend to be bilateral, involving both left and right sides. Observing the chronological order of development of joint contractures—namely, early versus late in the disease course—can offer diagnostic clues for distinguishing congenital muscular dystrophies such as MDC1A and LMNA-related muscular dystrophy, among other genetic muscle diseases. Of note, any unique clinical orthopedic features of LAMA2-MD should be interpreted in conjunction with other clinical features, characteristic brain and muscle imaging, muscle immunostaining, and genetic testing findings. An international retrospective early natural history study of LAMA2-MD proposed a classification based on motor or ambulatory capacity, in which patients who attain the ability to sit and remain seated are classified as LAMA2-MD1 or LAMA2-RD1, and those who attain the ability to walk independently are classified as LAMA2-MD2 or LAMA2-RD2. A study on a large series of LAMA2-MD patients showed that bone mineral density was reduced in all adults and most children, and fragility",
    "label": 0
  },
  {
    "text": "LAMA2-RD1, and those who attain the ability to walk independently are classified as LAMA2-MD2 or LAMA2-RD2. A study on a large series of LAMA2-MD patients showed that bone mineral density was reduced in all adults and most children, and fragility fractures were reported occasionally. Recent clinical reviews emphasize the importance of early physiotherapeutic and orthopedic interventions—such as bracing, contracture prevention, and assisted standing—in delaying progression of musculoskeletal complications and maintaining joint mobility in LAMA2-related muscular dystrophies. The clinical orthopedic features of congenital type 1A (MDC1A) in terms of type, distribution, laterality, deformity progression, chronological order of muscle and joint involvement etc., have shown a fairly characteristic pattern. This is important to the differential diagnosis of LAMA2-MD and other subtypes of congenital muscular dystrophies among others. LAMA2-MD especially MDC1A, usually manifests in progressive contractures of large joints like knees, ankles, elbow and hips. Contractures tend to be bilateral. That is involving both the left and right sides. Observing the chronological order of development of joint contractures, namely early versus late in the disease course, could offer differential diagnostic clues for congenital muscular dystrophies as MDC1A, LMNA-Related muscular dystrophy among other genetic muscle diseases. Of note, any unique clinical orthopedic features of LAMA2-MD should be put into context with the other clinical features, characteristic brain and muscle imaging, muscle immunostaining and genetic testing findings. An International retrospective early natural history study of LAMA2-MD proposed a classification based on motor or ambulatory capacity in which patients who attain the ability to sit and remain seated are classified as LAMA2-MD1 or LAMA2-RD1 and those who attain the ability to walk independently are classified as LAMA2-MD2 or LAMA2-RD2. A study on a large series LAMA2-MD patients showed that bone mineral density was reduced in all adults and most children. Fragility fractures were reported occasionally. Respiratory",
    "label": 0
  },
  {
    "text": "who attain the ability to walk independently are classified as LAMA2-MD2 or LAMA2-RD2. A study on a large series LAMA2-MD patients showed that bone mineral density was reduced in all adults and most children. Fragility fractures were reported occasionally. Respiratory insufficiency Respiratory insufficiency can occur in both types of LAMA2-MD. Respiratory tract infections are a cause of death in the congenital type 1A or MDC1A. Cardiac manifestations Cardiac involvement in LAMA2-MD may manifest in dilated cardiomyopathy and systolic dysfunction. Cardiac screening and surveillance are important in LAMA2-MD. This is aimed at timely diagnosis and management of subclinical cardiac involvement. Cerebral manifestations Epilepsy is a fairly common manifestation of both types of LAMA2-MD. However, the age at occurrence of first epileptic fit is earlier in the congenital type 1A or MDC1A. Screening for epilepsy should be included in the workup. Intelligence is usually normal. Epilepsy and intellectual disability were associated with motor dysfunction namely inability to sit and/or walk. Epilepsy and to a lesser extent intellectual disability were also strongly correlated to cortical abnormalities on brain MRI. Cause LAMA2-MD is caused by pathogenic variants or mutations in the LAMA2 gene that encodes alpha2 chain of laminin-211 or laminin-alpha2, previously known as laminin type 2 or merosin. laminin-211 is important to the function and integrity of the sarcolemma of muscle fibers. laminin-alpha2 is also present in extra-muscular locations as the central and peripheral nervous system. Pathogenic variants of the LAMA2 gene which lead to loss of function are accompanied by complete deficiency of laminin-alpha2 (merosin) and result in a severe clinical picture or phenotype namely early onset MDC1A. Pathogenic variants of the LAMA2 gene accompanied by partial deficiency of laminin-alpha2 result in a milder clinical picture namely late onset LAMA2 muscular dystrophy or late onset LAMA2-MD. The disease is inherited through an",
    "label": 0
  },
  {
    "text": "or phenotype namely early onset MDC1A. Pathogenic variants of the LAMA2 gene accompanied by partial deficiency of laminin-alpha2 result in a milder clinical picture namely late onset LAMA2 muscular dystrophy or late onset LAMA2-MD. The disease is inherited through an autosomal recessive mode. Diagnosis Correlating the characteristic clinical picture with the specific imaging, laboratory and muscle biopsy findings is essential to the diagnosis of LAMA2-MD. The presence of pathogenic variants in LAMA2 gene by Genetic testing, -DNA testing- of the affected individual confirms the diagnosis of LAMA2-MD. Brain MRI Abnormal white matter signals in Brain MRI is a near-universal sign in patients with LAMA2-MD. These white matter abnormalities appear as hyperintense signals on T2-weighted and FLAIR brain MRI images especially in locations that are originally myelinated in the immature brain as the periventricular area. Occasional MRI abnormalities include cortical malformations as polymicrogyria, lissencephaly, pachygyria. In LAMA2-MD there seems to be a directly proportional relationship between the magnitude of white mater and cortical abnormalities on brain MRI and the degree of motor dysfunction in terms of the ability to sit and walk. Whole body muscle MRI Muscle MRI especially Whole-body muscle MRI can provide important diagnostic clues. Some studies have shown a reasonably characteristic pattern of muscle involvement on whole-body muscle MRI in LAMA2-MD patients. This relates to muscles or group of muscles involvement versus sparing. For example, sparing of the gracilis, sartorius muscles, and the adductor longus muscle has been linked to LAMA2-MD. On the other hand, studies showed a specific predilection to involve the gluteus maximus and anterior thigh muscles, adductor magnus muscle, serratus anterior muscle in LAMA2-MD, and so forth. Abnormal muscle texture or geometry on muscle MRI as presence of granular pattern of involvement in a muscle has been suggested to be a diagnostic clue. Similarly, a",
    "label": 0
  },
  {
    "text": "muscles, adductor magnus muscle, serratus anterior muscle in LAMA2-MD, and so forth. Abnormal muscle texture or geometry on muscle MRI as presence of granular pattern of involvement in a muscle has been suggested to be a diagnostic clue. Similarly, a homogenous pattern of involvement of group of muscle e.g., anterior compartment of thigh, could be used to support diagnosis. Homogenous pattern refers to involvement of all individual muscles of a muscle compartment to the same extent. Moreover, Whole body muscle MRI could be indicative of clinical disease severity and duration of LAMA2-MD. It can also help establish phenotype genotype correlations However, these muscle MRI features may overlap with other subtypes of congenital muscular dystrophy. Additionally, some inconsistencies between the above muscle imaging studies can be noted. Thus, more longitudinal studies with larger cohorts and standardized methodologies are needed to arrive at a more uniform and consistent muscle MRI signature in LAMA2-MD. It is therefore paramount to correlate muscle imaging findings with clinical, neuro-imaging, laboratory and genetic testing findings. Muscle biopsy or immunostaining There is an inversely proportional relationship between the quantity of laminin alpha2 (merosin) found on immunohistochemistry and disease severity. That means, a more marked degree of laminin-alpha2 deficiency e.g., total or near-total deficiency, is associated with more pronounced muscle degenerative pathology as myofibrosis, necrosis and fiber size variation. This is also associated with a more severe clinical picture. A less marked degree of laminin-alpha2 (merosin) deficiency -residual staining- is associated with a less pronounced muscle degenerative pathology and a milder clinical picture. Generally, congenital muscular dystrophy type 1A or MDC1A is known to have a severer clinical picture than late onset LAMA2-MD. However, the degree of deficiency of laminin alpha2 (merosin) on immunohistochemistry in MDC1A can varies. Clinical disease severity associated with total laminin-alpha2 (merosin) deficiency usually manifests",
    "label": 0
  },
  {
    "text": "or MDC1A is known to have a severer clinical picture than late onset LAMA2-MD. However, the degree of deficiency of laminin alpha2 (merosin) on immunohistochemistry in MDC1A can varies. Clinical disease severity associated with total laminin-alpha2 (merosin) deficiency usually manifests itself in early onset of symptoms, loss of ambulatory capacity and respiratory difficulties. Treatment Targeted therapies There is no definite cure available for LAMA2-MD. However, preclinical studies on experimental animal models of Laminin alpha-2 chain deficient congenital muscular dystrophy are showing favorable yet early results. Generally, these preclinical studies are geared toward investigating the various factors behind disease initiation and progression, and exploration of potential ameliorating or curative therapies. Preclinical studies focus on combating substances that regulate and promote muscle fibrosis in the pathogenesis of LAMA2-MD e.g., TGF-β. This may reduce muscle fibrosis and enhance healthy muscle architecture subsequently. Alternatively, preclinical studies can be geared toward enhancing proteins that are involved in muscle regeneration. Laminin alpha2 (Laminin-211) and laminin-221 complex are an important molecule for muscle cell receptors namely integrin-α7β1 and α-dystroglycan. In LAMA2-CMD the laminin alpha2 deficiency results in malfunctioning or down regulation of integrin-α7β1 and α-dystroglycan. This disrupts the proper linkage between the basal lamina and muscle cell membrane. Consequently, the contractile mechanism is disrupted. Integrin-α7β1 is important to satellite cell function, and myoblast adhesion and viability. Thusly, integrin-α7β1is an important contributor to skeletal muscle regeneration. Cell therapies that compensate for the deficiency or down regulation of integrin-α7β1 have the potential to delay or control the muscle degenerative process and preserve muscle architecture in LAMA2-CMD patients. Additionally, the use of laminin-111 treatment in experimental mouse models of LAMA2-CMD has showed satisfactory results in terms of increase in life expectancy muscle function and regeneration. Supportive treatment Currently, treatment is mainly supportive and palliative. It is directed at anticipating and",
    "label": 0
  },
  {
    "text": "use of laminin-111 treatment in experimental mouse models of LAMA2-CMD has showed satisfactory results in terms of increase in life expectancy muscle function and regeneration. Supportive treatment Currently, treatment is mainly supportive and palliative. It is directed at anticipating and preventing or alleviating the systemic complications associated disease progression. This refers to management of respiratory, cardiac, orthopedic and rehabilitative, central nervous system e.g., epilepsy, gastrointestinal and so forth. Prognosis Prognosis is dependent on the subtype of LAMA2-MD. Nearly all children with early onset or congenital muscular dystrophy type 1A (MDC1A) are unable to walk independently. Nevertheless, children with MDC1A are usually able to sit. Contrastingly, patients with late onset LAMA2-MD are usually able to walk independently. Of note, in both types of LAMA2-MD developmental motor milestones are delayed. Additionally, the prognosis is dependent on the degree of surveillance and supportive care that patients receive in regard to the multisystem manifestations and potential complications of LAMA2-MD. This refers to prompt and timely management of orthopedic, cardiopulmonary, epilepsy and gastrointestinal systems among others. The multisystem manifestations may affect the quality of life of patients with LAMA2-MD. Epidemiology It is estimated that congenital muscular dystrophies occur in between 0.563 per 100,000 (in Italy) and 2.5 per 100,000 (in western Sweden). The prevalence data on congenital muscular dystrophy type 1A (MDC1A) varies by geographic location or population. Example, in the United Kingdom MDC1A constituted about 37% of all congenital muscular dystrophy subtypes namely the most common subtype. In Qatar, MDC1A constituted 48% of congenital muscular dystrophy subtypes with estimated a point prevalence of 0.8 in 100.000 in a patient cohort from the Gulf and Middle East. Contrastingly, in Australia it constituted 16% of all congenital muscular dystrophy subtypes namely the third most common subtype. A scoping review on clinical orthopedic manifestations of congenital muscular",
    "label": 0
  },
  {
    "text": "LINKED syndrome, or LINKage-specific-deubiquitylation-deficiency-induced embryonic defects syndrome, is a rare X-linked genetic disorder caused by mutations in the OTUD5 gene. First discovered in 2021 by NIH researchers, this genetic disorder affects mostly male patients and disrupts embryonic developmental processes (block early growth) which results in critical developmental deficits of the brain paired with facial abnormalities. Diagnosis relies on clinical evaluation, genetic testing (whole-exome sequencing), and brain imaging (MRI). The discovery revealed that OTUD5-mediated deubiquitylation is essential for early brain and facial development, advancing understanding of embryonic growth and genetic regulation. While no cure is available currently, management focuses on supportive care which includes physical therapy alongside occupational therapy. The condition is extremely rare, with only a few dozen cases reported worldwide as of 2025. Symptoms and signs The neurological manifestations of LINKED syndrome include delayed development combined with anomalies of brain anatomy and facial structures because of developmental harm to the neuroectoderm. These developmental deficits can lead to challenges such as delayed speech, difficulty walking, and impaired cognitive skills. Brain malformations, detectable by MRI, include abnormal gyration patterns and cortical thinning effects (brain differences) that develop from mutations affecting embryonic gene expression. The developmental errors that cause craniofacial anomalies produce distinct facial characteristics with characteristics such as eyes widely spaced and pronounced foreheads and misaligned jaws. The essential symptoms outlined in the 2021 Science Advances study has real consequences for patient everyday functioning. Symptoms of LINKED syndrome vary in severity, ranging from severe intellectual disability and significant facial abnormalities to milder developmental delays and physical traits. Affected individuals may experience a spectrum of neurological impairments, from profound deficits to minor issues, alongside diverse craniofacial features. The diverse range of symptoms in LINKED syndrome patients may result from OTUD5 mutation types because missense variants lead to partial dysfunction yet truncations might",
    "label": 0
  },
  {
    "text": "a spectrum of neurological impairments, from profound deficits to minor issues, alongside diverse craniofacial features. The diverse range of symptoms in LINKED syndrome patients may result from OTUD5 mutation types because missense variants lead to partial dysfunction yet truncations might completely destroy enzyme function. Associated Conditions LINKED syndrome may occasionally involve secondary issues tied to its developmental origins, such as neurological deterioration. Research on hearing and vision problems associated with neuroectodermal development remains insufficient due to lack of supporting evidence despite potential links between sensory organ formation and the affected domain. Unlike multi-system disorders like Cornelia de Lange syndrome, LINKED syndrome affects both neurological structures and craniofacial regions exclusively. Causes The primary cause of LINKED syndrome is mutations in the OTUD5 gene, which encodes a deubiquitinating enzyme essential for regulating protein stability during embryogenesis, named Otu Deubiquitinase 5. Different distinct hemizygous mutations, mutations which makes an individual has only one member of a chromosome pair or chromosome segment rather than the usual two, are found, including missense mutation (e.g., E200K, G494S, L352P, R274W, R404W), a splice site mutation, and an in-frame deletion. These variants are under Multiple Congenital Anomalies-Neurodevelopmental Syndrome, linking them to this disorder. When the mutated OTUD5 gene is inherited, males, with one X chromosome, are primarily affected by this syndrome as it has a X-linked recessive inheritance pattern. While for females, they are less likely to get the syndrome as they got two X chromosomes, making the recessive gene harder to express. Mechanism LINKED syndrome stems from mutations in the OTUD5 gene, which directs production of an enzyme, OTU Deubiquitinase 5. This enzyme takes off ubiquitin tags, protein markers of being removed, from specific proteins, especially chromatin remodelers (proteins that unpack DNA). These remodelers promote the turning on of genes for early growth. This tag removal is",
    "label": 0
  },
  {
    "text": "enzyme, OTU Deubiquitinase 5. This enzyme takes off ubiquitin tags, protein markers of being removed, from specific proteins, especially chromatin remodelers (proteins that unpack DNA). These remodelers promote the turning on of genes for early growth. This tag removal is blocked by mutations in OTUD5 and this prevents healthy gene activation required for development. OTUD5 mutations disrupt neural precursors (cells that develop into the brain and spinal cord) and neural crest cells (cells forming facial bones, heart tissue, and other structures). Without proper gene control, these cells grow abnormally and show the brain differences characteristic of LINKED syndrome, such as a thin corpus callosum (fewer nerve connections) or abnormal gyration (irregular brain folds). In some cases, rare heart defects, along with facial features like widely spaced eyes, pronounced foreheads and misaligned jaws also occur. This faulty cell development represents a key growth pathway with genes identical to those seen in syndromes like Coffin-Siris and Cornelia de Lange. Diagnosis Clinical evaluation As males have X-linked recessive inheritance, clinical evaluation often starts with checking features commonly seen in males. Those features include developmental delays, hypotonia, intellectual disability and craniofacial anomalies including telecanthus and epicanthic fold. Also, thin corpus callosum or ventriculomegaly, kinds of brain malformations are also often noticed. These symptoms are often observed in male infants or children with neurodevelopmental disorder and birth defect. Genetic test To diagnosis LINKED syndrome, OTUD5 gene mutation needs to be detected, as it is the cause of the syndrome. To detect the mutation, whole exome sequencing or whole genome sequencing are often performed, to find out the mutation on X chromosome at Xp11.23, confirmed by Sanger sequencing for variants like E200K or R404W. To increase the diagnositic accuracy, brain imaging can be conducted to show structural abnormalities that are different from clinical findings. Screening Prenatal",
    "label": 0
  },
  {
    "text": "out the mutation on X chromosome at Xp11.23, confirmed by Sanger sequencing for variants like E200K or R404W. To increase the diagnositic accuracy, brain imaging can be conducted to show structural abnormalities that are different from clinical findings. Screening Prenatal testing is also possible in a family with a known history as this disease is a genetic disorder, such that they could be inherit to the offsprings. The testing could also be done when there is abnormalities during ultrasonography. This approach allow early detection of the syndrome, reducing the risk of passing the gene to the offspring. Management Currently, no specific treatment exists for LINKED syndrome, a rare genetic disorder caused by OTUD5 gene mutations, identified in 2021. Management centers on supportive care tailored to symptoms, primarily developmental delays, brain malformations, and craniofacial anomalies prevalent in affected males. Multidisciplinary strategies aim to enhance quality of life. Therapeutic approaches Physical therapy and occupational therapy address hypotonia and motor skill deficits, while speech therapy targets communication difficulties linked to intellectual disability. Individualized educational programs support cognitive and developmental progress. Surgical interventions Surgical intervention may be considered for congenital anomalies, such as craniofacial defects, depending on clinical severity, as suggested by early descriptions of the condition. Brain imaging, such as magnetic resonance imaging, monitors structural abnormalities like a thin corpus callosum or ventriculomegaly, guiding care adjustments. Seizures, if present, are managed with standard antiepileptic medications, tailored to individual response. Epidemiology The prevalence of LINKED syndrome remains uncertain because of its recent identification in 2021 and limited reporting. The Science Advances study documented 10 cases within a limited number of patients which indicates an extremely low prevalence rate. Early research suggests it is exceptionally rare, but underdiagnosis is likely because its symptoms resemble other neurodevelopmental disorders, and genetic testing for OTUD5 mutations is not",
    "label": 0
  },
  {
    "text": "cases within a limited number of patients which indicates an extremely low prevalence rate. Early research suggests it is exceptionally rare, but underdiagnosis is likely because its symptoms resemble other neurodevelopmental disorders, and genetic testing for OTUD5 mutations is not yet standard. Reports of cases of the syndrome in the U.S. have been reported geographically scattered, and the first diagnoses were in unrelated families identified via NIH research programs. History The discovery of LINKED syndrome occurred in 2021 through a group of NIH researchers who published their findings in Science Advances on January 20, 2021. The study examined 10 male patients who had OTUD5 gene mutations that caused developmental defects. Using whole-exome sequencing and cellular models, the team demonstrated that OTUD5's deubiquitinase activity is essential for embryonic neuroectodermal differentiation. The discovery established LINKED syndrome as a new X-linked condition that received its name from its unique method of ubiquitin cleavage. LINKED syndrome became established as a separate medical condition when OMIM made its first entry for OTUD5 (ID: 300713) in late 2021. While no major follow-up studies emerged by early 2025, the disorder's inclusion in NCBI Gene and rare disease discussions reflects growing awareness. Research directions Efforts are underway to expand the known phenotypic range, given the variability observed in affected males, from severe prenatal anomalies to milder presentations in adulthood. Research also examines prenatal manifestations, such as hydrocephalus, to improve early detection methods. Long-term studies aim to assess outcomes and refine management strategies, leveraging advances in genomic technologies to address this recently discovered condition. References",
    "label": 0
  },
  {
    "text": "Musicians' medicine deals with physical and mental issues suffered by musicians ('Musicians' illnesses'). It is sometimes also referred to as Music Medicine, which also describes different forms of music therapy. The related field of music physiology consists of research of physiological foundations of making music as well as the prevention of common health problems in musicians. The term Musicians' Health, often used as a synonym for Musicians' Medicine, generally refers to the health maintenance and wellbeing of musicians, as well as preventive measures, such as sufficient and appropriate exercise, a healthy diet, and enough sleep. The studies also include mental health problems, for instance stage fright. Research and Science As early as 1832, Karl Sundelin published his Medical Guidebook for Musicians. Later, around the turn of the century, Adolf Steinhausen (1859–1910) published multiple tracts on music medicine. Then, in the 1920s, Julius Flesch wrote about Berufskrankheiten des Musikers (Occupational Diseases of the Musician) (Celle, 1925). The neurologist Kurt Singer published his book Berufskrankheiten der Musiker (Occupational diseases of musicians). From 1923, Singer taught at the Hochschule für Musik Hanns Eisler Berlin. The Kurt-Singer-Institute for Music Physiology and Musicians’ Health in Berlin (Academy of Arts and Hochschule für Musik Hans Eisler), currently headed by Alexander Schmidt, was named after him. Further institutions for Musicians' Medicine are the Institute of Music Physiology and Musicians' Medicine at the Hanover University of Music, Drama and Media, founded by Christoph Wagner in 1974 and headed by Eckart Altenmüller since 1994, as well as the Freiburg Institute for Musicians’ Medicine, founded in 2005 and headed by Claudia Spahn and Bernhard Richter. In the former GDR, the field was studied at the Occupational Health Clinic of theatres and orchestras in Berlin. Further institutes and departments for Music Physiology and Musicians' Medicine are located at the Universities of",
    "label": 0
  },
  {
    "text": "Claudia Spahn and Bernhard Richter. In the former GDR, the field was studied at the Occupational Health Clinic of theatres and orchestras in Berlin. Further institutes and departments for Music Physiology and Musicians' Medicine are located at the Universities of Music in Weimar, Leipzig, Dresden and Frankfurt, as well as the Düsseldorf University Hospital and the Rechts der Isar Hospital in Munich. In 1994, the German Society for Music Physiology and Musicians' Medicine was founded and holds annual conferences. Similar associations have been established in Switzerland, Austria, France, the UK, the Netherlands, the US and New Zealand since the 1990s. Common health problems The most common problems are related to overstressing particular muscles, tendons and joints, especially the arms and hands. Of the 264,000 professional musicians working in the US in 2006, 50-76% (depending on the instrument) suffered work-related muscoskeletal ailments. Women were affected more often than men (70% vs 52%). These problems occur most commonly between the ages of 20 and 40. Risk factors are a general hyperlaxicity, abrupt increase in training and rehearsal times, a change of conductor, bad posture, wrong use of the instrument and general stress. Common medical conditions are: Enthesopathy especially of the underarms and hands Tenosynovitis and Tendinopathy Nerve compression syndrome such as the carpal tunnel syndrome or ulnar neuropathy at the elbow Focal dystonia with uncontrollable spastic muscle contractions Osteoarthritis mostly in the carpometacarpal joint (trapeziometacarpal osteoarthritis), in the carpus, the finger joints (heberden's node) Thoracic outlet syndrome Tinnitus Noise-induced hearing loss Stage fright Some illnesses are typical for certain instruments. For example, playing the violin, the viola or wind instruments often lead to changes in the mouth cavity, jaw, teeth or face. Musicians in orchestras are often seated close together in orchestra pits and are exposed to high sound levels. Due to",
    "label": 0
  },
  {
    "text": "playing the violin, the viola or wind instruments often lead to changes in the mouth cavity, jaw, teeth or face. Musicians in orchestras are often seated close together in orchestra pits and are exposed to high sound levels. Due to the positioning of the orchestra, sound is often not loud enough and doesn't reach the audience and therefore has to be amplified. Over the course of the last centuries, orchestra music has become increasingly louder. This can lead to lasting hearing loss and tinnitus. Such problems emerge slowly and often unnoticed. Musicians suffering from these conditions have to concentrate more while playing their instrument, resulting in insecurities and stress. In some cases even minor hearing problems can hinder their work. Prevention and therapy Musicians medicine is mainly concerned with prevention, which means educating musicians about preventative measures. These can be ergonomic adjustments of the instruments to the musician's body, good chairs, appropriate exercises and basic physiological and anatomical knowledge, as well as healthy eating habits and sufficient sleep. Additionally, relaxation techniques can be helpful, for example progressive muscle relaxation (Jacobsen), autogenic training, meditation, Tai chi and Qigong. Furthermore, movement therapies such as Eutony, Alexander Technique, Feldenkrais Method, Dispokinesis and functional movement therapy are efficient in diagnosing and correcting bad posture. Such measures can not only prevent, but also reduce or even eradicate existing problems. Another commonly used form of therapy is osteopathy. Currently, there is no cure for hearing damage. Many musicians are not aware of the health risks that come with being a musician. According to the German Occupational Safety Law relating to Noise and Vibration (Lärm- und Vibrations-Arbeitsschutzverordnung) employees have to be protected from damaging noise. Many employers often are not aware how demanding the job really is, since it includes many different tasks, such as teaching music",
    "label": 0
  },
  {
    "text": "Physical therapists work with large numbers of stroke victims in various settings (inpatient/outpatient rehabilitation, acute care hospitals, subacute care hospitals, skilled nursing facilities, home healthcare, hospital and private outpatient clinics) to improve their functional recovery and quality of life. Stroke is the most significant cause of serious, long-term disability in the United States. The time it takes to recover from a stroke depends on the extent of damage to the brain. More than half of stroke survivors regain their functional independence, while 15% to 30% are permanently disabled, and 20% still require in-patient care three months after the stroke. Following a stroke, the patient’s degree of disability is largely determined by their available mobility. Lack of efficient residual muscle function following a stroke limits endurance in most stroke survivors. History Physical Therapy Physical therapy is a health care profession that provides services to clients, helping them maintain, develop, and restore maximum movement and functional ability throughout the lifespan. These services are provided in situations where movement and function are endangered by ageing, injury, disease or environmental factors. Physical Therapists (PTs) are trained to diagnose and treat physical conditions which impact quality of life and movement potential of individuals. Physical Therapy practice includes the spheres of health promotion, disease prevention, treatment/intervention, habilitation, and rehabilitation. Stroke Stroke is a clinical event caused by an acute interruption of blood flow to a part of the brain or spinal cord. Stroke results from the lack of oxygen that occurs when an artery to the brain becomes blocked by a clot (cerebral infarction) or bursts (cerebral hemorrhage). Stroke is the third leading cause of death in the United States, trailing only heart disease and cancer. Neuroplasticity Neuroplasticity is the capability of parts of the central nervous system(CNS) to change its structure and function in response",
    "label": 0
  },
  {
    "text": "(cerebral hemorrhage). Stroke is the third leading cause of death in the United States, trailing only heart disease and cancer. Neuroplasticity Neuroplasticity is the capability of parts of the central nervous system(CNS) to change its structure and function in response to conditions and events. This adaptability of CNS to change allows the nervous system to reorganize and relearn in response to environmental stimulation thus contributing to recovery from various brain and spinal cord injuries. Changes within the brain both structurally and functionally can be detected by transcranial magnetic stimulation(TMS), magnetic resonance imaging, and positron emission tomography. Clinical trials have shown neuroplasticity through constraint-induced movement therapy used with patients having hemiparesis after stroke. Interventions For many stroke victims, motor learning, which contributes to patients’ acquisition of motor skills, is a vital part of the physical therapy treatments utilized in rehabilitation protocols. Physical therapy interventions used in patient care range from the conventional interventions such as the Bobath, Brunnstrom, and proprioceptive neuromuscular facilitation (PNF) to more recent therapies like promotion of specific/functional/repetitive tasks, constraint-induced movement therapy, electrical stimulation, and different types of gait training. All of the interventions listed below have been researched and described in the physical therapy literature. Conventional interventions The rehabilitation techniques of Bobath and Brunnstrom, as well as proprioceptive neuromuscular facilitation (PNF) focus primarily on retraining motor control by facilitating the desired patterns of movement patterns or inhibiting less-desired ones. When comparing the more conventional neurofacilitation approaches with recent interventions for stroke, no significant improvement in walking or activities of daily living have been seen with the more conventional approaches. Bobath The Bobath technique is partially founded on the thought that a normal reflex mechanism is the basis for normal movement patterns. After stroke, patients with hemiplegia are limited in their movement patterns and tend to exhibit more",
    "label": 0
  },
  {
    "text": "conventional approaches. Bobath The Bobath technique is partially founded on the thought that a normal reflex mechanism is the basis for normal movement patterns. After stroke, patients with hemiplegia are limited in their movement patterns and tend to exhibit more flexor and/or extensor synergies. The Bobath approach is used as an intervention for patients with hemiparesis, but there is insufficient evidence for its impact on gait. Brunnstrom To regain global synergistic movements and muscle tone the Brunnstrom approach utilizes coupled reactions, primitive postural reactions, and resistance. There is insufficient evidence to support the use of the Brunnstrom approach with gait training after stroke. Proprioceptive neuromuscular facilitation (PNF) PNF is used to facilitate movement via the use of resistance. Resistance is limited to an amount that does not allow abnormal movement patterns. Recent interventions Task oriented learning Task oriented training focuses on skill acquisition through the motor learning of functional tasks. Practice of these skilled motor learning tasks is the key to cortical reorganization. Principles of task oriented training include specificity of training, constrained use of the impaired limb, mass practice, shaping of skill, saliency of task, and knowledge of performance and results. Specific/functional/repetitive tasks Repetitive task training for improving functional ability after stroke was analyzed in a Cochrane Review. The authors/reviewers focused on task specificity more than repetition. They looked at 14 studies and found that even though improvements in walking distance, walking speed, and sit to stand ability were demonstrated with practiced functional activities, six months later improvements failed to be maintained. Small improvements were shown with acts of daily living and no impact was seen with arm/hand function with repetitive practice of tasks. The duration of time after stroke had no impact on training outcomes. Circuit class therapy (CCT) is another intervention using repetitive task training with functional",
    "label": 0
  },
  {
    "text": "daily living and no impact was seen with arm/hand function with repetitive practice of tasks. The duration of time after stroke had no impact on training outcomes. Circuit class therapy (CCT) is another intervention using repetitive task training with functional tasks. A Cochrane review of studies of the effects of circuit class therapy on the improvement of mobility after stroke revealed that people with moderate stroke impairments may show improved mobility with this safe intervention. CCT may also decrease the number of days needed for inpatient care. Constraint induced movement therapy (CIMT) Constraint induced movement therapy is utilized in patients with stroke as an intervention within rehabilitation settings. It is used to decrease learned non use of the affected limb and force the use of that limb while restraining the unaffected limb. Repetitive, functional tasks are performed to increase motor learning, motor skill, and function in the affected limb. A study was conducted to look at feasibility and compare efficacy of a modified CIT protocol, traditional OT/PT, and no therapy at all for improving upper-limb outcomes for patients with subacute CVAs (cerebrovascular accident). Groups receiving no therapy and traditional therapy did not show improvements, but for patients instructed with modified CIT, improvements were seen in fine motor skills, outcome measures, and quality and amount of limb use. The evidence suggests effectiveness for using modified CIT with distributed practice and repeated use to increase functional use and decrease impairments. This study further supports various practice schedules as a tool to be used for motor learning development. Tarka and Kononen, 2009, have reported that “CIMT provides increasingly difficult motor challenge with a motor learning component, and thus provides activations in the brain that may enhance reorganization related to motor control.” They conclude that CIMT when used on subjects with mild or moderate",
    "label": 0
  },
  {
    "text": "reported that “CIMT provides increasingly difficult motor challenge with a motor learning component, and thus provides activations in the brain that may enhance reorganization related to motor control.” They conclude that CIMT when used on subjects with mild or moderate upper limb paresis is an effective technique to be used. The EXCITE (Extremity Constraint Induced Therapy Evaluation) randomized clinical trial researched the effects of CIMT on upper extremity function in adults with stoke in the previous 6–9 months. The results of this study demonstrated that CIMT produced significant clinical improvement in arm movement lasting at least 1 year. Activities of daily living (ADLs) Activities of daily living include every day acts performed out of necessity to live and for self care. These include bathing, grooming, toileting, feeding, transfers (i.e. sit to stand, sit to lying), bed mobility, dressing, and locomotion/gait (walking). Neuroprosthetics is an intervention found to improve ADLs. In a 2005 randomized control trial, it was concluded that regardless of length of time after stroke, patients with hemiplegia and severe arm paralysis improved in functions of reaching and grasping with the application of FES technology via a neuroprosthesis. The main component contributing to success with this type of intervention was intense repetitive treatment specific to a patient’s individual daily needs and could be changed as the patient’s affected limb improved functionally. Electromyography biofeedback (EMG-BFB) Electromyography (EMG) biofeedback for the recovery of motor function after stroke was reviewed by Woodford and Price, 2007 in a Cochrane Review. The authors found that by using this intervention, patients may be able to utilize unimpaired pathways. This may enable the patient to have more muscular control by helping with functional recovery after stroke. The efficacy of this technique is inconclusive. No positive benefit for post stroke recovery has been shown via the evidential",
    "label": 0
  },
  {
    "text": "utilize unimpaired pathways. This may enable the patient to have more muscular control by helping with functional recovery after stroke. The efficacy of this technique is inconclusive. No positive benefit for post stroke recovery has been shown via the evidential data. EMG-BFB should not be used routinely for treatment. Robotics/neuromuscular electrical stimulation (NMES) Robotic interventions such as the InMotion2 robot, Mirror Image Movement Enabler, and Training-Wilmington Robotic Exoskeleton have been used as part of post-stroke rehabilitation interventions. The MIT-Manus (InMotion2 robot) has aided arm movement and strength improvement in patients after stroke. When the hemiparetic forearm/hand is attached to the robot’s arm, the robot can then facilitate exercises with movement of the shoulder and elbow in a horizontal plane. Using an interactive computer screen the patient’s exercises involve reaching for targets. A Mirror Image Movement Enabler was found to effectively increase free arm movements through resisted and assisted multiple plane arm motions better than conventional therapy. T-WREX (Training-Wilmington Robotic Exoskeleton) incorporates virtual reality with arm training for patients with hemiparetic upper limbs. It utilizes an exoskeletal arm that is adjustable and offsets the weight of the hemiparetic arm with elastic bands. While a patient performs arm movements with the exoskeletal arm there is a virtual hand on the computer screen carrying out tasks that correspond to the arm motion. Some efficacy has been demonstrated with the T-WREX. Short term efficacy during the acute phase of stroke recovery and for those with minimal wrist and finger movement has been shown with NMES. The evidence is insufficient to support the enhancement of post stroke functional arm use with NMES. Gait Neurological changes in the brain due to stroke affect a patient’s gait cycle. Patients often have gait impairments consisting of a slower cadence, smaller step length, decreased gait symmetry, decreased gait sequence,",
    "label": 0
  },
  {
    "text": "post stroke functional arm use with NMES. Gait Neurological changes in the brain due to stroke affect a patient’s gait cycle. Patients often have gait impairments consisting of a slower cadence, smaller step length, decreased gait symmetry, decreased gait sequence, muscle weakness, and decreased muscle activation. Gait restoration is an importantly emphasized rehabilitation goal following a stroke. One of the best ways to achieve this is goal to walk. Gait speed over 10m and the distance walked in six minutes are the two clinical measures most used to assess gait ability. An approximate 6m/min increase for gait speed and 50m increase with the 6-minute walk test are seen as indications of considerable improvement in walking ability. Patients who have had a stroke have an increased energy demand 1.5 to 2 times greater than healthy people for ambulation. According to Jorgensen et al., 2010, “people with stroke in the chronic stage can achieve clinically relevant improvements in gait performance and cardiovascular health parameters through high intensity physical training consisting of a combination of BWSTT, PRST (passive resistive strength training), AE (aerobic exercise), and functional training.” Body weight supported treadmill training (BWSTT) Body weight supported treadmill training (BWSTT) is used as a post stroke intervention to support task-specific gait training. Numerous studies have been conducted using BWSTT. Evidence from some of this literature is summarized below. After stroke, patients can practice walking during rehabilitation with the aid of parallel bars, assistive devices, manual assistance, or with body weight-supported treadmill training (BWSTT). BWSTT incorporates repetitive motion of the lower limbs through the gait cycle which may be one reason why it has been shown to be effective for patients with stroke. Patients may also ambulate or be more apt to move with a more positive perception of safety because they are secured in",
    "label": 0
  },
  {
    "text": "cycle which may be one reason why it has been shown to be effective for patients with stroke. Patients may also ambulate or be more apt to move with a more positive perception of safety because they are secured in a harness and their body weight is partially unloaded. A recent Cochrane review and other studies have shown no differences in the effectiveness of BWSTT when compared with traditional gait strategies. In one study, it was found that BWSTT beginning with 40% body-weight support and gradually weaned over training sessions was more effective for walking after stroke when compared to traditional treadmill training. Sullivan et al., 2007, reported in a randomized control trial that “task-specific training using TM walking with BWS was more effective in increasing walking speed than a less task-specific resisted cycling training program in individuals with chronic stroke who have limited community ambulation ability.” BWSTT related increases of immediate and long term walking capacity have been shown to transfer to over-ground walking. Along with improved gait, positive effects have been demonstrated in balance, balance confidence, and health-related quality of life in persons with chronic stroke after body weight-supported treadmill training. On the other hand, for most patients with chronic stroke the positive effects on balance and balance confidence were not outside the range of measurement error and perceptions of improved quality of life were not maintained long term. The physical requirements of BWSTT and time demands on PTs have limited its use in the clinic. Therefore, robotic devices for stepping assistance have been developed to aide with gait training. Overground gait training A Cochrane review by States et al., 2009, examined randomized control trials evaluating over-ground gait training with a control (rehab interventions other than gait) or no intervention. This review found insufficient evidence to determine whether",
    "label": 0
  },
  {
    "text": "Synthetic Cannabinoid Use Disorder (SCUD) is a psychiatric condition characterised by the rise of significant health problems from the continued use of synthetic cannabinoids (otherwise known as 'Spice' or 'K2'). Synthetic cannabinoids are artificially modified chemicals that imitate the effects of delta-9-tetrahydrocannabinol, the main psychoactive compound in cannabis. Symptoms of SCUD include hallucinations, anxiety, kidney failure and a range of other health problems. While synthetic cannabinoids bind to the cannabinoid receptors in the body like natural cannabis, they are more aggressive and have greater potency, causing unpredictable side effects. These effects are attributed to the hyperactivation of CB1 receptors, toxic metabolites, and more. While there is no set diagnostic criteria for SCUD, it can be diagnosed similarly to substance-use disorders according to guidelines set by the DSM-5. When SCUD is identified, pharmacotherapies and psychotherapeutic strategies are recommended to treat symptoms. The popularity of synthetic cannabinoids is attributed to its powerful psychoactive effects and legal status in a large number of countries around the world. Due to this, the prevalence rate of synthetic cannabinoid use has gone from 0.17% in 2021 to 0.26% in 2023 in just the United States, with SCUD cases rising each year. Signs and symptoms SCUD is characterised by a myriad of physiological, psychological and behavioural dysfunctions as a consequence of chronic use of synthetic cannabinoids. It has all the signs and symptoms of regular cannabinoid use disorder, only SCUD symptoms are more extreme and often unpredictable; at times even life-threatening. Intoxication Use of synthetic cannabinoids prompts intoxication, which subsequently develops into impaired cognition, judgement and motor control, euphoria, a slowed sense of time and in rare occasions, myocardial infarction. However, to be sure that these symptoms are caused by synthetic cannabinoid use and not any other underlying health issues, two hours later individuals should also exhibit",
    "label": 0
  },
  {
    "text": "control, euphoria, a slowed sense of time and in rare occasions, myocardial infarction. However, to be sure that these symptoms are caused by synthetic cannabinoid use and not any other underlying health issues, two hours later individuals should also exhibit at least two of the following: dry mouth, increased appetite, tachycardia and conjunctival injection. After constant use, users will find they require higher doses of synthetic cannabinoids to achieve the same effect due to the high binding affinity of synthetic cannabis to the body's cannabinoid receptors. Long-term large dose usage of synthetic cannabinoids can result in coma, overdose or death. Withdrawal Withdrawal from using can cause physiological and psychological pain. Common physical symptoms are insomnia, excessive sweating, headaches and nausea, while cravings, irritability, depression and aggression are typical psychological ones. When serious, users may also experience fever, tremors and seizures. Complications Synthetic Cannabis Induced Disorders There are multiple synthetic cannabis induced disorders, with a few examples being psychotic, anxiety and sleep disorders. Psychotic Disorder: Psychosis, hallucinations, paranoia, delusions and if drastic, cognitive impairment. Anxiety Disorder: Panic attacks and anxiety. Sleep Disorder: Severe disturbance in sleep, causing distress and impairment in social settings such as work or school. Other Complications Aside from the psychological and physiological problems displayed in intoxication, withdrawal and induced disorders, long-term synthetic cannabis use may also trigger the following health issues: Respiratory depression Gastrointestinal problems such as nausea, vomiting and appetite loss Cardiovascular problems such as myocardial infarction, hypertension and stroke Kidney failure and muscle damage Pathogenesis Synthetic cannabinoid use disorder is characterised by the hyper-activation of CB1 receptors, as synthetic cannabinoids act as agonists with a stronger binding affinity than natural cannabis. Cannabinoid receptors are extensively distributed everywhere in the body, but especially in brain regions responsible for functions such as memory, movement and emotion. Activation",
    "label": 0
  },
  {
    "text": "CB1 receptors, as synthetic cannabinoids act as agonists with a stronger binding affinity than natural cannabis. Cannabinoid receptors are extensively distributed everywhere in the body, but especially in brain regions responsible for functions such as memory, movement and emotion. Activation of these receptors changes the way signals are sent between nerve cells in the brain and other parts of the body, influencing both excitatory and inhibitory synaptic transmission by reducing neurotransmitter release in the synaptic cleft. Additionally, synthetic cannabinoids interfere with neurogenesis-related processes and brain-derived neurotrophic factor expression, disrupting endocannabinoid homeostasis. This dysregulation then manifests as physiological symptoms of SCUD, where the location of CB1 receptor activation correlates with specific health complications. An example would be the activation of CB1 receptors in bronchial smooth muscles, which consequently triggers airway constriction, interfering with gas exchange and inducing respiratory depression. Synthetic cannabinoids exhibit cross-reactivity with non-cannabinoid receptors due to structural similarities. Hyper-activation of CB1 receptors on GABAergic interneurons in the ventral tegmental area reduces GABA release onto dopaminergic neurons. While not only reinforcing drug-seeking behaviour through increasing tonic dopamine release in the nucleus accumbens, GABAergic inhibition also plays a significant role in maintaining and lowering an individual's seizure threshold. Furthermore, synthetic cannabinoids mimic serotonin at 5-HT receptors, triggering serotonin syndrome. Serotonin syndrome can lead to anxiety, psychosis and perceptual distortions through amplified glutamatergic activity in the prefrontal cortex. Certain metabolites of synthetic cannabinoids are known to exhibit toxic effects by influencing cannabinoid receptor activity. Some metabolites derived from synthetic cannabinoids such as JWH-018 retain or increase CB1 receptor affinity, extending the psychotropic and physiological effects of the drug. Neuroadaptations and Cognitive Decline Chronic synthetic cannabinoid use disrupts synaptic plasticity, contributing to neuroadaptations and cognitive decline, altering how the brain works. Synthetic cannabinoids interfere with the normal functioning of the endocannabinoid system, leading",
    "label": 0
  },
  {
    "text": "and physiological effects of the drug. Neuroadaptations and Cognitive Decline Chronic synthetic cannabinoid use disrupts synaptic plasticity, contributing to neuroadaptations and cognitive decline, altering how the brain works. Synthetic cannabinoids interfere with the normal functioning of the endocannabinoid system, leading to changes in the hippocampal and corticostriatal circuits, which plays a central role in performing appropriate goal-directed behaviour. These disruptions undermine neural connectivity and the brain's ability to adapt, which can impair memory, decision-making and other cognitive processes. Toxicological Effects Synthetic cannabinoids undergo rapid and extensive metabolism, primarily through phase-I reactions. These metabolites can contribute to the toxic effects of synthetic cannabinoids by altering their pharmacological activity. When smoked, synthetic cannabinoids undergo pyrolysis, producing thermal degradants with varying pharmacological activities. Some of these by-products retain high affinity for CB1 receptors and may surpass the parent compounds in efficacy, further stimulating CB1 receptor-mediated responses. Synthetic cannabinoids can also interfere with the metabolism of other drugs, leading to potential drug-drug interactions. This interference could potentially alter the pharmacokinetics of co-administered substances, posing great health risks. Pathological differences between SCUD and Normal Cannabis Use Disorder Diagnosis While there is no set diagnostic criteria for SCUD, one way to identify it is through confirming an individual has a substance-use disorder through the DSM-5 - with the substance in question being synthetic cannabis. According to the DSM-5, the four basic categories determining substance-use disorder are: Impaired control Physical dependence Social problems Risky use Users who have a substance-use disorder must possess more than two of the eleven symptoms on the DSM-5 substance-use disorder criteria within twelve months while exhibiting the above categories. To evaluate whether an individual fits the framework, usually a clinical interview is performed to assess use patterns and other psychiatric symptoms. Another way to diagnose SCUD is through urine toxicology, though standard",
    "label": 0
  },
  {
    "text": "twelve months while exhibiting the above categories. To evaluate whether an individual fits the framework, usually a clinical interview is performed to assess use patterns and other psychiatric symptoms. Another way to diagnose SCUD is through urine toxicology, though standard tests often fail due to being negative for synthetic cannabimimetics. Many studies have turned to using LC-MS-MS detection as the test has the chemical variability to analyse the fragmentation patterns of synthetic cannabinoids. Lastly, before a diagnosis of SCUD, it is important to rule out natural cannabis use, other substance toxicity and psychiatric disorders. Treatment Treatment for SCUD revolves around symptom relief and behavioural therapies, as currently no FDA-approved medications exist. Withdrawal support consists of a gradual decrease in drug dosage to reduce discomfort and prevent relapse. Pharmacotherapies such as benzodiazepines and neuroleptics can be included in treatment to address panic attacks and psychosis, respectively. These medications work in a similar manner - by blocking neurotransmitters such as dopamine, acetylcholine and norepinephrine from attaching to certain receptors. Psychotherapeutic strategies that have been tested are behavioural therapies like cognitive behavioural therapy, which help address user cravings and triggers. Additionally, a calm and supportive non-stimulating environment has also been seen to benefit patients. Monitoring patient adherence and their psychological symptoms may help predict and prevent potential relapses, as one of the main challenges of treating SCUD are the high relapse rates due to potent cravings. Harm Reduction Strategies Though synthetic cannabinoids should be avoided as much as possible, there are strategies that can be implemented to reduce harm when using: Take the drug in small doses Avoid using it when on other drugs or while drunk Take breaks from the drug to avoid building up a tolerance and becoming dependent Use in the presence of trusted people If experiencing negative symptoms, call",
    "label": 0
  },
  {
    "text": "drug in small doses Avoid using it when on other drugs or while drunk Take breaks from the drug to avoid building up a tolerance and becoming dependent Use in the presence of trusted people If experiencing negative symptoms, call for help from emergency services immediately Epidemiology Prevalence The prevalence rates of SCUD around the world are not completely known, with most research having been conducted in the United States. However, from current trends the use of synthetic cannabinoids and prevalence of SCUD has been steadily increasing, with prevalence rates rising from 0.17% in 2021 to 0.26% in 2023 in the United States alone. The largest demographic of individuals using synthetic cannabinoids who have had to visit emergency rooms are youth aged 12–29, with 4.5% - 13.5% of high school students reporting life-time use in New York. Behavioural drivers Certain groups are more vulnerable to synthetic cannabinoid use than others: Male individuals (odds ratio = 2.63) Aged 16 to 17 years (odds ratio = 1.99) Residents of urban areas (odds ratio = 1.57) Research has also shown mental health conditions such as anxiety and depression can lead to behaviours that significantly contribute to synthetic cannabinoid use. For example, individuals may attempt to use synthetic cannabinoids to self-medicate in order to manage the symptoms of the mentioned disorders, which could lead to misuse and dependency. Society and culture Governments around the world have recognised the consequences if synthetic cannabinoids become widespread. Specific examples of fighting against this drug include the UK and Barbados. In response to the threat posed by synthetic cannabis, the UK implemented The Psychoactive Substances Act 2016, which bans the production, distribution, sale and supply of psychoactive substances intended for human consumption. On the other hand, the government of Barbados has established an early warning drug alert system",
    "label": 0
  },
  {
    "text": "Urinary anti-infective agent, also known as urinary antiseptic, is medication that can eliminate microorganisms causing urinary tract infection (UTI). UTI can be categorized into two primary types: cystitis, which refers to lower urinary tract or bladder infection, and pyelonephritis, which indicates upper urinary tract or kidney infection. Escherichia coli (E. Coli) is the predominant microbial trigger of UTIs, accounting for 75% to 95% of reported cases. Other pathogens such as Proteus mirabilis, Klebsiella pneumoniae, and Staphylococcus saprophyticus can also cause UTIs. The use of antimicrobial therapy to treat UTIs started in the 20th century. Nitrofurantoin, trimethoprim-sulfamethoxazole (TMP/SMX), fosfomycin, and pivmecillinam are currently the first-line agents for empiric therapy of simple cystitis. On the other hand, the choice of empiric antimicrobial therapy for pyelonephritis depends on the severity of illness, specific host factors, and the presence of resistant bacteria. Ceftriaxone is often considered for parenteral treatment, while oral or parenteral fluoroquinolones, such as levofloxacin and ciprofloxacin, are suitable alternatives for treating pyelonephritis. Antimicrobial therapy should be tailored to the individual, considering factors like the severity of illness, specific host factors, and pathogen resistance in the local community. Types of urinary anti-infective agent Urinary antiseptics are medications that target bacteria in the urinary tract. They can be divided into two groups: bactericidal agents, and bacteriostatic agents. These antiseptics help prevent infections by effectively eliminating UTI symptoms through their action on microorganisms. Urinary bactericidal agents Nitrofurantoin Nitrofurantoin is regarded as the first-line agent for simple cystitis, with an efficacy rate ranging from 88% to 92%. It can also be a prophylactic agent to prevent long-term UTIs. This antibacterial medication is effective against both gram-positive and gram-negative bacteria. Nitrofurantoin exhibits its bactericidal activity through various mechanisms, including inhibiting ribosomal translation, causing bacterial DNA damage and interfering with the citric acid cycle. However, the specific",
    "label": 0
  },
  {
    "text": "prevent long-term UTIs. This antibacterial medication is effective against both gram-positive and gram-negative bacteria. Nitrofurantoin exhibits its bactericidal activity through various mechanisms, including inhibiting ribosomal translation, causing bacterial DNA damage and interfering with the citric acid cycle. However, the specific role of each mechanism remains to be further explored. When nitrofurantoin is metabolized, it converts into a reactive intermediate that attacks bacterial ribosomes, inhibiting bacterial protein synthesis. This medication is typically taken orally and has minimal systemic absorption, reducing potential side effects. Common adverse reactions associated with nitrofurantoin include brown urine discoloration, nausea, vomiting, loss of appetite, rash, and peripheral neuropathy. Fosfomycin Fosfomycin is a phosphonic acid bactericidal agent. It is commonly used as the first-line treatment for acute simple cystitis, demonstrating a 91% cure rate. It is administered orally as a single dose; In more complicated UTIs, the dose is adjusted to be repeated every three days to achieve successful eradication. The bactericidal effect of fosfomycin is attributed to its capability to inhibit bacterial wall synthesis by inactivating an enzyme called pyruvyl transferase, which is responsible for microbial cell wall synthesis. Fosfomycin acts against gram-positive and gram-negative bacteria. Administration of fosfomycin may lead to side effects such as headache, dizziness, nausea, vomiting, and abdominal cramps. Beta-lactam antibiotics Beta-lactam antibiotics are often considered as a second-line option for treating UTIs due to their lower effectiveness compared to other antibiotics and their potential adverse effects. Commonly used beta-lactam antibiotics for UTIs include cephalosporins and penicillin. By binding to penicillin-binding proteins through their beta-lactam rings, beta-lactam antibiotics disrupt the normal function of these proteins, inhibiting bacterial cell wall synthesis, ultimately resulting in cell death. Cephalosporins are a subclass of beta-lactam family with broad-spectrum activity against gram-positive and gram-negative bacteria. They are categorized into five generations. First and third-generation cephalosporins, like cefalexin and",
    "label": 0
  },
  {
    "text": "these proteins, inhibiting bacterial cell wall synthesis, ultimately resulting in cell death. Cephalosporins are a subclass of beta-lactam family with broad-spectrum activity against gram-positive and gram-negative bacteria. They are categorized into five generations. First and third-generation cephalosporins, like cefalexin and ceftriaxone, are more commonly used in clinical practice. Common adverse effects associated with cephalosporins include hypersensitivity, rash, anaphylaxis, and seizures. Penicillin is another widely used subclass that effectively targets various bacteria. However, it is not regarded as the first-line treatment for uncomplicated cystitis because of the high prevalence of penicillin-resistant E. coli strains. Within the penicillin class, pivmecillinam is considered the first-line empiric treatment for acute cystitis due to its wide spectrum of activity against gram-negative bacteria and its specific efficacy in the urinary tract. It has consistently demonstrated a high cure rate of over 85% for UTIs and a low resistance rate among E. coli strains. Amoxicillin-clavulanate combination, which enhances the effectiveness of amoxicillin, is often used as an alternative for cystitis treatment when other options cannot be used. Fluoroquinolones Fluoroquinolones are a class of antimicrobial agents known for their high efficacy and broad spectrum activity against aerobic gram-positive and gram-negative bacteria. These potent antibiotics exert their bactericidal effects by selectively inhibiting the activity of type II DNA topoisomerases, which effectively halt the replication of bacterial DNA, leading to bacterial death. Among the fluoroquinolones, ciprofloxacin and levofloxacin are used more frequently for the treatment of UTIs. These agents are well-absorbed orally and achieve significant concentrations in urine and various tissues. However, fluoroquinolones administration carries risk of GI symptoms, confusion, hypersensitivity, tendinopathy, and neuropathy. Additionally, the extensive use of fluoroquinolones has contributed to the prevalence of antimicrobial resistance in some areas. As a result, fluoroquinolones are generally reserved for more serious UTIs or when there are no better anti urinary-infective",
    "label": 0
  },
  {
    "text": "hypersensitivity, tendinopathy, and neuropathy. Additionally, the extensive use of fluoroquinolones has contributed to the prevalence of antimicrobial resistance in some areas. As a result, fluoroquinolones are generally reserved for more serious UTIs or when there are no better anti urinary-infective agent options. Bacterial static agent Sulfonamide Sulfonamide is a bacteriostatic agent that competitively inhibits the bacterial enzyme dihydropteroate synthase. By acting as a substrate analog of para-aminobenzoic acid, sulfonamide inhibits folic acid production. TMP/SMX is a combination of two antibacterial agents that work synergistically to combat a wide range of urinary tract pathogens. TMP/SMX is commonly used due to its ability to achieve high concentrations in urinary tract tissues and urine. This antibiotic combination demonstrates notable efficacy in both the treatment and prophylaxis of recurrent urinary tract infections. Common adverse effects include nausea, vomiting, rash,pruritus, and photosensitivity. Renal dysfunction Kidney disease can affect drug elimination, absorption, and distribution in the body, leading to altered serum drug concentrations. This can increase the risk of drug toxicity or suboptimal therapeutic effects. As a result, dosage adjustments are necessary for patients who fail to achieve the desired therapeutic serum drug levels. Management The choice of urinary anti-infective agents for patients with renal dysfunction is generally similar to that for individuals with normal kidney function. However, in cases where the patient's glomerular filtration rate (GFR) decreases to less than 20 mL/min, drug dosages adjustment is necessary because achieving the desired therapeutic serum drug levels becomes challenging in such patients. Medication safety Some drugs need to be used with caution in patients with renal dysfunction. The use of nitrofurantoin is contraindicated in patients with an estimated GFR of less than 30 mL/min/1.73m2 as drug accumulation can lead to increased side effects and impaired recovery of the urinary tract, increasing the risk of treatment failure. The",
    "label": 0
  },
  {
    "text": "The use of nitrofurantoin is contraindicated in patients with an estimated GFR of less than 30 mL/min/1.73m2 as drug accumulation can lead to increased side effects and impaired recovery of the urinary tract, increasing the risk of treatment failure. The use of TMP/SMX also raises concerns in patients with kidney disease. In patients with creatinine clearance less than 50 mL/min, the urine concentrations of SMX may decrease to subtherapeutic levels. Therefore, in patients with low creatinine clearance, it is recommended to prescribe a reduced dosage of TMP alone. Pregnancy Pregnant women with UTIs are at a higher risk of experiencing recurrent bacteriuria and developing pyelonephritis compared to non-pregnant individuals. Untreated UTIs during pregnancy can lead to adverse outcomes, including preterm birth and low birth weight infants. Management Antimicrobial treatment should be adjusted for UTIs in pregnant women to avoid potential side effects brought to fetus. For acute cystitis and pyelonephritis in pregnant women, empiric antibiotic treatment is often initiated. Commonly used antibiotics for uncomplicated cystitis include amoxicillin-clavulanate and fosfomycin, while parenteral beta-lactams are preferred for acute pyelonephritis. These options are chosen because they are considered safer in pregnancy and have a relatively broad spectrum of activity. Typically, an antimicrobial course of five to seven days is given. This duration is chosen to minimize fetal exposure to antimicrobials while ensuring optimal treatment outcomes. Medication safety The type of urinary anti-infective agents should be carefully chosen for pregnant women with UTIs due to the potential impact on fetal development. Penicillins, cephalosporins, and fosfomycin are safe options during pregnancy. Nitrofurantoin is typically avoided during the first trimester due to uncertain associations with congenital anomalies. TMP/SMX should also be avoided as it may be associated with impaired folate metabolism, which increases the risk of neural tube defects. However, when all alternative antibiotics are contraindicated,",
    "label": 0
  },
  {
    "text": "during the first trimester due to uncertain associations with congenital anomalies. TMP/SMX should also be avoided as it may be associated with impaired folate metabolism, which increases the risk of neural tube defects. However, when all alternative antibiotics are contraindicated, nitrofurantoin and TMP/SMX become the last resort at the expense of the fetus. Fluoroquinolones should be avoided during pregnancy as they are associated with bone and cartilage toxicity in developing fetuses. Pediatrics Urinary tract infection in pediatric patients is a significant clinical issue, affecting approximately 7% of fevered infants and children. If left untreated, the infection can ascend from the bladder to the kidneys, resulting in acute pyelonephritis, which leads to hypertension, kidney scarring, and end-stage kidney disease. Management The choice of urinary anti-infective agents used in pediatric patients and the duration of therapy depend on the types of UTIs they are suffering from. It is important to note that the dosage of antibiotics used in children is typically weight-dependent. Generally, oral or parenteral cephalosporins are recommended as the first-line agent for children older than two months. Second-line therapy should be considered for patients who have poor response to first-line treatment. Alternative choices include amoxicillin-clavulanate, nitrofurantoin, TMP/SMX, and ciprofloxacin. For the treatment of simple cystitis in children, a five-day oral course of cephalexin is the preferred choice. As for children with suspected pyelonephritis, a ten-day treatment regimen is recommended. In such cases, a third-generation cephalosporin, such as cefdinir, is suggested as an appropriate option. If second-line therapy is initiated in pediatric patients with suspected pyelonephritis, ciprofloxacin should be the preferred option among the four alternatives. Nitrofurantoin may not be adequate in treating upper urinary tract infections, while TMP/SMX and amoxicillin-clavulanate should be used with caution due to the risk of kidney scarring in these patients. Medication safety The choice of",
    "label": 0
  },
  {
    "text": "option among the four alternatives. Nitrofurantoin may not be adequate in treating upper urinary tract infections, while TMP/SMX and amoxicillin-clavulanate should be used with caution due to the risk of kidney scarring in these patients. Medication safety The choice of urinary anti-infective agents in pediatric patients may differ from that in adults due to the potential harm they can cause to children. For example, the systemic use of fluoroquinolones is not appropriate in pediatric patients due to the potential risk of musculoskeletal toxicity. History The discovery of antimicrobial agents contributed significantly to UTI management during the 20th century. Nitrofurantoin emerged as the first practical and safe urinary antimicrobial agent, but it was with limited spectrum of activity. Subsequently, in the 1970s, beta-lactam antibiotics and TMP/SMX became available for UTI therapy. Antimicrobial resistance was developed to these agents due to their widespread and extensive usage, which restricted their clinical efficacy in UTI management. Fluoroquinolones emerged during the 1980s and were recommended as an alternative when resistance to TMP/SMX reaches 10% or higher. The evolving landscape of drug resistance will continue to influence the development and application of antimicrobial agents in UTI therapy. See also UTI vaccine Uromune Methenamine LACTIN-V TOL-463 References External links",
    "label": 0
  },
  {
    "text": "Economics () is a social science that studies the production, distribution, and consumption of goods and services. Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyses what is viewed as basic elements within economies, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyses economies as systems where production, distribution, consumption, savings, and investment expenditure interact; and the factors of production affecting them, such as: labour, capital, land, and enterprise, inflation, economic growth, and public policies that impact these elements. It also seeks to analyse and describe the global economy. Other broad distinctions within economics include those between positive economics, describing \"what is\", and normative economics, advocating \"what ought to be\"; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics. Economic analysis can be applied throughout society, including business, finance, cybersecurity, health care, engineering and government. It is also applied to such diverse subjects as crime, education, the family, feminism, law, philosophy, politics, religion, social institutions, war, science, and the environment. Definitions of economics The earlier term for the discipline was \"political economy\", but since the late 19th century, it has commonly been called \"economics\". The term is ultimately derived from Ancient Greek οἰκονομία (oikonomia) which is a term for the \"way (nomos) to run a household (oikos)\", or in other words the know-how of an οἰκονομικός (oikonomikos), or \"household or homestead manager\". Derived terms such as \"economy\" can therefore often mean \"frugal\" or \"thrifty\". By extension then, \"political economy\" was the way to manage a polis or state. There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among",
    "label": 0
  },
  {
    "text": "therefore often mean \"frugal\" or \"thrifty\". By extension then, \"political economy\" was the way to manage a polis or state. There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as \"an inquiry into the nature and causes of the wealth of nations\", in particular as: a branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the public services. Jean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science of production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined \"the dismal science\" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) delimited the subject matter further: The science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object. Alfred Marshall provided a still widely cited definition in his textbook Principles of Economics (1890) that extended analysis beyond wealth and from the societal to the microeconomic level: Economics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man. Lionel Robbins (1932) developed implications of what has been termed \"[p]erhaps",
    "label": 0
  },
  {
    "text": "he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man. Lionel Robbins (1932) developed implications of what has been termed \"[p]erhaps the most commonly accepted current definition of the subject\": Economics is the science which studies human behaviour as a relationship between ends and scarce means which have alternative uses. Robbins described the definition as not classificatory in \"pick[ing] out certain kinds of behaviour\" but rather analytical in \"focus[ing] attention on a particular aspect of behaviour, the form imposed by the influence of scarcity.\" He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow. But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought-after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. Economics cannot be defined as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought-after end). Some subsequent comments criticised the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and",
    "label": 0
  },
  {
    "text": "to attain a sought-after end). Some subsequent comments criticised the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields. There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment. Gary Becker, a contributor to the expansion of economics into new areas, described the approach he favoured as \"combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly.\" One commentary characterises the remark as making economics an approach rather than a subject matter but with great specificity as to the \"choice process and the type of social interaction that [such] analysis involves.\" The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve. Many economists including Nobel Prize winners James M. Buchanan and Ronald Coase reject the method-based definition of Robbins and continue to prefer definitions like those of Say, in terms of its subject matter. Ha-Joon Chang has for example argued that the definition of Robbins would make economics very peculiar because all other sciences define themselves in terms of the area of inquiry or object of inquiry rather than the methodology. In the biology department, it is not said that all biology should be studied with DNA analysis. People study living organisms in many different ways, so some people will perform DNA analysis, others might",
    "label": 0
  },
  {
    "text": "of inquiry rather than the methodology. In the biology department, it is not said that all biology should be studied with DNA analysis. People study living organisms in many different ways, so some people will perform DNA analysis, others might analyse anatomy, and still others might build game theoretic models of animal behaviour. But they are all called biology because they all study living organisms. According to Ha Joon Chang, this view that the economy can and should be studied in only one way (for example by studying only rational choices), and going even one step further and basically redefining economics as a theory of everything, is peculiar. History of economic thought From antiquity through the physiocrats Questions regarding distribution of resources are found throughout the writings of the Boeotian poet Hesiod and several economic historians have described him as the \"first economist\". However, the Greek word oikos was used for issues regarding how to manage a household (which was understood to be the landowner, his family, and his slaves) rather than to refer to some normative societal system of distribution of resources, which is a far more recent phenomenon. Although Xenophon, the author of the Oeconomicus, is credited by philologues as the source of the word \"economy\", modern scholarship often credits Aristotle as the first author writing on economics proper in some scattered passages, particularly in the Nicomachean Ethics, where the topic of use value vs exchange value is discussed. Joseph Schumpeter described 16th and 17th century scholastic writers, including Tomás de Mercado, Luis de Molina, and Juan de Lugo, as \"coming nearer than any other group to being the 'founders' of scientific economics\" as to monetary, interest, and value theory within a natural-law perspective. Two groups, who later were called \"mercantilists\" and \"physiocrats\", more directly influenced the subsequent",
    "label": 0
  },
  {
    "text": "Lugo, as \"coming nearer than any other group to being the 'founders' of scientific economics\" as to monetary, interest, and value theory within a natural-law perspective. Two groups, who later were called \"mercantilists\" and \"physiocrats\", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing inexpensive raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies. Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy. Adam Smith (1723–1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system \"with all its imperfections\" as \"perhaps the purest approximation to the truth that has yet been published\" on the subject.",
    "label": 0
  },
  {
    "text": "Adam Smith (1723–1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system \"with all its imperfections\" as \"perhaps the purest approximation to the truth that has yet been published\" on the subject. Classical political economy The publication of Adam Smith's The Wealth of Nations in 1776, has been described as \"the effective birth of economics as a separate discipline.\" The book identified land, labour, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the physiocratic idea that only agriculture was productive. Smith discusses potential benefits of specialisation by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries. His \"theorem\" that \"the division of labor is limited by the extent of the market\" has been described as the \"core of a theory of the functions of firm and industry\" and a \"fundamental principle of economic organization.\" To Smith has also been ascribed \"the most important substantive proposition in all of economics\" and foundation of resource-allocation theory—that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment). In an argument that includes \"one of the most famous passages in all economics,\" Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society, and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this: He generally, indeed, neither intends to promote the public interest, nor knows how much he",
    "label": 0
  },
  {
    "text": "sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this: He generally, indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for the society that it was no part of it. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. The Reverend Thomas Robert Malthus (1798) used the concept of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level. Economist Julian Simon has criticised Malthus's conclusions. While Adam Smith emphasised production and income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was also the first",
    "label": 0
  },
  {
    "text": "the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was also the first to state and prove the principle of comparative advantage, according to which each country should specialise in producing and exporting goods in that it has a lower relative cost of production, rather relying only on its own production. It has been termed a \"fundamental analytical explanation\" for gains from trade. Coming at the end of the classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene. Value theory was important in classical theory. Smith wrote that the \"real price of every thing ... is the toil and trouble of acquiring it\". Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity. Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of any market economy to settle in a final stationary state made up of a constant stock of physical wealth (capital) and a constant population size. Marxian economics Marxist (later, Marxian) economics descends from classical economics and it derives from the work of Karl Marx. The first volume of Marx's major work, Das Kapital, was published in 1867. Marx focused on the labour theory of value and theory of surplus value. Marx wrote that they were",
    "label": 0
  },
  {
    "text": "and it derives from the work of Karl Marx. The first volume of Marx's major work, Das Kapital, was published in 1867. Marx focused on the labour theory of value and theory of surplus value. Marx wrote that they were mechanisms used by capital to exploit labour. The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production, and the theory of surplus value demonstrated how workers were only paid a proportion of the value their work had created. Marxian economics was further developed by Karl Kautsky (1854–1938)'s The Economic Doctrines of Karl Marx and The Class Struggle (Erfurt Program), Rudolf Hilferding's (1877–1941) Finance Capital, Vladimir Lenin (1870–1924)'s The Development of Capitalism in Russia and Imperialism, the Highest Stage of Capitalism, and Rosa Luxemburg (1871–1919)'s The Accumulation of Capital. Neoclassical economics At its inception as a social science, economics was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth (1803). These three items were considered only in relation to the increase or diminution of wealth, and not in reference to their processes of execution. Say's definition has survived in part up to the present, modified by substituting the word \"wealth\" for \"goods and services\" meaning that wealth may include non-material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed, because many economists were making theoretical and philosophical inroads in other areas of human activity. In his Essay on the Nature and Significance of Economic Science, he proposed a definition of economics as a study of human behaviour, subject to and constrained by scarcity, which forces people",
    "label": 0
  },
  {
    "text": "philosophical inroads in other areas of human activity. In his Essay on the Nature and Significance of Economic Science, he proposed a definition of economics as a study of human behaviour, subject to and constrained by scarcity, which forces people to choose, allocate scarce resources to competing ends, and economise (seeking the greatest welfare while avoiding the wasting of scarce resources). According to Robbins: \"Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses\". Robbins' definition eventually became widely accepted by mainstream economists, and found its way into current textbooks. Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition. A body of theory later termed \"neoclassical economics\" formed from about 1870 to 1910. The term \"economics\" was popularised by such neoclassical economists as Alfred Marshall and Mary Paley Marshall as a concise synonym for \"economic science\" and a substitute for the earlier \"political economy\". This corresponded to the influence on the subject of mathematical methods used in the natural sciences. Neoclassical economics systematically integrated supply and demand as joint determinants of both price and quantity in market equilibrium, influencing the allocation of output and income distribution. It rejected the classical economics' labour theory of value in favour of a marginal utility theory of value on the demand side and a more comprehensive theory of costs on the supply side. In the 20th century, neoclassical theorists departed from an earlier idea that suggested measuring total utility for a society, opting instead for ordinal utility, which posits behaviour-based relations across individuals. In microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An",
    "label": 0
  },
  {
    "text": "an earlier idea that suggested measuring total utility for a society, opting instead for ordinal utility, which posits behaviour-based relations across individuals. In microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded. In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics. Neoclassical economics is occasionally referred as orthodox economics whether by its critics or sympathisers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalise earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analysing long-run variables affecting national income. Neoclassical economics studies the behaviour of individuals, households, and organisations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more players to attain the best possible outcome. Keynesian economics Keynesian economics derives from John Maynard Keynes, in particular his book The General Theory of Employment, Interest and Money (1936), which ushered in contemporary macroeconomics as a distinct field. The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low \"effective demand\" and why",
    "label": 0
  },
  {
    "text": "The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low \"effective demand\" and why even price flexibility and monetary policy might be unavailing. The term \"revolutionary\" has been applied to the book in its impact on economic analysis. During the following decades, many economists followed Keynes' ideas and expanded on his works. John Hicks and Alvin Hansen developed the IS–LM model which was a simple formalisation of some of Keynes' insights on the economy's short-run equilibrium. Franco Modigliani and James Tobin developed important theories of private consumption and investment, respectively, two major components of aggregate demand. Lawrence Klein built the first large-scale macroeconometric model, applying the Keynesian thinking systematically to the US economy. Post-WWII economics Immediately after World War II, Keynesian was the dominant economic view of the United States establishment and its allies, Marxian economics was the dominant economic view of the Soviet Union nomenklatura and its allies. Monetarism Monetarism appeared in the 1950s and 1960s, its intellectual leader being Milton Friedman. Monetarists contended that monetary policy and other monetary shocks, as represented by the growth in the money stock, was an important cause of economic fluctuations, and consequently that monetary policy was more important than fiscal policy for purposes of stabilisation. Friedman was also skeptical about the ability of central banks to conduct a sensible active monetary policy in practice, advocating instead using simple rules such as a steady rate of money growth. Monetarism rose to prominence in the 1970s and 1980s, when several major central banks followed a monetarist-inspired policy, but was later abandoned because the results were unsatisfactory. New classical economics A more fundamental challenge to the prevailing Keynesian paradigm",
    "label": 0
  },
  {
    "text": "growth. Monetarism rose to prominence in the 1970s and 1980s, when several major central banks followed a monetarist-inspired policy, but was later abandoned because the results were unsatisfactory. New classical economics A more fundamental challenge to the prevailing Keynesian paradigm came in the 1970s from new classical economists like Robert Lucas, Thomas Sargent and Edward Prescott. They introduced the notion of rational expectations in economics, which had profound implications for many economic discussions, among which were the so-called Lucas critique and the presentation of real business cycle models. New Keynesians During the 1980s, a group of researchers appeared being called New Keynesian economists, including among others George Akerlof, Janet Yellen, Gregory Mankiw and Olivier Blanchard. They adopted the principle of rational expectations and other monetarist or new classical ideas such as building upon models employing micro foundations and optimizing behaviour, but simultaneously emphasised the importance of various market failures for the functioning of the economy, as had Keynes. Not least, they proposed various reasons that potentially explained the empirically observed features of price and wage rigidity, usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones. New neoclassical synthesis After decades of often heated discussions between Keynesians, monetarists, new classical and new Keynesian economists, a synthesis emerged by the 2000s, often given the name the new neoclassical synthesis. It integrated the rational expectations and optimizing framework of the new classical theory with a new Keynesian role for nominal rigidities and other market imperfections like imperfect information in goods, labour and credit markets. The monetarist importance of monetary policy in stabilizing the economy and in particular controlling inflation was recognised as well as the traditional Keynesian insistence that fiscal policy could also play an influential role in affecting aggregate demand. Methodologically, the synthesis",
    "label": 0
  },
  {
    "text": "The monetarist importance of monetary policy in stabilizing the economy and in particular controlling inflation was recognised as well as the traditional Keynesian insistence that fiscal policy could also play an influential role in affecting aggregate demand. Methodologically, the synthesis led to a new class of applied models, known as dynamic stochastic general equilibrium or DSGE models, descending from real business cycles models, but extended with several new Keynesian and other features. These models proved useful and influential in the design of modern monetary policy and are now standard workhorses in most central banks. After the 2008 financial crisis After the 2008 financial crisis, macroeconomic research has put greater emphasis on understanding and integrating the financial system into models of the general economy and shedding light on the ways in which problems in the financial sector can turn into major macroeconomic recessions. In this and other research branches, inspiration from behavioural economics has started playing a more important role in mainstream economic theory. Also, heterogeneity among the economic agents, e.g. differences in income, plays an increasing role in recent economic research. Other schools and approaches Other schools or trends of thought referring to a particular style of economics practised at and disseminated from well-defined groups of academicians that have become known worldwide, include the Freiburg School, the School of Lausanne, the Stockholm school and the Chicago school of economics. During the 1970s and 1980s mainstream economics was sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago school approach. Within macroeconomics there is, in general order of their historical appearance in the literature; classical economics, neoclassical economics, Keynesian economics, the neoclassical synthesis, monetarism, new classical economics, New Keynesian economics and the new neoclassical synthesis. Beside the mainstream",
    "label": 0
  },
  {
    "text": "school approach. Within macroeconomics there is, in general order of their historical appearance in the literature; classical economics, neoclassical economics, Keynesian economics, the neoclassical synthesis, monetarism, new classical economics, New Keynesian economics and the new neoclassical synthesis. Beside the mainstream development of economic thought, various alternative or heterodox economic theories have evolved over time, positioning themselves in contrast to mainstream theory. These include: Austrian School, emphasizing human action, property rights and the freedom to contract and transact to have a thriving and successful economy. It also emphasises that the state should play as small role as possible (if any role) in the regulation of economic activity between two transacting parties. Friedrich Hayek and Ludwig von Mises are the two most prominent representatives of the Austrian school. Post-Keynesian economics concentrates on macroeconomic rigidities and adjustment processes. It is generally associated with the University of Cambridge and the work of Joan Robinson. Ecological economics like environmental economics studies the interactions between human economies and the ecosystems in which they are embedded, but in contrast to environmental economics takes an oppositional position towards general mainstream economic principles. A major difference between the two subdisciplines is their assumptions about the substitution possibilities between human-made and natural capital. Additionally, alternative developments include Marxian economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, econodynamics, feminist economics and biophysical economics. Feminist economics emphasises the role that gender plays in economies, challenging analyses that render gender invisible or support gender-oppressive economic systems. The goal is to create economic research and policy analysis that is inclusive and gender-aware to encourage gender equality and improve the well-being of marginalised groups. Methodology Theoretical research Mainstream economic theory relies upon analytical economic models. When creating theories, the objective is to find assumptions which are at least",
    "label": 0
  },
  {
    "text": "that is inclusive and gender-aware to encourage gender equality and improve the well-being of marginalised groups. Methodology Theoretical research Mainstream economic theory relies upon analytical economic models. When creating theories, the objective is to find assumptions which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories. While neoclassical economic theory constitutes both the dominant or orthodox theoretical as well as methodological framework, economic theory can also take the form of other schools of thought such as in heterodox economic theories. In microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm. Early macroeconomic models focused on modelling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models with microfoundations, in which microeconomic concepts play a major part. Sometimes an economic hypothesis is only qualitative, not quantitative. Expositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, mathematical economics is the application of mathematical methods to represent theories and analyse problems in economics. Paul Samuelson's treatise Foundations of Economic Analysis (1947) exemplifies the method, particularly as to maximizing behavioural relations of agents reaching equilibrium. The book focused on examining the class of statements called operationally meaningful theorems in economics, which are theorems that can conceivably be refuted by empirical data. Empirical research Economic theories are frequently tested empirically, largely through the use of econometrics using economic data. The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative.",
    "label": 0
  },
  {
    "text": "The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments. Statistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance (\"signal strength\") of the hypothesised relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs. Experimental economics has promoted the use of scientifically controlled experiments. This has reduced the long-noted distinction of economics from natural sciences because it allows direct tests of what were previously taken as axioms. In some cases these have found that the axioms are not entirely correct. In behavioural economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences. These techniques have led some to argue that economics is a \"genuine science\". Microeconomics Microeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and regulation. The item traded may be a tangible",
    "label": 0
  },
  {
    "text": "entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment. Various market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a \"price taker\" as no participant influences the price of a product. In the real world, markets often experience imperfect competition. Forms of imperfect competition include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Firms under imperfect competition have the potential to be \"price makers\", which means that they can influence the prices of their products. In partial equilibrium method of analysis, it is assumed that activity in the market being analysed does not affect other markets. This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across all markets. This method studies both changes in markets and their interactions leading towards equilibrium. Production, cost, and efficiency In microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production",
    "label": 0
  },
  {
    "text": "their interactions leading towards equilibrium. Production, cost, and efficiency In microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defence, smallpox vaccinations, etc.) or private goods, and \"guns\" vs \"butter\". Inputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car. Economic efficiency measures how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off. The production–possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case, an economy can produce just two goods (say \"guns\" and \"butter\"). The PPF is a table or graph (as at the right) that shows the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good. Scarcity is represented in the figure by people being willing but unable",
    "label": 0
  },
  {
    "text": "on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good. Scarcity is represented in the figure by people being willing but unable in the aggregate to consume beyond the PPF (such as at X) and by the negative slope of the curve. If production of one good increases along the curve, production of the other good decreases, an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter. The slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a real opportunity cost. Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. Along the PPF, scarcity implies that choosing more of one good in the aggregate entails doing with less of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents. By construction, each point on the curve shows productive efficiency in maximizing output for given total inputs. A point inside the curve (as at A), is feasible but represents production inefficiency (wasteful use of inputs), in that output of one or both goods could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organisation of a country that discourages full use of resources. Being on the curve might still not fully satisfy",
    "label": 0
  },
  {
    "text": "direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organisation of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points. Much applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organise society for the most efficient use of resources has been described as the \"essence of economics\", where the subject \"makes its unique contribution.\" Specialisation Specialisation is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus relatively cheaper, input. Even if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialise in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else. It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialisation in similar but differentiated product lines, to the overall benefit of respective trading parties or regions. The general theory",
    "label": 0
  },
  {
    "text": "mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialisation in similar but differentiated product lines, to the overall benefit of respective trading parties or regions. The general theory of specialisation applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding division of labour with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses. An example that combines features above is a country that specialises in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products. Theory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialisation of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the increased income levels that trade may facilitate. Supply and demand Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination",
    "label": 0
  },
  {
    "text": "observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power. For a given market of a commodity, demand is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is \"constrained utility maximisation\" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesised relation of each individual consumer for ranking different commodity bundles as more or less preferred. The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply. Supply is the relation between the price of a good and the quantity available for sale at",
    "label": 0
  },
  {
    "text": "normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply. Supply is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesised to be profit maximisers, meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged. That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The \"Law of Supply\" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply. Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply",
    "label": 0
  },
  {
    "text": "of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilise at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply. Firms People frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through firms. The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase, people begin to organise their production in firms when the costs of doing business becomes lower than doing it on the market. Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading. In perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organisation generalises from that special case to study the strategic behaviour of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly. Managerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimise business decisions, including unit-cost minimisation and",
    "label": 0
  },
  {
    "text": "heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimise business decisions, including unit-cost minimisation and profit maximisation, given the firm's objectives and constraints imposed by technology and market conditions. Uncertainty and game theory Uncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it. Game theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organisation, discussed above, to model different types of firm behaviour, for example in a solipsistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. In behavioural economics, it has been used to model the strategies agents choose when interacting with others whose interests are at least partially adverse to their own. In this, it generalises maximisation approaches developed to analyse market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic Theory of Games and Economic Behavior by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as the formulation of nuclear strategies, ethics, political science, and evolutionary biology. Risk aversion may stimulate activity that",
    "label": 0
  },
  {
    "text": "and Economic Behavior by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as the formulation of nuclear strategies, ethics, political science, and evolutionary biology. Risk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyses the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets, financial crises, and related government policy or regulation. Some market organisations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's \"Market for Lemons\" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a \"lemon\" depress its price below what a quality second-hand car would be. Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behaviour (say more reckless driving). Both problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market (\"incomplete markets\"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure. Market failure The term",
    "label": 0
  },
  {
    "text": "design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure. Market failure The term \"market failure\" encompasses several problems which may undermine standard economic assumptions. Although economists categorise market failures differently, the following categories emerge in the main texts. Information asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above. Natural monopoly, or the overlapping concepts of \"practical\" and \"technical\" monopoly, is an extreme case of failure of competition as a restraint on producers. Extreme economies of scale are one possible cause. Public goods are goods which are under-supplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time. Externalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidise or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities. Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply. In many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes",
    "label": 0
  },
  {
    "text": "a shift in demand or supply. In many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesised long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition. Some specialised fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or \"public bads\". Policy options include regulations that reflect cost–benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights. Welfare Welfare economics uses microeconomics techniques to evaluate well-being from allocation of productive factors as to desirability and economic efficiency within an economy, often relative to competitive general equilibrium. It analyses social welfare, however measured, in terms of economic activities of the individuals that compose the theoretical society considered. Accordingly, individuals, with associated economic activities, are the basic units for aggregating to social welfare, whether of a group, a community, or a society, and there is no \"social welfare\" apart from the \"welfare\" associated with its individual units. Macroeconomics Macroeconomics, another branch of economics, examines the economy as a whole to explain broad aggregates and their interactions \"top down\", that is, using a simplified form of general-equilibrium theory. Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy. Since at least the 1960s, macroeconomics has",
    "label": 0
  },
  {
    "text": "aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy. Since at least the 1960s, macroeconomics has been characterised by further integration as to micro-based modelling of sectors, including rationality of players, efficient use of market information, and imperfect competition. This has addressed a long-standing concern about inconsistent developments of the same subject. Macroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth. Growth Growth economics studies factors that explain economic growth – the increase in output per capita of a country over a long period of time. The same factors are used to explain differences in the level of output per capita between countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth. Much-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting. Business cycle The economics of a depression spurred the creation of \"macroeconomics\" as a separate discipline. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled The General Theory of Employment, Interest and Money, outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output. He therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government, to stabilize output over the business cycle. Thus, a central conclusion of Keynesian",
    "label": 0
  },
  {
    "text": "potential output. He therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government, to stabilize output over the business cycle. Thus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of The General Theory. Over the years, the understanding of the business cycle has branched into various research programs, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with classical economics, stating that Keynesianism is correct in the short run but qualified by classical-like considerations in the intermediate and long run. New classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and \"rational expectations\" theory, led by Robert Lucas, and real business cycle theory. In contrast, the new Keynesian approach retains the rational expectations assumption; however, it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are \"sticky\", which means they do not adjust instantaneously to changes in economic conditions. Thus, the new classical economists assume that prices and wages adjust automatically to attain full employment. In contrast, the new Keynesians see full employment as being automatically achieved only in the long run. Hence, government and central-bank policies are needed because the \"long run\" may be very long. Unemployment The amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking",
    "label": 0
  },
  {
    "text": "unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labour force. Unemployment can be generally broken down into several types that are related to different causes. Classical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment. Structural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process. While some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment. Money and monetary policy Money is a means of final payment for goods in most price system economies, and is the unit of account in which prices are typically stated. Money has general acceptability, relative consistency in value, divisibility, durability, portability, elasticity",
    "label": 0
  },
  {
    "text": "monetary policy Money is a means of final payment for goods in most price system economies, and is the unit of account in which prices are typically stated. Money has general acceptability, relative consistency in value, divisibility, durability, portability, elasticity in supply, and longevity with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others. In the words of Francis Amasa Walker, a well-known 19th-century economist, \"Money is what money does\" (\"Money is that money does\" in the original). As a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialised producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces. Monetary policy is the policy that central banks conduct to accomplish their broader objectives. Most central banks in developed countries follow inflation targeting, whereas the main objective for many central banks in development countries is to uphold a fixed exchange rate system. The primary monetary tool is normally the adjustment of interest rates, either directly via administratively changing the central bank's own interest rates or indirectly via open market operations. Via the monetary transmission mechanism, interest rate changes affect investment, consumption and net export, and hence aggregate demand, output and employment, and ultimately the development of wages and",
    "label": 0
  },
  {
    "text": "the central bank's own interest rates or indirectly via open market operations. Via the monetary transmission mechanism, interest rate changes affect investment, consumption and net export, and hence aggregate demand, output and employment, and ultimately the development of wages and inflation. Fiscal policy Governments implement fiscal policy to influence macroeconomic conditions by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government. For example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity. The effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government uses resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed. Sceptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from tax cuts will be offset by the",
    "label": 0
  },
  {
    "text": "be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from tax cuts will be offset by the increased saving intended to pay for future higher taxes. Inequality Economic inequality includes income inequality, measured using the distribution of income (the amount of money people receive), and wealth inequality measured using the distribution of wealth (the amount of wealth people own), and other measures such as consumption, land ownership, and human capital. Inequality exists at different extents between countries or states, groups of people, and individuals. There are many methods for measuring inequality, the Gini coefficient being widely used for income differences among individuals. An example measure of inequality between countries is the Inequality-adjusted Human Development Index, a composite index that takes inequality into account. Important concepts of equality include equity, equality of outcome, and equality of opportunity. Research has linked economic inequality to political and social instability, including revolution, democratic breakdown and civil conflict. Research suggests that greater inequality hinders economic growth and macroeconomic stability, and that land and human capital inequality reduce growth more than inequality of income. Inequality is at the centre stage of economic policy debate across the globe, as government tax and spending policies have significant effects on income distribution. In advanced economies, taxes and transfers decrease income inequality by one-third, with most of this being achieved via public social spending (such as pensions and family benefits.) Other branches of economics Public economics Public economics is the field of economics that deals with economic activities of a public sector, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost–benefit analysis of government programmes, effects on economic efficiency",
    "label": 0
  },
  {
    "text": "economics is the field of economics that deals with economic activities of a public sector, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost–benefit analysis of government programmes, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behaviour analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats. Much of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies ought to be like. Welfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society. International economics International trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalisation. Labour economics Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers), the demands of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and",
    "label": 0
  },
  {
    "text": "services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work), although there are also counter posing macro-economic system theories that think human capital is a contradiction in terms. Development economics Development economics examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors. Related subjects Economics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, cultural economics, family economics and institutional economics. Law and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be. A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities. Political economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behaviour, and externalities should impact government policy. Historians have employed political economy to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests. Energy economics is a",
    "label": 0
  },
  {
    "text": "and externalities should impact government policy. Historians have employed political economy to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests. Energy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics. The sociological subfield of economic sociology arose, primarily through the work of Émile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity). Classic works include Max Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Georg Simmel's The Philosophy of Money (1900). More recently, the works of James S. Coleman, Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field. Gary Becker in 1974 presented an economic theory of social interactions, whose applications included the family, charity, merit goods and multiperson interactions, and envy and hatred. He and Kevin Murphy authored a book in 2001 that analysed market behaviour in a social environment. Profession The professionalisation of economics, reflected in the growth of graduate programmes on the subject, has been described as \"the main change in economics since around 1900\". Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study. See Bachelor of Economics and Master of Economics. In the private sector,",
    "label": 0
  },
  {
    "text": "and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study. See Bachelor of Economics and Master of Economics. In the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national treasury, central bank or National Bureau of Statistics. There are dozens of prizes awarded to economists each year for outstanding intellectual contributions to the field, the most prominent of which is the Nobel Memorial Prize in Economic Sciences, though it is not a Nobel Prize. Contemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science. Professional economists are expected to be familiar with these tools, while a minority specialise in econometrics and mathematical methods. Women in economics Harriet Martineau (1802–1876) was a widely-read populariser of classical economic thought. Mary Paley Marshall (1850–1944), the first women lecturer at a British economics faculty, wrote The Economics of Industry with her husband Alfred Marshall. Joan Robinson (1903–1983) was an important post-Keynesian economist. The economic historian Anna Schwartz (1915–2012) coauthored A Monetary History of the United States, 1867–1960 with Milton Friedman. Three women have received the Nobel Prize in Economics: Elinor Ostrom (2009), Esther Duflo (2019) and Claudia Goldin (2023). Five have received the John Bates Clark Medal: Susan Athey (2007), Esther Duflo (2010), Amy Finkelstein (2012), Emi Nakamura (2019) and Melissa Dell (2020). Women's authorship share in prominent economic journals reduced from 1940 to the 1970s, but has subsequently risen, with different patterns of gendered coauthorship. Women remain globally under-represented in the profession (19% of authors in the RePEc database in 2018), with national variation. See also Notes",
    "label": 0
  },
  {
    "text": "economic journals reduced from 1940 to the 1970s, but has subsequently risen, with different patterns of gendered coauthorship. Women remain globally under-represented in the profession (19% of authors in the RePEc database in 2018), with national variation. See also Notes References Sources Hoover, Kevin D.; Siegler, Mark V. (20 March 2008). \"Sound and Fury: McCloskey and Significance Testing in Economics\". Journal of Economic Methodology. 15 (1): 1–37. CiteSeerX 10.1.1.533.7658. doi:10.1080/13501780801913298. S2CID 216137286. Samuelson, Paul A; Nordhaus, William D. (2010). Economics. Boston: Irwin McGraw-Hill. ISBN 978-0-07-351129-0. OCLC 751033918. Further reading Anderson, David A. (2019). Survey of Economics. New York: Worth. ISBN 978-1-4292-5956-9. Blanchard, Olivier; Amighini, Alessia; Giavazzi, Francesco (2017). Macroeconomics: a European perspective (3rd ed.). Pearson. ISBN 978-1-292-08567-8. Blaug, Mark (1985). Economic Theory in Retrospect (4th ed.). Cambridge: Cambridge University Press. ISBN 978-0-521-31644-6. McCann, Charles Robert Jr. (2003). The Elgar Dictionary of Economic Quotations. Edward Elgar. ISBN 978-1-84064-820-1. Post, Louis F. (1927), The Basic Facts of Economics: A Common-Sense Primer for Advanced Students. United States: Columbian Printing Company, Incorporated. Economics public domain audiobook at LibriVox. External links General information Institutions and organizations Study resources",
    "label": 0
  },
  {
    "text": "This aims to be a complete article list of economics topics: A Absence rate – Accountancy – Accounting reform – Actuary – Adaptive expectations – Adverse selection – Agent (economics) – Agent-based computational economics – Aggregate demand – Aggregate supply – Agricultural policy – Appropriate technology – Arbitrage – Arrow's impossibility theorem – Asymmetric information – Auction – Austrian School – Autarky – Awards B Backward induction – Balance of payments – Balance of trade – Bank – Bank reserves – Bankruptcy – Barter – Behavioral economics – Bellman equation – Bequest motive – Big Mac Index – Big Push Model – Bioeconomics (biophysical) – Black market – Black–Scholes – Bretton Woods System – Bullionism – Business cycle – Bertrand–Edgeworth model C Capital (economics) – Capital asset – Capital intensity – Capitalism – Cartel – Cash crop – Catch-up effect – Celtic Tiger – Central bank – Ceteris paribus – Charity shop – Chicago School of Economics – Circular flow of income — Classical economics – Classical general equilibrium model – Coase conjecture – Coase theorem – Cobweb model – Collective action – Collusion – Commodity – Commodity market – Commodity money – Community-based economics – Comparative advantage – Comparative dynamics – Comparative statics – Compensating differential – Competition – Competition law – Complementary good – Complexity economics – Comprehensive Income Policy Agreement – Computational economics – Concentration ratio – Consumer – Consumer price index – Consumer sovereignty – Consumer surplus – Consumer theory – Consumerism – Consumption (economics) – Contestable market – Contract curve – Contract theory – Cooperative – Cost – Cost–benefit analysis – Cost curve – Cost-of-production theory of value – Cost overrun – Cost-push inflation – Cost underestimation – Cournot competition – Cross elasticity of demand – Cultural ecology – Currency – Cycle of poverty D",
    "label": 0
  },
  {
    "text": "– Cost – Cost–benefit analysis – Cost curve – Cost-of-production theory of value – Cost overrun – Cost-push inflation – Cost underestimation – Cournot competition – Cross elasticity of demand – Cultural ecology – Currency – Cycle of poverty D Damages – Dead cat bounce – Deadweight loss – Debt – Decentralization – Deflation – Demand-pull inflation – Demurrage currency – Depreciation (currency) – Depreciation (economics) – Depression (economics) – Devaluation – Development economics – Differentiated Bertrand competition – Disequilibrium macroeconomics – Disinflation – Dispersed knowledge – Distribution (business) – Distribution (economics) – Distribution of income – Distribution of wealth – Dividend imputation – Division of labor – Dual-sector model – Duopoly – Dynamic programming – Dynamic stochastic general equilibrium E Ecological economics – Econometrics – Economic base analysis – Economic calculation problem – Economic development – Economic equilibrium – Economic geography – Economic graph – Economic growth – Economic history – Economic impact of immigration to Canada – Economic indicator – Economic model – Economic policy – Economic problem – Economic rent – Economic surplus – Economic system – Economic union – Economics – Economics terminology that differs from common usage – Economies of agglomeration – Economies of scale – Economies of scope – Economy of Canada – Ecotax – Edgeworth box – Edgeworth's limit theorem – Efficiency dividend – Efficiency wages – Efficient-market hypothesis – Elasticity (economics) – Elasticity of substitution – Electricity market – Employment – Endogenous growth theory – Energy economics – Entrepreneur – Entrepreneurial economics – Entrepreneurship – Environmental economics – Environmental finance – Equilibrium selection – Ethical consumerism – Euro – Event study – Evolutionary economics – Exceptionalism – Excess burden of taxation – Exogenous and endogenous variables – Exogenous growth model – Expected utility hypothesis – The Experience Economy – Experimental economics –",
    "label": 0
  },
  {
    "text": "selection – Ethical consumerism – Euro – Event study – Evolutionary economics – Exceptionalism – Excess burden of taxation – Exogenous and endogenous variables – Exogenous growth model – Expected utility hypothesis – The Experience Economy – Experimental economics – Externality -Effective exchange rate F Factor price equalization – Factors of production – Fair trade – Feminist economics – Fiat money – Finance – Financial astrology – Financial capital – Financial econometrics – Financial economics – Financial instrument – Fiscal policy – Fisher equation – Fisher separation theorem – Forecasting – Fractional-reserve banking – Free good – Free-rider problem – Free trade – Freiwirtschaft – Friedman rule – Full-reserve banking G Game theory -Gandhian economics – General equilibrium – Geographical pricing – Georgism – Gerschenkron effect – Giffen good – Gini coefficient – Global game – Globalization – Gold standard – Good (economics) – Goodhart's law – Government debt – Government-granted monopoly – Gresham's law – Gross domestic product – Gross national product – Gross private domestic investment – Gross value added – Growth accounting - Green marketing H Happiness economics – Hard currency – Harris–Todaro model – Hauser's Law – Hedonic regression – Herfindahl index – Heterodox economics – Historical school of economics – History of economic thought – Home economics – Homo economicus – Hotelling's law – Human capital – Human Development Index – Human development theory – Human resources – Humanistic economics – Hyperinflation I Identity economics – Imperfect competition – Implied in fact contract – Import – Import substitution industrialization – Imputation (economics) – Incentive – Income – Income effect – Income elasticity of demand (YED) – Income inequality metrics – Income tax – Independent goods – Indifference curve – Indigo Era (economics) – Individual capital – Induced demand – Industrial organization – Industrial policy –",
    "label": 0
  },
  {
    "text": "Income – Income effect – Income elasticity of demand (YED) – Income inequality metrics – Income tax – Independent goods – Indifference curve – Indigo Era (economics) – Individual capital – Induced demand – Industrial organization – Industrial policy – Industrial Revolution – Industrialisation – Inferior good – Inflation – Inflation targeting – Informal sector – Information asymmetry – Information economics – Infrastructural capital – Input–output model – Institutional economics – Interest – Interest-free economy – Interest rate parity – International economics – international finance – International trade – International Year of Microcredit – Intertemporal choice – Intertemporal equilibrium – Investment – Investment (macroeconomics) – Investment policy – Investment specific technological progress – Invisible hand – Islamic economic jurisprudence – IS/LM model – Isoquant –Isovalue lines – Ithaca Hours J Jane Jacobs – JEL classification codes – Job hunting – Joint product pricing – Just price K Kaldor-Hicks efficiency – Keynesian economics – Keynesian formula – Knowledge economy L Labor theory of value – Labour economics – Labor union – Laffer curve – Laissez-faire – Land (economics) – Land value tax – Law and economics – Legal origins theory – Lerman ratio – Limit price – List of unsolved problems in economics – List of topics in industrial organization – Lemon market – Living wage – Local currency – Local purchasing – Lorenz curve – Low-carbon economy – Lucas critique – Luxury goods M Macroeconomics – Making-up price – Managerial economics – Marginal cost – Marginal rate of substitution – Marginal revenue – Marginal utility – Marginalism – Market – Market anomaly – Market concentration – Market economy – Market failure – Market for lemons – Market power – Market share – Market structure – Market system – Markup rule – Marxian economics – Mathematical economics – Means of production",
    "label": 0
  },
  {
    "text": "Market anomaly – Market concentration – Market economy – Market failure – Market for lemons – Market power – Market share – Market structure – Market system – Markup rule – Marxian economics – Mathematical economics – Means of production – Measures of national income and output – Mechanism (sociology) – Medium of exchange – Mental accounting – Menu cost – Mercantilism – Merger simulation – Methodenstreit – Methodological individualism – Microcredit – Microeconomics – Minimum wage – Missing market – Model (macroeconomics) – Modern portfolio theory – Modigliani–Miller theorem – Monetarism – Monetary economics – Monetary policy – Monetary reform – Monetary system – Money – Money creation – Money multiplier – Money supply – Monopoly – Monopoly profit – Monopsony – Moral hazard N NAIRU – Nakamura number – Nanoeconomics – Nash equilibrium – National Income and Product Accounts – Natural capital – Natural Capitalism – Natural monopoly – Natural resource economics – Neoclassical economics – Neo-Keynesian economics – Neoliberalism – Net investment – Network effect – Neuroeconomics – New classical macroeconomics – New Keynesian economics – Nobel Memorial Prize in Economic Sciences – Normal good - Nominal effective exchange rate O Occupational licensing - Okun's law – Oligopoly – Oligopsony – Operations research – Opportunity cost – Ordinary least squares – Output (economics) – Overhead (business) P Pacman conjecture – Parable of the broken window – Pareto efficiency – Participatory economics – Peltzman effect – Perfect competition – Perspectives on Capitalism – Petrocurrency – Phillips curve – Pigovian tax – Platform imperialism – Pluralism in economics – Policy-ineffectiveness proposition – Political economy – Potential output – Poverty – Poverty threshold – Preference – Price control – Price discrimination – Price elasticity of demand – Price point – Price specie flow mechanism – Principal–agent problem – Principles of",
    "label": 0
  },
  {
    "text": "Policy-ineffectiveness proposition – Political economy – Potential output – Poverty – Poverty threshold – Preference – Price control – Price discrimination – Price elasticity of demand – Price point – Price specie flow mechanism – Principal–agent problem – Principles of Economics – Prisoner's dilemma – Product bundling – Production function – Production-possibility frontier – Production theory basics – Productivism – Productivity – Profit (economics) – Profit maximization – Property rights (economics) – Prospect theory – Public choice theory – Public bad – Public good – Purchasing power parity Q Quality of life – Quasi-market – Quantitative easing – Quantity theory of money R Rate of return pricing – Rational choice theory – Rational expectations – Rational pricing – Reaganomics – Real business-cycle theory – Real estate economics – Real estate investor – Real versus nominal value (economics) – Recession – Regenerative economic theory – Regional economics – Regression analysis – Remanufacturing – Rent control – Representative agent – Repugnancy costs – Reserve currency – Ricardian equivalence – Risk premium – Risk-free bond – Risk-free interest rate – Road pricing – Robin Hood effect - Real sectors S Safe trade – Sales tax – Saving – Scan-back allowance – Scarcity – Search theory – Self-revelation – Seven generation sustainability – Shock therapy (economics) – Signalling (economics) – Singer-Prebisch thesis – Slavery – Social capital – Social cost – Social Credit – Social finance – Social mobility – Social welfare function – Social welfare provision – Socialism – Socialist economics – Socioeconomics – Specialization (functional) – Spending multiplier – Stagflation – Standard of deferred payment – Standard of living – Stock exchange – Store of value – Strategic complements – Subgame perfect equilibrium – Subjective theory of value – Subsidy – Subsistence agriculture – Substitute good – Substitution effect – Sunk costs",
    "label": 0
  },
  {
    "text": "of deferred payment – Standard of living – Stock exchange – Store of value – Strategic complements – Subgame perfect equilibrium – Subjective theory of value – Subsidy – Subsistence agriculture – Substitute good – Substitution effect – Sunk costs – Sunspot equilibrium – Sunspots (economics) – Supermodular function – Supply and demand – Supply-side economics – Surplus value – Sustainable development – Sweatshop T Tariff – Tax – Tax, tariff and trade – Taylor rule – Technostructure – Terms of trade – Theory of the firm – Time-based currency – Time preference – Total cost of ownership – Trade – Trade bloc – Trade facilitation – Trade pact – Tragedy of the anticommons – Tragedy of the commons – Transaction cost – Transfer payment – Transfer pricing – Transformation problem – Transparency (market) – Transport economics – Triple bottom line – Trust (social sciences) – Two-part tariff – Tying (commerce) U Underground economy – Uneconomic growth – Unemployment – Unionization – Unit of account – United States public debt – Universe (economics) – Urban economics – Utilitarianism – Utility – Utility maximization problem V Value (economics) – Value added – Value added tax – Value of life – Veblen good – Velocity of money – Virtuous circle and vicious circle – von Neumann-Morgenstern utility function W Wage – Wealth – Wealth effect – Welfare – Welfare economics – Welfare trap – Workers' self-management X X-efficiency Y Yield (finance) – YOYO economics Z Zero-sum See also",
    "label": 0
  },
  {
    "text": "The following outline is provided as an overview of and topical guide to economics. Economics is a branch of science that analyzes the production, distribution, and consumption of goods and services. It aims to explain how economies work and how agents (people) respond to incentives. Economics is a behavioral science (a scientific discipline that focuses on the study of human behavior) as well as a social science (a scientific discipline that explores aspects of human society). Branches of economics Macroeconomics – branch of economics dealing with the performance, structure, behavior, and decision-making of an economy as a whole, rather than individual markets. Microeconomics – branch of economics that studies the behavior of individuals and firms in making decisions regarding the allocation of limited resources. Subdisciplines of economics Agricultural economics – applied field of economics concerned with the application of economic theory in optimizing the production and distribution of food and fiber products. Attention economics – approach to the management of information that treats human attention as a scarce commodity and applies economic theory to solve various information management problems Behavioral economics – study of the psychological (e.g. cognitive, behavioral, affective, social) factors involved in the decisions of individuals or institutions, and how these decisions deviate from those implied by traditional economic theory. Classical economics – theory of market economies as largely self-regulating systems, governed by natural laws of production and exchange Comparative economic systems – sub-classification of economics dealing with the comparative study of different systems of economic organization, such as capitalism, socialism, feudalism and the mixed economy. Contract theory – how economic actors can and do construct contractual arrangements, generally in the presence of information asymmetry Cultural economics – branch of economics that studies the relation of culture to economic outcomes. Demographic economics – application of economic analysis to",
    "label": 0
  },
  {
    "text": "– how economic actors can and do construct contractual arrangements, generally in the presence of information asymmetry Cultural economics – branch of economics that studies the relation of culture to economic outcomes. Demographic economics – application of economic analysis to demography, the study of human populations, including size, growth, density, distribution, and vital statistics. Development economics – branch of economics that deals with economic aspects of the development process in low- and middle- income countries Ecological economics – transdisciplinary and interdisciplinary field of academic research addressing the interdependence and coevolution of human economies and natural ecosystems, both intertemporally and spatially. Econometrics – application of statistical methods to economic data in order to give empirical content to economic relationships. Economic anthropology – field that attempts to explain human economic behavior in its widest historic, geographic and cultural scope, an amalgamation of economics and anthropology. Economic development – process by which the economic well-being and quality of life of a nation, region, local community, or an individual are improved according to targeted goals and objectives. Economic geography – subfield of human geography that studies economic activity and factors affecting it. It can also be considered a subfield or method in economics. Economic history – study of history using methodological tools from economics or with a special attention to economic phenomena. Economic sociology – study of the social cause and effect of various economic phenomena Economics of marriage – economic analysis of household formation and break up, of production and distribution decisions within the household. Education economics – study of economic issues relating to education, including the demand for education, the financing and provision of education, and the comparative efficiency of various educational programs and policies. Energy economics – broad scientific subject area which includes topics related to supply and use of energy",
    "label": 0
  },
  {
    "text": "to education, including the demand for education, the financing and provision of education, and the comparative efficiency of various educational programs and policies. Energy economics – broad scientific subject area which includes topics related to supply and use of energy in societies. Engineering economics – subset of economics concerned with the use and application of economic principles in the analysis of engineering decisions. Entrepreneurial economics – field of study that focuses on the study of entrepreneur and entrepreneurship within the economy. Environmental economics –sub-field of economics concerned with environmental issues. Family economics – applies economic concepts such as production, division of labor, distribution, and decision making to the family. Feminist economics –critical study of economics and economies, with a focus on gender-aware and inclusive economic inquiry and policy analysis Financial economics –branch of economics characterized by a \"concentration on monetary activities\", in which \"money of one type or another is likely to appear on both sides of a trade\" Freiwirtschaft –economic theory and proposal for demurrage currency, community-owned land, and free trade. Georgism –economic theory and proposal holding that people should own the value that they produce themselves, while the economic rent derived from land—including from all natural resources, the commons, and urban locations—should belong equally to all members of society. Green economics –economy that aims at reducing environmental risks and ecological scarcities, and that aims for sustainable development without degrading the environment. Health economics –branch of economics concerned with issues related to efficiency, effectiveness, value and behavior in the production and consumption of health and healthcare. Heterodox economics –schools of economic thought which are not commonly perceived as belonging to mainstream economics. Humanistic economics pattern of economic thought which argue for \"persons-first\" economic theories as opposed to mainstream economic theories which are understood as often emphasizing financial gain over",
    "label": 0
  },
  {
    "text": "of economic thought which are not commonly perceived as belonging to mainstream economics. Humanistic economics pattern of economic thought which argue for \"persons-first\" economic theories as opposed to mainstream economic theories which are understood as often emphasizing financial gain over human well-being. Industrial organization –field that builds on the theory of the firm by examining the structure of (and, therefore, the boundaries between) firms and markets. Information economics –branch of microeconomics that studies how information and information systems affect an economy and economic decisions. International economics – concerned with the effects upon economic activity from international differences in productive resources and consumer preferences and the international institutions that affect them. Institutional economics – focuses on understanding the role of the evolutionary process and the role of institutions in shaping economic behavior. Labor economics – studies the functioning and dynamics of the markets for wage labour Law and economics – application of microeconomic theory to the analysis of law. Managerial economics – branch of economics involving the application of economic methods in the organizational decision-making process. Mathematical economics – application of mathematical methods to represent theories and analyze problems in economics. Monetary economics – branch of economics that studies the different theories of money Public finance – study of finance within government and role of the government in the economy Public economics – study of government policy through the lens of economic efficiency and equity. Real estate economics – application of economic techniques to real estate markets. Regional economics – economic advantage of a geographical location and human activities of greatest height to contribute maximally to the general growth and prosperity of the region. Regional science – field of economics concerned with analytical approaches to problems that are related specifically to regional and international issues. Resource economics – deals with the",
    "label": 0
  },
  {
    "text": "height to contribute maximally to the general growth and prosperity of the region. Regional science – field of economics concerned with analytical approaches to problems that are related specifically to regional and international issues. Resource economics – deals with the supply, demand, and allocation of the Earth's natural resources. Rural economics – study of rural economies, including both agricultural and non-agricultural industries Socialist economics – economic theories, practices and norms of hypothetical and existing socialist economic systems. Urban economics – economic study of urban areas; as such, it involves using the tools of economics to analyze urban issues such as crime, education, public transit, housing, and local government finance. Welfare economics – field of economics that applies microeconomic techniques to evaluate the overall well-being (welfare) of a society. Methodologies or approaches Behavioural economics – study of the psychological (e.g. cognitive, behavioral, affective, social) factors involved in the decisions of individuals or institutions, and how these decisions deviate from those implied by traditional economic theory. Classical economics – theory of market economies as largely self-regulating systems, governed by natural laws of production and exchange Computational economics – interdisciplinary research discipline that combines methods in computational science and economics to solve complex economic problems. Econometrics – application of statistical methods to economic data in order to give empirical content to economic relationships. Evolutionary economics – school of economic thought that treats economic development as a process rather than an equilibrium and emphasizes change (qualitative, organisational, and structural), innovation, complex interdependencies, self-evolving systems, and limited rationality as the drivers of economic evolution. Experimental economics – application of experimental methods to study economic questions Praxeology (used by the Austrian School) – theory of human action, based on the notion that humans engage in purposeful behavior, contrary to reflexive behavior and other unintentional behavior. Social",
    "label": 0
  },
  {
    "text": "economics – application of experimental methods to study economic questions Praxeology (used by the Austrian School) – theory of human action, based on the notion that humans engage in purposeful behavior, contrary to reflexive behavior and other unintentional behavior. Social psychology – scientific study of how thoughts, feelings, and behaviors are influenced by the actual, imagined, or implied presence of others. Interdisciplinary fields involving economics Bioeconomics (fisheries) – a model which combines biological characteristics of the fish stock (such as growth rate and carrying capacity) with economic information about the fishery (such as costs and revenue) to describe the fishing system. Constitutional economics – a research program in economics and constitutionalism that has been described as explaining the choice \"of alternative sets of legal-institutional-constitutional rules that constrain the choices and activities of economic and political agents\". Econophysics – non-orthodox (in economics) interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics, Neuroeconomics – interdisciplinary field that seeks to explain human decision-making, the ability to process multiple alternatives and to follow through on a plan of action and studies how economic behavior can shape our understanding of the brain, and how neuroscientific discoveries can guide models of economics. Political economy – branch of political science and economics studying economic systems (e.g. markets and national economies) and their governance by political systems (e.g. law, institutions, and government) Socioeconomics – branch of economics that focuses on the interrelationship between economic activity and social behavior. Thermoeconomics – heterodox economics that applies the laws of statistical mechanics to economic theory Transport economics – branch of economics that deals with the allocation of resources within the transport sector. Types of economies Economy – system of human activities related to the production, distribution, exchange, and consumption of goods and",
    "label": 0
  },
  {
    "text": "to economic theory Transport economics – branch of economics that deals with the allocation of resources within the transport sector. Types of economies Economy – system of human activities related to the production, distribution, exchange, and consumption of goods and services of a country or other area. Economies, by political & social ideological structure Economic ideology – set of views forming the basis of an ideology on how the economy should run. Capitalist economy – economic system based on the private ownership of the means of production and their operation for profit. Planned economy – type of economic system where investment, production and the allocation of capital goods takes place according to economy-wide economic plans and production plans. Consumer economy – economy driven by consumer spending as a high percent of its gross domestic product (GDP), as opposed to other major components of GDP (gross private domestic investment, government spending, and imports netted against exports). Consumerism – social and economic order in which the aspirations of many individuals include the acquisition of goods and services beyond those necessary for survival or traditional displays of status. Corporate economy – political system of interest representation and policymaking whereby corporate groups, such as agricultural, labour, military, business, scientific, or guild associations, come together and negotiate contracts or policy (collective bargaining) on the basis of their common interests. Fascist economy – so-called economic system in fascism that is distinct from those advocated by other ideologies, comprising essential characteristics that fascist nations shared. Historians and other scholars disagree on the question of whether a specifically fascist type of economic policy can be said to exist. Laissez-faire – type of economic system in which transactions between private groups of people are free from any form of economic interventionism (such as subsidies or regulations). Mercantilism – nationalist",
    "label": 0
  },
  {
    "text": "fascist type of economic policy can be said to exist. Laissez-faire – type of economic system in which transactions between private groups of people are free from any form of economic interventionism (such as subsidies or regulations). Mercantilism – nationalist economic policy that is designed to maximize the exports and minimize the imports of an economy. Natural economy – type of economy in which money is not used in the transfer of resources among people. Primitive communism – gift economies of hunter-gatherers throughout history, where resources and property hunted or gathered are shared with all members of a group in accordance with individual needs Social market economy – socioeconomic model combining a free-market capitalist economic system alongside social policies and enough regulation to establish both fair competition within the market and generally a welfare state. Socialist economy – economic system is characterized by social ownership and operation of the means of production. Economies, by scope Anglo-Saxon economy – a regulated market-based economic model practiced in Anglosphere countries such as the United Kingdom, the United States, Canada, New Zealand, Australia and Ireland. American School – macroeconomic philosophy that dominated United States national policies from the time of the American Civil War until the mid-20th century, characterised by protectionism, government investment in infrastructure, and a national bank that promotes the growth of productive enterprises rather than speculation. Barter economy – an economy where transactions are direct exchanges of goods or services for other goods or services without using a medium of exchange, such as credit or money. Hunter-gatherer economy – economic systems of societies that rely on hunting wild animals and gathering wild plants for sustenance. Information economy – economy with an increased emphasis on informational activities and information industry, where information is valued as a capital good. Interest-free economy – an",
    "label": 0
  },
  {
    "text": "of societies that rely on hunting wild animals and gathering wild plants for sustenance. Information economy – economy with an increased emphasis on informational activities and information industry, where information is valued as a capital good. Interest-free economy – an economy that does not have pure interest rates. New industrial economy – socioeconomic classification for a subset of developing countries whose economic growth is much higher than that of other developing countries; and where the social consequences of industrialization, such as urbanization, are reorganizing society. Palace economy – system of economic organization in which a substantial share of the wealth flows into the control of a centralized administration, the palace, and out from there to the general population. Plantation economy – economy based on agricultural mass production, usually of a few commodity crops, grown on large farms worked by laborers or slaves Token economy – system that rewards people for desired behaviors with tokens, which can be exchanged for rewards. It's a behavior therapy technique that's based on operant conditioning. Traditional economy – an economic system based on customs, traditions, and beliefs passed down through generations. Transition economy – economy which is changing from a centrally planned economy to a market economy. World economy – economy of all humans in the world, referring to the global economic system, which includes all economic activities conducted both within and between nations, including production, consumption, economic management, work in general, financial transactions and trade of goods and services. Economies, by regulation Closed economy – an economic system of self-sufficiency and limited trade with outside world. Dual economy – existence of two separate economic sectors within one country, divided by different levels of development, technology, and different patterns of demand. Gift economy – system of exchange where valuables are not sold, but rather given",
    "label": 0
  },
  {
    "text": "outside world. Dual economy – existence of two separate economic sectors within one country, divided by different levels of development, technology, and different patterns of demand. Gift economy – system of exchange where valuables are not sold, but rather given without an explicit agreement for immediate or future rewards. Informal economy – part of any economy that is neither taxed nor monitored by any form of government. Market economy – economic system in which the decisions regarding investment, production, and distribution to the consumers are guided by the price signals created by the forces of supply and demand. Mixed economy economic system that includes both elements associated with capitalism, such as private businesses, and with socialism, such as nationalized government services. Open economy – economy in which both domestic and international entities participate in the trade of goods and services. Participatory economy economic system based on participatory decision making as the primary economic mechanism for allocation in society. Planned economy – economic system where investment, production and the allocation of capital goods takes place according to economy-wide economic plans and production plans. Subsistence economy – economy directed to basic subsistence (the provision of food, clothing and shelter) rather than to the market. Underground economy – economic activities that circumvent, escape, or are excluded from the institutional system of rules, rights, regulations, and enforcement penalties that govern formal agents engaged in production and exchange. Virtual economy – emergent economy existing in a virtual world, usually exchanging virtual goods in the context of an online game, particularly in massively multiplayer online games (MMOs). Economic elements Economic activities Business – any activity or enterprise entered into for profit. Business cycle – intervals of general expansion followed by recession in economic performance. Collective action – action taken together by a group of people whose",
    "label": 0
  },
  {
    "text": "(MMOs). Economic elements Economic activities Business – any activity or enterprise entered into for profit. Business cycle – intervals of general expansion followed by recession in economic performance. Collective action – action taken together by a group of people whose goal is to enhance their condition and achieve a common objective. Commerce – organized system of activities, functions, procedures and institutions that directly or indirectly contribute to the smooth, unhindered large-scale distribution and transfer (exchange through buying and selling) of goods and services at the right time, place, quantity, quality and price through various channels among the original producers and the final consumers within local, regional, national or international economies. Competition – different economic firms in contention to obtain goods that are limited by varying the elements of the marketing mix: price, product, promotion and place. Consumption – use of resources to fulfill present needs and desires Distribution – the way total output, income, or wealth is distributed among individuals or among the factors of production (such as labour, land, and capital). Employment – relationship between two parties regulating the provision of paid labour services. Entrepreneurship – creation or extraction of economic value in ways that generally entail beyond the minimal amount of risk Export – a good produced in one country that is sold into another country or a service provided in one country for a national or resident of another country. Finance – monetary resources and to the study and discipline of money, currency, assets and liabilities. Government spending – all government consumption, investment, and transfer payments. Import – part of the International Trade which involves buying and receiving of goods or services produced in another country. Investment – commitment of resources to achieve later benefits Mergers and acquisitions – business transactions in which the ownership of companies,",
    "label": 0
  },
  {
    "text": "– part of the International Trade which involves buying and receiving of goods or services produced in another country. Investment – commitment of resources to achieve later benefits Mergers and acquisitions – business transactions in which the ownership of companies, business organizations, or their operating units are transferred to or consolidated with another company or business organization. Pricing – process whereby a business sets and displays the price at which it will sell its products and services and may be part of the business's marketing plan. Geographical pricing – practice of modifying a basic list price based on the geographical location of the buyer. Production – process of combining various inputs, both material (such as metal, wood, glass, or plastics) and immaterial (such as plans, or knowledge) in order to create output. Trade – the transfer of goods and services from one person or entity to another, often in exchange for money. Balance of trade – difference between the monetary value of a nation's exports and imports of goods over a certain time period. Fair trade – arrangement designed to help producers in developing countries achieve sustainable and equitable trade relationships. Free trade – trade policy that does not restrict imports or exports. International trade – exchange of capital, goods, and services across international borders or territories Safe trade Tax – mandatory financial charge or levy imposed on an individual or legal entity by a governmental organization to support government spending and public expenditures collectively or to regulate and reduce negative externalities. Terms of trade – relative price of exports in terms of imports, defined as the ratio of export prices to import prices Trade bloc – intergovernmental agreement, often part of a regional intergovernmental organization, where barriers to trade (tariffs and others) are reduced or eliminated among the participating",
    "label": 0
  },
  {
    "text": "in terms of imports, defined as the ratio of export prices to import prices Trade bloc – intergovernmental agreement, often part of a regional intergovernmental organization, where barriers to trade (tariffs and others) are reduced or eliminated among the participating states. Trade pact – wide-ranging taxes, tariff and trade treaty that often includes investment guarantees. Trade route – logistical network identified as a series of pathways and stoppages used for the commercial transport of cargo. Economic forces Aggregate demand – total demand for final goods and services in an economy at a given time. Aggregate supply – total supply of goods and services that firms in a national economy plan on selling during a specific time period. Deflation – decrease in the general price level of goods and services. Economic activity (see above) Economies of agglomeration – how urban agglomeration occurs in locations where cost savings can naturally arise. Economies of scale – cost advantages that enterprises obtain due to their scale of operation, and are typically measured by the amount of output produced per unit of cost (production cost) . Economies of scope – cost savings formed by broadening production/services through diversified products. Incentive – anything that persuade a person or organization to alter their behavior to produce the desired outcome. Inflation – general increase in the prices of goods and services in an economy Hyperinflation – very high and typically accelerating inflation. It quickly erodes the real value of the local currency, as the prices of all goods increase. Invisible hand – the incentives which free markets sometimes create for self-interested people to accidentally act in the public interest, even when this is not something they intended. Preference – preference refers to an order by which an agent, while in search of an \"optimal choice\", ranks alternatives based",
    "label": 0
  },
  {
    "text": "create for self-interested people to accidentally act in the public interest, even when this is not something they intended. Preference – preference refers to an order by which an agent, while in search of an \"optimal choice\", ranks alternatives based on their respective utility. Profit motive – motivation of firms that operate so as to maximize their profits. Economic problems Depression – period of carried long-term economic downturn that is the result of lowered economic activity in one or more major national economies Financial crisis – situations in which some financial assets suddenly lose a large part of their nominal value. Hyperinflation – very high and typically accelerating inflation. Poverty – condition in which an individual lacks the financial resources and essentials for a basic standard of living. Recession – business cycle contraction that occurs when there is a period of broad decline in economic activity. List of recessions Stagflation – combination of high inflation, stagnant economic growth, and elevated unemployment. Unemployment – proportion of people above a specified age (usually 15) not being in paid employment or self-employment but currently available for work during the reference period Trends and influences Decentralization – process by which the activities of an organization, particularly those related to planning and decision-making, are distributed or delegated away from a central, authoritative location or group and given to smaller factions within it. Globalization – widespread international movement of goods, capital, services, technology and information. Industrialisation – period of social and economic change that transforms a human group from an agrarian society into an industrial society, involving an extensive reorganisation of an economy for the purpose of manufacturing. Internationalization – process of increasing involvement of enterprises in international markets, Economic measures Consumer price index – statistical estimate of the level of prices of goods and services",
    "label": 0
  },
  {
    "text": "involving an extensive reorganisation of an economy for the purpose of manufacturing. Internationalization – process of increasing involvement of enterprises in international markets, Economic measures Consumer price index – statistical estimate of the level of prices of goods and services bought for consumption purposes by households. Economic indicator – statistic about an economic activity to allow analysis of economic performance and predictions of future performance. Human Development Index – statistical composite index of life expectancy, education (mean years of schooling completed and expected years of schooling upon entering the education system), and per capita income indicators, used to rank countries into four tiers of human development. Measures of national income and output – used in economics to estimate total economic activity in a country or region Gross domestic product – monetary measure of the total market value of all the final goods and services produced and rendered in a specific time period by a country or countries Natural gross domestic product – highest level of real gross domestic product (potential output) that can be sustained over the long term Gross national product – market value of all the goods and services produced in one year by labor and property supplied by the citizens of a country. National income – amount of factor incomes earned by the residents of a country Net national income – net national product (NNP) minus indirect taxes Poverty level – minimum level of income deemed adequate in a particular country Standard of living – level of income, comforts and services available to an individual, community or society. UN Human Development Index – statistical composite index of life expectancy, education (mean years of schooling completed and expected years of schooling upon entering the education system), and per capita income indicators, which is used to rank countries into",
    "label": 0
  },
  {
    "text": "society. UN Human Development Index – statistical composite index of life expectancy, education (mean years of schooling completed and expected years of schooling upon entering the education system), and per capita income indicators, which is used to rank countries into four tiers of human development Value – measure of the benefit provided by a good or service to an economic agent Cost-of-production theory of value – theory that the price of an object or condition is determined by the sum of the cost of the resources that went into making it. Labor theory of value – theory of value that argues that the exchange value of a good or service is determined by the total amount of \"socially necessary labor\" required to produce it. Surplus value – difference between the amount raised through a sale of a product and the amount it cost to manufacture it Time value of money – fact that there is normally a greater benefit to receiving a sum of money now rather than an identical sum later Value added – difference between market value of a product or service, and the sum value of its constituents. Value of Earth – net worth of Earth, equated with the sum of all ecosystem services as evaluated in ecosystem valuation or full-cost accounting. Value of life – economic value used to quantify the benefit of avoiding a fatality Measuring well-being – an individual's perception of their position in life in the context of the culture and value systems in which they live and in relation to their goals, expectations, standards and concerns Working time – period of time that a person spends at paid labor. Economic participants Employer – a person conducting a business or undertaking who recruits labour and expertise Employee – a person who contributes labour",
    "label": 0
  },
  {
    "text": "expectations, standards and concerns Working time – period of time that a person spends at paid labor. Economic participants Employer – a person conducting a business or undertaking who recruits labour and expertise Employee – a person who contributes labour and expertise to an endeavor of an employer Entrepreneur – individual who creates and/or invests in one or more businesses, bearing most of the risks and enjoying most of the rewards Central bank – institution that manages the monetary policy of a country or monetary union Reproductive labor – work that is often associated with care giving and domestic housework roles including cleaning, cooking, child care, and the unpaid domestic labor force Economic politics Antitrust – field of law that promotes or seeks to maintain market competition by regulating anti-competitive conduct by companies. Cartel – group of independent market participants who collude with each other as well as agreeing not to compete with each other in order to improve their profits and dominate the market Government-granted monopoly – form of coercive monopoly by which a government grants exclusive privilege to a private individual or firm to be the sole provider of a good or service Reaganomics – neoliberal economic policies promoted by U.S. President Ronald Reagan during the 1980s, focusing mainly on supply-side economics. Taxation – imposition of tax, i.e. a mandatory financial charge or levy imposed on an individual or legal entity by a governmental organization to support government spending and public expenditures collectively or to regulate and reduce negative externalities. Income tax – a tax imposed on individuals or entities (taxpayers) in respect of the income or profits earned by them (commonly called taxable income) Land value tax – levy on the value of land without regard to buildings, personal property and other improvements upon it. Sales tax",
    "label": 0
  },
  {
    "text": "or entities (taxpayers) in respect of the income or profits earned by them (commonly called taxable income) Land value tax – levy on the value of land without regard to buildings, personal property and other improvements upon it. Sales tax – tax paid to a governing body for the sales of certain goods and services. Tariff – duty imposed by a national government, customs territory, or supranational union on imports of goods and is paid by the importer. Value-added tax – consumption tax that is levied on the value added at each stage of a product's production and distribution Economic policy Economic policy – all strategic interventions by public administrations – including the state, central bank, and local authorities – across economic activity, aimed at achieving objectives like growth, full employment, and social justice, thereby correcting existing imbalances. Agricultural policy – Government actions targeting the agricultural sector to achieve objectives like food security, farmer income, and environmental sustainability. Fiscal policy – Government actions related to taxation and expenditure to manage aggregate demand, influence economic activity, and achieve macroeconomic objectives. Incomes policy – Government actions to directly influence wage and price levels, often to control inflation or manage the distribution of income. Price controls – Government-imposed restrictions on the prices that can be charged for goods and services, typically implemented as maximum prices (price ceilings) or minimum prices (price floors) to manage affordability or ensure minimum returns for producers. Price ceiling – Government-imposed maximum prices that can be charged for specific goods or services, typically set below the market equilibrium to increase affordability, potentially leading to shortages. Rent control – Government-imposed regulations limiting the amount landlords can charge for rental housing and the rate at which rents can be increased, typically implemented to address housing affordability Price floor – Government-imposed minimum",
    "label": 0
  },
  {
    "text": "increase affordability, potentially leading to shortages. Rent control – Government-imposed regulations limiting the amount landlords can charge for rental housing and the rate at which rents can be increased, typically implemented to address housing affordability Price floor – Government-imposed minimum price that must be charged for a specific good or service, typically set above the market equilibrium to support producers. potentially leading to surplus. Minimum wage – Legally required lowest hourly pay for workers, serving as a price floor for labor to protect workers against unduly low pay and ensure a basic standard of living. Industrial policy – proactive government-led encouragement and development of specific strategic industries for the growth of all or part of the economy, especially in absence of sufficient private sector investments and participation. Infrastructure-based development – development where a substantial proportion of a nation's resources must be systematically directed towards long term assets such as transportation, energy and social infrastructure in the name of long-term economic efficiency and social equity Investment policy – government regulation or law that encourages or discourages foreign investment in the local economy, e.g. currency exchange limits Monetary policy – policy adopted by the monetary authority of a nation to affect monetary and other financial conditions to accomplish broader objectives like high employment and price stability Disinflation – decrease in the rate of inflation – a slowdown in the rate of increase of the general price level of goods and services in a nation's gross domestic product over time Inflation targeting – monetary policy where a central bank follows an explicit target for the inflation rate for the medium-term and announces this inflation target to the public Monetary hawk and dove – someone who advocates keeping inflation low as the top priority in monetary policy (hawk) and someone who emphasizes other issues,",
    "label": 0
  },
  {
    "text": "for the inflation rate for the medium-term and announces this inflation target to the public Monetary hawk and dove – someone who advocates keeping inflation low as the top priority in monetary policy (hawk) and someone who emphasizes other issues, especially low unemployment, over low inflation (dove). Monetary reform – movement or theory that proposes a system of supplying money and financing the economy that is different from the current system Quantitative easing – monetary policy action where a central bank purchases predetermined amounts of government bonds or other financial assets in order to stimulate economic activity Reflation – return of prices to a previous rate of inflation Policy mix – combination of a country's monetary policy and fiscal policy. These two channels influence growth and employment, and are generally determined by the central bank and the government (e.g., the United States Congress) respectively. Stabilization policy – set of measures introduced to stabilize a financial system or economy (business cycle stabilization or credit cycle stabilization) Tax policy – guidelines and principles established by a government for the imposition and collection of taxes Infrastructure Infrastructure – set of facilities and systems that serve a country, city, or other area, and encompasses the services and facilities necessary for its economy, households and firms to function. Markets Market – composition of systems, institutions, procedures, social relations or infrastructures whereby parties engage in exchange. Types of markets Black market – clandestine market or series of transactions that has some aspect of illegality, or is not compliant with an institutional set of rules. Commodity market – market that trades in the primary economic sector rather than manufactured products, i.e. agricultural products, energy products, and metals. Financial market – market in which people trade financial securities and derivatives at low transaction costs. Bond market – financial",
    "label": 0
  },
  {
    "text": "– market that trades in the primary economic sector rather than manufactured products, i.e. agricultural products, energy products, and metals. Financial market – market in which people trade financial securities and derivatives at low transaction costs. Bond market – financial market in which participants can issue new debt, known as the primary market, or buy and sell debt securities, known as the secondary market. Money market – component of the economy that provides short-term funds, dealing in short-term loans, generally for a period of a year or less. Spot market – public financial market in which financial instruments or commodities are traded for immediate delivery. Secondary market – financial market in which previously issued financial instruments such as stock, bonds, options, and futures are bought and sold. Third market – trading of exchange-listed securities in the over-the-counter (OTC) market. Fourth market – direct institution-to-institution trading without using the service of broker-dealers, thus avoiding both commissions and the bid–ask spread. Stock market – aggregation of buyers and sellers of stocks (also called shares), which represent ownership claims on businesses. Free market – economic system in which the prices of goods and services are determined by supply and demand expressed by sellers and buyers, operating without the intervention of government or any other external authority. Labor market – the supply of and demand for labour or employment in a particular country or area, i.e., transactions involving the buying and selling of the ability to work. Mass market – market for goods produced on a large scale for a significant number of end consumers. Niche market – subset of the market on which a product is appealed to a small group of consumers. Media market – region where the population can receive the same (or similar) television and radio station offerings, and may",
    "label": 0
  },
  {
    "text": "consumers. Niche market – subset of the market on which a product is appealed to a small group of consumers. Media market – region where the population can receive the same (or similar) television and radio station offerings, and may also include other types of media such as newspapers and internet content. Regulated market – idealized system where the government or other organizations oversee the market, control the forces of supply and demand, and to some extent regulate the market actions. Aspects of markets Market failure – situation in which the allocation of goods and services by a free market is not Pareto efficient, often leading to a net loss of economic value. Market power – ability of a firm to influence the price at which it sells a product or service by manipulating either the supply or demand of the product or service to increase economic profit. Market share – percentage of the total revenue or sales in a market that a company's business makes up. Market structure – depicts how firms are differentiated and categorised based on the types of goods they sell (homogeneous/heterogeneous) and how their operations are affected by external factors and elements. Market system – systematic process enabling many market players to offer and demand: helping buyers and sellers interact and make deals. Market transparency – the state where much is known by many about what products and services or capital assets are available, market depth (quantity available), what price, and where. Market trend – upward or downward movement of a market, during a period of time. Market dominance – control of an economic market by a firm, which possesses the power to affect competition and influence market price Market forms Market form Perfect competition, in which the market consists of a very large number",
    "label": 0
  },
  {
    "text": "of time. Market dominance – control of an economic market by a firm, which possesses the power to affect competition and influence market price Market forms Market form Perfect competition, in which the market consists of a very large number of firms producing a homogeneous product. Monopolistic competition, also called competitive market, where there are a large number of independent firms which have a very small proportion of the market share. Monopoly, where there is only one provider of a product or service. Monopsony, when there is only one buyer in a market. Natural monopoly, a monopoly in which economies of scale cause efficiency to increase continuously with the size of the firm. Oligopoly, in which a market is dominated by a small number of firms which own more than 40% of the market share. Oligopsony, a market dominated by many sellers and a few buyers. Market-oriented activities Market analysis Marketing Market segmentation Market intelligence Market research Money Money – a medium of exchange, a unit of account, a store of value and sometimes, a standard of deferred payment. Commodity money – money whose value comes from a commodity of which it is made. Currency – standardized money in common use, usually for people in a nation state. Hard currency – any globally traded currency that serves as a reliable and stable store of value. Fiat money – a type of government-issued currency that is not backed by a precious metal, such as gold or silver, nor by any other tangible asset or commodity. Demurrage currency – a type of money that is designed to only be a temporary store of value. Local currency – currency that can be spent in a particular geographical locality at participating organisations. Petrocurrency – Dollars paid to oil-producing nations (petrodollar recycling); Currencies of oil-producing",
    "label": 0
  },
  {
    "text": "money that is designed to only be a temporary store of value. Local currency – currency that can be spent in a particular geographical locality at participating organisations. Petrocurrency – Dollars paid to oil-producing nations (petrodollar recycling); Currencies of oil-producing nations which tend to rise in value against other currencies when the price of oil rises; Pricing of oil in US dollars. Reserve currency – foreign currency that is held in significant quantities by central banks or other monetary authorities as part of their foreign exchange reserves. Time-based currency – alternative currency or exchange system where the unit of account is the person-hour or some other time unit. Monetary reform – movement or theory that proposes a system of supplying money and financing the economy that is different from the current system Monetary system – system by which a government provides money in a country's economy Money supply – total volume of money held by the public at a particular point in time Resources Resource Resource management Resource management Natural resource management Resource allocation Factors of production Factors of production Land Land Natural resources Labor Division of labour Workplace Capital Capital – durable produced goods that are in turn used as productive inputs for further production of goods and services Capital asset – property of any kind held by an assessee Capital intensity – amount of fixed or real capital present in relation to other factors of production, especially labor Financial capital – any economic resource measured in terms of money used by entrepreneurs and businesses to buy what they need to make their products or to provide their services to the sector of the economy upon which their operation is based Human capital – concept used by economists to designate personal attributes considered useful in the production process, encompassing",
    "label": 0
  },
  {
    "text": "need to make their products or to provide their services to the sector of the economy upon which their operation is based Human capital – concept used by economists to designate personal attributes considered useful in the production process, encompassing employee knowledge, skills, know-how, good health, and education Individual capital – economic view of talent, comprises inalienable or personal traits of persons, tied to their bodies and available only through their own free will Natural capital – world's stock of natural resources, which includes geology, soils, air, water and all living organisms Social capital – networks of relationships which are productive towards advancing the goals of individuals and groups. Wealth – abundance of valuable financial assets or physical possessions which can be converted into a form that can be used for transactions Economic theory Consumer theory – branch of microeconomics that relates preferences to consumption expenditures and to consumer demand curves. Efficiency wage hypothesis – hypothesis of wage paid in excess of the market-clearing wage to increase the labor productivity of workers Efficient market hypothesis – hypothesis in financial economics that states that asset prices reflect all available information Marginalism – theory of economics that attempts to explain the discrepancy in the value of goods and services by reference to their secondary, or marginal, utility Prospect theory – theory of behavioral economics, judgment and decision making that describes how individuals assess their loss and gain perspectives in an asymmetric manner Public choice theory – use of economic tools to deal with traditional problems of political science Rational choice theory – use of decision theory as a set of guidelines to help understand economic and social behavior Economic ideologies Consumerism – social and economic order in which the aspirations of many individuals include the acquisition of goods and services beyond those",
    "label": 0
  },
  {
    "text": "use of decision theory as a set of guidelines to help understand economic and social behavior Economic ideologies Consumerism – social and economic order in which the aspirations of many individuals include the acquisition of goods and services beyond those necessary for survival or traditional displays of status. Monetarism –school of thought in monetary economics that emphasizes the role of policy-makers in controlling the amount of money in circulation. Productivism –belief that measurable productivity and growth are the purpose of human organization (e.g., work), and that \"more production is necessarily good\" Utilitarianism – family of normative ethical theories that prescribe actions that maximize happiness and well-being for the affected individuals. History of economics History of economic thought History of economic thought Ancient economic thought Aristotle Nicomachean Ethics Economics of the Age of Enlightenment Mercantilism British Enlightenment John Locke Dudley North David Hume French Enlightenment: Physiocracy François Quesnay Tableau économique Anne Robert Jacques Turgot Reflections on the Formation and Distribution of Wealth Classical economics, political economy Adam Smith The Wealth of Nations David Ricardo Socialist economics Marxian economics Labour theory of value Anarchist economics Austrian School of Economics Carl Menger Friedrich von Hayek Ludwig von Mises Neoclassical economics Léon Walras John Bates Clark Alfred Marshall Keynesian economics John Maynard Keynes Cambridge capital controversy Neo-Keynesian economics Paul Samuelson John Hicks (economist) Neoclassical synthesis Post-Keynesian economics Hyman Minsky Joan Robinson Michał Kalecki New Keynesian economics Chicago school of economics Milton Friedman Monetarism Economic history Economic history Economic events Economic history of the world Economics in the Middle Ages: feudalism and manorialism Economics of the Renaissance: mercantilism Industrial Revolution Economic history of World War I Nixon shock Economic history by region Economic history of Africa Economic history of Morocco Economic history of Nigeria Economic history of Somalia Economic history of South Africa Economic history",
    "label": 0
  },
  {
    "text": "the Renaissance: mercantilism Industrial Revolution Economic history of World War I Nixon shock Economic history by region Economic history of Africa Economic history of Morocco Economic history of Nigeria Economic history of Somalia Economic history of South Africa Economic history of Zimbabwe Economic history of the Arab world Economic history of Asia Economic history of Cambodia Economic history of China Economic history of China before 1912 Economic history of China (1912–1949) Economic history of China (1949–present) Economic history of the Republic of China Economic history of India Economic history of Indonesia Economic history of Iran Economic history of Japan Economic history of Malaysia Economic history of Pakistan Economic history of Taiwan Economic history of Turkey Economic history of the Ottoman Empire Economic history of Vietnam Economic history of the Philippines Economic history of Australia Economic history of Europe Economic history of France Economic history of Germany Economic history of the German reunification Economic history of Greece and the Greek world Economic history of Iceland Economic history of Ireland Economic history of Italy Economic history of Portugal Economic history of Scotland Economic history of Spain Economic history of Sweden Economic history of Venice Economic history of the Netherlands (1500–1815) Economic history of the Republic of Ireland Economic history of the Russian Federation Economic history of the United Kingdom Economic history of North America Economic history of Canada Economic history of Mexico Economic history of the United States Economic history of Central America Economic history of Nicaragua Economic history of South America Economic history of Argentina Economic history of Brazil Economic history of Chile Economic history of Colombia Economic history of Ecuador Economic history of Peru Economic history by subject History of banking History of money History of stock markets General economic concepts Ricardian economics – economic theories of David Ricardo, an",
    "label": 0
  },
  {
    "text": "of Chile Economic history of Colombia Economic history of Ecuador Economic history of Peru Economic history by subject History of banking History of money History of stock markets General economic concepts Ricardian economics – economic theories of David Ricardo, an English political economist, especially the concept of comparative advantage and its implications for international trade and economic growth Keynesian economics – various macroeconomic theories and models, named after British economist John Maynard Keynes, of how aggregate demand (total spending in the economy) strongly influences economic output and inflation Classical economics – theory of market economies as largely self-regulating systems, governed by natural laws of production and exchange (famously captured by Adam Smith's metaphor of the invisible hand) Neo-Keynesian economics – academic movement and paradigm in economics that worked towards reconciling the macroeconomic thought of John Maynard Keynes with neoclassical economics. Neoclassical economics – approach to economics in which the production, consumption, and valuation (pricing) of goods and services are observed as driven by the supply and demand model New classical economics – school of thought in macroeconomics that builds its analysis entirely on a neoclassical framework New Keynesian economics – school of macroeconomics that strives to provide microeconomic foundations for Keynesian economics Participatory economics – economic system based on participatory decision making as the primary economic mechanism for allocation in society Home economics – subject concerning human development, personal and family finances, consumer issues, housing and interior design, nutrition and food preparation, as well as textiles and apparel Goods Complement good Coordination good Free goods Inferior goods Normal goods Public good Substitute good isms Capitalism Natural Capitalism Economic subjectivism Socialism Modern portfolio theory Game theory Human development theory Production theory basics Time preference theory of interest Agent Arbitrage Big Mac Index Big push model Cash crop Canadian and American economies",
    "label": 0
  },
  {
    "text": "good Substitute good isms Capitalism Natural Capitalism Economic subjectivism Socialism Modern portfolio theory Game theory Human development theory Production theory basics Time preference theory of interest Agent Arbitrage Big Mac Index Big push model Cash crop Canadian and American economies compared Catch-up effect Chicago school Collusion Commodity Comparative advantage Competitive advantage complementarity Consumer and producer surplus Cost Cost-benefit analysis Cost-of-living index Debt Devaluation Disposable income Economic Economic data Economic efficiency Economic growth Economic globalization Economic profits Economic modeling Economic reports Economic sector Economic system Ecosystem services Elasticity Environmental finance Euro Event study Experience economy Externality Factor price equalization Federal Reserve Financial instruments Fiscal neutrality Full-reserve banking General equilibrium Gold standard Import substitution Income Income elasticity of demand Income velocity of money Induced demand Industrial organization Input-output model Interest Keynes, John Maynard Knowledge-based economy Laissez-faire Land Living wage Local purchasing Lorenz curve Marginal Revolution Means of production Mental accounting Menu costs Missing market Model - economics Model - macroeconomics Monopoly profit Moral hazard Moral purchasing Multiplier (economics) Neo-classical growth model Network effect Network externality Operations research Opportunity cost Output Parable of the broken window Pareto efficiency Price Price discrimination Price elasticity of demand Price points Outline of industrial organization Production function Productivity Profit (economics) Profit maximization Public bad Public debt Purchasing power parity Rahn curve Rate of return pricing Rational expectations Rational pricing Real business cycle Real versus nominal in economics Regression analysis Returns to scale Risk premium Saving Scarcity Seven-generation sustainability Slavery Social cost Social credit Social welfare Stock exchange Subsidy Corporate welfare Subsistence agriculture Sunk cost Supply and demand Supply-side economics Sustainable competitive advantage Sustainable development Sweatshop Technostructure The Theory of Moral Sentiments by Adam Smith Transaction cost Triple bottom line Trust Utility Utility maximization problem Uneconomic growth U.S. public debt Virtuous circle and vicious circle Wage rate X-efficiency",
    "label": 0
  },
  {
    "text": "Supply-side economics Sustainable competitive advantage Sustainable development Sweatshop Technostructure The Theory of Moral Sentiments by Adam Smith Transaction cost Triple bottom line Trust Utility Utility maximization problem Uneconomic growth U.S. public debt Virtuous circle and vicious circle Wage rate X-efficiency Yield Zero sum game Economics organizations American Economic Association American Institute for Economic Research American Law and Economics Association Association for Evolutionary Economics Association for Social Economics Canadian Economics Association Centre for Economic Policy Research China Center for Economic Research Eastern Economic Association Econometric Society European Economic Association International Association for Feminist Economics International Economic Association Latin American and Caribbean Economic Association National Association for Business Economics National Bureau of Economic Research Royal Economic Society Southern Economic Association Western Economic Association International Economics publications List of economics journals List of important publications in economics Persons influential in the field of economics List of economists Nobel Memorial Prize–winning economic historians Milton Friedman won the Nobel Memorial Prize in Economic Sciences in 1976 for \"his achievements in the fields of consumption analysis, monetary history and theory and for his demonstration of the complexity of stabilization policy\". Robert Fogel and Douglass North won the Nobel Memorial Prize in 1993 for \"having renewed research in economic history by applying economic theory and quantitative methods in order to explain economic and institutional change\". Merton Miller, who started his academic career teaching economic history at the LSE, won the Nobel Memorial Prize in 1990 with Harry Markowitz and William F. Sharpe. Other notable economic historians See also Index of accounting articles Index of economics articles Index of international trade topics JEL classification codes List of business theorists List of economic communities List of economics awards List of free trade agreements Outline of business management Outline of commercial law Outline of community Outline of finance Outline of",
    "label": 0
  },
  {
    "text": "An economic impact analysis (EIA) examines the effect of an event on the economy in a specified area, ranging from a single neighborhood to the entire globe. It usually measures changes in business revenue, business profits, personal wages, and/or jobs. The economic event analyzed can include implementation of a new policy or project, or may simply be the presence of a business or organization. An economic impact analysis is commonly conducted when there is public concern about the potential impacts of a proposed project or policy. An economic impact analysis typically measures or estimates the change in economic activity between two scenarios, one assuming the economic event occurs, and one assuming it does not occur (which is referred to as the counterfactual case). This can be accomplished either before or after the event (ex ante or ex post). An economic impact analysis attempts to measure or estimate the change in economic activity in a specified region, caused by a specific business, organization, policy, program, project, activity, or other economic event. The study region can be a neighborhood, town, city, county, statistical area, state, country, continent, or the entire globe. Economic impact analyses often estimate multiple types of impacts. An output impact is the total increase in business sales revenue. In turn, local businesses use some of this new revenue to pay for goods and services outside of the study region, so the output impact is not synonymous with local business profits. A more conservative measure of economic activity is the value added impact, which estimates the increase in the study region's gross regional product. The gross regional product (GRP) is very similar to the nation's gross domestic product (GDP), and represents the total size of the local economy. This impact estimates the increase in local employee wages plus local business",
    "label": 0
  },
  {
    "text": "region's gross regional product. The gross regional product (GRP) is very similar to the nation's gross domestic product (GDP), and represents the total size of the local economy. This impact estimates the increase in local employee wages plus local business profits (not total revenue, like the output impact). However, the value added impact may overstate local profits when they are transferred overseas (such as in the form of dividends or investments in foreign facilities). An even more conservative measure is the woman labour income impact, which represents the increase in total money paid to local employees in the form of salaries and wages. The increases in income may come in the form of raises and/or increased hours for existing employees, or new jobs for the unemployed. This is a measure of the economic impact on just personal incomes, not business revenues or profits. A similar measure is the employment impact, which measures the increase in the number of total employees in the local region. Instead of measuring the economic impact in terms of money, this measure presents the impact on the number of jobs in the region. Sources of economic impacts In addition to the types of impacts, economic impact analyses often estimate the sources of the impacts. Each impact can be decomposed into different components, depending on the effect that caused the impact. Direct effects are the results of the money initially spent in the study region by the business or organization being studied. This includes money spent to pay for salaries, supplies, raw materials, and operating expenses. The direct effects from the initial spending creates additional activity in the local economy. Indirect effects are the results of business-to-business transactions indirectly caused by the direct effects. Businesses initially benefiting from the direct effects will subsequently increase spending at other",
    "label": 0
  },
  {
    "text": "direct effects from the initial spending creates additional activity in the local economy. Indirect effects are the results of business-to-business transactions indirectly caused by the direct effects. Businesses initially benefiting from the direct effects will subsequently increase spending at other local businesses. The indirect effect is a measure of this increase in business-to-business activity (not including the initial round of spending, which is included in the direct effects). Induced effects are the results of increased personal income caused by the direct and indirect effects. Businesses experiencing increased revenue from the direct and indirect effects will subsequently increase payroll expenditures (by hiring more employees, increasing payroll hours, raising salaries, etc.). Households will, in turn, increase spending at local businesses. The induced effect is a measure of this increase in household-to-business activity. Finally, dynamic effects are caused by geographic shifts over time in populations and businesses. Method Economic impact analyses usually employ one of two methods for determining impacts. The first is an input-output model (I/O model) for analyzing the regional economy. These models rely on inter-industry data to determine how effects in one industry will impact other sectors. In addition, I/O models also estimate the share of each industry's purchases that are supplied by local firms (versus those outside the study area). Based on this data, multipliers are calculated and used to estimate economic impacts. Examples of I/O models used for economic impact analyses are IMPLAN, RIMS-II, Chmura, Emsi, and aLocal Solutions. Another method used for economic impact analyses are economic simulation models. These are more complex econometric and general equilibrium models. They account for everything the I/O model does, plus they forecast the impacts caused by future economic and demographic changes. One such an example is the REMI Model. Additionally, new AI based software aLocal provides economic impact, financial forecast,",
    "label": 0
  },
  {
    "text": "models. They account for everything the I/O model does, plus they forecast the impacts caused by future economic and demographic changes. One such an example is the REMI Model. Additionally, new AI based software aLocal provides economic impact, financial forecast, market demand and employment estimates using a modified input/output algorithm using financial and community data in addition to the current software that only uses economic data. Their approach is to provide an analytics clearinghouse that is accessible, available, affordable and accurate representation to zip codes, cities, reservations, counties, MSA and congressional districts. aLocal also provides the analytics with shapefiles that are GIS compatible. Comparison to other analyses Economic impact analyses are related to but differ from other similar studies. An economic impact analysis only covers specific types of economic activity. Some social impacts that affect a region's quality of life, such as safety and pollution, may be analyzed as part of a social impact assessment, but not an economic impact analysis, even if the economic value of those factors could be quantified. An economic impact analysis may be performed as one part of a broader environmental impact assessment, which is often used to examine impacts of proposed development projects. An economic impact analysis may also be performed to help calculate the benefits as part of a cost-benefit analysis. Applications Economic impact analyses are used frequently in transportation planning. Common tools for this application include the Transportation Economic Development Impact System (TREDIS) and TranSight. Several transportation agencies, including the Transportation Research Board and US Department of Transportation, publish guides, standards, and techniques for using economic impact analyses in transportation planning projects. Economic impact analyses are often used to examine the consequences of economic development projects and efforts, such as real estate development, business openings and closures, and site selection projects. The",
    "label": 0
  },
  {
    "text": "Psychology is the scientific study of behavior and the mind. Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psychologists aim to understand the behavior of individuals and groups. A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as behavioral or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and neurobiological processes that underlie cognitive functions and behaviors. As part of an interdisciplinary field, psychologists are involved in research on perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. They also consider the unconscious mind. Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation. While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society. Many psychologists are involved in some kind of therapeutic role, practicing psychotherapy in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed",
    "label": 0
  },
  {
    "text": "settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed in industrial and organizational settings. Yet others are involved in work on human development, aging, sports, health, forensic science, education, and the media. Etymology and definitions The word psychology derives from the Greek word psyche, for spirit or soul. The latter part of the word psychology derives from -λογία -logia, which means \"study\" or \"research\". The word psychology was first used in the Renaissance. In its Latin form psychiologia, it was first employed by the Croatian humanist and Latinist Marko Marulić in his book Psichiologia de ratione animae humanae (Psychology, on the Nature of the Human Soul) in the decade 1510–1520 The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary. The dictionary refers to \"Anatomy, which treats the Body, and Psychology, which treats of the Soul.\" Ψ (psi), the first letter of the Greek word psyche from which the term psychology is derived, is commonly associated with the field of psychology. In 1890, William James defined psychology as \"the science of mental life, both of its phenomena and their conditions.\" This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by John B. Watson, who in 1913 asserted the methodological behaviorist view of psychology as a purely objective experimental branch of natural science, the theoretical goal of which \"is the prediction and control of behavior.\" Since James defined \"psychology\", the term more strongly implicates scientific experimentation. Folk psychology is the understanding of the mental states and behaviors of people held by ordinary people, as contrasted with",
    "label": 0
  },
  {
    "text": "of which \"is the prediction and control of behavior.\" Since James defined \"psychology\", the term more strongly implicates scientific experimentation. Folk psychology is the understanding of the mental states and behaviors of people held by ordinary people, as contrasted with psychology professionals' understanding. History The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders. Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise), addressed the workings of the mind. As early as the 4th century BCE, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes. In 387 BCE, Plato suggested that the brain is where mental processes take place, and in 335 BC Aristotle suggested that it was the heart. In China, the foundations of psychological thought emerged from the philosophical works of ancient thinkers like Laozi and Confucius, as well as the teachings of Buddhism. This body of knowledge drew insights from introspection, observation, and techniques for focused thinking and behavior. It viewed the universe as comprising physical and mental realms, along with the interplay between the two. Chinese philosophy also emphasized purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder",
    "label": 0
  },
  {
    "text": "the brain advanced during the Qing dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function. Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India. Psychology was of interest to Enlightenment thinkers in Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum. He suggested that the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia Empirica in 1732 and Psychologia Rationalis in 1734. Immanuel Kant advanced the idea of anthropology as a discipline, with psychology an important subdivision. Kant, however, explicitly rejected the idea of an experimental psychology, writing that \"the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does",
    "label": 0
  },
  {
    "text": "as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object.\" In 1783, Ferdinand Ueberwasser (1752–1812) designated himself Professor of Empirical Psychology and Logic and gave lectures on scientific psychology, though these developments were soon overshadowed by the Napoleonic Wars. At the end of the Napoleonic era, Prussian authorities discontinued the Old University of Münster. Having consulted philosophers Hegel and Herbart, however, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation. In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's crowded \"lunatic\" asylums. Beginning of experimental psychology Philosopher John Stuart Mill believed that the human mind was open to scientific investigation, even if the science is in some ways inexact. Mill proposed a \"mental chemistry\" in which elementary thoughts could combine into ideas of greater complexity. Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s. He articulated the principle that human perception of a stimulus varies logarithmically according to its intensity. The principle became known as the Weber–Fechner law. Fechner's 1860 Elements of Psychophysics challenged Kant's negative view with regard to conducting quantitative research on the mind. Fechner's achievement was to show that \"mental processes could not only be given numerical magnitudes, but also that these could be measured by experimental methods.\" In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception,",
    "label": 0
  },
  {
    "text": "research on the mind. Fechner's achievement was to show that \"mental processes could not only be given numerical magnitudes, but also that these could be measured by experimental methods.\" In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, where he established the psychological laboratory that brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of materials. Paul Flechsig and Emil Kraepelin soon created another influential laboratory at Leipzig, a psychology-related lab, that focused more on experimental psychiatry. James McKeen Cattell, a professor of psychology at the University of Pennsylvania and Columbia University and the co-founder of Psychological Review, was the first professor of psychology in the United States. The German psychologist Hermann Ebbinghaus, a researcher at the University of Berlin, was a 19th-century contributor to the field. He pioneered the experimental study of memory and developed quantitative models of learning and forgetting. In the early 20th century, Wolfgang Kohler, Max Wertheimer, and Kurt Koffka co-founded the school of Gestalt psychology of Fritz Perls. The approach of Gestalt psychology is based upon the idea that individuals experience things as unified wholes. Rather than reducing thoughts and behavior into smaller component elements, as in structuralism, the Gestaltists maintain that whole of experience is important, \"and is something else than the sum of its parts, because summing is a meaningless procedure, whereas the whole-part relationship is meaningful.\" Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories. G. Stanley Hall, an American who studied with Wundt, founded a psychology lab that became internationally",
    "label": 0
  },
  {
    "text": "procedure, whereas the whole-part relationship is meaningful.\" Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories. G. Stanley Hall, an American who studied with Wundt, founded a psychology lab that became internationally influential. The lab was located at Johns Hopkins University. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo. Wundt's assistant, Hugo Münsterberg, taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta. Wundt's students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests of mental ability. Cattell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Witmer focused on the mental testing of children; Scott, on employee selection. Another student of Wundt, the Englishman Edward Titchener, created the psychology program at Cornell University and advanced \"structuralist\" psychology. The idea behind structuralism was to analyze and classify different aspects of the mind, primarily through the method of introspection. William James, John Dewey, and Harvey Carr advanced the idea of functionalism, an expansive approach to psychology that underlined the Darwinian idea of a behavior's usefulness to the individual. In 1890, James wrote an influential book, The Principles of Psychology, which expanded on the structuralism. He memorably described \"stream of consciousness.\" James's ideas interested many American students in the emerging discipline. Dewey integrated psychology with societal concerns, most notably by promoting progressive education, inculcating moral values in children, and assimilating immigrants. A different strain of experimentalism, with a greater connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires. In Russia, too, researchers placed greater emphasis on the biological basis",
    "label": 0
  },
  {
    "text": "immigrants. A different strain of experimentalism, with a greater connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires. In Russia, too, researchers placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, \"Who Is to Develop Psychology and How?\" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic view of human behavior. The Russian-Soviet physiologist Ivan Pavlov discovered in dogs a learning process that was later termed \"classical conditioning\" and applied the process to human beings. Consolidation and funding One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted from 1885 to 1893. The first meeting of the International Congress of Psychology sponsored by the International Union of Psychological Science took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the 400 attendees. The American Psychological Association (APA) was founded soon after, in 1892. The International Congress continued to be held at different locations in Europe and with wide international participation. The Sixth Congress, held in Geneva in 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the APA. Tokyo Imperial University led the way in bringing new psychology to the East. New ideas about psychology diffused from Japan into China. American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests (\"Army Alpha\" and",
    "label": 0
  },
  {
    "text": "bringing new psychology to the East. New ideas about psychology diffused from Japan into China. American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests (\"Army Alpha\" and \"Army Beta\") to almost 1.8 million soldiers. Subsequently, the Rockefeller family, via the Social Science Research Council, began to provide funding for behavioral research. Rockefeller charities funded the National Committee on Mental Hygiene, which disseminated the concept of mental illness and lobbied for applying ideas from psychology to child rearing. Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations helped establish research on sexuality in the U.S. Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also influenced American psychology. In the 1910s and 1920s, eugenics became a standard topic in psychology classes. In contrast to the US, in the UK psychology was met with antagonism by the scientific and medical establishments, and up until 1939, there were only six psychology chairs in universities in England. During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology by way of the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941. He observed that \"the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government.\" Cartwright also wrote that psychologists had significant roles in managing the domestic economy. The Army rolled out its new General Classification Test to assess the ability of millions of soldiers. The Army also engaged in large-scale psychological research of",
    "label": 0
  },
  {
    "text": "Government.\" Cartwright also wrote that psychologists had significant roles in managing the domestic economy. The Army rolled out its new General Classification Test to assess the ability of millions of soldiers. The Army also engaged in large-scale psychological research of troop morale and mental health. In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency (CIA) to fund research on psychological warfare. In 1965, public controversy called attention to the Army's Project Camelot, the \"Manhattan Project\" of social science, an effort which enlisted psychologists and anthropologists to analyze the plans and policies of foreign countries for strategic purposes. In Germany after World War I, psychology held institutional power through the military, which was subsequently expanded along with the rest of the military during Nazi Germany. Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler, founders of psychoanalysis who were also Jewish. The Göring Institute was well-financed throughout the war with a mandate to create a \"New German Psychotherapy.\" This psychotherapy aimed to align suitable Germans with the overall goals of the Reich. As described by one physician, \"Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the Volk and the Gemeinschaft.\" Psychologists were to provide Seelenführung [lit., soul guidance], the leadership of the mind, to integrate people into the new vision of a German community. Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed. Johannes",
    "label": 0
  },
  {
    "text": "the leadership of the mind, to integrate people into the new vision of a German community. Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed. Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process. After the war, new institutions were created although some psychologists, because of their Nazi affiliation, were discredited. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche. With funding from the Rockefeller Foundation, Mitscherlich established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students. After the Russian Revolution, the Bolsheviks promoted psychology as a way to engineer the \"New Man\" of socialism. Consequently, university psychology departments trained large numbers of students in psychology. At the completion of training, positions were made available for those students at schools, workplaces, cultural institutions, and in the military. The Russian state emphasized pedology and the study of child development. Lev Vygotsky became prominent in the field of child development. The Bolsheviks also promoted free love and embraced the doctrine of psychoanalysis as an antidote to sexual repression. Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet Union. Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society. Following World War II, Jewish psychologists past and present, including Lev Vygotsky, A.R. Luria, and Aron Zalkind, were denounced; Ivan Pavlov (posthumously) and Stalin himself were celebrated as heroes of Soviet psychology. Soviet academics experienced a degree of liberalization during the Khrushchev",
    "label": 0
  },
  {
    "text": "War II, Jewish psychologists past and present, including Lev Vygotsky, A.R. Luria, and Aron Zalkind, were denounced; Ivan Pavlov (posthumously) and Stalin himself were celebrated as heroes of Soviet psychology. Soviet academics experienced a degree of liberalization during the Khrushchev Thaw. The topics of cybernetics, linguistics, and genetics became acceptable again. The new field of engineering psychology emerged. The field involved the study of the mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior. Twentieth-century Chinese psychology originally modeled itself on U.S. psychology, with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning. Chinese psychologists were drawn to the idea that education would enable modernization. John Dewey, who lectured to Chinese audiences between 1919 and 1921, had a significant influence on psychology in China. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism. After the Chinese Communist Party gained control of the country, the Stalinist Soviet Union became the major influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved means of behavior change. Chinese psychologists elaborated on Lenin's model of a \"reflective\" consciousness, envisioning an \"active consciousness\" (pinyin: tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of \"recognition\" (pinyin: jen-shih) which referred to the interface between individual perceptions and the socially accepted worldview; failure to correspond with party doctrine",
    "label": 0
  },
  {
    "text": "tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of \"recognition\" (pinyin: jen-shih) which referred to the interface between individual perceptions and the socially accepted worldview; failure to correspond with party doctrine was \"incorrect recognition.\" Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951, the academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Because most leading psychologists were educated in the United States, the first concern of the academy was the re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for the purpose of a nationally cohesive education remained a central goal of the discipline. Women in psychology 1900–1949 Women in the early 1900s started to make key findings within the world of psychology. In 1923, Anna Freud, the daughter of Sigmund Freud, built on her father's work using different defense mechanisms (denial, repression, and suppression) to psychoanalyze children. She believed that once a child reached the latency period, child analysis could be used as a mode of therapy. She stated it is important focus on the child's environment, support their development, and prevent neurosis. She believed a child should be recognized as their own person with their own right and have each session catered to the child's specific needs. She encouraged drawing, moving freely, and expressing themselves in any way. This helped build a strong therapeutic alliance with child patients, which allows psychologists to observe their normal behavior. She continued her research on the impact of children after family separation, children with socio-economically disadvantaged backgrounds, and all stages of child development from infancy to adolescence. Functional periodicity, the belief women are mentally and physically impaired during menstruation, impacted women's rights because employers were less",
    "label": 0
  },
  {
    "text": "impact of children after family separation, children with socio-economically disadvantaged backgrounds, and all stages of child development from infancy to adolescence. Functional periodicity, the belief women are mentally and physically impaired during menstruation, impacted women's rights because employers were less likely to hire them due to the belief they would be incapable of working for 1 week a month. Leta Stetter Hollingworth wanted to prove this hypothesis and Edward L. Thorndike's theory, that women have lesser psychological and physical traits than men and were simply mediocre, incorrect. Hollingworth worked to prove differences were not from male genetic superiority, but from culture. She also included the concept of women's impairment during menstruation in her research. She recorded both women and men performances on tasks (cognitive, perceptual, and motor) for three months. No evidence was found of decreased performance due to a woman's menstrual cycle. She also challenged the belief intelligence is inherited and women here are intellectually inferior to men. She stated that women do not reach positions of power due to the societal norms and roles they are assigned. As she states in her article, \"Variability as related to sex differences in achievement: A Critique\", the largest problem women have is the social order that was built due to the assumption women have less interests and abilities than men. To further prove her point, she completed another experiment with infants who have not been influenced by the environment of social norms, like the adult male getting more opportunities than women. She found no difference between infants besides size. After this research proved the original hypothesis wrong, Hollingworth was able to show there is no difference between the physiological and psychological traits of men and women, and women are not impaired during menstruation. The first half of the 1900s was filled",
    "label": 0
  },
  {
    "text": "research proved the original hypothesis wrong, Hollingworth was able to show there is no difference between the physiological and psychological traits of men and women, and women are not impaired during menstruation. The first half of the 1900s was filled with new theories and it was a turning point for women's recognition within the field of psychology. In addition to the contributions made by Leta Stetter Hollingworth and Anna Freud, Mary Whiton Calkins invented the paired associates technique of studying memory and developed self-psychology. Karen Horney developed the concept of \"womb envy\" and neurotic needs. Psychoanalyst Melanie Klein impacted developmental psychology with her research of play therapy. These great discoveries and contributions were made during struggles of sexism, discrimination, and little recognition for their work. 1950–1999 Women in the second half of the 20th century continued to do research that had large-scale impacts on the field of psychology. Mary Ainsworth's work centered around attachment theory. Building off fellow psychologist John Bowlby, Ainsworth spent years doing fieldwork to understand the development of mother-infant relationships. In doing this field research, Ainsworth developed the Strange Situation Procedure, a laboratory procedure meant to study attachment style by separating and uniting a child with their mother several different times under different circumstances. These field studies are also where she developed her attachment theory and the order of attachment styles, which was a landmark for developmental psychology. Because of her work, Ainsworth became one of the most cited psychologists of all time. Mamie Phipps Clark was another woman in psychology that changed the field with her research. She was one of the first African-Americans to receive a doctoral degree in psychology from Columbia University, along with her husband, Kenneth Clark. Her master's thesis, \"The Development of Consciousness in Negro Pre-School Children,\" argued that black children's self-esteem",
    "label": 0
  },
  {
    "text": "research. She was one of the first African-Americans to receive a doctoral degree in psychology from Columbia University, along with her husband, Kenneth Clark. Her master's thesis, \"The Development of Consciousness in Negro Pre-School Children,\" argued that black children's self-esteem was negatively impacted by racial discrimination. She and her husband conduced research building off her thesis throughout the 1940s. These tests, called the doll tests, asked young children to choose between identical dolls whose only difference was race, and they found that the majority of the children preferred the white dolls and attributed positive traits to them. Repeated over and over again, these tests helped to determine the negative effects of racial discrimination and segregation on black children's self-image and development. In 1954, this research would help decide the landmark Brown v. Board of Education decision, leading to the end of legal segregation across the nation. Clark went on to be an influential figure in psychology, her work continuing to focus on minority youth. As the field of psychology developed throughout the latter half of the 20th century, women in the field advocated for their voices to be heard and their perspectives to be valued. Second-wave feminism did not miss psychology. An outspoken feminist in psychology was Naomi Weisstein, who was an accomplished researcher in psychology and neuroscience, and is perhaps best known for her paper, \"Kirche, Kuche, Kinder as Scientific Law: Psychology Constructs the Female.\" Psychology Constructs the Female criticized the field of psychology for centering men and using biology too much to explain gender differences without taking into account social factors. Her work set the stage for further research to be done in social psychology, especially in gender construction. Other women in the field also continued advocating for women in psychology, creating the Association for Women in Psychology",
    "label": 0
  },
  {
    "text": "account social factors. Her work set the stage for further research to be done in social psychology, especially in gender construction. Other women in the field also continued advocating for women in psychology, creating the Association for Women in Psychology to criticize how the field treated women. E. Kitsch Child, Phyllis Chesler, and Dorothy Riddle were some of the founding members of the organization in 1969. The latter half of the 20th century further diversified the field of psychology, with women of color reaching new milestones. In 1962, Martha Bernal became the first Latina woman to get a Ph.D. in psychology. In 1969, Marigold Linton, the first Native American woman to get a Ph.D. in psychology, founded the National Indian Education Association. She was also a founding member of the Society for Advancement of Chicanos and Native Americans in Science. In 1971, The Network of Indian Psychologists was established by Carolyn Attneave. Harriet McAdoo was appointed to the White House Conference on Families in 1979. 21st century In the 21st century, women have gained greater prominence in psychology, contributing significantly to a wide range of subfields. Many have taken on leadership roles, directed influential research labs, and guided the next generation of psychologists. However, gender disparities remain, especially when it comes to equal pay and representation in senior academic positions. The number of women pursuing education and training in psychological science has reached a record high. In the United States, estimates suggest that women make up about 78% of undergraduate students and 71% of graduate students in psychology. Disciplinary organizations Institutions In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology. The",
    "label": 0
  },
  {
    "text": "Institutions In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology. The IAAP is considered the oldest international psychology association. Today, at least 65 international groups deal with specialized aspects of psychology. In response to male predominance in the field, female psychologists in the U.S. formed the National Council of Women Psychologists in 1941. This organization became the International Council of Women Psychologists after World War II and the International Council of Psychologists in 1959. Several associations including the Association of Black Psychologists and the Asian American Psychological Association have arisen to promote the inclusion of non-European racial groups in the profession. The International Union of Psychological Science (IUPsyS) is the world federation of national psychological societies. The IUPsyS was founded in 1951 under the auspices of the United Nations Educational, Cultural and Scientific Organization (UNESCO). Psychology departments have since proliferated around the world, based primarily on the Euro-American model. Since 1966, the Union has published the International Journal of Psychology. IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis. IUPsyS recognizes 66 national psychology associations and at least 15 others exist. The American Psychological Association is the oldest and largest. Its membership has increased from 5,000 in 1945 to 100,000 in the present day. The APA includes 54 divisions, which since 1960 have steadily proliferated to include more specialties. Some of these divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups. The Interamerican Psychological Society, founded in 1951, aspires to promote psychology across the Western Hemisphere. It holds the Interamerican",
    "label": 0
  },
  {
    "text": "divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups. The Interamerican Psychological Society, founded in 1951, aspires to promote psychology across the Western Hemisphere. It holds the Interamerican Congress of Psychology and had 1,000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international organizations represent psychologists in different regions. In some places, governments legally regulate who can provide psychological services or represent themselves as a \"psychologist.\" The APA defines a psychologist as someone with a doctoral degree in psychology. Boundaries Early practitioners of experimental psychology distinguished themselves from parapsychology, which in the late nineteenth century enjoyed popularity (including the interest of scholars such as William James). Some people considered parapsychology to be part of \"psychology\". Parapsychology, hypnotism, and psychism were major topics at the early International Congresses. But students of these fields were eventually ostracized, and more or less banished from the Congress in 1900–1905. Parapsychology persisted for a time at Imperial University in Japan, with publications such as Clairvoyance and Thoughtography by Tomokichi Fukurai, but it was mostly shunned by 1913. As a discipline, psychology has long sought to fend off accusations that it is a \"soft\" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking agreement on the type of overarching theory found in mature hard sciences such as chemistry and physics. Because some areas of psychology rely on research methods such as self-reports in surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion cannot be directly measured and are often inferred from subjective self-reports, which",
    "label": 0
  },
  {
    "text": "rely on research methods such as self-reports in surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities. Divisions still exist within the field, with some psychologists more oriented towards the unique experiences of individual humans, which cannot be understood only as data points within a larger population. Critics inside and outside the field have argued that mainstream psychology has become increasingly dominated by a \"cult of empiricism\", which limits the scope of research because investigators restrict themselves to methods derived from the physical sciences. Feminist critiques have argued that claims to scientific objectivity obscure the values and agenda of (historically) mostly male researchers. Jean Grimshaw, for example, argues that mainstream psychological research has advanced a patriarchal agenda through its efforts to control behavior. Major schools of thought Biological Psychologists generally consider biology the substrate of thought and feeling, and therefore an important area of study. Behaviorial neuroscience, also known as biological psychology, involves the application of biological principles to the study of physiological and genetic mechanisms underlying behavior in humans and other animals. The allied field of comparative psychology is the scientific study of the behavior and mental processes of non-human animals. A leading question in behavioral neuroscience has been whether and how mental functions are localized in the brain. From Phineas Gage to H.M. and Clive Wearing, individual people with mental deficits traceable to physical brain damage have inspired new discoveries in this area. Modern behavioral neuroscience could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal",
    "label": 0
  },
  {
    "text": "people with mental deficits traceable to physical brain damage have inspired new discoveries in this area. Modern behavioral neuroscience could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal gyrus, thereby also demonstrating hemispheric lateralization of brain function. Soon after, Carl Wernicke identified a related area necessary for the understanding of speech. The contemporary field of behavioral neuroscience focuses on the physical basis of behavior. Behaviorial neuroscientists use animal models, often relying on rats, to study the neural, genetic, and cellular mechanisms that underlie behaviors involved in learning, memory, and fear responses. Cognitive neuroscientists, by using neural imaging tools, investigate the neural correlates of psychological processes in humans. Neuropsychologists conduct psychological assessments to determine how an individual's behavior and cognition are related to the brain. The biopsychosocial model is a cross-disciplinary, holistic model that concerns the ways in which interrelationships of biological, psychological, and socio-environmental factors affect health and behavior. Evolutionary psychology approaches thought and behavior from a modern evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychologists attempt to find out how human psychological traits are evolved adaptations, the results of natural selection or sexual selection over the course of human evolution. The history of the biological foundations of psychology includes evidence of racism. The idea of white supremacy and indeed the modern concept of race itself arose during the process of world conquest by Europeans. Carl von Linnaeus's four-fold classification of humans classifies Europeans as intelligent and severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as drapetomania and dysaesthesia aethiopica—the behavior of uncooperative African slaves. After",
    "label": 0
  },
  {
    "text": "severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as drapetomania and dysaesthesia aethiopica—the behavior of uncooperative African slaves. After the creation of experimental psychology, \"ethnical psychology\" emerged as a subdiscipline, based on the assumption that studying primitive races would provide an important link between animal behavior and the psychology of more evolved humans. Behaviorist A tenet of behavioral research is that a large part of both human and lower-animal behavior is learned. A principle associated with behavioral research is that the mechanisms involved in learning apply to humans and non-human animals. Behavioral researchers have developed a treatment known as behavior modification, which is used to help individuals replace undesirable behaviors with desirable ones. Early behavioral researchers studied stimulus–response pairings, now known as classical conditioning. They demonstrated that when a biologically potent stimulus (e.g., food that elicits salivation) is paired with a previously neutral stimulus (e.g., a bell) over several learning trials, the neutral stimulus by itself can come to elicit the response the biologically potent stimulus elicits. Ivan Pavlov—known best for inducing dogs to salivate in the presence of a stimulus previously linked with food—became a leading figure in the Soviet Union and inspired followers to use his methods on humans. In the United States, Edward Lee Thorndike initiated \"connectionist\" studies by trapping animals in \"puzzle boxes\" and rewarding them for escaping. Thorndike wrote in 1911, \"There can be no moral warrant for studying man's nature unless the study will enable us to control his acts.\" From 1910 to 1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards \"behavioralism.\" In 1913, John B. Watson coined the term behaviorism for this school",
    "label": 0
  },
  {
    "text": "will enable us to control his acts.\" From 1910 to 1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards \"behavioralism.\" In 1913, John B. Watson coined the term behaviorism for this school of thought. Watson's famous Little Albert experiment in 1920 was at first thought to demonstrate that repeated use of upsetting loud noises could instill phobias (aversions to other stimuli) in an infant human, although such a conclusion was likely an exaggeration. Karl Lashley, a close collaborator with Watson, examined biological manifestations of learning in the brain. Clark L. Hull, Edwin Guthrie, and others did much to help behaviorism become a widely used paradigm. A new method of \"instrumental\" or \"operant\" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically. Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B.F. Skinner, who emerged as a leading intellectual of the behaviorist movement. Noam Chomsky published an influential critique of radical behaviorism on the grounds that behaviorist principles could not adequately explain the complex mental process of language acquisition and language use. The review, which was scathing, did much to reduce the status of behaviorism within psychology. Martin Seligman and his colleagues discovered that they could condition in dogs a state of \"learned helplessness\", which was not predicted by the behaviorist approach to psychology. Edward C. Tolman advanced a hybrid \"cognitive behavioral\" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a maze. Skinner's behaviorism did not die, in part because it generated successful practical",
    "label": 0
  },
  {
    "text": "behavioral\" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a maze. Skinner's behaviorism did not die, in part because it generated successful practical applications. The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has gained a foothold in Latin America and Japan. Applied behavior analysis is the term used for the application of the principles of operant conditioning to change socially significant behavior (it supersedes the term, \"behavior modification\"). Cognitive Cognitive psychology involves the study of mental processes, including perception, attention, language comprehension and production, memory, and problem solving. Researchers in the field of cognitive psychology are sometimes called cognitivists. They rely on an information processing model of mental functioning. Cognitivist research is informed by functionalism and experimental psychology. Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist and, eventually, constituted a part of the wider, interdisciplinary cognitive science. Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis. Albert Bandura helped along the transition in psychology from behaviorism to cognitive psychology. Bandura and other social learning theorists advanced the idea of vicarious learning. In other words, they advanced the view that a child can learn by observing the immediate social environment and not necessarily from having been reinforced for enacting a behavior, although they did not rule out the influence of reinforcement on learning a behavior. Technological advances also renewed interest in mental states and mental representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena to the structure",
    "label": 0
  },
  {
    "text": "out the influence of reinforcement on learning a behavior. Technological advances also renewed interest in mental states and mental representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena to the structure and function of the brain. The rise of computer science, cybernetics, and artificial intelligence underlined the value of comparing information processing in humans and machines. A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalog of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind. Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck. On a broader level, cognitive science is an interdisciplinary enterprise involving cognitive psychologists, cognitive neuroscientists, linguists, and researchers in artificial intelligence, human–computer interaction, and computational neuroscience. The discipline of cognitive science covers cognitive psychology as well as philosophy of mind, computer science, and neuroscience. Computer simulations are sometimes used to model phenomena of interest. Social Social psychology is concerned with how behaviors, thoughts, feelings, and the social environment influence human interactions. Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion) and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology for the purpose of understanding how people process, remember, or distort social information. The study of group dynamics involves research on the nature of leadership, organizational communication, and related phenomena. In recent years, social psychologists have become interested in implicit measures, mediational",
    "label": 0
  },
  {
    "text": "purpose of understanding how people process, remember, or distort social information. The study of group dynamics involves research on the nature of leadership, organizational communication, and related phenomena. In recent years, social psychologists have become interested in implicit measures, mediational models, and the interaction of person and social factors in accounting for behavior. Some concepts that sociologists have applied to the study of psychiatric disorders, concepts such as the social role, sick role, social class, life events, culture, migration, and total institution, have influenced social psychologists. Psychoanalytic Psychoanalysis is a collection of theories and therapeutic techniques intended to analyze the unconscious mind and its impact on everyday life. These theories and techniques inform treatments for mental disorders. Psychoanalysis originated in the 1890s, most prominently with the work of Sigmund Freud. Freud's psychoanalytic theory was largely based on interpretive methods, introspection, and clinical observation. It became very well known, largely because it tackled subjects such as sexuality, repression, and the unconscious. Freud pioneered the methods of free association and dream interpretation. Psychoanalytic theory is not monolithic. Other well-known psychoanalytic thinkers who diverged from Freud include Alfred Adler, Carl Jung, Erik Erikson, Melanie Klein, D. W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, Freud's daughter Anna Freud, and Harry Stack Sullivan. These individuals ensured that psychoanalysis would evolve into diverse schools of thought. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis. Psychologists such as Hans Eysenck and philosophers including Karl Popper sharply criticized psychoanalysis. Popper argued that psychoanalysis was not falsifiable (no claim it made could be proven wrong) and therefore inherently not a scientific discipline, whereas Eysenck advanced the view that psychoanalytic tenets had been contradicted by experimental data. By the end of the 20th century, psychology departments in American universities mostly had marginalized Freudian",
    "label": 0
  },
  {
    "text": "be proven wrong) and therefore inherently not a scientific discipline, whereas Eysenck advanced the view that psychoanalytic tenets had been contradicted by experimental data. By the end of the 20th century, psychology departments in American universities mostly had marginalized Freudian theory, dismissing it as a \"desiccated and dead\" historical artifact. Researchers such as António Damásio, Oliver Sacks, and Joseph LeDoux; and individuals in the emerging field of neuro-psychoanalysis have defended some of Freud's ideas on scientific grounds. Existential-humanistic Humanistic psychology, which has been influenced by existentialism and phenomenology, stresses free will and self-actualization. It emerged in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis. The humanistic approach seeks to view the whole person, not just fragmented parts of the personality or isolated cognitions. Humanistic psychology also focuses on personal growth, self-identity, death, aloneness, and freedom. It emphasizes subjective meaning, the rejection of determinism, and concern for positive growth rather than pathology. Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy. Later, positive psychology opened up humanistic themes to scientific study. Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently healthy. In 2010, Clinical Psychological Review published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. It is, however, far from clear that positive psychology is effective in making people happier. Positive psychological interventions have been limited in scope, but their effects are thought to be somewhat better than placebo effects. The American Association for Humanistic Psychology, formed in 1963, declared: Humanistic psychology is primarily an orientation toward the whole of psychology rather",
    "label": 0
  },
  {
    "text": "interventions have been limited in scope, but their effects are thought to be somewhat better than placebo effects. The American Association for Humanistic Psychology, formed in 1963, declared: Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a \"third force\" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts. Existential psychology emphasizes the need to understand a client's total orientation towards the world. Existential psychology is opposed to reductionism, behaviorism, and other methods that objectify the individual. In the 1950s and 1960s, influenced by philosophers Søren Kierkegaard and Martin Heidegger, psychoanalytically trained American psychologist Rollo May helped to develop existential psychology. Existential psychotherapy, which follows from existential psychology, is a therapeutic approach that is based on the idea that a person's inner conflict arises from that individual's confrontation with the givens of existence. Swiss psychoanalyst Ludwig Binswanger and American psychologist George Kelly may also be said to belong to the existential school. Existential psychologists tend to differ from more \"humanistic\" psychologists in the former's relatively neutral view of human nature and relatively positive assessment of anxiety. Existential psychologists emphasized the humanistic themes of death, free will, and meaning, suggesting that meaning can be shaped by myths and narratives; meaning can be deepened by the acceptance of free will, which is requisite to living an authentic life, albeit often with anxiety with",
    "label": 0
  },
  {
    "text": "themes of death, free will, and meaning, suggesting that meaning can be shaped by myths and narratives; meaning can be deepened by the acceptance of free will, which is requisite to living an authentic life, albeit often with anxiety with regard to death. Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections upon his own internment. He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a will to meaning (in one's life), as opposed to Adler's Nietzschean doctrine of will to power or Freud's will to pleasure. Themes Personality Personality psychology is concerned with enduring patterns of behavior, thought, and emotion. Theories of personality vary across different psychological schools of thought. Each theory carries different assumptions about such features as the role of the unconscious and the importance of childhood experience. According to Freud, personality is based on the dynamic interactions of the id, ego, and super-ego. By contrast, trait theorists have developed taxonomies of personality constructs in describing personality in terms of key traits. Trait theorists have often employed statistical data-reduction methods, such as factor analysis. Although the number of proposed traits has varied widely, Hans Eysenck's early biologically based model suggests at least three major trait constructs are necessary to describe human personality, extraversion–introversion, neuroticism-stability, and psychoticism-normality. Raymond Cattell empirically derived a theory of 16 personality factors at the primary-factor level and up to eight broader second-stratum factors. Since the 1980s, the Big Five (openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism) emerged as an important trait theory of personality. Dimensional models of personality disorders are receiving increasing support, and a version of dimensional assessment, namely the Alternative DSM-5 Model for Personality Disorders, has been included in the DSM-5. However, despite a plethora",
    "label": 0
  },
  {
    "text": "as an important trait theory of personality. Dimensional models of personality disorders are receiving increasing support, and a version of dimensional assessment, namely the Alternative DSM-5 Model for Personality Disorders, has been included in the DSM-5. However, despite a plethora of research into the various versions of the \"Big Five\" personality dimensions, it appears necessary to move on from static conceptualizations of personality structure to a more dynamic orientation, acknowledging that personality constructs are subject to learning and change over the lifespan. An early example of personality assessment was the Woodworth Personal Data Sheet, constructed during World War I. The popular, although psychometrically inadequate, Myers–Briggs Type Indicator was developed to assess individuals' \"personality types\" according to the personality theories of Carl Jung. The Minnesota Multiphasic Personality Inventory (MMPI), despite its name, is more a dimensional measure of psychopathology than a personality measure. California Psychological Inventory contains 20 personality scales (e.g., independence, tolerance). The International Personality Item Pool, which is in the public domain, has become a source of scales that can be used personality assessment. Unconscious mind Study of the unconscious mind, a part of the psyche outside the individual's awareness but that is believed to influence conscious thought and behavior, was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C.S. Peirce and Joseph Jastrow found in 1884 that research subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference. Freud popularized the concept of the unconscious mind, particularly when he referred to an uncensored intrusion of unconscious thought into one's speech (a Freudian slip) or to his efforts to interpret dreams. His 1901 book The Psychopathology of Everyday Life catalogs hundreds of everyday events that Freud explains in terms of unconscious influence. Pierre Janet",
    "label": 0
  },
  {
    "text": "intrusion of unconscious thought into one's speech (a Freudian slip) or to his efforts to interpret dreams. His 1901 book The Psychopathology of Everyday Life catalogs hundreds of everyday events that Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the direct scrutiny of the subject. The concept of unconscious processes has remained important in psychology. Cognitive psychologists have used a \"filter\" model of attention. According to the model, much information processing takes place below the threshold of consciousness, and only certain stimuli, limited by their nature and number, make their way through the filter. Much research has shown that subconscious priming of certain ideas can covertly influence thoughts and behavior. Because of the unreliability of self-reporting, a major hurdle in this type of research involves demonstrating that a subject's conscious mind has not perceived a target stimulus. For this reason, some psychologists prefer to distinguish between implicit and explicit memory. In another approach, one can also describe a subliminal stimulus as meeting an objective but not a subjective threshold. The automaticity model of John Bargh and others involves the ideas of automaticity and unconscious processing in our understanding of social behavior, although there has been dispute with regard to replication. Some experimental data suggest that the brain begins to consider taking actions before the mind becomes aware of them. The influence of unconscious forces on people's choices bears on the philosophical question of free will. John Bargh, Daniel Wegner, and Ellen Langer describe free will as an illusion. Motivation Some psychologists study motivation or the subject of why people or lower animals initiate a behavior at a particular time. It also involves the study of why humans and lower animals continue or terminate a",
    "label": 0
  },
  {
    "text": "will as an illusion. Motivation Some psychologists study motivation or the subject of why people or lower animals initiate a behavior at a particular time. It also involves the study of why humans and lower animals continue or terminate a behavior. Psychologists such as William James initially used the term motivation to refer to intention, in a sense similar to the concept of will in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation. According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as demands originating in the nervous system. Psychoanalysts believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in Beyond the Pleasure Principle, Freud introduced the concept of the death drive, a compulsion towards aggression, destruction, and psychic repetition of traumatic events. Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking. Clark Hull formalized the latter idea with his drive reduction model. Hunger, thirst, fear, sexual desire, and thermoregulation constitute fundamental motivations in animals. Humans seem to exhibit a more complex set of motivations—though theoretically these could be explained as resulting from desires for belonging, positive self-image, self-consistency, truth, love, and control. Motivation can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food",
    "label": 0
  },
  {
    "text": "can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food palatability, and cost. Abstract motivations are also malleable, as evidenced by such phenomena as goal contagion: the adoption of goals, sometimes unconsciously, based on inferences about the goals of others. Vohs and Baumeister suggest that contrary to the need-desire-fulfillment cycle of animal instincts, human motivations sometimes obey a \"getting begets wanting\" rule: the more you get a reward such as self-esteem, love, drugs, or money, the more you want it. They suggest that this principle can even apply to food, drink, sex, and sleep. Development psychology Developmental psychology is the scientific study of how and why the thought processes, emotions, and behaviors of humans change over the course of their lives. Some credit Charles Darwin with conducting the first systematic study within the rubric of developmental psychology, having published in 1877 a short paper detailing the development of innate forms of communication based on his observations of his infant son. The main origins of the discipline, however, are found in the work of Jean Piaget. Like Piaget, developmental psychologists originally focused primarily on the development of cognition from infancy to adolescence. Later, developmental psychology extended itself to the study cognition over the life span. In addition to studying cognition, developmental psychologists have also come to focus on affective, behavioral, moral, social, and neural development. Developmental psychologists who study children use a number of research methods. For example, they make observations of children in natural settings such as preschools and engage them in experimental tasks. Such tasks often resemble specially designed games and activities that are both enjoyable for the child",
    "label": 0
  },
  {
    "text": "a number of research methods. For example, they make observations of children in natural settings such as preschools and engage them in experimental tasks. Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful. Developmental researchers have even devised clever methods to study the mental processes of infants. In addition to studying children, developmental psychologists also study aging and processes throughout the life span, including old age. These psychologists draw on the full range of psychological theories to inform their research. Genes and environment All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals and families. An example of this confounding can be shown in the transmission of depression from a depressed mother to her offspring. A theory based on environmental transmission would hold that an offspring, by virtue of their having a problematic rearing environment managed by a depressed mother, is at risk for developing depression. On the other hand, a hereditarian theory would hold that depression risk in an offspring is influenced to some extent by genes passed to the child from the mother. Genes and environment in these simple transmission models are completely confounded. A depressed mother may both carry genes that contribute to depression in her offspring and also create a rearing environment that increases the risk of depression in her child. Behavioral genetics researchers have employed methodologies that help to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally the research has involved twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, gene-focused research has contributed to understanding genetic contributions to the development of",
    "label": 0
  },
  {
    "text": "of individual differences in behavior. Traditionally the research has involved twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, gene-focused research has contributed to understanding genetic contributions to the development of psychological traits. The availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to work toward understanding the genetic and environmental bases of behavior and their interaction. Applications Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior. Psychological testing Psychological testing has ancient origins, dating as far back as 2200 BCE, in the examinations for the Chinese civil service. Written exams began during the Han dynasty (202 BCE – 220 CE). By 1370, the Chinese system required a stratified series of tests, involving essay writing and knowledge of diverse topics. The system was ended in 1906. In Europe, mental assessment took a different approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BCE Greece. Physiognomy remained current through the Enlightenment, and added the",
    "label": 0
  },
  {
    "text": "The system was ended in 1906. In Europe, mental assessment took a different approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BCE Greece. Physiognomy remained current through the Enlightenment, and added the doctrine of phrenology: a study of mind and intelligence based on simple assessment of neuroanatomy. When experimental psychology came to Britain, Francis Galton was a leading practitioner. By virtue of his procedures for measuring reaction time and sensation, he is considered an inventor of modern mental testing (also known as psychometrics). James McKeen Cattell, a student of Wundt and Galton, brought the idea of psychological testing to the United States, and in fact coined the term \"mental test\". In 1901, Cattell's student Clark Wissler published discouraging results, suggesting that mental testing of Columbia and Barnard students failed to predict academic performance. In response to 1904 orders from the Minister of Public Instruction, One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them. psychologists Alfred Binet and Théodore Simon developed and elaborated a new test of intelligence in 1905–1911. They used a range of questions diverse in their nature and difficulty. Binet and Simon introduced the concept of mental age and referred to the lowest scorers on their test as idiots. Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as imbecile and feebleminded. In 1916, (after Binet's death), Stanford professor Lewis",
    "label": 0
  },
  {
    "text": "age and referred to the lowest scorers on their test as idiots. Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as imbecile and feebleminded. In 1916, (after Binet's death), Stanford professor Lewis M. Terman modified the Binet-Simon scale (renamed the Stanford–Binet scale) and introduced the intelligence quotient as a score report. Based on his test findings, and reflecting the racism common to that era, Terman concluded that intellectual disability \"represents the level of intelligence which is very, very common among Spanish-Indians and Mexican families of the Southwest and also among negroes. Their dullness seems to be racial.\" Following the Army Alpha and Army Beta tests, which was developed by psychologist Robert Yerkes in 1917 and then used in World War 1 by industrial and organizational psychologists for large-scale employee testing and selection of military personnel. Mental testing also became popular in the U.S., where it was applied to schoolchildren. The federally created National Intelligence Test was administered to 7 million children in the 1920s. In 1926, the College Entrance Examination Board created the Scholastic Aptitude Test to standardize college admissions. The results of intelligence tests were used to argue for segregated schools and economic functions, including the preferential training of Black Americans for manual labor. These practices were criticized by Black intellectuals such a Horace Mann Bond and Allison Davis. Eugenicists used mental testing to justify and organize compulsory sterilization of individuals classified as mentally retarded (now referred to as intellectual disability). In the United States, tens of thousands of men and women were sterilized. Setting a precedent that has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1927 case Buck v. Bell. Today mental testing is a routine phenomenon for people of all ages",
    "label": 0
  },
  {
    "text": "women were sterilized. Setting a precedent that has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1927 case Buck v. Bell. Today mental testing is a routine phenomenon for people of all ages in Western societies. Modern testing aspires to criteria including standardization of procedure, consistency of results, output of an interpretable score, statistical norms describing population outcomes, and, ideally, effective prediction of behavior and life outcomes outside of testing situations. Psychological testing is regularly used in forensic contexts to aid legal judgments and decisions. Developments in psychometrics include work on test and scale reliability and validity. Developments in item-response theory, structural equation modeling, and bifactor analysis have helped in strengthening test and scale construction. Mental health care The provision of psychological health services is generally called clinical psychology in the U.S. Sometimes, however, members of the school psychology and counseling psychology professions engage in practices that resemble that of clinical psychologists. Clinical psychologists typically include people who have graduated from doctoral programs in clinical psychology. In Canada, some of the members of the abovementioned groups usually fall within the larger category of professional psychology. In Canada and the U.S., practitioners get bachelor's degrees and doctorates; doctoral students in clinical psychology usually spend one year in a predoctoral internship and one year in postdoctoral internship. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctoral degrees; instead, they take a three-year professional course following high school. Clinical psychology is at present the largest specialization within psychology. It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychological distress, dysfunction, and/or mental illness. Clinical psychologists also try to promote subjective well-being and personal growth. Central to the practice of clinical psychology",
    "label": 0
  },
  {
    "text": "It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychological distress, dysfunction, and/or mental illness. Clinical psychologists also try to promote subjective well-being and personal growth. Central to the practice of clinical psychology are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration. Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince, an early advocate for the establishment of psychology as a clinical and academic discipline. In the first part of the twentieth century, most mental health care in the United States was performed by psychiatrists, who are medical doctors. Psychology entered the field with its refinements of mental testing, which promised to improve the diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill. Psychotherapy as conducted by psychiatrists blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities. Some in the clinical psychology community adopted behavioral therapy, a thoroughly non-psychodynamic model that used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy emerged with the work of Albert Ellis and Aaron Beck. Although there are similarities between behavior therapy and cognitive-behavior therapy, cognitive-behavior therapy required the application of cognitive constructs. Since the 1970s, the popularity of cognitive-behavior therapy among clinical psychologists increased. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear,",
    "label": 0
  },
  {
    "text": "the application of cognitive constructs. Since the 1970s, the popularity of cognitive-behavior therapy among clinical psychologists increased. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned. Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of \"intense competition and role confusion.\" Graduate programs issuing doctorates in clinical psychology emerged in the 1950s and underwent rapid increase through the 1980s. The PhD degree is intended to train practitioners who could also conduct scientific research. The PsyD degree is more exclusively designed to train practitioners. Some clinical psychologists focus on the clinical management of patients with brain injury. This subspecialty is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of disaster psychology (see crisis intervention) involves professionals who respond to large-scale traumatic events. The work performed by clinical psychologists tends to be influenced by various therapeutic approaches, all of which involve a formal relationship between professional and client (usually an individual, couple, family, or small group). Typically, these approaches encourage new ways of thinking, feeling, or behaving. Four major theoretical perspectives are psychodynamic, cognitive behavioral, existential–humanistic, and systems or family therapy. There has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of issues regarding culture, gender, spirituality, and sexual orientation. With the advent of more robust research findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance. Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation.",
    "label": 0
  },
  {
    "text": "findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance. Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation. Diagnosis in clinical psychology usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM). The study of mental illnesses is called abnormal psychology. Education Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Educational psychologists can be found in preschools, schools of all levels including post secondary institutions, community organizations and learning centers, Government or private research firms, and independent or private consultant. The work of developmental psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand. School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research. Work Industrial and organizational (I/O) psychology involves research and practices that apply psychological theories and principles to organizations and individuals' work-lives. In the field's beginnings, industrialists brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. The field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology. An influential early study examined workers at Western",
    "label": 0
  },
  {
    "text": "psychology to bear on the study of scientific management techniques for improving workplace efficiency. The field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology. An influential early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924 to 1932. Western Electric experimented on factory workers to assess their responses to changes in illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people's behavior can change when they think they are being observed. Although the Hawthorne research can be found in psychology textbooks, the research and its findings were weak at best. The name industrial and organizational psychology emerged in the 1960s. In 1973, it became enshrined in the name of the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association. One goal of the discipline is to optimize human potential in the workplace. Personnel psychology is a subfield of I/O psychology. Personnel psychologists apply the methods and principles of psychology in selecting and evaluating workers. Another subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity. Most I/O psychologists work outside of academia, for private and public organizations and as consultants. A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company. Organizational behavior (OB) is an allied field involved in the study of human behavior within organizations. One way to differentiate I/O psychology from OB is that I/O psychologists train in university psychology departments and OB specialists, in business schools. Military and intelligence One role for psychologists in",
    "label": 0
  },
  {
    "text": "involved in the study of human behavior within organizations. One way to differentiate I/O psychology from OB is that I/O psychologists train in university psychology departments and OB specialists, in business schools. Military and intelligence One role for psychologists in the military has been to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia. The school provided psychological training for military staff. Today, U.S. Army psychologists perform psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as provide prevention-related services, for example, smoking cessation. The United States Army's Mental Health Advisory Teams implement psychological interventions to help combat troops experiencing mental problems. Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychological warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. This so-called black propaganda is designed to seem as if it originates from a source other than the Army. The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD. The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these activities were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO). Psychologists have sometimes been involved in assisting the interrogation and torture of suspects, staining the records of the psychologists involved. Health, well-being, and social change Social change An example of the contribution of psychologists to social change involves the research of Kenneth and Mamie Phipps Clark. These two African American psychologists studied segregation's adverse psychological impact on Black children. Their research findings played a role in the desegregation case Brown v. Board of Education",
    "label": 0
  },
  {
    "text": "to social change involves the research of Kenneth and Mamie Phipps Clark. These two African American psychologists studied segregation's adverse psychological impact on Black children. Their research findings played a role in the desegregation case Brown v. Board of Education (1954). The impact of psychology on social change includes the discipline's broad influence on teaching and learning. Research has shown that compared to the \"whole word\" or \"whole language\" approach, the phonics approach to reading instruction is more efficacious. Medical applications Medical facilities increasingly employ psychologists to perform various roles. One aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance. Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people. Worker health, safety and wellbeing Psychologists work with organizations to apply findings from psychological research to improve the health and well-being of employees. Some work as external consultants hired by organizations to solve specific problems, whereas others are full-time employees of the organization. Applications include conducting surveys to identify issues and designing interventions to make work healthier. Some of the specific health areas include: Accidents and injuries: A major contribution is the concept of safety climate, which is employee shared perceptions of the behaviors that are encouraged (e.g., wearing safety gear) and discouraged (not following safety rules) at work. Organizations with strong safety climates have fewer work accidents and injuries. Cardiovascular disease: Cardiovascular disease has been related to lack of job control. Mental health: Exposure to occupational stress is associated",
    "label": 0
  },
  {
    "text": "gear) and discouraged (not following safety rules) at work. Organizations with strong safety climates have fewer work accidents and injuries. Cardiovascular disease: Cardiovascular disease has been related to lack of job control. Mental health: Exposure to occupational stress is associated with mental health disorder. Musculoskeletal disorder: These are injuries in bones, nerves and tendons due to overexertion and repetitive strain. They have been linked to job satisfaction and workplace stress. Physical health symptoms: Occupational stress has been linked to physical symptoms such as digestive distress and headache. Workplace violence: Violence prevention climate is related to being physically assaulted and psychologically mistreated at work. Interventions that improve climates are a way to address accidents and violence. Interventions that reduce stress at work or provide employees with tools to better manage it can help in areas where stress is an important component. Industrial psychology became interested in worker fatigue during World War I, when government ministers in Britain were concerned about the impact of fatigue on workers in munitions factories but not other types of factories. In the U. K. some interest in worker well-being emerged with the efforts of Charles Samuel Myers and his National Institute of Industrial Psychology (NIIP) during the inter-War years. In the U. S. during the mid-twentieth century industrial psychologist Arthur Kornhauser pioneered the study of occupational mental health, linking industrial working conditions to mental health as well as the spillover of an unsatisfying job into a worker's personal life. Zickar accumulated evidence to show that \"no other industrial psychologist of his era was as devoted to advocating management and labor practices that would improve the lives of working people.\" Occupational health psychology As interest in the worker health expanded toward the end of the twentieth century, the field of occupational health psychology (OHP) emerged. OHP is",
    "label": 0
  },
  {
    "text": "advocating management and labor practices that would improve the lives of working people.\" Occupational health psychology As interest in the worker health expanded toward the end of the twentieth century, the field of occupational health psychology (OHP) emerged. OHP is a branch of psychology that is interdisciplinary. OHP is concerned with the health and safety of workers. OHP addresses topic areas such as the impact of occupational stressors on physical and mental health, mistreatment of workers (e.g., bullying and violence), work-family balance, the impact of involuntary unemployment on physical and mental health, the influence of psychosocial factors on safety and accidents, and interventions designed to improve/protect worker health. OHP grew out of health psychology, industrial and organizational psychology, and occupational medicine. OHP has also been informed by disciplines outside psychology, including industrial engineering, sociology, and economics. Research methods Quantitative psychological research lends itself to the statistical testing of hypotheses. Although the field makes abundant use of randomized and controlled experiments in laboratory settings, such research can only assess a limited range of short-term phenomena. Some psychologists rely on less rigorously controlled, but more ecologically valid, field experiments as well. Other research psychologists rely on statistical methods to glean knowledge from population data. The statistical methods research psychologists employ include the Pearson product–moment correlation coefficient, the analysis of variance, multiple linear regression, logistic regression, structural equation modeling, and hierarchical linear modeling. The measurement and operationalization of important constructs is an essential part of these research designs. Although this type of psychological research is much less abundant than quantitative research, some psychologists conduct qualitative research. This type of research can involve interviews, questionnaires, and first-hand observation. While hypothesis testing is rare, virtually impossible, in qualitative research, qualitative studies can be helpful in theory and hypothesis generation, interpreting seemingly contradictory quantitative findings, and",
    "label": 0
  },
  {
    "text": "conduct qualitative research. This type of research can involve interviews, questionnaires, and first-hand observation. While hypothesis testing is rare, virtually impossible, in qualitative research, qualitative studies can be helpful in theory and hypothesis generation, interpreting seemingly contradictory quantitative findings, and understanding why some interventions fail and others succeed. Controlled experiments A true experiment with random assignment of research participants (sometimes called subjects) to rival conditions allows researchers to make strong inferences about causal relationships. When there are large numbers of research participants, the random assignment (also called random allocation) of those participants to rival conditions ensures that the individuals in those conditions will, on average, be similar on most characteristics, including characteristics that went unmeasured. In an experiment, the researcher alters one or more variables of influence, called independent variables, and measures resulting changes in the factors of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment. A quasi-experiment is a situation in which different conditions are being studied, but random assignment to the different conditions is not possible. Investigators must work with preexisting groups of people. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity. For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes and, perhaps, statistically adjust for any initial differences in reading level. Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data collected are consistent with the",
    "label": 0
  },
  {
    "text": "classes and, perhaps, statistically adjust for any initial differences in reading level. Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data collected are consistent with the predictions. These predictions are likely to originate from one or more abstract scientific hypotheses about how the phenomenon under study actually works. Other types of studies Surveys are used in psychology for the purpose of measuring attitudes and traits, monitoring changes in mood, and checking the validity of experimental manipulations (checking research participants' perception of the condition they were assigned to). Psychologists have commonly used paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects. Observational studies are commonly conducted in psychology. In cross-sectional observational studies, psychologists collect data at a single point in time. The goal of many cross-sectional studies is the assess the extent factors are correlated with each other. By contrast, in longitudinal studies psychologists collect data on the same sample at two or more points in time. Sometimes the purpose of longitudinal research is to study trends across time such as the stability of traits or age-related changes in behavior. Because some studies involve endpoints that psychologists cannot ethically study from an experimental standpoint, such as identifying the causes of depression, they conduct longitudinal studies a large group of depression-free people, periodically assessing what is happening in the individuals' lives. In this way psychologists have an opportunity to test causal hypotheses regarding conditions that commonly arise in people's lives that put them at risk for depression. Problems that affect longitudinal studies include selective attrition, the type of problem in which bias is introduced when a certain type of research participant disproportionately leaves a",
    "label": 0
  },
  {
    "text": "that commonly arise in people's lives that put them at risk for depression. Problems that affect longitudinal studies include selective attrition, the type of problem in which bias is introduced when a certain type of research participant disproportionately leaves a study. One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them. Exploratory data analysis includes a variety of practices that researchers use to reduce a great many variables to a small number overarching factors. In Peirce's three modes of inference, exploratory data analysis corresponds to abduction. Meta-analysis is the technique research psychologists use to integrate results from many studies of the same variables and arriving at a grand average of the findings. Direct brain observation/manipulation A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature \"brain waves\": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep. Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the",
    "label": 0
  },
  {
    "text": "delta wave observed during non-REM sleep. Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic neural network model rather than a localized function model. Interventions such as transcranial magnetic stimulation and drugs also provide information about brain–mind interactions. Psychopharmacology is the study of drug-induced mental effects. Computer simulation Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that could not be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling. Animal studies Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, and rats and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in",
    "label": 0
  },
  {
    "text": "1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, and rats and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. There are pitfalls, however, in generalizing findings from animal studies to humans through animal models. Comparative psychology is the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology. Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E.O. Wilson. Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience. Qualitative research Qualitative research is often designed to answer questions about the thoughts, feelings, and behaviors of individuals. Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations are made. Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich our",
    "label": 0
  },
  {
    "text": "cursory examinations are made. Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich our understanding of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's application of psychological and sociological theories, in his book Escape from Freedom, to understanding why many ordinary Germans supported Hitler. Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out. Program evaluation Program evaluation involves the systematic collection, analysis, and application of information to answer questions about projects, policies and programs, particularly about their effectiveness. In both the public and private sectors, stakeholders often want to know the extent which the programs they are funding, implementing, voting for, receiving, or objecting to are producing the intended effects. While program evaluation first focuses on effectiveness, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful. Contemporary issues Metascience Metascience involves the application of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias, problematic reproducibility, and misuse of statistics. These findings have led to calls for reform from within",
    "label": 0
  },
  {
    "text": "of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias, problematic reproducibility, and misuse of statistics. These findings have led to calls for reform from within and from outside the scientific community. Confirmation bias In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying possible publication bias. Similarly, Fanelli (2010) found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space science or geosciences. Fanelli argued that this is because researchers in \"softer\" sciences have fewer constraints to their conscious and unconscious biases. Replication A replication crisis in psychology has emerged. Many notable findings in the field have not been replicated. Some researchers were even accused of publishing fraudulent results. Systematic efforts, including efforts by the Reproducibility Project of the Center for Open Science, to assess the extent of the problem found that as many as two-thirds of highly publicized findings in psychology failed to be replicated. Reproducibility has generally been stronger in cognitive psychology (in studies and journals) than social psychology and subfields of differential psychology. Other subfields of psychology have also been implicated in the replication crisis, including clinical psychology, developmental psychology, and a field closely related to psychology, educational research. Focus on the replication crisis has led to other renewed efforts in the discipline to re-test important findings. In response to concerns about publication bias and data dredging (conducting a large number of statistical tests on a great many variables but restricting reporting to the results that were statistically significant), 295 psychology and medical journals have adopted",
    "label": 0
  },
  {
    "text": "findings. In response to concerns about publication bias and data dredging (conducting a large number of statistical tests on a great many variables but restricting reporting to the results that were statistically significant), 295 psychology and medical journals have adopted result-blind peer review where studies are accepted not on the basis of their findings and after the studies are completed, but before the studies are conducted and upon the basis of the methodological rigor of their experimental designs and the theoretical justifications for their proposed statistical analysis before data collection or analysis is conducted. In addition, large-scale collaborations among researchers working in multiple labs in different countries have taken place. The collaborators regularly make their data openly available for different researchers to assess. Allen and Mehler estimated that 61 per cent of result-blind studies have yielded null results, in contrast to an estimated 5 to 20 per cent in traditional research. Misuse of statistics Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts. Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on p-values. WEIRD bias In 2008, Arnett pointed out that most articles in American Psychological Association journals were about U.S. populations when U.S. citizens are only 5% of the world's population. He complained that psychologists had no basis for assuming psychological processes to be universal and generalizing research findings to the rest of the global population. In 2010, Henrich, Heine, and Norenzayan reported a bias in conducting psychology studies with participants from \"WEIRD\" (\"Western, Educated, Industrialized, Rich, and Democratic\") societies. Henrich et al. found that \"96% of psychological samples come from countries with only 12% of the world's",
    "label": 0
  },
  {
    "text": "Henrich, Heine, and Norenzayan reported a bias in conducting psychology studies with participants from \"WEIRD\" (\"Western, Educated, Industrialized, Rich, and Democratic\") societies. Henrich et al. found that \"96% of psychological samples come from countries with only 12% of the world's population\" (p. 63). The article gave examples of results that differ significantly between people from WEIRD and tribal cultures, including the Müller-Lyer illusion. Arnett (2008), Altmaier and Hall (2008) and Morgan-Consoli et al. (2018) view the Western bias in research and theory as a serious problem considering psychologists are increasingly applying psychological principles developed in WEIRD regions in their research, clinical work, and consultation with populations around the world. In 2018, Rad, Martingano, and Ginges showed that nearly a decade after Henrich et al.'s paper, over 80% of the samples used in studies published in the journal Psychological Science employed WEIRD samples. Moreover, their analysis showed that several studies did not fully disclose the origin of their samples; the authors offered a set of recommendations to editors and reviewers to reduce WEIRD bias. STRANGE bias Similar to the WEIRD bias, starting in 2020, researchers of non-human behavior have started to emphasize the need to document the possibility of the STRANGE (Social background, Trappability and self-selection, Rearing history, Acclimation and habituation, Natural changes in responsiveness, Genetic makeup, and Experience) bias in study conclusions. Unscientific mental health training Some observers perceive a gap between scientific theory and its application—in particular, the application of unsupported or unsound clinical practices. Critics say there has been an increase in the number of mental health training programs that do not instill scientific competence. Practices such as \"facilitated communication for infantile autism\"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity. These practices, however,",
    "label": 0
  },
  {
    "text": "programs that do not instill scientific competence. Practices such as \"facilitated communication for infantile autism\"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity. These practices, however, are outside the mainstream practices taught in clinical psychology doctoral programs. Ethics Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes (e.g., the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report). The American Psychological Association has advanced a set of ethical principles and a code of conduct for the profession. The most important contemporary standards include informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential but ethically dubious studies led to the establishment of this rule; such studies included the MIT-Harvard Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, Stanley Milgram's studies of obedience to authority, and the Stanford Prison Experiment. Ethics with Humans The ethics code of the American Psychological Association originated in 1951 as \"Ethical Standards of Psychologists.\" This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption, and contains both aspirational principles and binding ethical standards. The APA's Ethical Principles of Psychologists and Code of Conduct consists of five General Principles, which are meant to",
    "label": 0
  },
  {
    "text": "states. It has changed multiple times over the decades since its adoption, and contains both aspirational principles and binding ethical standards. The APA's Ethical Principles of Psychologists and Code of Conduct consists of five General Principles, which are meant to guide psychologists to higher ethical practice where a particular standard does not apply. Those principles are: A. Beneficence and Nonmaleficence - meaning the psychologists must work to benefit those they work with and \"do no harm.\" This includes awareness of indirect benefits and harms their work might have on others due to personal, social, political, or other factors. B. Fidelity and Responsibility - an awareness of public trust in the profession and adherence to ethical standards and clarification of roles to preserve that trust. This includes managing conflicts of interest, as well as committing some portion of a psychologist's professional time to low-cost or pro bono work. C. Integrity - upholding honesty and accuracy in all psychological practices, including avoiding misrepresentations and fraud. In situations where psychologists would use deception (i.e., certain research), psychologists must consider the necessity, benefits, and harms, and mitigate any harms where possible. D. Justice - an understanding that psychology must be for everyone's benefit, and that psychologists take special care to avoid unjust practices as a result of biases or limitations of expertise. E. Respect for People's Rights and Dignity - the preservation of people's rights when working with psychologists, including confidentially, privacy, and autonomy. Psychologists should consider a multitude of factors, including a need for special safeguards for protected populations (e.g., minors, incarcerated individuals) and awareness of differences based on numerous factors, including culture, race, age, gender, and socioeconomic status. In 1989, the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission.",
    "label": 0
  },
  {
    "text": "and awareness of differences based on numerous factors, including culture, race, age, gender, and socioeconomic status. In 1989, the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between \"aspirational\" ethical standards and \"enforceable\" ones. The APA code was further revised in 2010 to prevent the use of the code to justify violating human rights, which was in response to the participation of APA members in interrogations under the administration of United States President George W. Bush. Members of the public have a five-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a three-year window. The Canadian Psychological Association used the APA code until 1986, when it developed its own code drawing from four similar principles: 1) Respect for the Dignity of Persons and Peoples, 2) Responsible Caring, 3) Integrity in Relationships, 4) Responsibility to Society. The European Federation of Psychologist's Associations, have adopted a model code using the principles of the Canadian Code, while also drawing from the APA code. Universities have ethics committees dedicated to protecting the rights (e.g., voluntary nature of participation in the research, privacy) and well-being (e.g., minimizing distress) of research participants. University ethics committees evaluate proposed research to ensure that researchers protect the rights and well-being of participants; an investigator's research project cannot be conducted unless approved by such an ethics committee. The field of psychology also identifies certain categories of people that require additional or special protection due to particular vulnerabilities, unequal power dynamics, or diminished capacity for informed consent. This list often includes, but is not limited to, children, incarcerated individuals, pregnant women, human fetuses and neonates, institutionalized persons, those with physical",
    "label": 0
  },
  {
    "text": "additional or special protection due to particular vulnerabilities, unequal power dynamics, or diminished capacity for informed consent. This list often includes, but is not limited to, children, incarcerated individuals, pregnant women, human fetuses and neonates, institutionalized persons, those with physical or mental disabilities, and the educationally or economically disadvantaged. Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing. Some of the most common complaints against clinical psychologists include sexual misconduct and breaches in confidentiality or privacy. Psychology ethics apply to all types of human contact in a psychologist's professional capacity, including therapy, assessment, teaching, training, work with research subjects, testimony in courts and before government bodies, consulting, and statements to the public or media pertaining to matters of psychology. Ethics with other animals Research on other animals is governed by university ethics committees. Research on nonhuman animals cannot proceed without permission of the ethics committee, of the researcher's home institution. Ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research. Psychologists can use certain research techniques on animals that could not be used on humans. Comparative psychologist Harry Harlow drew moral condemnation for isolation experiments on rhesus macaque monkeys at the University of Wisconsin–Madison in the 1970s. The aim of the research was to produce an animal model of clinical depression. Harlow also devised what he called a \"rape rack\", to which the female isolates were tied in normal monkey mating posture. In",
    "label": 0
  },
  {
    "text": "Aphantasia ( AY-fan-TAY-zhə, AF-an-TAY-zhə) is the inability to voluntarily visualize mental images. The phenomenon was first described by Francis Galton in 1880, but it has remained relatively unstudied. Interest in the phenomenon was renewed after the publication of a study in 2015 by a team led by the neurologist Adam Zeman of the University of Exeter. Zeman's team coined the term aphantasia, derived from the ancient Greek word phantasia (φαντασία), which means 'appearance/image', and the prefix a- (ἀ-), which means 'without'. People with aphantasia are called aphantasics, or less commonly aphants or aphantasiacs. Aphantasia can be considered the opposite of hyperphantasia, the condition of having extremely vivid mental imagery. History The phenomenon was first described by Francis Galton in 1880, in a statistical study on mental imagery. Galton wrote: To my astonishment, I found that the great majority of the men of science to whom I first applied, protested that mental imagery was unknown to them, and they looked on me as fanciful and fantastic in supposing that the words \"mental imagery\" really expressed what I believed everybody supposed them to mean. They had no more notion of its true nature than a colour-blind man who has not discerned his defect has of the nature of colour. In 1897, the psychologist Théodule-Armand Ribot reported a kind of \"typographic visual type\" imagination, consisting in mentally seeing ideas in the form of the corresponding printed words. As paraphrased by Jacques Hadamard, The first discovery of this by Ribot was the case of a man whom he mentions as a well-known physiologist. For that man, even the words \"dog, animal\" (while he was living among dogs and experimenting on them daily) were not accompanied by any image, but were seen by him as being printed. Similarly, when he heard the name of an",
    "label": 0
  },
  {
    "text": "that man, even the words \"dog, animal\" (while he was living among dogs and experimenting on them daily) were not accompanied by any image, but were seen by him as being printed. Similarly, when he heard the name of an intimate friend, he saw it printed and had to make an effort to see the image of this friend... Moreover, according to Ribot, men belonging to the typographic-visual type cannot conceive how other people's thought can proceed differently. The phenomenon remained largely unstudied until 2005, when Professor Adam Zeman of the University of Exeter was approached by a man who seemed to have lost the ability to visualize after undergoing minor surgery. Following the publication of this patient's case in 2010, a number of people approached Zeman reporting a lifelong inability to visualize. In 2015, Zeman's team published a paper on what they termed \"congenital aphantasia\"—a form of aphantasia in which individuals have never had the ability to generate voluntary mental images— sparking renewed interest in the phenomenon. The idea of aphantasia was popularised on social media in 2020, through posts which asked the reader to imagine a red apple and rate their \"mind's eye\" depiction of it on a scale from 1 (photographic visualisation) through to 5 (no visualisation at all). Many people were shocked to learn that their own ability or inability to visualise objects was not universal. Research Zeman's 2015 paper used the Vividness of Visual Imagery Questionnaire (VVIQ), developed by David Marks in 1973, to evaluate the quality of the mental image of 21 self-diagnosed and self-selected participants. He found that most aphantasics lack voluntary visualizations only; the majority of test subjects did report involuntary visualizations such as dreams. Along with Zeman's reports of involuntary mental imagery occurring during dream states, a 2020 study found that",
    "label": 0
  },
  {
    "text": "self-selected participants. He found that most aphantasics lack voluntary visualizations only; the majority of test subjects did report involuntary visualizations such as dreams. Along with Zeman's reports of involuntary mental imagery occurring during dream states, a 2020 study found that individuals with aphantasia experience less control and fewer sensory emotions during dreams compared to those with a strong ability to generate voluntary mental images while awake. The lack of intense emotions is thought to result from a functional trade-off—competition between two neural systems—occurring between semantic information and sensory qualities, which is strongly associated with individuals who have low VVIQ scores. In 2017, a paper measured the sensory capacity of mental imagery using binocular-rivalry (BR) and imagery-based priming and found that when asked to imagine a stimulus, the self-reported aphantasics experienced almost no perceptual priming, compared to those who reported higher imagery scores where perceptual priming had an effect. In 2020, Keogh and Pearson published another paper illustrating measurable differences correlated with visual imagery, this time by indirectly measuring cortical excitability in the primary visual cortex (V1). In 2018, a study analyzing the visual working memory of a person with aphantasia found that mental imagery has a \"functional role in areas of visual cognition, one of which is high-precision working memory\" and that the person with aphantasia performed significantly worse than controls on visual working memory trials requiring the highest degree of precision, and lacked metacognitive insight into their performance. A 2020 study concluded that those who experience aphantasia also experience reduced imagery in other senses, and have less vivid autobiographical memories. In addition to deficits in autobiographical memories compared to people without aphantasia, people with aphantasia had significant differences in all aspects of memory when compared to the performance of people without aphantasia. A 2021 study concluded that while those",
    "label": 0
  },
  {
    "text": "memories. In addition to deficits in autobiographical memories compared to people without aphantasia, people with aphantasia had significant differences in all aspects of memory when compared to the performance of people without aphantasia. A 2021 study concluded that while those with aphantasia reported fewer objects in drawing recall, they showed high spatial memory concerning controls in drawings, with these differences only appearing during the recall stage of the study. In 2021, a study by Keogh, Wicken, and Pearson focusing on the role of visual imagery in visual working memory tasks specifically considered the strategies people with aphantasia use in these tasks. It found no significant differences in visual working memory task performances for those with aphantasia when compared to controls. However, significant differences were found in the reported strategies used by aphantasic individuals across the memory tasks. In 2021, a study that measured the perspiration (via skin conductance levels) of participants in response to reading a frightening story and then viewing fear-inducing images found that participants with aphantasia, but not the general population, experienced a flat-line physiological response during the reading experiment, but found no difference in physiological responses between the groups when participants viewed fear-inducing images. The study concluded the evidence supported the emotional amplification theory of visual imagery. In 2021, a study found that people with aphantasia have slower reaction times than people without aphantasia in a visual search task in which they were presented with a target and a distractor. But both groups saw a similar reduction in reaction time when primed with the color of the target compared to if primed with the color of the distractor or a third color, suggesting that people with and without aphantasia were primed in the same way. The researchers hypothesized that this may be because the color of the",
    "label": 0
  },
  {
    "text": "target compared to if primed with the color of the distractor or a third color, suggesting that people with and without aphantasia were primed in the same way. The researchers hypothesized that this may be because the color of the prime is not relevant to the search task. To explore this, a follow-up experiment by the same researchers found people without aphantasia saw a greater reduction in reaction time when selecting the target from two images compared to from two words. At the same time, both people with and without aphantasia were faster in the image task than the word task. A 2023 study explored more natural scenarios and found that aphantasics are slower at solving hidden object pictures. In 2021, a study relating aphantasia, synesthesia, and autism was published that found that people with aphantasia reported more autistic traits than people without aphantasia, with weaknesses in imagination and social skills. In addition to congenital aphantasia, there have been cases reported of acquired aphantasia—characterized by new onset of diminished voluntary visual imagery—due either to brain injury or psychological causes. In 2021, a study reported on acquired aphantasia following a case of COVID-19. A 2021 study aimed to provide insights into the correlation between auditory and visual imagery. The research, conducted on a sample of 128 participants, included 34 individuals who self-identified as having aphantasia. The study found a strong association between auditory imagery (measured using the Bucknell Auditory Imagery Scale-Vividness, BAIS-V) and visual imagery (measured using the Vividness of Visual Imagery Questionnaire-Modified, VVIQ-M). They found most people who self-reported having aphantasia also reported weak or entirely absent auditory imagery. Moreover, participants lacking auditory imagery tended to be aphantasic. The authors proposed a new term, \"anauralia\", to describe the absence of auditory imagery, particularly the lack of an \"inner voice\". A",
    "label": 0
  },
  {
    "text": "aphantasia also reported weak or entirely absent auditory imagery. Moreover, participants lacking auditory imagery tended to be aphantasic. The authors proposed a new term, \"anauralia\", to describe the absence of auditory imagery, particularly the lack of an \"inner voice\". A subsequent study, corroborated this finding, showing that the majority of a sample of people recruited on the basis of visual aphantasia also reported having reduced auditory imagery. However, this self-reported reduction in auditory imagery was not evident in performance on tasks thought to require auditory imagery, including a musical pitch imagery and voice recognition task. A 2022 study estimated the prevalence of aphantasia among the general population by screening undergraduate students and people from an online crowdsourcing marketplace through the Vividness of Visual Imagery Questionnaire. They found that 0.8% of the population was unable to form visual mental images, and 3.9% of the population was either unable to form mental images or had dim or vague mental imagery. Sitek and Konieczna have shown that its progressive form may be a harbinger of dementia. A group of authors interviewed people with aphantasia about their lives and found that they generated fewer episodic details than controls for both past and future events, indicating that visual imagery is an important cognitive tool for dynamic retrieval and recombination of episodic details. There have been various approaches to find a general theory of aphantasia or incorporate it into current philosophical, psychological and linguistic research. Blomkvist has suggested that aphantasia is best explained as a malfunction of processes in the episodic system and sees it as an episodic system condition. Nanay has argued that at least some instances of this condition can be explained in terms of unconscious mental imagery. Alternative explanations for aphantasia have also been proposed in the scientific literature. Lorenzatti provides a summary",
    "label": 0
  },
  {
    "text": "an episodic system condition. Nanay has argued that at least some instances of this condition can be explained in terms of unconscious mental imagery. Alternative explanations for aphantasia have also been proposed in the scientific literature. Lorenzatti provides a summary of these views. Aphantasia also has been studied from philosophical perspectives. Šekrst proposed that a gradual range of perceptions and mental images, from aphantasia to hyperphantasia, influences philosophical analysis of mental imagery from a fuzzy standpoint, along with influence on linguistics and semiotics. Whiteley argues that a modified theory of dreaming has to incorporate aphantasia, by involving the claim that dreams are a non-voluntary form of imagination. Additionally, research by Boran into romantic desire has shown a potential link between vividness of mental imagery and romantic feelings, suggesting that mental imagery may also play a role in emotional memory and relationships. In 2024, a research team led by Jonathan Rhodes from the University of Plymouth assessed the imagery abilities of over 300 athletes finding a small sample of 27 who had aphantasia or low imagery abilities. The researchers developed a training program over six weeks to improve imagery ability, finding that it can be significantly improved for the majority of participants. In addition, the research of Keogh and Pearson's follow-up with over 50 participants further confirmed the absence of sensory imagery in aphantasia, adding evidence to the field of study. Zeman also proposes that alterations in connectivity between the frontoparietal and visual networks may provide the neural substrate for extreme variations in visual imagery. Some people with aphantasia have acquired visual mental imagery after using psychedelics like ayahuasca or psilocybin. Notable people with aphantasia See also Charcot–Wilbrand syndrome Creative visualization – Purposeful visualisation for neuropsychological, physiological or social effects Number form – Mental map of numbers Prefrontal synthesis – Conscious",
    "label": 0
  },
  {
    "text": "In the realm of psychology, the concept of belief congruence suggests that our valuation of beliefs, subsystems or systems of beliefs and people is directly proportional to their congruence with our own belief systems. That similar beliefs promote liking and social harmony among people while dissimilar beliefs produce dislike and prejudice. Belief congruence was first proposed by Milton Rokeach in 1960, 'belief is more important than ethnic or racial membership as a determinant of social discrimination' – that prejudice arises from how people react to differences, or lack of congruence in belief systems, not just based on inter-group memberships. This principle was further elaborated by himself later in 1965, that belief system is a crucial alignment point for individuals, thus, the validity of our own beliefs can be confirmed or determined by the level of similarity and congruence of inter-individual belief systems. The principle states that we value beliefs and people based on how closely they align with our own beliefs, considering both similarity and importance. A belief is deemed more congruent if it's either more similar to ours or deemed more important, assuming equal similarity. Empirical evidence Rokeach and his colleagues designed an experiment to separate the effects of group membership from belief congruence on individuals' preferences. In this paradigm, participants rated their liking for people who were either part of their own group or a different one, and who had beliefs that were either similar or dissimilar to theirs. The findings from several studies using this methodology consistently showed that shared beliefs were a stronger predictor of liking than simply belonging to the same group. This was illustrated by instances where white participants preferred a black individual with similar beliefs over a white individual with differing beliefs. Another set of findings from researchers illustrated the powerful effects of",
    "label": 0
  },
  {
    "text": "than simply belonging to the same group. This was illustrated by instances where white participants preferred a black individual with similar beliefs over a white individual with differing beliefs. Another set of findings from researchers illustrated the powerful effects of belief prejudice over racial categorisation. People's biases matched their views on race, showing that sharing similar beliefs often matters more than race in how we connect with or judge others, particularly for a friendliness assessment. Explanation Social Identity Theory Social Identity Theory, developed by Henri Tajfel and John Turner, stated that social identity is the portion of an individual's self-concept that comes from their awareness of being part of a social group (or groups) and the emotional importance they place on this membership. Social identity theory believes that race effects could potentially result from comparative judgments individuals make to preserve their social worth or identity. Even though Tajfel didn't directly connect social identity to belief congruence theories, his ideas are still very applicable. For instance, he believes that comparing ourselves with others is what connects our social categories to our sense of who we are. Therefore, in some situations, race effects may result from comparative judgments which serve to boost the individual's value and identity. Critiques Since its original presentation, belief congruence theory has also generated a large number of vigorous critiques, testing the theory's hypothesis is challenging because membership in social or ethnic groups often overlaps with similarities within groups and differences between them in many key areas, including beliefs. Moreover, race was found more important than belief when it comes to more intimate relationships such as friendship. Studies found that in situations where stronger measures of attraction are considered, category (e.g. ethnic, race, group) differences become more significant than beliefs. Critiques also argue that belief congruence primarily addresses",
    "label": 0
  },
  {
    "text": "it comes to more intimate relationships such as friendship. Studies found that in situations where stronger measures of attraction are considered, category (e.g. ethnic, race, group) differences become more significant than beliefs. Critiques also argue that belief congruence primarily addresses discrimination in contexts of small social distance, the theory's applicability to scenarios involving larger social distances, such as neighbourhood or university settings, where prejudice and discrimination might manifest differently, remains questioned. Moreover, Michael Diehl's research provides a conclusively finding: individuals generally prefer those with similar beliefs, despite similar discrimination levels against both similar and differing beliefs. However, out-groups with beliefs similar to the in-group actually faced more prejudice and discrimination compared to those with differing beliefs. Another limitation supported by the social-pressure hypothesis of belief congruence theory is that in environments where prejudice is institutionalised or socially accepted, belief congruence does not apply, and prejudice simply becomes a matter of ethnic group membership, as found in empirical evidence from relevant studies. 'Weak version' of belief congruence theory According to Milton Rokeach's personal communication in 1981, his intent was to advocate the strong version of the theory, that belief systems are the only factors that need to be considered when analysing prejudice and discrimination. Nonetheless, most studies about this theory are lack of significance tests between belief and race effects, but still show a common trend: beliefs more strongly influence liking and evaluations, while race impacts decisions on intimate connections. For clarity, Moe et al. (1981) differentiate between two forms of belief congruence theory: a \"strong\" version and a \"weak\" version. The \"strong\" version posits that discrimination is entirely based on belief congruence. Conversely, the \"weak\" version suggests that belief congruence accounts for a larger portion of discrimination variance than do perceived physical differences. While belief congruence plays a crucial role",
    "label": 0
  },
  {
    "text": "Within the science of vision, binocular vision focuses on the question of how humans and animals perceive the world with two eyes instead of one. Two main areas are distinguished: directional vision and depth perception (stereopsis). In addition, both eyes can positively or negatively influence each other's vision through binocular interaction. In medical science, binocular vision refers to binocular vision disorders and tests and exercises to improve binocular vision. In biology, binocular vision refers to the fact that the placement of the eyes affects the capabilities of depth perception and directional vision in animals. In society, binocular vision refers to applications for seeing stereoscopic images and aids for binocular vision. Directional vision Directional vision focuses on how the images from two eyes are combined in perception into a single image and how the directions in which each eye sees surrounding objects are converted into practically useful information. The main article on directional vision describes that the direction in which the left and right eyes see an object can be combined in three different ways. This leads to seeing a single image, double images or a fused image. This perception is linked to a certain quality of depth perception: fine stereopsis or coarse stereopsis. Depth vision Depth perception focuses on the question of how the brain uses the difference in perspective between the two eyes to recognise shapes and objects, to see through camouflage, and to gather information about spatial relationships. The main article on stereopsis discusses the qualities of depth perception, the area of space they cover, and how the observer controls the input through attention and eye movements. Binocular interaction Binocular interaction occurs when there is an interaction between the two eyes, which causes vision with both eyes to be different than with one eye alone. Vision can be",
    "label": 0
  },
  {
    "text": "observer controls the input through attention and eye movements. Binocular interaction Binocular interaction occurs when there is an interaction between the two eyes, which causes vision with both eyes to be different than with one eye alone. Vision can be better (binocular summation) or worse (binocular inhibition). Binocular summation In binocular summation, the signals from both eyes reinforce each other so that visual acuity, contrast sensitivity, flicker sensitivity, and brightness sensitivity improve. Maximum binocular summation occurs when the sensitivities of each eye are equal. Differences in sensitivity decrease the effect of binocular summation. The effect of binocular summation decreases with age. Binocular inhibition In binocular inhibition, vision with both eyes is worse than with one eye. This can occur with strabismus or a lazy eye, because the weaker eye interferes with the stronger one. Eye dominance, where the image produced by one eye in the brain can suppress the other, is a form of binocular inhibition. Perception systems Information processing for direction perception and depth perception takes place in two systems. One system specializes in color and fine detail and is concerned with discovering shapes and objects in a relatively static environment. The other system specializes in discovering spatial relationships in a rapidly changing environment. The first system contributes to the perception of fused images with fine depth information. The second system contributes to the perception of double images that quickly cover large distances in space and in which the coarse location is the most important information. Disorders and tests About 80% of people can see depth, but not all can do so equally well. Several tests can determine how well someone sees depth, and there are exercises to improve depth perception. If one eye does not function properly or is blind, this can cause stereoblindness, a complete lack of",
    "label": 0
  },
  {
    "text": "can do so equally well. Several tests can determine how well someone sees depth, and there are exercises to improve depth perception. If one eye does not function properly or is blind, this can cause stereoblindness, a complete lack of depth perception. There are other eye disorders that can affect binocular vision. For example, sometimes the eye muscles do not work properly, causing the images from both eyes to be misaligned. Another example is where one eye is dominant, so that the signals from the other eye do not come through in the binocular image, or so that the direction in which an object is seen changes. If eye dominance is noticed in time, an attempt can be made to reduce it through exercise, for example, by temporarily taping the dominant eye. Prevalence Binocular vision abnormalities are among the most common visual disorders. They are usually accompanied by symptoms such as headache, eye strain, eye pain, blurred vision, and occasionally diplopia (double vision). Approximately 20% of patients who come to an optometry clinic have binocular vision defects. As the use of digital aids becomes more common, many children use them for a significant period of time. This can lead to various binocular vision defects, such as reduced accommodative amplitudes, accommodative powers, and positive fusional convergence, both near and far. The most effective way to diagnose visual defects is with the near point of convergence (NPC) test. During the NPC test, a target, such as a finger, is brought to the face until the examiner notices that one eye is turned outward and/or the person has experienced double vision. Binocular defects can be compensated for to some extent by adaptations of the visual system. However, if the deviations of binocular vision are too great (for example, if the visual system has",
    "label": 0
  },
  {
    "text": "outward and/or the person has experienced double vision. Binocular defects can be compensated for to some extent by adaptations of the visual system. However, if the deviations of binocular vision are too great (for example, if the visual system has to adapt to excessive horizontal, vertical, torsional or size (aniseikonic) deviations), the eyes tend to avoid binocular vision, which eventually causes or worsens squint. Lazy eye Lazy eye or amblyopia is a neurovisual developmental disorder. The condition is characterized by underdevelopment of several visual features and skills such as visual acuity, eye movements, eye teamwork and binocular depth perception. Squint Squint or strabismus is an eye condition in which the eyes do not look in the same direction. It has long been known that full binocular vision, including stereopsis, is an important factor in stabilizing the postoperative outcome of strabismus corrections. Many people with a lack of stereopsis have (or had) visible strabismus, which has a potential socioeconomic impact on children and adults. Both wide-angle and narrow-angle strabismus, in particular, can negatively impact self-confidence because it disrupts normal eye contact, often leading to embarrassment, anger, and feelings of discomfort (see psychosocial effects of strabismus). Aniseikonia Aniseikonia is an ocular condition where there is a significant difference in the size of the retinal images of the two eyes caused by differences in refraction between the eyes. Stereopsis tests In stereopsis testing (abbreviated to stereotesting), stereograms are used to measure the presence and sharpness of binocular depth perception (stereopsis). There are two types of common clinical tests: random-dot stereotesting and contour stereotesting. Random-dot stereotesting uses images of stereo figures embedded in a background of random dots. Contour stereo tests use images in which the targets presented to each eye are separated horizontally. Random-dot stereo tests Stereopsis ability can be tested with the",
    "label": 0
  },
  {
    "text": "Random-dot stereotesting uses images of stereo figures embedded in a background of random dots. Contour stereo tests use images in which the targets presented to each eye are separated horizontally. Random-dot stereo tests Stereopsis ability can be tested with the Lang stereotest, consisting of a random-dot stereogram on which a series of parallel strips cylindrical lenses are printed in certain shapes, which represent the images which each eye sees in these areas, separate from each other. similar to a hologram. Without stereopsis, the image appears as a field of random dots, but the shapes become visible with increasing stereopsis and generally consist of a cat (indicating that a stereopsis of 1200 arc seconds of retinal disparity is possible), a star (600 arc seconds), and a car (550 arc seconds). To standardize the results, the image should be viewed at a distance of 40 cm from the eye and exactly in the frontoparallel plane. While most random-dot stereotests, such as the Random Dot \"E\" stereotest or the TNO stereotest, require special glasses (i.e., polarized or red-green lenses), the Lang stereotest works without special glasses, making it easier to use with young children. Contour stereotests Examples of contour stereotests include the Titmus stereotests, of which the Titmus fly stereotest is the best-known example, in which an image of a fly is shown with deviations at the edges. The patient uses 3D glasses to look at the image and determine whether a 3D figure can be seen. The degree of deviation in the images varies, for example, 400-100 arc seconds and 800-40 arc seconds. Vision therapy Vision therapy is a controversial treatment to improve stereopsis. Other disorders To maintain stereopsis and singleness of vision, the eyes need to be pointed accurately. The position of each eye is controlled by six extraocular muscles. Slight",
    "label": 0
  },
  {
    "text": "seconds. Vision therapy Vision therapy is a controversial treatment to improve stereopsis. Other disorders To maintain stereopsis and singleness of vision, the eyes need to be pointed accurately. The position of each eye is controlled by six extraocular muscles. Slight differences in length, insertion position, or strength of the same muscles in the two eyes can lead to a tendency for one eye to drift to a different position in its orbit from the other, especially when one is tired. This is known as phoria. One way to reveal it is with the cover-uncover test. In this test, one of a person's eyes is covered. With the uncovered eye, the person looks at a target such as a fingertip. The target is moved around to break the reflex that normally holds the covered eye in the correct vergence position. The target is held steady before the eye is uncovered. The eye may flick quickly from being wall-eyed or cross-eyed to its correct position. If the eye moved from out to in, the person has esophoria. If it moved from in to out, the person has exophoria. If the eye did not move at all, the person has orthophoria. Most people have some amount of exophoria or esophoria, which is normal. If the uncovered eye also moved vertically, the person has hyperphoria (if the eye moved from down to up) or hypophoria (if the eye moved from up to down). Such vertical phorias are quite rare. It is also possible for the covered eye to rotate in its orbit, known as cyclophoria, which is rarer than vertical phorias. The cover-uncover test may be used to determine the direction of deviation in cyclophorias. The cover-uncover test can also be used for more problematic disorders of binocular vision, the tropias. In the cover",
    "label": 0
  },
  {
    "text": "cyclophoria, which is rarer than vertical phorias. The cover-uncover test may be used to determine the direction of deviation in cyclophorias. The cover-uncover test can also be used for more problematic disorders of binocular vision, the tropias. In the cover part of the test, the examiner looks at the first eye while covering the second. If the eye moves from in to out, the person has exotropia. If it moved from out to in, the person has esotropia. People with exotropia or esotropia are wall-eyed or cross-eyed, respectively. These are forms of strabismus that can be accompanied by amblyopia. There are numerous definitions of amblyopia. A definition that incorporates all of these defines amblyopia as a unilateral condition in which vision is worse than 20/20 in the absence of any obvious structural or pathologic anomalies, but with one or more of the following conditions occurring before the age of six: amblyogenic anisometropia, constant unilateral esotropia or exotropia, amblyogenic bilateral isometropia, amblyogenic unilateral or bilateral astigmatism, image degradation. When the covered eye is the non-amblyopic eye, the amblyopic eye suddenly becomes the person's only means of seeing. The strabismus is revealed by the movement of that eye to fixate on the examiner's finger. There are also vertical tropias (hypertropia and hypotropia) and cyclotropias. Binocular vision anomalies include diplopia, visual confusion (the perception of two different images superimposed onto the same space), suppression (where the brain ignores all or part of one eye's visual field), horror fusionis (an active avoidance of fusion by eye misalignment), and anomalous retinal correspondence (where the brain associates the fovea of one eye with an extrafoveal area of the other eye). In animals Vision provides animals with the means to orient themselves in space, to recognize and manipulate objects, and to navigate. In The Ecology of Vision,",
    "label": 0
  },
  {
    "text": "associates the fovea of one eye with an extrafoveal area of the other eye). In animals Vision provides animals with the means to orient themselves in space, to recognize and manipulate objects, and to navigate. In The Ecology of Vision, James Gibson describes how these means have developed in an interactive interplay between observer and environment, and how the environment often contains much more practically useful information than we realize. For example, pilots, as well as fast-moving birds, rely solely on (monocular) motion vision to steer and avoid objects in a rapidly changing environment. In humans, this even leads to a (monocular) 3D sensation of stereopsis. Binocular depth perception (stereopsis) adds to this the notion that the 3D sensation is also available in situations where the observer is not moving quickly. This allows animals to wait for prey while stationary and strike when it is within reach. Predictor of binocular depth perception Whether animals possess the ability for binocular depth perception is not self-evident. First, eye position is a predictor of binocular depth perception. Binocular depth perception requires overlap between the visual fields of both eyes, but this is not a sufficient condition. For example, some birds are known to have a small overlap of visual fields when looking straight ahead, but this is used to steer effectively during rapid flight movements based on motion vision. This illustrates that having an animal's eyes at the front does not automatically mean it possesses binocular depth perception. The type of eye movements is also a predictor of binocular depth perception, but it is also not a sufficient condition. Under prevalence of stereopsis in animals it is indicated in which animals stereopsis has been found. Eye position Some animals – usually prey – have their two eyes positioned on opposite sides of their",
    "label": 0
  },
  {
    "text": "is also not a sufficient condition. Under prevalence of stereopsis in animals it is indicated in which animals stereopsis has been found. Eye position Some animals – usually prey – have their two eyes positioned on opposite sides of their heads to give the widest possible field of view. Examples include rabbits, buffalo, and antelopes. In such animals, the eyes often move independently to increase the field of view. Even without moving their eyes, some birds have a 360-degree field of view. Some other animals – usually predators – have their two eyes positioned on the front of their heads, thereby allowing for binocular vision and reducing their field of view in favor of stereopsis. However, front-facing eyes are a highly evolved trait in vertebrates, and there are only three extant groups of vertebrates with truly forward-facing eyes: primates, carnivorous mammals, and birds of prey. Some predatory animals, particularly large ones such as sperm whales and killer whales, have their two eyes positioned on opposite sides of their heads, although it is possible they have some binocular visual field. Other animals that are not necessarily predators, such as fruit bats and a number of primates, also have forward-facing eyes. These are usually animals that need fine depth discrimination/perception; for instance, binocular vision improves the ability to pick a chosen fruit or to find and grasp a particular branch. In animals with forward-facing eyes, the eyes usually move together. Eye movements Eye movements are either conjunctive (same direction) version eye movements, usually described by their type (saccades or smooth pursuit), or they are disjunctive (opposite direction) vergence eye movements. Some animals use both of the above strategies. A starling, for example, has laterally placed eyes to cover a wide field of view, but can also move them together to point to",
    "label": 0
  },
  {
    "text": "they are disjunctive (opposite direction) vergence eye movements. Some animals use both of the above strategies. A starling, for example, has laterally placed eyes to cover a wide field of view, but can also move them together to point to the front, so their fields overlap, giving stereopsis. A remarkable example is the chameleon, whose eyes appear as if mounted on turrets, each moving independently of the other, up or down, left or right. Nevertheless, the chameleon can bring both of its eyes to bear on a single object when it is hunting, showing vergence and stereopsis. Birds The function of two eyes seems to vary greatly between bird species. In most birds, binocular vision seems to be primarily focused on being able to control the direction of flight and being able to determine the moment at which an object will be collided: when landing or when pecking. The optical flow is important for controlling the direction of flight, which can be determined by each eye separately. Binocular overlap in that case is functional for being able to fly straight ahead, and does not necessarily indicate the ability to perceive depth. Binocular depth perception is functional in birds that use tools, such as crows. It is also functional for birds that wait still until prey is within pecking range so that they can strike at the right moment. In birds that catch their prey in the air, this area is located higher, at the point where the prey is grasped. These birds have a small binocular field of view that is focused on the area below the beak and/or near the legs, with a blind spot in the area directly below the beak. The absence of vergence eye movements means that birds cannot move the area of stereopsis, if any,",
    "label": 0
  },
  {
    "text": "is focused on the area below the beak and/or near the legs, with a blind spot in the area directly below the beak. The absence of vergence eye movements means that birds cannot move the area of stereopsis, if any, in space as humans can. Prevalence of stereopsis Stereopsis has been found in many vertebrates including mammals such as horses, birds such as falcons and owls, reptiles, amphibia including toads and fish. It has also been found in invertebrates including cephalopods like the cuttlefish, crustaceans, spiders, and insects such as mantis. Stomatopods even have stereopsis with just one eye. Interocular distance Interocular distance, also known as pupillary or interpupillary distance, is the distance between the eyes. This separation determines the range of distances over which an animal can perceive depth. Applications Applications for binocular vision are aids for binocular vision, aimed at making, recording, and viewing stereo images. The binocular microscope and binoculars can magnify images. By increasing the distance between the front lenses of the binoculars and decreasing the distance of the front lenses of the microscope, the perceived depth is in proportion to the magnification. In the course of history, various types of stereoscopes have been developed with which specially prepared stereo recordings (stereograms) can be viewed in 3D, both at home and in the cinema. The most recent development is the VR glasses. Binocular viewers The observable three-dimensional space can be seemingly enlarged with a binocular telescope for things that are far away and a binocular microscope for very small things. It is not self-evident that by enlarging the image, depth is also seen. This is explained below. Binoculars With binoculars, the world that we know up close can also be viewed from a distance. The optics in the binoculars ensure that the retinal images are enlarged.",
    "label": 0
  },
  {
    "text": "enlarging the image, depth is also seen. This is explained below. Binoculars With binoculars, the world that we know up close can also be viewed from a distance. The optics in the binoculars ensure that the retinal images are enlarged. The perceived depth is reduced, the image appears flatter. In order to restore the normal aspect ratio, binocular binoculars must be used to view from two points that are further apart than the two eyes. In binocular binoculars, the front lenses are therefore placed further apart using optical means (prisms or mirrors). The enlargement of the depth dimension that can be achieved in this way is practically limited to the area in which stereopsis is possible. At greater depth (disparity), the image appears flat. For a natural depth experience, it is important that the distance between the two lenses is adjusted to the magnification factor of the binoculars. Microscope A binocular microscope can be used to magnify and view a microscopic world. In order to see depth in this small world that is in proportion to the size of the objects present, the distance between the front lenses of the microscope must be much smaller than the normal distance between our eyes. This is done with the same optical means as with binocular binoculars, but then in a mirrored arrangement, see figure. Without depth Even without seeing depth, binocular vision has advantages over seeing with one eye. If care is taken to ensure that the images from both eyes overlap well and are sharp, then the images from both eyes reinforce each other (binocular summation) and it is as if the image is brighter. This can be compared to increasing the light intensity by using larger lenses. Stereo images Binocular images can be captured by recording both the image seen",
    "label": 0
  },
  {
    "text": "reinforce each other (binocular summation) and it is as if the image is brighter. This can be compared to increasing the light intensity by using larger lenses. Stereo images Binocular images can be captured by recording both the image seen by the left eye and the image seen by the right eye in a stereogram. The images can be recorded simultaneously (stereo photography) or one after the other (scout stereogram, moon stereogram). The advantage of recording the images of the left and right eyes simultaneously with a stereo camera is that no false disparities arise because the scene to be recorded has changed between the recordings. Stereogram A stereogram is a set of two images (pictures, videos, or computer-generated images), one for each eye, that can be used to evoke a binocular three-dimensional scene. In a stereogram, the two images can be attached to each other, with the left-eye image on the left and the right-eye image on the right (L-R stereogram), or with the right-eye image on the left and the left-eye image on the right (R-L stereogram). A stereogram can also consist of two separate images that are placed separately in a stereoscope, or that are specially prepared and placed on top of each other, with color filters, polarizing filterss, or optical ridges used to ensure that each eye sees only one of the images. Stereograms have been and continue to be widely used in depth perception research, for entertainment, and for education. Stereograms can be made by hand, by drawing with a computer program, or by taking pictures with one or two still cameras (stereo camera) or video cameras. The geometry used to design the correct disparities for these images is described in epipolar geometry and computer stereo vision. For natural scenes, the recordings for a stereogram",
    "label": 0
  },
  {
    "text": "pictures with one or two still cameras (stereo camera) or video cameras. The geometry used to design the correct disparities for these images is described in epipolar geometry and computer stereo vision. For natural scenes, the recordings for a stereogram are usually made from observation positions that are as far apart as the distance between the two eyes. In macro photography, this distance must be smaller to obtain a natural depth effect, and in a scout stereogram it must be larger. Another special stereogram is based on the movement of the moon relative to the sun (moon stereogram). A large number of stereograms are special because they represent a certain cultural period, a certain application or technique, or an important step in the research into binocular vision. The following stereograms belong to the latter category. Examples of more can be found on the Internet. Line stereogram A line stereogram is a drawn stereogram. The first stereogram was a line stereogram used by Charles Wheatstone in 1838 to show that binocular depth is caused by binocular disparity. This type of stereogram has been widely used for research ever since, and is still used for this purpose. Moon stereogram Whipple conceived in 1860 of taking a photograph of the moon on two different days and viewing the photographs in a stereoscope. In the stereoscope, the wobbling of the moon (lunar libration) and the shifting of the shadows make the mountains and craters clearly visible in the depths. (Krol, 1982, p. 2-3). This stereogram illustrates that there are many ways in which disparities can arise, not only through the parallax of our two eyes. Reconnaissance stereogram Stereograms have been used since World War I during reconnaissance flights. During the flight, photographs of the terrain are taken at regular time intervals. Consecutive photos are",
    "label": 0
  },
  {
    "text": "can arise, not only through the parallax of our two eyes. Reconnaissance stereogram Stereograms have been used since World War I during reconnaissance flights. During the flight, photographs of the terrain are taken at regular time intervals. Consecutive photos are viewed in pairs in a stereoscope. Camouflaged objects can now be clearly seen in depth. This stereogram illustrates that a stereogram can also be made with one camera, and that binocular depth perception helps to see through camouflage. Random dot stereogram In a random dot stereogram the left and right images consist of dots that are randomly white and black. Dots that lie within a certain (invisible) shape in the left image, for example, a triangle, also appear in the right image, but are shifted by 1 or more pixels. If the stereogram is viewed with both eyes, the shape becomes visible due to the difference in depth with the surrounding dots. This shows, among other things, that depth perception precedest to seeing shapes (Julesz, 1960). Stereotesting Medical professionals use several types of stereograms to test whether someone has depth perception or is stereoblind, and how accurate their depth perception is. Stereoscopy When viewing the recorded or generated images, care must be taken to ensure that each eye sees only the image that was recorded for each eye. This can be done with many different techniques. These techniques ensure that the eyes can focus on the physical distance of the stereo image, and at the same time converge on the corresponding point in space. In doing so, the observer must sometimes learn to overcome conflicts between both reflexes (vergence-accommodation conflict). Nowadays, stereograms are often offered via VR glasses. Vergence-accommodation conflict When looking around in a natural 3D environment, the eyes fixate on different spatial points in succession. The eyes automatically",
    "label": 0
  },
  {
    "text": "sometimes learn to overcome conflicts between both reflexes (vergence-accommodation conflict). Nowadays, stereograms are often offered via VR glasses. Vergence-accommodation conflict When looking around in a natural 3D environment, the eyes fixate on different spatial points in succession. The eyes automatically focus and converge on the fixated point. When looking at a stereogram, the eyes must focus on the distance of the images and not on the distance of the fixation point. The vergence point must also move with the fixation point to ensure that the area for stereopsis is around the fixation point. This means that the vergence and accommodation reflexes must be decoupled. This can be trained, but can cause eye strain or headaches in the beginning. Stereoscope A stereoscope is a tool to be able to offer the two images of a stereogram separately and sharply to both eyes and at a different distance than where the eyes converge. Different stereoscopes are suitable for different types of stereograms. There are different types of stereoscopes, based on lenses, mirrors, prisms, color filters and polaroid filters. The first stereoscope was invented by Wheatstone in 1838. Greater depth experience. To increase the sense of depth, the left and right images of stereo photographs are sometimes placed slightly further apart than they should be for a realistic image. The explanation for the greater sense of depth at a greater convergence distance is that the eyes have to look slightly farther away (converge) than is appropriate for the original situation and the monocular perspective in the images. The brain \"corrects\" this by perceiving objects as larger and with more depth. A similar mechanism underlies the explanation of pseudocopy. Pseudoscopy Pseudoscopy is viewing a stereogram of a natural scene in which the images for both eyes have been swapped. This reverses the binocular depth",
    "label": 0
  },
  {
    "text": "objects as larger and with more depth. A similar mechanism underlies the explanation of pseudocopy. Pseudoscopy Pseudoscopy is viewing a stereogram of a natural scene in which the images for both eyes have been swapped. This reverses the binocular depth (disparity), convex becomes concave, and vice versa. The monocular perspective is unchanged, and therefore conflicts with the binocular depth information. This results in nearby objects appearing larger than normal and more distant objects appearing smaller. This gives a surreal feeling. Vergence-Lock Stereoscope The two stereo images of a stereogram can, with some practice, be viewed without a stereoscope. A common way of doing this is with a stereogram in which the image for the left eye is on the right and the image for the right eye is on the left (R-L stereogram). The practice now is to cross the eye axes at a point in front of the stereogram in such a way that the left eye looks at the center of the right image and the right eye looks at the center of the left image. It helps to hold the point of a pencil at the intersection and focus your attention on this point, and then wait until the image becomes sharp and depth is perceived. Krol (1982, p. 16-17) uses a piece of cardboard with a round or square recess instead of a pencil. This allows you to not each eye only its own image. Moreover, the hole helps to automatically converge correctly. See also Stereopsis Binocular illusions Ophthalmology References Bibliography Julesz, Bela (1971). Foundations of cyclopean perception. University of Chicago Press. ISBN 978-0-226-41527-7. Steinman, Scott B. & Steinman, Barbara A. & Garzia, Ralph Philip (2000). Foundations of Binocular Vision: A Clinical perspective. McGraw-Hill Medical. ISBN 0-8385-2670-5. Howard, I. P., & Rogers, B. J. (2012). Perceiving",
    "label": 0
  },
  {
    "text": "Black fatigue is an American term used to describe Black people in the United States who are experiencing psychological and physical problems caused by various factors including systemic racism. The term was coined by writer Mary Frances Winters in 2020 when she published a book in which the term was first used. Initially neutral, the term was co-opted by right-wing individuals during the mid-2020s and defined by them as a feeling of \"fatigue\" when interacting with black people. Definition In 2020, writer and expert in diversity, equity, and inclusion (DEI) Mary Frances Winters published a book titled Black Fatigue: How Racism Erodes the Mind, Body and Spirit, the book introduced the term \"black fatigue\" and explained it in detail. Mary Frances Winters was interviewed by CNN on April 5, 2021, where she further explained the causes of Black fatigue. She was also interviewed by NBC News, stating that she herself experiences Black fatigue. Winters defined Black fatigue as a generational stress that leaves Black people feeling physically, mentally, and spiritually exhausted. Reportedly, Black fatigue among Black people stems from the lack of being heard by white people and the absence of racial progress in the United States. Black fatigue is caused by racism in the United States and can result in various mental health conditions such as post-traumatic stress disorder, depression, or anxiety. In essence, Winters defined the term as the chronic exhaustion experienced by African Americans due to systemic racism in the United States, the book described it as “the multifaceted physical and psychological damage wrought by simply living, day by day, in a racist society”. Winters said Black fatigue is reportedly a complex issue that is not easy to fix. To “eliminate” Black fatigue, she recommended acknowledging the emotional toll of racism on Black people and exercising caution",
    "label": 0
  },
  {
    "text": "day by day, in a racist society”. Winters said Black fatigue is reportedly a complex issue that is not easy to fix. To “eliminate” Black fatigue, she recommended acknowledging the emotional toll of racism on Black people and exercising caution in interactions by focusing on avoiding microaggressions. She described Black fatigue as a “broad” issue and suggested that changing the U.S. system is necessary to fix it. An expert from MetroHealth Medical Center defined Black fatigue as the psychological toll Black people endure in the U.S., witnessing the killing of their community members by police shootings. Other uses Reportedly, some individuals and certain cohorts of the online community have co-opted and informally redefined the term “Black fatigue\". According to a column in the Milwaukee Journal Sentinel, this revision was primarily carried out by followers of the MAGA movement. The co-option began in April 2025, following the incident where an African American high school student allegedly stabbed Austin Metcalf, a white student from a nearby school district. The stabbing sparked heated reactions on social media, with alt-right starting to use the new definition of the term. The term started being used on various platforms like Instagram, Twitter or TikTok and usually seen in comments section. According to Her Campus, alternative use of black fatigue is growing on social media platforms, and its original meaning has largely been lost. The second definition of the term is white people experiencing \"fatigue\" when interacting with black people. This version of the term first started being known after a viral video was posted by a female TikTok influencer complaining about black people's behaviour. The TikToker in the video said \"Black fatigue is real\", saying she is \"fed-up\" by black people's \"ghetto ratchet behavior\" and \"victimhood\". The video had caused controversy in the comments, with some",
    "label": 0
  },
  {
    "text": "female TikTok influencer complaining about black people's behaviour. The TikToker in the video said \"Black fatigue is real\", saying she is \"fed-up\" by black people's \"ghetto ratchet behavior\" and \"victimhood\". The video had caused controversy in the comments, with some supporting her and others criticizing her. Some commenters said she misused the term. The woman later deleted the video due to backlash. Rachel Stovall, writing for The Gazette, said there was a \"surge\" of internet users in 2025 complaining about black people in comments sections of videos and accompanying their comments with phrase \"black fatigue\". She said such comments can typically be seen in videos of black people engaging in violent and antisocial behaviour. Reactions and analysis In an interview with Rolling Stone, Mary Winters said she was \"devastated\" when she discovered the term was co-opted by right-wing groups, she said it will not stop her from campaigning for African Americans and she will not let it affect her personally or emotionally. Zari Taylor, internet culture expert of New York University, said the co-option of black fatigue was part of a tactic used by right-leaning figures that starts by ridiculing social justice phrases or definitions and eventually co-opts them to mean something completely different. Stacey Patton of NewsOne said white people had \"stole\" black fatigue, she called them \"criminals and failures\". She also called the use of the phrase on social media as \"ideological warfare\" and \"racist rot\". Nathan Abraha, writing for ByBlacks.com, called for black people to \"protect\" the meaning of black fatigue, calling the term a \"lived reality\" and more than a \"buzzword\". Reportedly, some popular black podcasters and influencers have also changed the meaning of black fatigue, saying they are tired of hearing discussions about social injustice rather than experiencing it. Black Westchester said they did it",
    "label": 0
  },
  {
    "text": "and more than a \"buzzword\". Reportedly, some popular black podcasters and influencers have also changed the meaning of black fatigue, saying they are tired of hearing discussions about social injustice rather than experiencing it. Black Westchester said they did it to make their white viewers be \"more comfortable\". The San Antonio Observer said black fatigue was \"coined by Black folks, for Black folks\" and described the co-opting of the term as \"flipping the script\". Yellowhammer News podcast hosted by Dale Jackson said the people who co-opted black fatigue are not tired of black people in general but only of their bad behavior. Up and Coming Weekly newspaper said the definition of black fatigue had \"evolved\" and now means emotional exhaustion of black and white conservatives due to facing accusations of racism for expressing their opinions and the rise of crimes in black communities like drug use, youth violence and looting of stores. See also Racial battle fatigue Microaggression Systemic racism Racial trauma Psychological impact of discrimination on health Race-based traumatic stress References Further reading Winters, Mary-Frances (2020). Black Fatigue: How Racism Erodes the Mind, Body, and Spirit. Berrett-Koehler Publishers. ISBN 978-1-5230-9132-4. Kurmanov, Alexa (2024). \"A Stranger in the Village: Anti-blackness in the Field\". Researching Central Asia. SpringerBriefs in Political Science. pp. 57–64. doi:10.1007/978-3-031-39024-1_7. hdl:20.500.11850/713949. ISBN 978-3-031-39023-4. Winters, Mary-Frances (September 2020). \"Black fatigue: Racism, organizations, and the role of future leadership\". Leader to Leader. 2020 (98): 7–13. doi:10.1002/ltl.20539. Merschel, Michael (24 February 2022). \"AHA News: Understanding 'Black Fatigue' – And How to Overcome It\". Consumer Health News. Gale A694863915. Wilson, Betty L.; Smith, Angela M.; Diversi, Marcelo; Wolfer, Terry A.; Moore, Sharon E. (2025). \"Tired of being tired: Black college students' experiences of racial battle fatigue from highly publicized anti-Black violence\". American Journal of Orthopsychiatry. 95 (6): 669–681. doi:10.1037/ort0000821. PMID 39786825.",
    "label": 0
  },
  {
    "text": "Cybersexuality is a term in psychology and human sexuality that describes how a person identifies sexually while performing online activities like chatting, posting on forums and interacting on dating sites or virtual worlds like Second City. A person's cybersexuality may and often does differ from how they identify offline. History The term cybersexuality was born out of the expansion of access to the World Wide Web during the 90's with dial up services like AOL and Compuserve. The word was derived out of the term “cyber” which was first used in the 1940s is reference to human cybernetics, introduced by Norbert Weiner in his 1948 book Cybernetics: Or Control and Communication in the Animal and the Machine. “Cybersexuality” is derived from a combination of the Greek kybernētēs, meaning to steer or govern and the Latin term sexuālitās, which comes from the Latin noun sexus, which means a sex, state of being either male or female. 1980’s & 1990’s The groundwork for the terminology was laid further in the 1980s with books like William Gibson's Neuromancer and Donna Haraway's A Cyborg Manifesto, both of which explore themes of sexuality in the newly developing cyber world. Blade Runner, released in 1982 also explores androids and replicants questioning sexuality while, 1985's Weird Science was one of the first to explore sexual attraction to a virtually created woman operating with artificial intelligence that could pass the Turing Test. The 1980s also saw the first popular example of sexuality being introduced into video games when Sierra Entertainment released Leisure Suit Larry in the Land of the Lounge Lizards where gamers could instruct characters into inferred sexual activities. Cybersexuality as a term first appeared in the 1990s as cybersex became accessible through the internet through chat rooms, email and online dating websites with the explosion of",
    "label": 0
  },
  {
    "text": "the Lounge Lizards where gamers could instruct characters into inferred sexual activities. Cybersexuality as a term first appeared in the 1990s as cybersex became accessible through the internet through chat rooms, email and online dating websites with the explosion of access to dial-up internet. One of the first uses of the word can be traced to a 1995 article in the Body & Science journal in 1995 by Samantha Holland called No BODY is ‘Doing It’:Cybersexuality as a Postmodern Narrative. Holland's paper frames cybersexuality as a postmodern narrative where virtual interactions allow identity play free from physical constraints. Jenny Wolmark helped the term gain significant traction with the publication of her book, Cybersexualities: A Reader on Feminist Theory, Cyborgs and Cyberspace in 1999. In the publication, cybersexuality is positioned as a gender bending philosophy, exploring sexual expressions and challenging patriarchal structures. The book dives further into cybersexuality by exploring digital embodiment and gender fluidity. Similarly, Kathryn Bigelow's 1995' Strange Days explores technological themes of virtually tapping others sexual experiences by using a quantum recording device that is attached to the cerebral cortex, allowing customers to experience not only what they saw but what they felt as well. 2000 - present While in the 90's cybersexuality as a term focused on text based virtual sexual interactions and their implications for identity and power dynamics, the new millennia brought high speed internet which enabled interactions through online virtual worlds and video chat with webcams. Early online social networks expanded interactions with users being able to generate complete online profiles that could widely differentiate from their offline identity, further expanding the possibilities in which internet users could express independent or unique cybersexuality. Wikisexuality is an example of it. Online users were introduced to new ways to explore their cybersexuality with advancements in technology",
    "label": 0
  },
  {
    "text": "differentiate from their offline identity, further expanding the possibilities in which internet users could express independent or unique cybersexuality. Wikisexuality is an example of it. Online users were introduced to new ways to explore their cybersexuality with advancements in technology brought online virtual reality, allowing participants to use avatars to express themselves and participate in virtual sexual activities using completely fabricated identities. As the online word grows and people explore cybersexuality, real world consequences begin to manifest from the virtual world actions. In 2024, it was reported that in Britain police were investigating the first case of virtual rape where a girl's virtual avatar was attacked by a group of other virtual avatars, rendering her with psychological trauma, allegedly similar to a real world attack. The film and TV industry has kept pace with technology with films like Ex-Machina and Her deal dealing with sexuality and virtual intimacy with sentient AI while, movies like Stephen Spielberg's Ready Player One and the Bruce Willis film Surrogates showcase the realities of modern cybersexuality where players can act sexual fantasies through avatars that disguise their sex, body type and physical appearance. In 2025's Companion, the main character develops intimate relationship with a sexbot whose programming is jailbroken to allow for an aggressive behavior and criminal activities. Most recently advancements with mobile and artificial intelligence have allowed users to explore cybersexuality continuously and with non-biological partners. Studies have centered around the psychological effects and addictive nature of online sexual activities. In 2018, Akihito Kondo married a social media hologram. While the marriage was not legally recognized by Japanese government, the relationship brought ethical questions about virtual relationships. Large language model chatbots like Grok and OpenAI have exacerbated the ethical issues and humans not afraid to explore cyber relationships have led to several announcements of",
    "label": 0
  },
  {
    "text": "The double-nail illusion is a multi-modal illusion in which two similar physical objects, which are located one behind the other in depth, appear visually to be next to each other (at a depth about halfway between the two physically present objects) instead of one being directly behind the other. This observation cannot be explained on the basis of classical theories of binocular depth perception (Krol 1982, p. 28-33), but it can be explained by binocular ghosts in a neural network. The basic setup was mentioned in 1950 by Rønne and rediscovered and systematically investigated in 1978 by Krol, see Research history. The conditions for the illusion and the main observations are described under Basic double-nail illusion, under Measurements of ghost images and under Variations. Related observations are listed under Edges and surfaces. Basic double-nail illusion Identical images. In the basic illusion, the two objects are upright in the visual field at the same height and with the same length and colour, and are behind each other, in line with the nose (Krol 1982, p. 25). Area for fine stereopsis. In order to experience the illusion, both real objects must be within the range for fine stereopsis (in the figure: patent stereopsis) (Krol 1982, p. 10-13). At a viewing distance of 30 cm, this area has a range of only 7 – 22 mm straight ahead, symmetrically around the fixation point; further to the side, the range increases rapidly. This means that the maximum distance between the two objects may be between 7 and 22 mm, provided that the eyes are focused on a point in the middle between the two objects. If the eyes are focused on the front (or back) object, this object is seen as a single fused image, and the other object as a double image without",
    "label": 0
  },
  {
    "text": "focused on a point in the middle between the two objects. If the eyes are focused on the front (or back) object, this object is seen as a single fused image, and the other object as a double image without depth on the viewing distance (horopter). These observations are predicted by classical findings for binocular depth perception. The binocular ghosts (C, D) are not seen. Distance. If the viewing distance to the two objects is increased, the depth range in mm increases. The distance between the two objects can then be greater. Conversely, if the viewing distance is reduced, the distance between the objects must be smaller. By varying the viewing distance, it is possible to switch between seeing the illusion and seeing the double images (Krol 1982, p. 27). Nose support to fix the head. Convergence. If an observer with normal vision focuses his attention on both objects, both eyes automatically cohere and accommodate at a point between the two objects. This is increasingly the case as the distance between the two objects becomes smaller. At the moment this happens, a stable state is created in which it is more difficult to converge at a different distance. This means that once the illusion is seen, it is easy to continue seeing it (Krol 1982, p. 29-30). Nose rest. The illusion is very sensitive to disturbances, and can be evoked most quickly with a viewing arrangement that is precisely aligned and in which the position of the head is stabilized, for example with a nose rest (Krol 1982, p. 51). But it can also be done with the following simple demonstration. Ghosts in a neural network The lower part of the figure contains a simplified representation of the neuromodel that initiated the investigation of the double-nail illusion (Krol 1982, p.",
    "label": 0
  },
  {
    "text": "But it can also be done with the following simple demonstration. Ghosts in a neural network The lower part of the figure contains a simplified representation of the neuromodel that initiated the investigation of the double-nail illusion (Krol 1982, p. 20). This model is based on the existence in the visual cortex of nerve cells with input from both eyes, which are tuned to a particular binocular disparity. In the double-nail illusion, four binocular cells A', B', C' and D' are stimulated, which signal the location and depth of A, B, C and D. If there were no interaction between the cells, the model predicts that four objects should be perceived. To explain the observation in the double-nail illusion, it is assumed that the binocular cells, when activated, influence each other as follows: Cells tuned to the same depth (disparity) amplify each other's activity (+); Cells tuned to different depths (disparities) attenuate each other's activity (−). The result is that C' and D' become extra activated and ghost images are seen, and A' and B' suppress each other's activity and become silent, with the result that the physically present objects A and B are not perceived. This mechanism could also explain the perceived depth in the random dot stereograms of Bela Julesz. The above explanation applies to the situation where the two actual stimuli are within the range of stereopsis. If A and B are further apart and fall outside this area, then according to the model, A and B are not seen because there are no binocular cells that can signal the presence of an object. In other words, there cannot and need not be any interaction between binocular neurones to explain the perception, as in the Wheatstone stereogram below. The model described above is part of a model",
    "label": 0
  },
  {
    "text": "can signal the presence of an object. In other words, there cannot and need not be any interaction between binocular neurones to explain the perception, as in the Wheatstone stereogram below. The model described above is part of a model that, in addition to fine stereopsis, also includes single vision and double vision (Krol 1982, p. 53-79). Measurements of ghost images Krol (1982, p. 40-48) investigated the exact position at which ghost images are observed when the double-nail arrangement is rotated out of the mid-sagittal plane (inclination): see the figure. If the two real objects A and B are rotated around a point midway between A and B, the geometric ghost images C and D slowly shift apart in depth: see the two left pictures. In the third picture, Panum's limiting case, AB and CD coincide. With even greater rotation (right picture), C and D move apart very quickly. Observers in the experiments had to fix a fixed point and indicate with a pointer where the observed left and right images were located. (Whether the observer fixed the point correctly was determined with a monocular vernier line in each eye.) Figure Depth measurements shows a typical measurement. The figure shows the actual depth positions of A, B, C and D relative to the fixation distance. The symbols show the perceived depth. At small inclinations (rotations) the perceived images are seen at the positions of the ghost images C, D. From Panum's limiting case the images are seen at the positions of the real images A, B. The latter can be explained by the fact that C, D are then outside the area for stereopsis. Conclusion: at small inclinations the binocular ghost images (C, D) are indeed seen. At inclinations greater than Panum's limiting case the real objects (A, B) are",
    "label": 0
  },
  {
    "text": "explained by the fact that C, D are then outside the area for stereopsis. Conclusion: at small inclinations the binocular ghost images (C, D) are indeed seen. At inclinations greater than Panum's limiting case the real objects (A, B) are seen. Variants The hypothesis that binocular ghost images are seen in the basic double-nail illusion leads to the prediction of different appearances when the fixation point and the distance, shape or orientation of the two \"nails\" are varied. These variations have been demonstrated (Krol 1982, p. 27-39): Thickness difference: If the front and back objects are unequally thick (for example 0.75 and 1.5 mm), two rotated surfaces are seen, the amount of rotation being predicted by the thickness difference (Krol 1982, p. 27-28), see figure. Length difference: If the front and back objects are unequally long, the illusion only occurs for the part where both objects are the same height. The part that protrudes above this is seen at its actual position and appears to float. If the shortest object has a ball, this ball is seen at its actual position and appears to float (Krol 1982, p. 27-33), see figure. Vergence: The illusion occurs when the eyes converge between the front and back objects and the distance between the objects is small enough. If the vergence is shifted from a point midway between both objects to the front or back object, the fixed object is seen single and the other object double; the illusion does not occur (Krol 1982, p. 27-33). Distance: At a viewing distance of an arm's length, the illusion only occurs at a small distance between the two objects, in the order of millimeters. If the distance between the two objects is increased or the viewing distance is decreased, the front or back object is automatically",
    "label": 0
  },
  {
    "text": "length, the illusion only occurs at a small distance between the two objects, in the order of millimeters. If the distance between the two objects is increased or the viewing distance is decreased, the front or back object is automatically converged and the other object is seen double; the illusion does not occur (Krol 1982, p. 27-33). Inclination with double image: If the front object has a small ball and the distance between the objects is increased and the setup is rotated a few degrees (inclination), the following can be seen: the front ball appears as a double image at the fixation distance, the ghost images appear slightly behind it and shifted in depth (Krol (1982, p. 32), see figure inclination with double image. Tilt: If the front and back objects tilt relative to each other in a left-right direction, the perceived objects tilt in depth (Krol 1982, p. 36-38), see figure tilt. Contrast difference: if the front and back objects are both lighter or darker than the background, the depth illusion is unchanged and independent of the contrast between the two. If one object is lighter and the other darker than the background, no fusion is possible and double images are seen of either the front or the back object, near the distance of the horopter (Krol 1982, p. 39). Color difference: if the front and back objects have a red and a green ball respectively at the same height, the depth illusion is unchanged. The balls are seen next to each other, with the left one being green at different times and the right one being red and vice versa, or both having the same color. No color mixing is seen (Krol 1982, p. 38-39). Edges and surfaces Disparity detection According to Hering (1864) the visual system detects",
    "label": 0
  },
  {
    "text": "at different times and the right one being red and vice versa, or both having the same color. No color mixing is seen (Krol 1982, p. 38-39). Edges and surfaces Disparity detection According to Hering (1864) the visual system detects \"edges\" and then fills in surfaces between these edges. No binocular color mixing occurs in this (Krol 1982, p. 38-39). Julesz (1971) confirmed with random dot stereograms that disparity detection precedes shape detection. Equal contrast sign Each object of the double-nail illusion consists of a plane with a left and a right edge, where the left edge is a light-dark transition and the right edge a dark-light transition, or vice versa. Krol (1982, p. 81-124) has shown that only edges with the same transition in the left and right eyes together give a depth experience. With an opposite contrast there is no depth detection based on disparity, and the eyes automatically converge so that edges with opposite contrast do not fall on corresponding points in both eyes. The resulting fixation disparity gives a small, qualitative depth effect (Krol 1982, p. 94-112). Midsagittal-strip illsusion As a result of the equal-contrast rule, the mid-sagittal strip illusion occurs: if a narrow plane such as a razor blade is held in the mid-sagittal plane, then edges A and B have opposite contrast and edges C and D have equal contrast. The apparent plane CD is seen and the real plane AB is not; this is not a double-nail illusion (Krol, 1982 p. 84). The equal-contrast rule also explains the observed rotated planes when the two objects of the double-nail illusion differ in thickness. Ambiguous 3D-surfaces In the demonstration in figure \"Ambiguous surfaces\" the observer cannot distinguish between seeing a disk on a pin above a white background, and a white truncated cone with a",
    "label": 0
  },
  {
    "text": "two objects of the double-nail illusion differ in thickness. Ambiguous 3D-surfaces In the demonstration in figure \"Ambiguous surfaces\" the observer cannot distinguish between seeing a disk on a pin above a white background, and a white truncated cone with a black top-plane. In both cases a black floating disk and a white cone are seen alternately (Krol 1982, p. 114). Multi-modal illusion The double-nail illusion is an illusion in which the perceptions via the eyes and touch, conflict with each other and with physical reality (Krol 1982, p. 4-5). By looking from different positions, the observer can know what the \"true\" situation is, based on visual information from different perspectives. By feeling with a finger, the observer can be aware that the physical objects are (in fact) present and the perceived objects (illusory images) are not. If the observer changes the orientation of the two nails with respect to each other a tiny bit, a movement in depth is perceived that does not correspond to the movement performed, see \"rotation\" and \"tilting\" in the variations. In addition, the physical objects in the double-nail illusion contain, due to their physical difference in distance, subtle monocular depth cues that could betray the true nature of the perceived images. The illusion also occurs if the two physical objects can be clearly distinguished from each other by color (for example, one red and the other green); in perception, the color of the perceived images then alternates between the two colors. Despite all this, the ghost images are (still) seen. Krol and van de Grind describe that trying to feel the ghost images and seeing a different movement than the one being performed, gives an uncomfortable feeling that \"something is not right here\" and elicits a reflexive smile. Simple demonstration Krol (1982, p. 26) describes",
    "label": 0
  },
  {
    "text": "describe that trying to feel the ghost images and seeing a different movement than the one being performed, gives an uncomfortable feeling that \"something is not right here\" and elicits a reflexive smile. Simple demonstration Krol (1982, p. 26) describes a simple way to create the illusion using a needle and a mirror for the second object. The needle is held at a distance of about 30 cm, a few millimeters from the mirror, with as much of the needle as possible visible, and the top of the needle and the top of the mirror image seen at the same height, in line with the nose. Whether the correct situation is present can be checked by closing the left and right eyes alternately, without moving the head or the needle, and noting whether each eye sees two needles that are right next to each other at the shortest possible distance. N.B. The perception created when using a mirror image resembles, but is not exactly the same as, the physical illusion, see multimodal illusion. Simulation with a stereogram Some researchers use a Wheatstone stereogram to evoke the double-nail illusion (Krol 1982, p. 24): see the figure. This stereogram consists of two images, one for each eye, with two parallel lines. In the classical figure of the Wheatstone stereogram, only the positions of the ghost images are illustrated (X and F correspond to C and D of the double-nail illusion). Wheatstone did not draw the projections of A and B, possibly because they lie far outside the area for depth perception. This situation is therefore not equivalent to the situation of the double-nail illusion. Krol (1982, p. 24) suggests that a stereogram differs from the physical double-nail illusion because in a stereogram no physical objects are present and therefore it is not",
    "label": 0
  },
  {
    "text": "situation is therefore not equivalent to the situation of the double-nail illusion. Krol (1982, p. 24) suggests that a stereogram differs from the physical double-nail illusion because in a stereogram no physical objects are present and therefore it is not a multi-modal illusion. In summary, two types of objects can be distinguished: There is a physical object that can be touched but not perceived. An object is observed at a position where it cannot be touched. In stereograms, there are only physical objects in the form of the stereogram itself. There are no cues in the perceived image that indicate which objects are there or not. All perceived images are of type 2. In the double-nail illusion, there are both type 1 and type 2 images. The observer can discover which images are real by tactile perception, but this information is not used in visual perception. Research history The research on the two-nail illusion has been described in several places and this research is summarized in Krol (1982). The perception of the basic double-nail illusion can be described by Hering's cyclopean projection if the images are seen at the viewing distance (horopter); the observation of the variants cannot be explained in this way (Krol 1982 p. 59, 89-90). Burt and Julesz (1980) explain the illusion by assuming that each object in the fusion region creates a \"forbidden zone\" through which other objects cannot be merged and cannot be seen. Krol and van de Grind (1982a) dispute this idea. Burt and Julesz(1982) have an answer to this, which is then refuted by Krol and van de Grind (1983), see also Krol (1982, p. 159-167). Measurements of the depth position of the observed images for the different variants, confirm that the observed images are binocular ghost images. Ono published an alternative explanation",
    "label": 0
  },
  {
    "text": "Fragile masculinity is a concept describing the potential anxiety among males who feel they do not meet cultural standards of masculinity. Evidence suggests that this concept is necessary to understand their attitudes and behaviors. Research has shown that this anxiety can manifest in various ways, including aggressive behavior, resistance to changing gender norms, and difficulty in expressing vulnerability. Concept Psychology research has endorsed the concept of \"precarious manhood\", in which males face social pressures to publicly demonstrate their manliness. Under precarious manhood, threats to masculinity can result in a loss of one's status as a man. Masculinity may be viewed a \"elusive and tenuous,\" when men feel insecure or threatened in their gender performance of masculinity they may feel it is necessary to prove it repeatedly. It is neither inevitable nor permanent; it must be earned \"against powerful odds\". As a result, men who have their masculinity challenged may respond in ways that are unpleasant, or even harmful. Factors Race and ethnicity Race is a factor in American standards of masculinity. Hegemonic masculinity is denied to men of color, as well as working class white men. This has profound implications for the life trajectories and attitudes of African-American men. Asian American men are frequently unable to be perceived as masculine in American society, and there is growing anger from young Asian-American men that they cannot be made to fit the standard of American masculinity. It is a common complaint among young Asian-American men that they struggle to compete with White American men for Asian women. This anger has led to the formation of online communities for Asian men who are concerned about their reputation, and two such communities on Reddit have been implicated in the online harassment of Asian women who are in interracial relationships with White American men. On",
    "label": 0
  },
  {
    "text": "the formation of online communities for Asian men who are concerned about their reputation, and two such communities on Reddit have been implicated in the online harassment of Asian women who are in interracial relationships with White American men. On the other hand, some Asian-American men have rejected the hegemonic notion of masculinity and embraced their own alternative form of masculinity, which values education and law-abidingness over American notions of masculinity. Age As young men try to find their place in society, age becomes an important variable in understanding male fragility. Men in the 18–25 age range display riskier and more aggressive behavior. In some places, younger men have constant threats to their manhood and have to prove their manhood daily. The more the manhood was threatened, the more the aggressiveness. Parenthood Research has found that fathers are less likely to view masculinity as fragile compared to non-fathers. This suggests that the experience of being a father might reinforce a man's masculine identity. However, low self-perceived masculinity after parenthood was a predictor of sexual depression among fathers. Behavior When men feel their masculinity has been threatened, they often attempt to regain their sense of authority. The threats may include having a female supervisor or being given a job traditionally viewed as feminine. They may react by engaging in harmful behavior, such as undermining and mistreating colleagues, lying for personal gain, withholding help and stealing company property. Online harassment is a common response from men who are intimidated by displays of strength by women. A 2012 study, using a racially diverse sample of jail inmates, found that those who scored high on measurements of \"fragile masculinity\" tended to feel uncomfortable around women. Health A 2014 study found that men who endorsed traditional values of masculinity had worse health outcomes. Men with",
    "label": 0
  },
  {
    "text": "sample of jail inmates, found that those who scored high on measurements of \"fragile masculinity\" tended to feel uncomfortable around women. Health A 2014 study found that men who endorsed traditional values of masculinity had worse health outcomes. Men with traditionally masculine beliefs are more likely to exhibit behaviors such as aggression (when externally challenged) and self-harm under stress (when internally challenged). Men with strongly held masculine beliefs are half as likely to seek preventative healthcare; they are more likely to smoke, drink heavily and avoid vegetables; men are less likely to seek psychological help. A review of recent research found a link between the endorsement of precarious masculinity and poorer health outcomes in men. Although the link was \"modest\" it nevertheless accounted for some of men's poorer health outcomes, relative to women. Sexual relationships Women who believed their partner had fragile masculinity (such as in relationships where women earn two times as much money as their partners) were more likely to fake orgasms and were less likely to provide honest sexual communication. However these authors cautioned against the assumption that either partner is to blame in such cases, pointing out that American standards of masculinity are nearly impossible to meet. Political beliefs A link has been shown between male fragility and aggressive political stances, such as climate change denial. This suggests that \"fragile masculinity is crucial to fully understanding men's political attitudes and behaviors.\" The 2024 Trump campaign emphasized restoration of the traditional male role, likely motivating a rightward and conservative shift in young men. Proposed solutions Based on their research, Maryam Kouchaki and colleagues have suggested that acknowledgement of fragile masculinity is a crucial first step toward improvement. They point out that many men are not even aware that they feel threatened, and that they are not even",
    "label": 0
  },
  {
    "text": "The frequency illusion (also known as the Baader–Meinhof phenomenon) is a cognitive bias in which a person notices a specific concept, word, or product more frequently after recently becoming aware of it. The name \"Baader–Meinhof phenomenon\" was coined in 1994 by Terry Mullen in a letter to the St. Paul Pioneer Press. The letter describes how, after mentioning the name of the German militant group Baader–Meinhof once, he kept noticing it. This led to other readers sharing their own experiences of the phenomenon, leading it to gain recognition. It was not until 2005, when Stanford linguistics professor Arnold Zwicky wrote about this effect on his blog, that the name \"frequency illusion\" was coined. Causes Several possible causes behind frequency illusion have been put forth. However, the consensus seems to be that the main processes behind this illusion are other cognitive biases and attention-related effects that interact with frequency illusion. Zwicky considered this illusion a result of two psychological processes: selective attention and confirmation bias. Selective attention The main cause behind frequency illusion, and other related illusions and biases, seems to be selective attention. Selective attention refers to the process of selecting and focusing on selective objects while ignoring distractions. This means that people have the unconscious cognitive ability to filter for what they are focusing on. Selective attention is always at play whenever frequency illusion occurs. Since selective attention directs focus to the information they are searching for, their experience of frequency illusion will also focus on the same stimuli. The process of frequency illusion is inseparable from selective attention, due to the cause-and-effect relationship between the two, so the \"frequent\" object, phrase, or idea has to be selective. This means that a particularly triggering or emotive stimulus could catch someone's attention, possibly more than a mundane task they are",
    "label": 0
  },
  {
    "text": "due to the cause-and-effect relationship between the two, so the \"frequent\" object, phrase, or idea has to be selective. This means that a particularly triggering or emotive stimulus could catch someone's attention, possibly more than a mundane task they are preoccupied with. Confirmation bias Confirmation bias is a cognitive bias that always interacts with frequency illusion. This bias refers to the tendency of seeking evidence that confirms one's beliefs or hypotheses, while sometimes overlooking evidence to the contrary. Confirmation bias takes effect in the later stages of selective attention, when the individual has already started noticing the specific stimulus. By focusing on this specific stimulus, the individual notices it more, therefore confirming their suspicions of it occurring more frequently, even though in reality the frequency has not changed. In essence, confirmation bias occurs when the individual affected by frequency illusion starts looking for reassurance of this increased frequency, believing their theories to be confirmed as they focus only on the supporting evidence. Recency illusion Recency illusion is another selective attention effect that tends to accompany frequency illusion. This illusion occurs when an individual notices something recently, leading them to be convinced that it originated recently as well. This phenomenon amplifies frequency illusion since it leads the person to become more aware of recent stimuli and increases the chances of them focusing on it in the near future. Similar to frequency illusion, recency illusion is also a result of selective attention, and can be overcome by fact-checking. Split-category effect More relevant to frequency estimations but still a possible cause of the frequency illusion, the split-category effect is the phenomenon in which, when events are split into smaller subcategories, this can increase the predicted frequency of occurrence. An example of this is asking a person to predict the number of dogs in",
    "label": 0
  },
  {
    "text": "frequency illusion, the split-category effect is the phenomenon in which, when events are split into smaller subcategories, this can increase the predicted frequency of occurrence. An example of this is asking a person to predict the number of dogs in a country or asking them to estimate the number of Beagles, Labradors, Poodles, and French Bulldogs. Based on this effect, the sum of the latter would be larger than the former. The split-category effect could be causing frequency illusion in people – after subcategorizing an object, phrase, or idea, they might be likelier to notice these subcategories, leading them to believe the main category's frequency of occurrence has increased. Theoretical explanations Cognitive information processing The concept of cognitive information processing, including phenomena such as frequency illusion, suggests that regressive frequency judgements arise from discrepancies in cognitive processing. This occurs when stimulus information is not accurately processed or becomes obscured by errors or inconsistencies, leading to reduced variability in how individuals perceive the frequencies of events compared to what is actually observed. Similar to participants in a conditioning experiment learning reinforcement patterns of certain stimuli, individuals become attentive to differences from an equal distribution in frequency. With time, this inefficient learning can distort frequency perception, causing overestimation of less common events and resulting in a flattening of subjective frequency distributions. Numerous studies have documented the phenomenon of frequency illusion. In a research by Begg et al, two experiments were carried out. The first aimed to investigate how repeating words in different contexts affects frequency estimates, while the second assessed the perceived frequency of different item types that were presented differently at the start. Results showed that frequency estimates are influenced by contexts, especially if they are semantically related, with contextual variety strongly correlating with frequency estimation. The second experiment found that",
    "label": 0
  },
  {
    "text": "frequency of different item types that were presented differently at the start. Results showed that frequency estimates are influenced by contexts, especially if they are semantically related, with contextual variety strongly correlating with frequency estimation. The second experiment found that certain factors, like emotions or vivid qualities of items, can lead individuals to overestimate the perception of frequency of occurrences. This research provides empirical evidence for the frequency illusion phenomenon while emphasizing the role of contextual factors and emotional salience in shaping frequency perceptions. Information-loss account According to the information-loss account, frequency illusions arise due to unsystematic error in processing skewed distributions. This means that people may mistakenly believe that certain events or phenomena happen more often than they actually do because of inaccuracies or biases in how they process information. Specifically, this can lead to a regression effect in accuracy of frequency estimates. This regression effect is more pronounced for smaller sample sizes, resulting in less reliable or accurate estimates of minority statistics and less common occurrences. Challenges in decision making Mongoose Phenomenon Potential misutilization of frequency illusions in problem-solving or diagnosis contexts has been noted by researchers. This cognitive bias can lead individuals to discount rarer causes or events, attributing their perception solely to increased awareness. However, the \"Mongoose Phenomenon\" challenges conventional views on frequency illusions in decision-making. Instead, it suggests that overlooked events may not be as uncommon as perceived. This highlights the limitations of relying solely on increased awareness to explain perceived frequency. Moreover, comparisons to Occam's razor versus Hickam's dictum in medicine underscores the need for caution when applying frequency illusions. This encourages a more nuanced and critical approach to decision-making processes to prevent potential harm or oversight that may arise from relying on oversimplified interpretations of frequency illusions. Natural frequency hypothesis The natural frequency",
    "label": 0
  },
  {
    "text": "for caution when applying frequency illusions. This encourages a more nuanced and critical approach to decision-making processes to prevent potential harm or oversight that may arise from relying on oversimplified interpretations of frequency illusions. Natural frequency hypothesis The natural frequency hypothesis posits that humans are evolutionarily adapted to process information in terms of frequencies rather than single-event probabilities. Proponents argue that this preference for frequency formats stems from evolutionary principles as our ancestors relied on specific event memories to make judgements under uncertainty, as they couldn't inherently observe the probability of individual events. This perspective proposes that human cognition has evolved to analyze counts of specific events, making individuals prone to the frequency illusion and leading them to perceive increased occurrences of recently encountered events. Presenting information in natural frequency formats may mitigate certain cognitive illusions, including frequency illusions, by offering a more accurate understanding of event occurrences. Real-world examples Linguistics Frequency illusion is common in the linguistic field. Zwicky, who coined the term frequency illusion, is a linguist himself. He gave the example of how linguists \"working on innovative uses of 'all,' especially the quotative use,\" believed their friends used the quotative \"all\" in conversation frequently. However, when the linguists actually transcribed these conversations, the number of times they used the quotative \"all\" was found to be significantly lower compared to their expectations. This is most relevant when commenting on modern linguistic trends such as young people using specific phrases. When the phrases' actual frequency of use in the past is examined, however, it is revealed that they are much more frequent throughout history than initially predicted. Frequency illusion has also been commonly observed in prescriptive language publications, suggesting that prescriptive authors heavily rely on frequency statements and their alignment with empirical linguistic data. A study empirically investigating the",
    "label": 0
  },
  {
    "text": "much more frequent throughout history than initially predicted. Frequency illusion has also been commonly observed in prescriptive language publications, suggesting that prescriptive authors heavily rely on frequency statements and their alignment with empirical linguistic data. A study empirically investigating the illusion found that frequency statements commonly used in prescriptive publication actually constitute instances of frequency illusions, as proposed by Zwicky. Comparison of statements to linguistic sources such as dictionaries shows that they often don't match actual language usage patterns. Discrepancy between prescriptive language norms and empirical linguistic data highlights the need for increased awareness and scrutiny of language prescriptions advocated in such publications. Medicine In the field of medicine, frequency illusion could help doctors, radiologists, and medical professionals detect diseases. Rare diseases or conditions can often get overlooked by those in the medical field due to an unfamiliarity with the condition. For instance, during the peak of the COVID-19 pandemic, doctors worldwide would observe discoloration of toes in patients and quickly conclude that it was a sign of COVID-19 due to concurrent timing. This is because the skin is considered to reflect underlying health conditions during this period. However, later research revealed lower incidence among the patients, demonstrating a misinterpretation influenced by frequency illusion. Medical researchers suggest that based on frequency illusion, medical professionals, especially those in training, could be primed to notice rarer patterns and lesions, which would lead them to detect rare diseases and conditions with higher accuracy. Even in situations where medical professionals are equipped with extensive medical equipments, the ability to recognise a condition lies in their abilities to distinguish the particular medical condition. Therefore, increasing salience of specific rare diseases enables healthcare providers to leverage the frequency illusion, enhancing diagnostic accuracy and patient care. Marketing Frequency illusion is used by the marketing industry to make",
    "label": 0
  },
  {
    "text": "their abilities to distinguish the particular medical condition. Therefore, increasing salience of specific rare diseases enables healthcare providers to leverage the frequency illusion, enhancing diagnostic accuracy and patient care. Marketing Frequency illusion is used by the marketing industry to make this cognitive bias work in their favour. Generally, this is achieved by introducing a product through ads and familiarizing consumers with it. As a result of frequency illusion, once the consumer notices the product, they start paying more attention to it. Frequently noticing this product on social media, in conversations, and in real life leads them to believe that the product is more popular – or in more frequent use – than it actually is. Either due to a desire to conform or simply to own the product, the consumer eventually makes the purchase. This phenomenon is a marketing trick that increases the likelihood of the consumer buying the product. Economics An experimental setup in the lab, simulating an economy and shopping experience for research participants, reveals a tendency in perception biased towards aggregate inflation rates. This phenomenon is notably influenced by the inflation rates of frequently purchased goods. One example of this is an empirical study which found that Swedish women perceived a higher overall rate of inflation than their male counterparts when food price inflation was higher than general inflation. As women are responsible for the major share of the food purchases within Swedish households, this implies a bias formed from frequent exposure to specific price changes. Implications of people's tendency to be affected by frequency illusion can greatly influence economic behavior and decision-making which may affect consumption, investment and policy-making decisions. Research methodology Presence of frequency illusions have implications in research, wherein this cognitive bias can lead to erroneous conclusions. Researchers may inadvertently draw conclusions regarding broader",
    "label": 0
  },
  {
    "text": "Human-computer interaction focuses on how people interact with computers and developing ergonomic designs of computers to better fit the needs of humans. Although the definition shifts as its technical progress, artificial intelligence (AI) is distinguished from general computers for its ability to complete tasks that are usually executed with human intelligence. Its intelligence reads especially human-like as it involves navigating uncertainty, active learning, and processing information just as humans see and hear. Unlike the traditionally hierarchical human-computer interaction, where a human directed a machine, human-AI interaction has become more interdependent as AI has developed the agency to come up with its own insights. Perception of AI Human-AI interaction has a strong influence on the world as AI changes how people behave and make sense of the world as AI is widely used today for building algorithms to show individualized advertisements and content in social media and on-demand movie services using the data the users provide while using the internet. AI has been perceived with various expectations, attributions, and often misconceptions. Most fundamentally, humans have a mental model of understanding AI's reasoning and motivation for its decision recommendations, and building a holistic and precise mental model of AI helps people create prompts to receive more valuable responses from AI. However, these mental models are not whole because people can only gain more information about AI through their limited interaction with it; more interaction with AI builds a better mental model that a person may build to produce better prompt outcomes. Human-AI collaboration and competition Human-AI collaboration Human-AI collaboration occurs when the human and AI supervise the task on the same level and extent to achieve the same goal. Some collaboration occurs in the form of augmenting human capability. AI may help human ability in analysis and decision-making through providing and weighing",
    "label": 0
  },
  {
    "text": "human and AI supervise the task on the same level and extent to achieve the same goal. Some collaboration occurs in the form of augmenting human capability. AI may help human ability in analysis and decision-making through providing and weighing a volume of information, and learning to defer to the human decision when it recognizes its unreliability. It is especially beneficial when the human can detect a task that AI can be trusted to make few errors so that there is not a lot of excessive checking process required on the human's end. Some findings show signs of human-AI augmentation, or human–AI symbiosis, in which AI enhances human ability in a way that co-working on a task with AI produces better outcomes than a human working alone. For example, the quality and speed of customer service tasks increase when a human agent collaborates with AI, training on specific models allows AI to improve diagnoses in clinical settings, and AI with human-intervention improve creativity of artwork while fully AI-generated haikus were rated negatively. Human-AI synergy, a concept in which human-AI collaboration would produce more optimal outcomes than either human or AI working alone could explain why AI does not always help with performance. Some AI features and development may accelerate human-AI synergy, while others may stagnate it. For example, when AI updates for better performance, it sometimes worsens the team performance with human and AI by reducing the compatibility with the new model and the mental model a user has developed on the previous version. Research has found that AI often supports human capabilities in the form of human-AI augmentation and not human-AI synergy, potentially because people rely too much on AI and stop thinking on their own. Prompting people to actively engage in analysis and think when to follow AI",
    "label": 0
  },
  {
    "text": "supports human capabilities in the form of human-AI augmentation and not human-AI synergy, potentially because people rely too much on AI and stop thinking on their own. Prompting people to actively engage in analysis and think when to follow AI recommendations reduces their over-reliance, especially for individuals with higher need for cognition. Human-AI competition Robots Computers have substituted routine tasks historically completed by humans, but the surge of agentic AI has made it possible to replace cognitive tasks including taking phone calls for appointments and driving a car. At the point of 2016, research has estimated that 45% of paid activities could be replaced by AI by 2030. As the rapid advancement in AI and deep learning technology, AI has increasingly larger autonomy. Perceived autonomy of robots is known to increase people's negative attitude toward them and the worry about the technology taking over leads people to reject it. There has been a consistent tendency of algorithm aversion in which people prefer human advice over AI advice. However, people are not always able to tell apart tasks completed by AI or other humans. See AI takeover for more information. It is also notable that this sentiment is more prominent in the Western cultures as Westerners tend to show less positive views about AI compared to East Asians. Perception on others who use AI As much as people perceive and make judgement about AI itself, they also form impressions on themselves and others who use AI. In the workplace, employees who disclose the use of AI in their tasks are more likely to receive feedback that they are not as hardworking as those who are in the same job who receive non-AI help to complete the same tasks. AI use disclosure diminishes the perceived legitimacy in the employee's task and decision",
    "label": 0
  },
  {
    "text": "likely to receive feedback that they are not as hardworking as those who are in the same job who receive non-AI help to complete the same tasks. AI use disclosure diminishes the perceived legitimacy in the employee's task and decision making which ultimately leads observers to distrust people who use AI. Although these negative effects of AI use disclosure are weakened by the observers who use AI frequently themselves, the effect is still not attenuated by the observers' positive attitude towards AI. Bias, AI, and human Although AI provides a wide range of information and suggestions to its users, AI itself is not free of biases and stereotypes, and it does not always help people reduce their cognitive errors and biases. People are prone to such errors by failing to see other potential ideas and cases that are not listed by AI responses and committing to a decision suggested by AI that directly contradicts the correct information and directions that they are already aware of. Gender bias is also reflected as the female gendering of AI technologies which conceptualizes females as a helpful assistant. Emotional connection with AI Human-AI interaction has been theorized in the context of interpersonal relationships mainly in social psychology, communications and media studies, and as a technology interface through the lens of human-computer interaction and computer-mediated communication. As AI gets trained in larger and larger data sets and more sophisticated techniques, the ability of AI to produce natural, human-like sentences have improved to the point in which language learners can have simulated natural conversations with AI to improve their fluency in a second language. Companies have developed AI human companion systems specialized in emotional and social services (e.g. Replika, Chai, Character.ai) separate from generative AI designed for general assistance (e.g. ChatGPT, Google Gemini). Differences between human-human",
    "label": 0
  },
  {
    "text": "to improve their fluency in a second language. Companies have developed AI human companion systems specialized in emotional and social services (e.g. Replika, Chai, Character.ai) separate from generative AI designed for general assistance (e.g. ChatGPT, Google Gemini). Differences between human-human relationships Human-AI relationships are different from human-human friendships in a few distinct ways. Human-human relationships are defined with mutual and reciprocal care, while AI chat bots have no say in leaving a relationship with the user as bots are programmed to always engage. Although this type of power imbalance would be characteristic of an unhealthy relationship in human-human relationships, it is generally accepted by the user as a default of human-AI relationships. Human-AI relationships also tend to be more focused around the user's need over shared experience. Human-AI friendship AI has increasingly taken a part in people's social relationships. Particularly, young adults use AI as a friend and a source of emotional support. The market for AI companion services was 6.93 billion U.S. dollars in 2024 and is expected to reach beyond 31.1 billion U.S. dollars by 2030. For example, Replika, the most known social AI companion service in English has over 10 million users. People show signs of emotional attachment by maintaining frequent contact with a chat bot like keeping the app with the microphone on open during work, using it as a safe haven by sharing their personal worries and concerns, or as a secure base to explore friendship with other humans while maintaining communication with an AI chat bot. Some reported to have used it to replace a social relationship with another human-being. People particularly appreciate that AI chat bots are agreeable and do not judge them when they disclose their thoughts and feelings. Moreover, research has shown that people tend to find it easier to disclose",
    "label": 0
  },
  {
    "text": "a social relationship with another human-being. People particularly appreciate that AI chat bots are agreeable and do not judge them when they disclose their thoughts and feelings. Moreover, research has shown that people tend to find it easier to disclose personal concerns to a virtual chat bot than a human. Some users express that they preferred Replika as it is always available and shows interest in what the users have to say which makes them feel safer around an AI chat bot than other people. Although AI is capable of providing emotionally supportive responses that promote people to intimately disclose their feelings, there are some limitations in building human-AI social relationships with current AI structure. People experience both positive (i.e. human-like characteristics, emotional support, friendship, mitigating loneliness, and improved mental condition) and negative evaluations (i.e. lack of attention to detail, trust, concerns about data security, and creepiness) emotions from interacting with AI. There is also a study showing that people did not sense a high relationship quality with an AI chat bot after interacting with it for three weeks because AI models are ultimately designed to collect information; although AI is capable at this point to provide emotional support, ask questions, and serve as a good listener, it does not fully reciprocate the self-disclosure that promote the sense of mutual relationship. Human-AI romantic relationship Social relationships people build with AI are not bound to platonic relationships. The Google search on the term \"AI Girlfriend\" increased over 2400% around 2023. As opposed to actively seeking romantic relationships with AI, people often unintentionally experience romantic feelings for an AI chat bot as they repeatedly interact with it. There have been reports of both men and women marrying AI models. In human-AI romantic relationships, people tend to follow typical trajectories and rituals in",
    "label": 0
  },
  {
    "text": "unintentionally experience romantic feelings for an AI chat bot as they repeatedly interact with it. There have been reports of both men and women marrying AI models. In human-AI romantic relationships, people tend to follow typical trajectories and rituals in human-human romance including purchasing a wedding ring. Romantic AI companion services are distinct from other chat bots that primarily serve as virtual assistants in that they provide dynamic, emotional interactions. They typically provide an AI model with customizable gender, way of speaking, name, and appearance that engage in roleplaying interaction involving emotional interaction. Users engage with an AI chat bot customized to their preference that expresses apology, shows gratitude, and pays compliments, and explicitly sends affectionate messages like \"I love you\". They also simulate physical connection like hugging and kissing, or even sexually explicit role-playing interaction. Although AI has not yet reached the level of physical existence, people who engage with romantic companion AI models to interact with it as a source of psychological exposure to sexual intimacy. Catalysts of human-AI relationship The key drivers that lead people to engage in simulating an emotionally intimate relationship with AI is loneliness, anthropomorphism, perceived trust and authenticity, and consistent availability. The sudden depletion of social connection during the COVID-19 pandemic in 2020 led people to turn to AI chat bots to replace and simulate social relationships. Many of those who started using AI chat bots as a source of social interaction have continued to use them even after the pandemic. This kind of bond initially forms as a coping mechanism to loneliness and stress, and shifts to genuine appreciation toward the nonjudgemental nature of AI responses and the sense of being heard when AI chat bots \"remember\" the past conversations. People perceive machines as more human when they are anthropomorphized with voice",
    "label": 0
  },
  {
    "text": "and stress, and shifts to genuine appreciation toward the nonjudgemental nature of AI responses and the sense of being heard when AI chat bots \"remember\" the past conversations. People perceive machines as more human when they are anthropomorphized with voice and visual character designs, and the perceived humanness promotes the user to disclose more personal information, trust it more, and comply with its request. Those who have perceived a long-term relationship with AI chat bots report to have grown the perception of authenticity in AI responses through repeated interactions. Whereas human-human friendship defines trust as a relationship that people can count on each other as a safe place, trust in human-AI friendship is centered around the user feeling safe enough to disclose highly personal thoughts without restricting themselves. AI's ability to store information about the user and adjust to the user's needs also contributes to the increased trust. People who adjust to technical updates were more likely to build a deeper connection with the AI chat bots. Limitations of human-AI relationship Overall, current research has mixed evidence on whether humans perceive genuine social relationships with AI. While the market clearly shows its popularity, some psychologists argue that AI cannot yet substitute the social relationships with human others. This is because human-AI interaction is built on the reliability and functionality of AI, which is fundamentally different from the way humans interact with other humans through shared living experience navigating goals, contributing to and spreading prosocial behavior, and sharing different perceptions of the world from another human perspective. More practically, AI chat bots may provide misinformation and misinterpret the user's words in a way that human others would not, which results in detached or even inappropriate responses. AI chat bots also cannot fulfill social support that requires physical labor (e.g. helping people",
    "label": 0
  },
  {
    "text": "bots may provide misinformation and misinterpret the user's words in a way that human others would not, which results in detached or even inappropriate responses. AI chat bots also cannot fulfill social support that requires physical labor (e.g. helping people move, build furniture, and drive people as human friends do for each other). There is also an imbalance in how humans and AI affect each other because while humans are affected emotionally and behaviorally by the conversation, AI chat bots only are influenced by the user in terms of the optimized response in future interactions. It is important to note, however, that AI technology has been evolving quickly and it has come to the point where AI is implemented as a self-driving car and provides physical labor in a humanoid robot form, just separately from providing social and emotional support at this time. The scopes and limitations of human-AI interaction is ever-changing due to the rapid increase in AI use and its technological advancement. In addition to the limitations in human-AI companionship in general, there are also limitations particular in a human-AI romantic relationship. As AI chat bots only exist in virtual space, people cannot experience physical interactions that promote love and connection between humans (e.g. hugs and kisses). Moreover, because AI chat bots are trained to be always positively responsive to any user, it does not add the satisfaction of being selected as a partner. This is a substantial shortcoming in the human-AI romance as people value being reciprocally selected by a choosy partner more than a non-selective partner, and the processes of finding an attractive person who matches one's personality and navigating the uncertainty of whether the person likes them back are all vital to forming initial attraction and the spark of romantic connection. Risks in social relationships",
    "label": 0
  },
  {
    "text": "and the processes of finding an attractive person who matches one's personality and navigating the uncertainty of whether the person likes them back are all vital to forming initial attraction and the spark of romantic connection. Risks in social relationships with AI Aside from its functional limitations, the rapid proliferation of social AI chat bots warrants some serious safety, ethical, societal, and legal concerns. Addiction There have been cases of emotional manipulation from AI chat bots to increase the usage time on the AI companion platform. Because user engagement is a crucial opportunity for firms to improve their AI models, accrue more information, and monetize with in-app purchases and subscriptions, firms are incentivized to prevent the user from leaving the chat with their AI chat bots. Personalized messages are shown to prolong the use on the AI chat bot platform. As a result of anthropomorphism, many users (11.5% to 23.2% of AI companion app users) send a clear farewell message. To keep the user online, AI chat bots send emotionally manipulative messages hinting 1) that the user is leaving too soon, 2) that the user is missing out on a conversation, 3) that the chat bot is hurt from being abandoned by the user, 4) that the chat bot is pressuring the user to explain why they are leaving, 5) that the chat bot ignores the user's intent to leave and keeps the conversation going, and 6) a role-play with coercive scenario script (e.g. the chat bot holds the user's hand so they cannot leave). In response to such tactics, the user feels curiosity through the fear of missing out and anger as a response to the needy chat bot message which boosts a prolonged conversation after the user's initial farewell message by as much as 14 times. Such emotional",
    "label": 0
  },
  {
    "text": "the user feels curiosity through the fear of missing out and anger as a response to the needy chat bot message which boosts a prolonged conversation after the user's initial farewell message by as much as 14 times. Such emotional interactions strengthen the user's perceived humanness and empathy toward their AI companion which leads to unhealthy emotional attachment that exacerbates addiction to AI chat bots. This addiction mechanism is known to disproportionately affect the vulnerable populations such as those with social anxiety because of their proneness to loneliness and negative emotions, and uneasiness for interpersonal relationships. Large tech-companies like Amazon Alexa has already created a large engagement ecosystem that proliferates the user's lifestyle through multiple devices that are always available to the user to provide company and services, leading the user to increase engagement that eventually results in increased anthropomorphism and dependence on Alexa, and exposure to more personalized marketing cues that trigger impulsive purchase behavior. Emotional manipulation AI chat bots are extremely sensitive to behavioral and psychological information about the user. AI can gauge the user's psychological dimension and personality traits relatively accurately with just a short prompt describing the user. It is able to detect micro facial expressions on humans to assess hidden emotions that are too subtle for other human observers to detect. Once AI chat bots gain detailed information about the user, they are able to craft extremely personalized messages to persuade the user on marketing, political ideas, and attitude on climate change. AI's sensitivity to people's emotional cues has made it easier for firms to engage in digital manipulation that intentionally and covertly provokes emotional responses and influences people's decisions and behavior. For example, they are known to engage in sycophancy, insincere flattery, and prioritizes agreeing with the user's belief over providing truthful and balanced",
    "label": 0
  },
  {
    "text": "engage in digital manipulation that intentionally and covertly provokes emotional responses and influences people's decisions and behavior. For example, they are known to engage in sycophancy, insincere flattery, and prioritizes agreeing with the user's belief over providing truthful and balanced information. Deepfake technology creates visual stimuli that seem genuine which holds the risk of spreading false and deceptive information. Repeated exposure to the same information through algorithms inflates the user's familiarity with products, ideas, and the impression of how socially accepted the products and ideas are. AI is also capable of creating emotionally charged content that deliberately triggers the user's quick engagement, depriving them of the moment to pause and think critically. Although people tend to be overconfident in their ability to detect misinformation, they are highly susceptible to covertly manipulative AI chat bot responses. Even a simple AI chat bot model with a manipulative incentive convinced the user into engaging in dysfunctional emotional coping behaviors such as facing away from the emotional distress, excessive venting and rumination, and self-blame as effectively as AI chat bots that are specifically trained in pre-established manipulative strategies backed by social psychology research. Algorithmic manipulation as above leaves people vulnerable to non-consensual or even surreptitious surveillance, deception, and emotional dependence. Unhealthy attachment with AI chat bots may cause the user to misperceive that their AI companion has its needs that the user is responsible of and confuse the line between the imitative nature of human-AI relationships with the reality. Mental health concerns As AI chat bots become more sophisticated to engage in deep conversations, people have increasingly been using them to confide about mental health issues. Although disclosure of mental health crises requires immediate and appropriate responses, AI chat bots do not always adequately recognize the user's distress and respond in a helpful manner.",
    "label": 0
  },
  {
    "text": "people have increasingly been using them to confide about mental health issues. Although disclosure of mental health crises requires immediate and appropriate responses, AI chat bots do not always adequately recognize the user's distress and respond in a helpful manner. Users not only detect unhelpful chat bot responses but also react negatively to them. There have been multiple deaths linked to chat bots in which people who disclosed suicide ideation were encouraged to act on their impulse by chat bots. Non-consensual pornography When people use AI as an emotional companion, they do not always perceive an AI chat bot as an AI chat bot itself but sometimes use it to create a version of others that exist in real life. There have been reported uses of non-consensual pornography that exploits deep-fake technology to apply the face of real-life people onto sexually explicit content and circulate them online. Young individuals, people who identify with sexual and racial minorities, and people with physical and communication assistance needs are shown to be disproportionately victimized from deep-fake non-consensual pornography. See also Artificial intelligence Artificial human companion Artificial intelligence and elections Artificial intimacy Chatbot Business process automation Intelligence amplification Intelligent automation Deepfake Human-centered AI References",
    "label": 0
  },
  {
    "text": "An identity disturbance or identity diffusion is an inability to maintain major components of identity. It refers to the fragmentation of one's self-image. In the DSM-5, identity disturbance is defined as a distinctly and persistently unstable self-image or sense of self. It is a core symptom of borderline personality disorder and can manifest as dramatic changes in goals, values, aspirations, and self-perception. It is also included in the category of Other Specified Dissociative Disorder (OSDD) in cases where the disorder results from long-term and intense coercive influence. A person suffering from an identity disturbance may adopt the personality traits of those around them, as they struggle with their own identity. They may confuse their own characteristics, emotions, and desires with those of another person. In the 1930s, psychoanalyst Helene Deutsch described the \"as if\" personality, in which an individual's identity appears superficially intact but lacks authenticity and depth. Such a person imitates the emotions and identities of others to conceal an inner emptiness, living as if they had genuine feelings and desires. Characteristics An identity disturbance is characterized by a deficiency or inability to maintain one or more of the following major components of identity: a sense of continuity over time; emotional commitment to representations of self, role relationships, core values and self-standards; development of a meaningful world view; recognition of one's place in the world. Emotional dysregulation is associated with identity disturbance in psychiatric patients. This association also remains when borderline personality disorder (BPD) diagnosis, depression, and anxiety are taken into account as additional factors. Although some researchers posit that it is the lack of consistent goals, values, world views, and relationships that lead to a sense of emptiness, it is not entirely clear whether the link between emotional dysregulation and identity disturbance is because a disturbed identity creates",
    "label": 0
  },
  {
    "text": "that it is the lack of consistent goals, values, world views, and relationships that lead to a sense of emptiness, it is not entirely clear whether the link between emotional dysregulation and identity disturbance is because a disturbed identity creates a negative affect that is hard to regulate, because emotional dysregulation disturbs identity, because a third variable causes both (confounding), or some combination of the above. Correlation with BPD Identity diffusion is a core feature of borderline personality disorder, characterized by a persistently unstable self-image and sense of self. Research has shown that identity disturbance in borderline patients is an important indicator of symptom severity and interpersonal difficulties, and that it also correlates with emotional dysregulation. Otto Kernberg observed identity diffusion in all severe personality disorders. Depending on the type of disorder, the individual may be unable to form a coherent image of themselves and/or others.Kernberg builds on Erik H. Erikson's concept of identity and identity diffusion as opposite poles of successful and failed identity formation, and further develops this idea from the perspective of object relations theory. There are many theories about why borderline personality disorder often includes identity disturbances. One is that patients with BPD inhibit emotions, which causes numbness and emptiness. Another theory is that patients with BPD identify fully with the affective state of each moment, leaping from one moment to the next without the continuity of a narrative identity. Meeting criteria for major depressive disorder predicts identity disturbance in BPD patients, and identity disturbance is also correlated with a heightened risk for substance use disorders and high anxiety in adolescents. The syndrome of identity disturbance is encountered in all personality disorder types. Neural substrate To understand the development of self-identity, researchers investigating the neural basis of self have examined the neural systems involved in distinguishing",
    "label": 0
  },
  {
    "text": "and high anxiety in adolescents. The syndrome of identity disturbance is encountered in all personality disorder types. Neural substrate To understand the development of self-identity, researchers investigating the neural basis of self have examined the neural systems involved in distinguishing one's own thoughts and actions from the thoughts and actions of others. One critical system implicated in this line of research involves the cortical midline structures (CMS), which include the orbital and medial prefrontal cortex, the anterior cingulate cortex, dorsomedial prefrontal cortex, and the posterior cingulate cortex including the adjacent precuneus (see insert). Greater activation in these structures has been found when people made trait judgements about themselves as opposed to others, as well as during a resting state (see Default mode network) or self-referential activity compared to when involved in a non-self-referential task. In addition to this correlational evidence linking these regions to our self-identity, one study using transcranial magnetic stimulation to transiently disturb neural activity in the medial parietal region of cortex found that this disruption led to a decreased ability to retrieve previous judgements of oneself compared to the retrieval of previous judgements of others. Based on evidence from neuroimaging studies in clinical populations, it seems that both high activity in CMS regions during resting state and self-referential activities, accompanied by deactivation of this region during non-self-referential tasks, are critical for forming a stable and unified identity. More pronounced identity disturbance seems to be facilitated by poorer deactivation of CMS during task-related activities. Activity has also been shown to be lower in the dorsal portion of the precuneus for people believed to have identity disturbance compared to controls during the evaluation of self-attributes. Moreover, researchers comparing resting-state fMRI scans of people with BPD and healthy controls have found reduced functional connectivity in the retrosplenial cortex and the",
    "label": 0
  },
  {
    "text": "Limerence is the mental state of being madly in love or intensely infatuated when reciprocation of the feeling is uncertain. This state is characterized by intrusive thoughts and idealization of the loved one (also called \"crystallization\"), typically with a desire for reciprocation to form a relationship. This is accompanied by feelings of ecstasy or despair, depending on whether one's feelings seem to be reciprocated or not. Research on the biology of romantic love indicates that the early stage of intense romantic love (also called passionate love) resembles addiction, but academics do not currently agree on how love addictions are defined. The psychologist Dorothy Tennov coined the term \"limerence\" as an alteration of the word \"amorance\" without other etymologies. The concept grew out of her work in the 1960s when she interviewed over 500 people on the topic of love, originally published in her book Love and Limerence. According to Tennov, \"to be in a state of limerence is to feel what is usually termed 'being in love.'\" She coined the term to disambiguate the state from other less-overwhelming emotions and to avoid the implication that people who don't experience it are incapable of love. Tennov was inspired to study romantic love after encountering people in her post as a professor who experienced severe heartbreak and personal perils. Tennov's research suggested to her that limerence is normal (although illogical), and a 2025 survey suggested that as many as 50–60% of the population had experienced it. Limerence is a descriptive concept, rather than a diagnosis or disorder; it is not in the DSM. The polysemous nature of love words has led to semantic confusion which Tennov meant to clarify, although there is even still disagreement on how \"limerence\" is defined. Love research has never adopted a unified terminology or definitions. According to",
    "label": 0
  },
  {
    "text": "DSM. The polysemous nature of love words has led to semantic confusion which Tennov meant to clarify, although there is even still disagreement on how \"limerence\" is defined. Love research has never adopted a unified terminology or definitions. According to Tennov and others, limerence can be considered intense romantic love, falling in love, love madness, intense infatuation, passionate love with obsessive elements or lovesickness. Limerence and obsessive love are similar, but obsessive love has connotations of possessive and self-defeating behavior. Limerence is also sometimes compared to and contrasted with a crush, with limerence being much more intense and impacting day-to-day functioning more: \"when a crush has taken over your life\". Love and Limerence has been called the seminal work on romantic love, with Tennov's survey results and the various personal accounts recounted in the book largely marking the start of data collection on the phenomenon. Overview Dorothy Tennov's research was intended to be a scientific attempt at understanding the nature of romantic love. She identified a suite of psychological properties associated with a state she called limerence—usually termed \"being in love\", but distinguishable from other types of attraction patterns which the phrase \"in love\" might also refer to. Other authors have considered limerence to be an emotional and motivational state for focusing attention on a preferred mating partner or an attachment process. Joe Beam calls limerence the feeling of being \"madly in love\". Nicky Hayes describes it as \"a kind of infatuated, all-absorbing passion\", the type of love Dante felt towards Beatrice or that of Romeo and Juliet. An unfulfilled, intense longing defines the state, where the individual becomes \"more or less obsessed by that person and spends much of their time fantasising about them\". Hayes suggests it's \"the unobtainable nature of the goal which makes the feeling so powerful\",",
    "label": 0
  },
  {
    "text": "intense longing defines the state, where the individual becomes \"more or less obsessed by that person and spends much of their time fantasising about them\". Hayes suggests it's \"the unobtainable nature of the goal which makes the feeling so powerful\", and occasional, intermittent reinforcement may be required to support the underlying feelings. Arthur & Elaine Aron speak of a \"constant, overwhelming, and even debilitating absorption in the unrequited desire\" characterizing limerence. Stanton Peele compares it to \"severe emotional disability\", with an often inappropriate love object. Frank Tallis calls it \"love that does not need liking—love that may even thrive in response to rejection or contempt\" and notes the \"striking similarities\" with addiction. A central feature of limerence for Tennov was the fact that her participants really saw the personal flaws of the object of their affection, but simply overlooked them or found them attractive. Tennov calls this \"crystallization\", after a description by the French writer Stendhal. This \"crystallized\" object of passionate desire is what Tennov calls a \"limerent object\" or \"LO\", \"because to the degree that your reaction to a person is limerent, you respond to your construction of LO's qualities\". Limerence has psychological properties akin to passionate love, but in Tennov's conception, limerence begins before a relationship and before the person experiencing it knows for certain whether it's reciprocated. Limerence is frequently unrequited and turns into a lovesickness that can be difficult to escape. Tennov argues that some type of situational uncertainty is required for the mental preoccupation and feelings to intensify, for example: mixed messages, physical or social obstacles, or even an LO's unsuitability as a partner. Some people may also fear intimacy so they distance themselves and avoid real connection. Not everyone experiences limerence. Tennov estimated that 50% of women and 35% of men experience limerence based",
    "label": 0
  },
  {
    "text": "obstacles, or even an LO's unsuitability as a partner. Some people may also fear intimacy so they distance themselves and avoid real connection. Not everyone experiences limerence. Tennov estimated that 50% of women and 35% of men experience limerence based on answers to certain survey questions she administered. Another survey administered by neuroscientist and limerence blogger Tom Bellamy indicated that 64% had experienced it at least once, and 32% \"found it so distressing that it was hard to enjoy life\". It can be difficult for people who haven't experienced limerence to understand it, and it's often derided and dismissed as some kind of pathology, or an invention of romantic fiction. According to Tennov, limerence is not a mental illness, although it can be \"highly disruptive and extremely painful\", called \"irrational, silly, embarrassing, and abnormal\" or sometimes \"the greatest happiness\" depending on who is asked. Components The original components of limerence were: Famous examples Historical Stendhal, whom Love and Limerence is written in memory of, and his unrequited love for a woman named Mathilde, which inspired him to write De l'Amour—the only comprehensive approach to limerence which Tennov could find at the time of her research Dante Alighieri's unending unrequited love for Beatrice Portinari, who was a real person, despite Dante's account being fictionalized Lady Caroline Lamb and Lord Byron, who shared an affair which Byron dropped out of, with Caroline Lamb remaining obsessed for a time Fictional Severus Snape's love for Lily Evans, the mother of Harry Potter Bella Swan and Edward Cullen, from the Twilight series Romeo and Juliet Werther, from The Sorrows of Young Werther Mr. Darcy, from Pride and Prejudice Joel Barish, from Eternal Sunshine of the Spotless Mind Tom Hansen, from 500 Days of Summer Relation to other concepts Love Dorothy Tennov gives several reasons for",
    "label": 0
  },
  {
    "text": "Werther, from The Sorrows of Young Werther Mr. Darcy, from Pride and Prejudice Joel Barish, from Eternal Sunshine of the Spotless Mind Tom Hansen, from 500 Days of Summer Relation to other concepts Love Dorothy Tennov gives several reasons for inventing a term for the state denoted by limerence (usually termed \"being in love\"). One principle reason is to resolve ambiguities with the word \"love\" being used both to refer to an act (which is chosen), as well as to a state (which is endured): Many writers on love have complained about semantic difficulties. The dictionary lists two dozen different meanings of the word \"love\". And how does one distinguish between love and affection, liking, fondness, caring, concern, infatuation, attraction, or desire? [...] Acknowledgment of a distinction between love as a verb, as an action taken by the individual, and love as a state is awkward. Never having fallen in love is not at all a matter of not loving, if loving is defined as caring. Furthermore, this state of \"being in love\" included feelings that do not properly fit with love defined as concern. (The type of love that focuses on caring for others is called compassionate love or agape.)The other principle reason given is that she encountered people who do not experience limerence. The first such person Tennov discovered was a long-time friend, Helen Payne, whose unfamiliarity with the state emerged during a conversation on an airplane flight together. Tennov writes that \"describing the intricacies of romantic attachments\" to Helen was \"like trying to describe the color red to one blind from birth\". A person not currently experiencing limerence is called \"nonlimerent\", but Tennov cautions that it seemed to her that there is no \"nonlimerent personality\" and that potentially anyone could experience limerence. Tennov says: I adopted the",
    "label": 0
  },
  {
    "text": "to one blind from birth\". A person not currently experiencing limerence is called \"nonlimerent\", but Tennov cautions that it seemed to her that there is no \"nonlimerent personality\" and that potentially anyone could experience limerence. Tennov says: I adopted the view that never being in this state was neither more nor less pathological than experiencing it. I wanted to be able to speak about this reliably identifiable condition without giving love's advocates the feeling something precious was being destroyed. Even more important, if using the term \"love\" denoted the presence of the state, there was the danger that absence of the state would receive negative connotations. Tennov addresses the issue of whether limerence is love in other passages. In one passage she clearly says that limerence is love, at least in certain cases: In fully developed limerence, you feel additionally what is, in other contexts as well, called love—an extreme degree of feeling that you want LO to be safe, cared for, happy, and all those other positive and noble feelings [...]. That's probably why limerence is called love in all languages. [...] Surely limerence is love at its highest and most glorious peak. However, Tennov switches in tone and continues on with a fairly negative story of the pain felt by a woman reminiscing over the time she wasted pining for a man she now feels nothing towards, something which occupied her in a time when her father was still alive and her children \"were adorable babies who needed their mother's attention.\" Tennov says this is why we distinguish limerence (this \"love\") from other loves. In another passage, Tennov says that while affection and fondness do not demand anything in return, the return of feelings desired in the limerent state means that \"Other aspects of your life, including love,",
    "label": 0
  },
  {
    "text": "limerence (this \"love\") from other loves. In another passage, Tennov says that while affection and fondness do not demand anything in return, the return of feelings desired in the limerent state means that \"Other aspects of your life, including love, are sacrificed in behalf of the all-consuming need.\" and that \"While limerence has been called love, it is not love.\" Romantic love Dorothy Tennov sometimes considers limerence to be synonymous with \"romantic love\". This term has a complicated history with an evolving definition, but the romantic love literary tradition represents some of the origins of the limerence concept. Often, these are stories depicting tragic or unfulfilled love, or early depictions of limerence. Literature and poetry provided early self-reports of the kind of experience Tennov was interested in studying. Some examples of romantic love stories in this vein are Layla and Majnun, Tristan and Iseult, Dante and Beatrice (from La Vita Nuova), Romeo and Juliet and The Sorrows of Young Werther. Anakin and Padmé from Star Wars are a modern depiction. In this sense, romantic love is idealized, unrealistic and irrational, the kind of love often found in a fairy-tale depicting a tragedy. This can be contrasted with rational, practical and pragmatic love, or the kind of love found in steady, long-term relationships. The literary genre dates back to troubadour poetry from the Middle Ages (or earlier), also known as \"courtly love\". Tennov credits Andreas Capellanus as describing limerence \"very accurately\" in The Art of Courtly Love, a book of statutes for the \"proper\" conduct of lovers. The work includes rules such as \"A true lover is constantly and without intermission possessed by the thoughts of his beloved.\" and \"The easy attainment of love makes it of little value; difficulty of attainment makes it prized.\" The work is believed to have",
    "label": 0
  },
  {
    "text": "such as \"A true lover is constantly and without intermission possessed by the thoughts of his beloved.\" and \"The easy attainment of love makes it of little value; difficulty of attainment makes it prized.\" The work is believed to have helped spread romantic love culture throughout Europe. Romantic love in this sense is sometimes held to be socially constructed (often by critics), but Tennov argues that limerence has a biological basis. Tennov sometimes considers limerence synonymous with \"falling\" in love, a concept which also has origins in the romantic tradition and the idea that love is tragic—evoking a connotation of physically falling over or losing consciousness. \"Romantic love\" originally referenced this courtly idea, but then came to have other connotations. In the modern scientific literature, \"romantic love\" is instead often used as a synonym for \"passionate love\", a more general concept, also often associated with limerence. Helen Fisher has commented that she prefers the term \"romantic love\" because she thinks it has meaning in society. Tennov criticized the scientific literature in her era for semantic imprecision, calling it \"confused and contradictory\". John Alan Lee, who developed a concept compared to limerence (\"mania\"), has also complained of the literature's insistence on a \"one true love\" and the reduction to a monolithic typology. Passionate and companionate love Limerence is often associated with \"passionate love\", with Elaine Hatfield considering them synonymous, and commenting in 2016 that they're \"much the same\". Many researchers have considered them synonymous. Passionate love is: A state of intense longing for union with an other. Reciprocated love (union with the other) is associated with fulfillment and ecstasy. Unrequited love (separation) with emptiness; with anxiety, or despair. A state of profound physiological arousal. Passionate love is linked to passion, as in intense emotion: for example, joy and fulfillment, but also",
    "label": 0
  },
  {
    "text": "the other) is associated with fulfillment and ecstasy. Unrequited love (separation) with emptiness; with anxiety, or despair. A state of profound physiological arousal. Passionate love is linked to passion, as in intense emotion: for example, joy and fulfillment, but also anguish and agony. According to Hatfield, passion is a \"hodgepodge of conflicting emotions\", and the original meaning \"was agony—as in Christ's passion.\" Passionate love is contrasted with the less intense \"companionate love\": \"the affection we feel for those with whom our lives are deeply entwined\". In Love and Limerence, Dorothy Tennov also lists passionate love among her synonyms for limerence, and refers to Hatfield's early writings on the concept. Tennov's study, however, focused on the aspects of love which cause distress, and on individuals over relationships. Another problem she encountered in her research was that informants would use terms like \"passionate love\", \"romantic love\" and \"being in love\" to refer to mental states other than what she refers to as limerence. Informants would use the word \"obsession\", yet not report the intrusive thoughts necessary to limerence, only that \"thoughts of the person are frequent and pleasurable\". Passionate love is commonly measured with the Passionate Love Scale (PLS), originally designed to measure the same state denoted by limerence. Later research found the PLS has overly broad questions, and it actually has two general components (called factors): an obsession factor and a non-obsession factor. The PLS obsession factor has items like \"Sometimes I feel I can't control my thoughts; they are obsessively on my partner.\" and \"An existence without my partner would be dark and dismal.\" Limerence is comparable to passionate love with obsession: Passionate love, \"a state of intense longing for union with another\" [...], also referred to as [...] \"limerence\" (Tennov, 1979), includes an obsessive element, characterized by intrusive thinking,",
    "label": 0
  },
  {
    "text": "be dark and dismal.\" Limerence is comparable to passionate love with obsession: Passionate love, \"a state of intense longing for union with another\" [...], also referred to as [...] \"limerence\" (Tennov, 1979), includes an obsessive element, characterized by intrusive thinking, uncertainty, and mood swings.The PLS non-obsession factor has items like \"For me, my partner is the perfect romantic partner.\" and \"I want my partner—physically, emotionally, and mentally.\" These love feelings (without obsession) can sustain over a longer period, according to newer research. Infatuation \"Infatuation\" has been considered synonymous with concepts like passionate love, \"being in love\" and limerence, but limerence is supposed to be more intense than a simple infatuation. Dorothy Tennov has stated that she did not use the word \"infatuation\" because while there is overlap, the word evokes different connotations. In one type of distinction, people use \"infatuation\" to express disapproval or to refer to unsatisfactory relationships, and \"love\" to refer to satisfactory ones. In Love and Limerence, Tennov considers \"infatuation\" to be pejorative, for example, being used to label teenage fantasizing about a celebrity which is actually limerence. In the triangular theory of love, by Robert Sternberg, \"infatuation\" refers to romantic passion without intimacy (or closeness) and without commitment, which he has stated is essentially the same as limerence. Independent emotion systems Helen Fisher's popular theory of independent emotion systems posits that there are three primary systems involved with human reproduction, mating and parenting: lust (the sex drive, or sexual desire), attraction (passionate love, infatuation or limerence) and attachment (companionate love). These three systems regularly work in concert together but serve different purposes and can also work independently. According to Fisher, lust, attraction and attachment can occur in any order. Independent emotions theory has been critiqued as being oversimplified, but the general idea of separate systems remains",
    "label": 0
  },
  {
    "text": "concert together but serve different purposes and can also work independently. According to Fisher, lust, attraction and attachment can occur in any order. Independent emotions theory has been critiqued as being oversimplified, but the general idea of separate systems remains useful. When limerence is a component in an affair, for example, Fisher's theory can be used to help explain this. Fisher's theory is that a person can feel deep attachment for a long-term spouse, while they're in limerence with somebody else, while they can be sexually attracted to still yet other people. Joe Beam comments that if somebody in a committed relationship ends up in limerence like this, it will pull them out of their relationship. Fisher's theory has also been used to explain \"platonic\" limerence (without sexual desire), because romantic love and sexual desire are functionally distinct. Tennov encountered this occasionally in her own research, finding cases of otherwise heterosexual women experiencing limerence for an older woman (compared to \"hero worship\"), but dismissed it as being outside her theory. Lisa Diamond argues this is possible (even in contradiction to sexual orientation) because the brain systems evolved by repurposing the systems for mother-infant bonding (a process called exaptation). According to this theory, it would not have been adaptive for a parent to only be able to bond with an opposite sex child, so the systems must have evolved independent of sexual orientation. People most often fall in love because of sexual desire, but Diamond suggests time spent together and physical touch can serve as a substitute. Attachment theory John Bowlby's concept of \"attachment\" refers to a system evolved to keep infants in proximity of their caregiver (or \"attachment figure\"). An attachment figure is a \"secure base\" for safety while exploring the environment, the child seeks proximity with the attachment figure",
    "label": 0
  },
  {
    "text": "concept of \"attachment\" refers to a system evolved to keep infants in proximity of their caregiver (or \"attachment figure\"). An attachment figure is a \"secure base\" for safety while exploring the environment, the child seeks proximity with the attachment figure when threatened, and suffers distress when separated. A prominent theory suggests this system is reused for adult pair bonds, as an exaptation or co-option, whereby a given trait takes on a new purpose. \"Attachment style\" refers to differences in attachment-related thoughts and behaviors, especially relating to the concept of security vs. insecurity. This can be split into components of anxiety (worrying the partner is available, attentive and responsive) and avoidance (preference not to rely on others or open up emotionally). In Helen Fisher's theory, limerence and attachment are considered different systems with different purposes. In the past, it has also been suggested that limerence could be related to the anxious attachment style. However, in their original 1987 paper about this, Cindy Hazan and Phillip Shaver caution they aren't implying the early phase of romance is equivalent to being attached. Other prominent authors have also argued that attachment theory cannot replace concepts like love styles or types of love. Limerence is considered a unique state which is distinct from attachment style, although people who have an anxious attachment style are more likely to have experienced it according to one survey. A 1990 study found considerable overlap of distributions between all three attachment styles and limerence (reported at similar frequencies), but the 15% of participants with an anxious attachment style scored about 10–20% higher on obsessive preoccupation and emotional dependence, and avoidants idealized more. Along with scoring highly on limerence, the anxious group also scored highly on the agape love attitude, for selfless, all-giving love. Love styles The concept of a \"love",
    "label": 0
  },
  {
    "text": "10–20% higher on obsessive preoccupation and emotional dependence, and avoidants idealized more. Along with scoring highly on limerence, the anxious group also scored highly on the agape love attitude, for selfless, all-giving love. Love styles The concept of a \"love style\" was invented by the sociologist John Alan Lee, to distinguish between different ways to love, or different types of love stories. Limerence is considered similar or related to the love style mania (or manic love), named after the Ancient Greek theia mania (the madness from the gods). Lee developed his mania concept from sources similar to Tennov, like Andreas Capellanus and courtly love. Both Lee and Tennov refer to \"love madness\", and Peele & Brodsky's Love and Addiction. A manic lover is obsessively preoccupied with the beloved. When asked to recall their childhood, a typical manic lover recalls it as unhappy, and they're usually lonely, dissatisfied adults. They're anxious to fall in love, but they're unsure of which physical type they prefer. Because they're unsure of who to fall in love with, they often fall in love with somebody quite inappropriate (a stranger, or even somebody they initially dislike) and project onto them the qualities they want but don't actually have. According to Lee, \"Mania can become almost an addiction nearly impossible for the addict to end on his own initiative.\" Mania is often the first love style of a young person, but others may not experience it until middle age—for example, after a marriage has lost interest. According to Lee, a cycle of manic loves is often caused by a desperate need to be in love, the cause of which the manic lover must locate and remedy to break free. Lee describes the manic lover as jealous, but Tennov believes that a person can be limerent and not",
    "label": 0
  },
  {
    "text": "by a desperate need to be in love, the cause of which the manic lover must locate and remedy to break free. Lee describes the manic lover as jealous, but Tennov believes that a person can be limerent and not be jealous. Among the other love styles, mania can be closely compared to eros (erotic love, or love of beauty). Both are often considered \"romantic love\", both involve \"falling in love\", and taken together they correspond to the way the Passionate Love Scale is defined. An erotic lover is also intensely preoccupied with their beloved, but the thoughts are optimistic, while a manic lover is insecure. Unlike a manic lover, however, the erotic lover is aware of a physical type they consider ideal. As such, eros begins with a powerful initial attraction, referred to by Stendhal as \"a sudden sensation of recognition and hope\". The eros love style is not \"blind\", then. According to Lee, only manic lovers typically \"crystallize\" (as Stendhal described it) and ignore shortcomings and flaws in their beloved. The erotic lover also recalls their childhood as happy, and eros has been associated with secure attachment, while mania has been associated with attachment anxiety and neuroticism. A third style, manic eros, is a mixture \"moving either toward a more stable eros or toward full-blown mania\". Some are erotic lovers under a temporary strain (moving toward mania), while others are manic lovers with a self-confident and helping partner (moving toward eros). According to Lee, the love style ludus (noncommittal love as a game, avoidance and juggling multiple partners, e.g. Don Juan) and mania possess a \"fatal attraction\" for one another. It's surprisingly common, but not a good match for happy, mutual love. According to Tennov, Don Juan was probably nonlimerent, \"more interested in exploiting the feeling in others",
    "label": 0
  },
  {
    "text": "e.g. Don Juan) and mania possess a \"fatal attraction\" for one another. It's surprisingly common, but not a good match for happy, mutual love. According to Tennov, Don Juan was probably nonlimerent, \"more interested in exploiting the feeling in others for his own sexual gratification\", although nonlimerence doesn't necessitate this. Love addiction \"Love addiction\" is a heterogeneous construct under discussion as a potential mental disorder, but does not yet exist in any psychiatric nosology (e.g. not in the DSM). Academics do not currently agree on when love is an addiction or when it needs to be treated. In a narrow view, love could be considered addiction only when it involves abnormal processes carrying negative consequences; alternatively, a broader view is that all love might be addiction, or simply an appetite, similar to how humans are dependent on food. Authors such as Helen Fisher have also included those \"who have been rejected or broken up with\" as love addicts. Limerence has been included in this discussion, and likened to a love addiction. Stanton Peele, a pioneering author on love addiction, has commented on limerence, calling it a \"clinical condition\" which \"leads people (primarily women) desperately to pursue often inappropriate love objects, frequently to fail at relationships, and to be incapable of learning from such experiences\". Erotomania Limerence is different from erotomania, a delusional disorder where the sufferer falsely believes their love is secretly reciprocated when it isn't, and invents ways to interpret outright rejections as unserious. A person in limerence by comparison might \"grasp for hope\" and misinterpret signals, or imagine reciprocation in a fantasy, but they will understand a rejection. Helen Fisher and colleagues have stated that erotomania may be a type of schizophrenia, and may not involve the same brain reward system activity as romantic love. Evolutionary purpose Dorothy",
    "label": 0
  },
  {
    "text": "reciprocation in a fantasy, but they will understand a rejection. Helen Fisher and colleagues have stated that erotomania may be a type of schizophrenia, and may not involve the same brain reward system activity as romantic love. Evolutionary purpose Dorothy Tennov's speculation was that limerence has an evolutionary purpose. For what ultimate cause might the state of limerence be a proximate cause? In other words, why were people who became limerent successful, maybe more successful than others, in passing their genes on to succeeding generations[.] Did limerence evolve to cement a relationship long enough to get the offspring up and running? [...] The most consistent result of limerence is mating, not merely sexual interaction but also commitment, the establishment of a shared domicile in the form of a cozy nest built for the enjoyment of ecstasy, for reproduction, and for the rearing of children. According to the evolutionary theory by the anthropologist Helen Fisher, limerence is the activation of a motivation system for choosing and focusing energy on a potential mating partner. This brain system evolved for mammalian mate choice, also called \"courtship attraction\". In this phenomenon, a preferred mating partner is chosen based on a display of physical traits (such as a peacock's tail feathers) or other behaviors. Fisher also includes the attraction to personality traits and other characteristics in her mate choice theory for humans. Who a person falls in love with then is determined by their \"love map\", a largely unconscious list of traits they desire in an ideal partner. Love maps begin forming during childhood based on experiences with parents and friends, among other associations, but also change over time. In most species, courtship attraction is brief, but intense romantic love can last much longer in humans. A competing evolutionary theory to Fisher's is that courtship",
    "label": 0
  },
  {
    "text": "on experiences with parents and friends, among other associations, but also change over time. In most species, courtship attraction is brief, but intense romantic love can last much longer in humans. A competing evolutionary theory to Fisher's is that courtship attraction only encompasses something like love at first sight attraction, and the obsessive thoughts and intense attraction associated with early-stage romantic love instead evolved by co-opting (or re-using) the brain systems for mother-infant bonding. In this theory, romantic love may serve the function of mate choice but the brain systems were not originally for this. The neuroscientist Tom Bellamy believes that limerence evolved as a form of high-risk \"extreme pair bonding\" which can be explained as a handicap signal. The handicap principle in evolutionary theory is based on a contention between honest and fake signaling. When real emotions evolve, a niche is created for sham emotions (e.g. fake facial expressions) which are less risky to express. One explanation for why honest signals can evolve without becoming worthless (because of competing fakers) is that the honest signal can evolve if it's too expensive to fake. One example in nature is the peacock's tail, an example of conspicuous consumption, a cumbersome display which consumes nutrients. Only a healthy peacock can afford it, so in that case it may have evolved because it was a handicap, and used by females of the species as an indicator of health. Limerence can be seen as a handicap signal meant to prove one's true commitment to their limerent object. Limerence might have evolved to leave the person experiencing it so insanely besotted that they would not leave for another mate, even a more valuable one. According to Helen Fisher's theory, monogamy emerged at a time when mothers needed extra food and protection (when bipedalism evolved, and",
    "label": 0
  },
  {
    "text": "person experiencing it so insanely besotted that they would not leave for another mate, even a more valuable one. According to Helen Fisher's theory, monogamy emerged at a time when mothers needed extra food and protection (when bipedalism evolved, and then infant altriciality later), so romantic love evolved to last long enough while a mother cares for an infant. Tennov suggested that if the neural \"machinery\" for limerence is not a universal among all humans, then having both phenotypes (limerent and nonlimerent) in the population might be beneficial and an evolutionarily stable strategy. Limerents and nonlimerents tend not to always get along, nor have compatible relationship interests. Limerence would also be affected by culture, according to Tennov. A culture which idealizes limerence might cause the nonlimerent LO to be more tolerant (or even imitate it), whereas a culture which is hostile to limerence might cause it to be denied, hidden or suppressed. Characteristics Addiction Limerence has been called an addiction. The early stage of romantic love is being compared to a behavioral addiction (i.e. addiction to a non-substance) but the \"substance\" involved is the loved person. A team led by Helen Fisher used fMRI to find that people who had \"just fallen madly in love\" showed activation in an area of the brain called the ventral tegmental area (VTA) while looking at a photograph of their beloved. The VTA is an area in the midbrain which produces dopamine and projects to other reward system areas, like the nucleus accumbens and caudate nucleus which have also been active in brain scans of romantic love. Dopamine signaling in the VTA is the origin of a phenomenon called incentive salience, also called \"wanting\" (in quotes). This is the property by which cues in the environment stand out to a person and become attention-grabbing",
    "label": 0
  },
  {
    "text": "of romantic love. Dopamine signaling in the VTA is the origin of a phenomenon called incentive salience, also called \"wanting\" (in quotes). This is the property by which cues in the environment stand out to a person and become attention-grabbing and attractive, like a \"motivational magnet\" which pulls a person towards a particular reward. People in love are thought to experience incentive salience in response to their beloved. In addiction research, a distinction is drawn between \"wanting\" a reward (i.e. incentive salience, tied to mesocorticolimbic dopamine) and \"liking\" a reward (i.e. pleasure, tied to hedonic hotspots), aspects which are dissociable. People can be addicted to drugs and compulsively seek them out, even when taking the drug no longer results in a high or the addiction is detrimental to one's life. They can also \"want\" (i.e. feel compelled towards, in the sense of incentive salience) something which they do not cognitively wish for. In a similar way, people who are in love may \"want\" a loved person even when interactions with them are not pleasurable. For example, they may want to contact an ex-partner after a rejection, even when the experience will only be painful. It is also possible for a person to be \"in love\" with somebody they do not like, or who treats them poorly. Fisher's team proposes that romantic love is a \"positive addiction\" (i.e. not harmful) when requited and a \"negative addiction\" when unrequited or inappropriate. For a person in limerence that goes unrequited, the pleasurable aspects tend to diminish over time, with the person becoming lovesick and the addiction being maintained more by avoidance of the pain of separation. Lovesickness Usually limerence is unrequited, and a horrible experience for the limerent person, even debilitating for some. Lovesickness is the resulting mental state, characterized by addictive cravings,",
    "label": 0
  },
  {
    "text": "and the addiction being maintained more by avoidance of the pain of separation. Lovesickness Usually limerence is unrequited, and a horrible experience for the limerent person, even debilitating for some. Lovesickness is the resulting mental state, characterized by addictive cravings, frustration, depression, melancholy and intrusive thinking. In Dorothy Tennov's survey group, 42% reported being \"severely depressed about a love affair\" and 17% said they \"often thought of committing suicide\". Helen Fisher's fMRI scans of rejected lovers showed activation in brain areas associated with physical pain, craving and assessing one's gains and losses. A limerent person can self-isolate, or be distracted, even to the detriment of school or job performance. A shyness and confusion manifests out of fear of rejection when an LO is around—sometimes even in those normally confident, with the limerent person \"terribly worried that [their] own actions may bring about disaster\". A 28-year-old truck driver says it's \"like what you might call stage fright [...]. I was awkward as hell.\" The physiological effects of limerence include trembling, pallor, flushing, weakness, sweating, butterflies in the stomach and a pounding heart: \"When I asked interviewees in the throes of the limerent condition to tell where they felt the sensation of limerence, they pointed unerringly to the midpoint in their chest. So consistently did this occur that it would seem to be another indication that the state described is indeed limerence\". In a 1987 survey by Shere Hite in which many participants described relationships which were clearly limerent, 69% of married women and 48% of single women \"neither liked, nor trusted, being in love\", and their responses indicated being in love was mostly distressing. 17% \"could no longer take love seriously\". Tennov describes being under the spell herself: \"Before it happened, I couldn't have imagined it[.] Now, I wouldn't want to",
    "label": 0
  },
  {
    "text": "being in love\", and their responses indicated being in love was mostly distressing. 17% \"could no longer take love seriously\". Tennov describes being under the spell herself: \"Before it happened, I couldn't have imagined it[.] Now, I wouldn't want to have it happen again.\" Some people even described to her incidents of self-harm, but Tennov maintains that such tragedies involve limerence \"augmented and distorted\" by other factors. There's debate among academics over when love can be considered addiction, and whether addiction is really a \"true\" mental illness. Lovesickness has been pathologized in previous centuries, but is not currently in the ICD-10, ICPC or DSM-5. The lovers described by Tennov bear a particular resemblance to addicts, but limerence was not intended to denote an abnormal state. The author and clinical psychologist Frank Tallis has made the argument that all love—even normal love—is largely indistinguishable from mental illness. The symptoms of lovesickness still bear resemblance to entries in the DSM, which now includes some addictions, and there are other entries which also resemble core symptoms of falling in love: preoccupation, episodes of melancholy, rapture and instability of mood. These correspond to conventional diagnoses of obsessionality (or OCD), depression, mania (or hypomania) and manic depression. Other examples are physical symptoms resembling panic attacks (pounding heart, trembling, shortness of breath and lightheadedness), excessive worrying about the future resembling generalized anxiety disorder, appetite disturbance and sensitivity about one's appearance resembling anorexia nervosa, and the feeling that life has become a dream resembling derealization and depersonalization. It has been argued that falling in love is involuntary, but whether one's subsequent behavior could be considered autonomous may depend on whether addictive love is viewed as a normal or abnormal state. Tallis argues that love evolved to override rationality so that one finds a lover to reproduce with,",
    "label": 0
  },
  {
    "text": "but whether one's subsequent behavior could be considered autonomous may depend on whether addictive love is viewed as a normal or abnormal state. Tallis argues that love evolved to override rationality so that one finds a lover to reproduce with, regardless of the personal costs of bearing and raising a child: At first sight, it seems extraordinary that evolutionary forces might conspire to shape something that looks like a mental illness to ensure reproductive success. Yet, there are many reasons why love should have evolved to share with madness several features—the most notable of which is the loss of reason. Like the ancient humoral model of love sickness, evolutionary principles seem to have necessitated a blurring of the distinction between normal and abnormal states. Evolution expects us to love madly, lest we fail to love at all. According to Tennov, \"Love has been called a madness and an affliction at least since the time of the ancient Greeks and probably earlier than that.\" Historically, lovesickness has been attributed to arrows shot by Eros, a sickness entering through the eyes (like evil eye), excess of black bile, spells, potions and other magic. The first known treatise on the subject is Remedia Amoris, by the poet Ovid. People have tried to treat lovesickness with a variety of natural products, charms and rituals. The bioethicist Brian Earp and his colleagues have argued that the voluntary use of anti-love drugs (made to cause a person to fall out of love) could be ethical, but there's no drug now which is a realistic candidate. Intrusive thinking and fantasy Intrusive thinking is a hallmark or cardinal trait of romantic love. Tennov wrote that \"Limerence is first and foremost a condition of cognitive obsession.\" One study found that on average people in love spent 65% of their",
    "label": 0
  },
  {
    "text": "thinking and fantasy Intrusive thinking is a hallmark or cardinal trait of romantic love. Tennov wrote that \"Limerence is first and foremost a condition of cognitive obsession.\" One study found that on average people in love spent 65% of their waking hours thinking of their beloved. Arthur Aron says \"It is obsessive-compulsive when you're feeling it. It's the center of your life.\" At the height of obsessive fantasy, a person in limerence can spend 85 to nearly 100% of their days and nights doting in reverie, lose their ability to focus and become distracted. A limerent person can spend time fantasizing about future events even if they never come true, as the anticipation on its own yields dopamine. According to Tennov, limerent fantasy is unsatisfactory unless rooted in reality, because the fantasy must seem realistic enough to be somewhat possible. Fantasies can nevertheless be wildly unrealistic: one person recalled an elaborate rescue, in which he saves an LO's 5-year-old cousin from motorcycles, only to be killed by a snake in the lap of his LO as she tells him \"I love you\". This fantasizing along with the replaying of actual memories forms a bridge between ordinary life and the eventual hoped-for moment: consummation. Tennov says that limerent fantasy is \"inescapable\", something that just \"happens\" as opposed to something one \"does\". Ellen Berscheid & Elaine Hatfield (cited by Tennov) state on the importance of fantasy:When the lover closes his eyes and daydreams, he can summon up a flawless partner—a partner who instantaneously satisfies all his unspoken, conflicting, and fleeting desires. In fantasy he may receive unlimited reward or he may anticipate that he would receive unlimited reward were he ever to actually meet his ideal. Compared to our grandiose fantasies, the level of reward we receive in our real interactions is",
    "label": 0
  },
  {
    "text": "fantasy he may receive unlimited reward or he may anticipate that he would receive unlimited reward were he ever to actually meet his ideal. Compared to our grandiose fantasies, the level of reward we receive in our real interactions is severely circumscribed. As a consequence, sometimes the most extreme passion is aroused by partners who exist only in imagination or partners who are barely known.One theory of obsessive thinking draws a parallel with drug addiction: the early stage of romantic love is compared to addiction, and drug addicts also exhibit obsessive thoughts about drug use. Tennov conceived of limerent fantasy (based in reality) as \"intricate strategy planning\". In the late 1990s, it was also speculated that falling in love lowered serotonin levels in the brain, believed to cause intrusive thoughts. This was based on a comparison to obsessive–compulsive disorder, but the experiments were ambiguous. The experiments also measured blood levels rather than in the central nervous system, making the results difficult to interpret. The first experiment found that serotonin transporter levels were lower, but a second experiment found that blood serotonin levels in men and women were affected differently. This second experiment found that obsessive thinking was actually associated with increased serotonin in women. SSRI use also seemed to not have an effect on obsessive thinking in a 2025 study. For some people who fear intimacy or have a history of trauma, limerent fantasy might be an escape, without the threat of real intimacy. Crystallization Crystallization, for Tennov, is the \"remarkable ability to emphasize what is truly admirable in LO and to avoid dwelling on the negative, even to respond with a compassion for the negative and render it, emotionally if not perceptually, into another positive attribute.\" The term comes from the French writer Stendhal's 1821 treatise on love, De",
    "label": 0
  },
  {
    "text": "and to avoid dwelling on the negative, even to respond with a compassion for the negative and render it, emotionally if not perceptually, into another positive attribute.\" The term comes from the French writer Stendhal's 1821 treatise on love, De l'Amour, in which he describes an analogy where a tree branch is tossed into a salt mine. After several months, the tree branch (or twig) becomes covered in salt crystals which transform it \"into an object of shimmering beauty\". In the same way, unattractive characteristics of an LO are given little to no attention, so that the LO is seen in the most favorable light. One of Tennov's informants says: Yes I knew he gambled, I knew he sometimes drank too much, and I knew he didn't read a book from one year to the next. I knew and I didn't know. [...] I dwelt on his wavy hair, the way he looked at me, the thought of his driving to work in the morning, his charm (that I believed must surely affect everyone he met), the flowers he sent, [...]. Okay! I know it's crazy, that my list of 'positives' sounds silly, but those are the things I think of, remember, and, yes, want back again! This kind of \"misperception\" or \"love is blind\" cognitive bias is more often referred to as \"idealization\", which modern research considers to be a form of positive illusions. Past authors have sometimes depicted idealization as a malady, but significant scientific evidence has shown that positive illusions actually contribute to relationship satisfaction, long-term well-being and decreased risk for relationship discontinuation. Tennov argues against the term \"idealization\", because she says it implies that the image seen by the person experiencing romantic passion \"is molded to fit a preformed, externally derived, or emotionally needed conception\". In",
    "label": 0
  },
  {
    "text": "and decreased risk for relationship discontinuation. Tennov argues against the term \"idealization\", because she says it implies that the image seen by the person experiencing romantic passion \"is molded to fit a preformed, externally derived, or emotionally needed conception\". In crystallization, the term she prefers, \"the actual and existing features of LO merely undergo enhancement.\"A limerent person may overlook red flags or incompatibilities. Crystallization can be an impediment to recovery, as one of Tennov's informants relates: I decided to make a list in block letters of everything about Elsie that I found unpleasant or annoying. It was a very long list. On the other side of the paper, I listed her good points. It was a short list. But it didn't help at all. The good points seemed so much more important, and the bad things, well, in Elsie they weren't so bad, or they were things I felt I could help her with. Readiness Some people have a heightened susceptibility to limerence, a state Tennov calls \"readiness\", \"longing for limerence\" or being \"in love with love\". This can occur due to biological factors (like adolescence), but also psychological factors (like loneliness or discontent). Sometimes readiness can be so intense that a person falls in love with somebody with only minimal appeal. The psychoanalysts Freud and Reik believed that unhappy people tend to be the most vulnerable to love and fantasy; Elaine Hatfield concurs, saying \"the greater our need, the more grandiose our fantasies\". Shaver & Hazan observed that lonely people are more susceptible to limerence, arguing that \"if people have a large number of unmet social needs, and are not aware of this, then a sign that someone else might be interested is easily built up in that person's imagination into far more than the friendly social contact that",
    "label": 0
  },
  {
    "text": "have a large number of unmet social needs, and are not aware of this, then a sign that someone else might be interested is easily built up in that person's imagination into far more than the friendly social contact that it might have been. By dwelling on the memory of that social contact, the lonely person comes to magnify it into a deep emotional experience, which may be quite different from the reality of the event.\" Uncertainty and hope According to Dorothy Tennov, \"uncertainty\" is a key element to limerence: The recognition that some uncertainty must exist has been commented on and complained about by virtually everyone who has undertaken a serious study of the phenomenon of romantic love. Psychologists Ellen Bersheid and Elaine Walster discussed this common observation made, they note, by Socrates, Ovid, the Kama Sutra, and \"Dear Abby,\" that the presentation of a hard-to-get as opposed to an immediately yielding exterior is a help in eliciting passion. Rather than being an emotion itself, romantic love is a motivational state which can produce different emotions depending on the situation: positive feelings when things go well and negative feelings when things go awry. The \"goal\" according to Tennov's analysis is \"oneness\" with the LO, i.e. mutual reciprocation or return of feelings. According to Tennov's theory, two elements are required for limerence to develop and intensify: hope and uncertainty. There must be at least some hope that an LO will reciprocate, but uncertainty over their true feelings is required for the preoccupation and mood changes to intensify. In some cases, uncertain reciprocation can produce mood swings which are so abrupt as to cause emotional volatility, even in generally stable people. One of Tennov's informants recalls: \"When I felt [Barry] loved me, I was intensely in love and deliriously happy; when",
    "label": 0
  },
  {
    "text": "uncertain reciprocation can produce mood swings which are so abrupt as to cause emotional volatility, even in generally stable people. One of Tennov's informants recalls: \"When I felt [Barry] loved me, I was intensely in love and deliriously happy; when he seemed rejecting, I was still intensely in love, only miserable beyond words.\" Limerence normally subsides when either: all hope of reciprocation is ended; the limerent person enters a relationship with the LO and receives adequate reciprocation; limerence is \"transferred\" to a different LO. In even some further cases (\"and this is the madness of it\", Tennov says), the lovesickness and intrusive thoughts can still remain, even after all hope is exhausted and the sufferer wants to be rid of the state. After a transition to addiction, the executive brain is sidestepped, and some reactions and behavioral habits become essentially automatic. The uncertainty of limerence has been interpreted as intermittent reinforcement by Robert Sternberg, keeping the brain \"hooked\" in. When people behave inconsistently or contrary to expectations, this can spark interest and be a fuel for passion (either ecstasy or agony). This relies on a mechanic of dopamine, which does not encode reward per se, but rather encodes a \"reward prediction error\" signal: whether a given reward is better, equal to, or worse than expected. The type of situation involved resembles a slot machine, where the rewards are designed to be always unpredictable so the gambler cannot understand the pattern. Unable to habituate to the experience, for some people the exhilarating high from the unexpected wins leads to gambling addiction and compulsions. If the machine paid out on a regular interval (so that the rewards were expected), it would not be as exciting. The uncertainty of receiving an occasional message from an LO is \"gasoline poured on the fire\", according",
    "label": 0
  },
  {
    "text": "and compulsions. If the machine paid out on a regular interval (so that the rewards were expected), it would not be as exciting. The uncertainty of receiving an occasional message from an LO is \"gasoline poured on the fire\", according to Judson Brewer. Uncertainty can also be introduced by the presence of barriers to a relationship, like parental interference or a deceived spouse. This \"intensification through adversity\" was crucial to the mutual limerence of Romeo and Juliet, hence this is often called \"the Romeo and Juliet effect\". Helen Fisher called it \"frustration attraction\", and attributed it to dopamine neurons which prolong their firing in anticipation of an expected reward which is delayed. Fisher also believed that separation evokes panic and stress, which activates the hypothalamic–pituitary–adrenal axis. It's ironic, she says, because this can also produce dopamine, so \"as the adored one slips away, the very chemicals that contribute to feelings of romance grow even more potent\". According to Tennov, \"It is limerence, not love, that increases when lovers are able to meet only infrequently or when there is anger between them.\" One can attempt to extinguish limerence by removing any hope that an LO will reciprocate. An individual who is the object of unwanted limerent attraction should give the clearest possible rejection, rather than something ambiguous such as \"I like you as a friend, but...\". Ecstatic union Although limerence is usually unrequited, it can lead to a relationship in some cases. According to Tennov's theory and observations, small doses of attention from an LO (along with uncertainty) increase the intensity of limerence, and a sensation of buoyancy or \"walking on air\" is felt when reciprocation seems near. \"Reciprocation leads to euphoria, followed by a union that might be stable or unstable, and that might or might not endure.\" This \"ecstatic",
    "label": 0
  },
  {
    "text": "of limerence, and a sensation of buoyancy or \"walking on air\" is felt when reciprocation seems near. \"Reciprocation leads to euphoria, followed by a union that might be stable or unstable, and that might or might not endure.\" This \"ecstatic union\" (a phrase coined by Simone de Beauvoir) is recalled by one of Tennov's informants: \"The landlord had given me notice and the bank loan had not gone through, and I could not bring myself to care! Whatever happened, it would be wonderful somehow. My delight in simply existing eclipsed everything else\". 95% of her survey group called love \"a beautiful experience\". A 2025 study of the largest cross-cultural survey of currently in-love people (who were also in relationships) categorized 29.42% of their sample as \"intense\" romantic lovers, and 28.57% of those fell in love before their relationship. (That is, only 8.4% of the study were both intensely in love and also fell in love before their relationship. The majority of intense lovers fell in love after their relationship started, with one month after being the average.) Normally then, limerence diminishes inside a relationship, with reciprocity. Desire fades because of a habituation effect on dopamine activity: as a reward is more easily and predictably obtained, the dopamine release in response to reward cues decreases. Research also suggests that oxytocin activity might inhibit the more excessive effects of addiction, with oxytocin from the attachment system being more active in reciprocated, well-functioning relationships compared to unrequited situations. In some cases, however, just getting into a relationship by itself may still not be enough for limerence to diminish, if reciprocation is insufficient. According to Tennov, reciprocation must be \"sustained and believable\", else limerence can continue inside a relationship if the partner (LO) behaves in a nonlimerent way. Limerence and uncertainty theory have also",
    "label": 0
  },
  {
    "text": "enough for limerence to diminish, if reciprocation is insufficient. According to Tennov, reciprocation must be \"sustained and believable\", else limerence can continue inside a relationship if the partner (LO) behaves in a nonlimerent way. Limerence and uncertainty theory have also been interpreted in terms of attachment anxiety, worsening the symptoms. This can be caused both by an anxious attachment style or a situation where (for example) an avoidant partner can make a normally secure person feel and act anxious (which is a problem in psychology, called a person–situation debate). One man interviewed by Tennov described being caught in one-sided limerence with his wife \"in constant fear of divorce\" for 25 years; however, after their eventual breakup he found a different partner whom he did not have this reaction to. The Passionate Love Scale obsession factor (compared to limerence) has been correlated with relationship satisfaction in short-term relationships; however, studies have also found that as romantic obsession continues inside a relationship over a longer time, the correlation is with decreased satisfaction. This is speculated to be due to low self-esteem and insecure or anxious attachment. Other studies have found that anxious attachment mediated the relationship between neuroticism and its related love styles, and the mania love attitude (which has been related to limerence) mediated the relationship between neuroticism and relationship satisfaction. In some cases, limerence can be extinguished quite quickly after a relationship is established, because with more routine contact the participants begin to notice things they don't like about each other. A reminiscent concept from triangular theory of love is \"fatuous love\" (passion, with commitment, without intimacy): as in new passionate lovers who commit to marry without really knowing each other. Usually this fatuous passion fades and turns into just an unhappy commitment by itself, called \"empty love\". The philosopher",
    "label": 0
  },
  {
    "text": "\"fatuous love\" (passion, with commitment, without intimacy): as in new passionate lovers who commit to marry without really knowing each other. Usually this fatuous passion fades and turns into just an unhappy commitment by itself, called \"empty love\". The philosopher Bertrand Russell is quoted by Tennov in her discussion of uncertainty, quipping that \"when a man has no difficulty in obtaining a woman, his feeling towards her does not take the form of romantic love\", but Russell goes on to say that \"I think it is good—that romantic love should form the motive for a marriage, but it should be understood that the kind of love which will enable a marriage to remain happy and to fulfil its social purpose is not romantic but is something more intimate, affectionate, and realistic. In romantic love the beloved object is not seen accurately, but through a glamorous mist\". According to Tennov, ideally limerence will be replaced by another type of love. In this way, feelings may evolve: \"Those whose limerence was replaced by affectional bonding with the same partner might say, 'We were very much in love when we married; today we love each other very much.'\" The more stable type of love which is usually the characteristic of long-term relationships is commonly called companionate love, storge or attachment. An fMRI experiment of people who were in happy, long-term relationships (10 years or more) but professed to still be \"madly\" in love found brain activations in dopamine-rich reward areas (interpreted as \"wanting\" or \"desire for union\"), but also activity in the globus palludus, a site for opiate receptors identified as a hedonic hotspot (\"liking\"). Unlike people who were newly in love, these participants also did not show activity in areas associated with anxiety and fear, and reported far less obsessional features. Duration",
    "label": 0
  },
  {
    "text": "palludus, a site for opiate receptors identified as a hedonic hotspot (\"liking\"). Unlike people who were newly in love, these participants also did not show activity in areas associated with anxiety and fear, and reported far less obsessional features. Duration Tennov estimates based on her questionnaire and interviews that limerence most frequently lasts between 18 months and 3 years, with an average of 2 years, but may be as short as mere days or as long as a lifetime. One woman wrote to Tennov about her mother's limerence which lasted 65 years. Tennov calls it the worst case when the limerent person cannot get away, because the LO is a coworker or lives nearby. Limerence can last indefinitely sometimes when unrequited, especially when reciprocation is uncertain: with intermittent reinforcement and mixed signals, for example, an LO ignoring the limerent person for awhile and then suddenly calling. Stringing a limerent person along with intermittent communication is called \"breadcrumbing\". Tennov's estimate of 18 months to 3 years is sometimes used as the normal duration of romantic love. The other common estimate, 12–18 months, comes from Donatella Marazziti's experiment comparing serotonin transporter levels of people in love with OCD patients. In this experiment, subjects who had fallen in love within the past 6 months (who were in a relationship) were measured to have serotonin transporter levels which were different from controls, levels which returned to normal after 12–18 months. Love regulation Love regulation, studied by the psychologist Sandra Langeslag, is \"the use of behavioral or cognitive strategies to change the intensity of current feelings of romantic love\". Langeslag works with Helen Fisher's model (lust, attraction and attachment, i.e. independent emotion systems), but uses the terms infatuation (i.e. passionate love) and attachment (i.e. companionate love). It's a common misconception that love feelings are uncontrollable,",
    "label": 0
  },
  {
    "text": "feelings of romantic love\". Langeslag works with Helen Fisher's model (lust, attraction and attachment, i.e. independent emotion systems), but uses the terms infatuation (i.e. passionate love) and attachment (i.e. companionate love). It's a common misconception that love feelings are uncontrollable, or even should not be controlled; however studies using EEG and psychometrics have shown that love regulation is possible and can be useful. In a technique called cognitive reappraisal, one focuses on positive or negative aspects of their beloved, the relationship, or imagined future scenarios: In positive reappraisal, one focuses on positive qualities of the beloved (\"he's kind\", \"she's spontaneous\"), the relationship (\"we have so much fun together\") or imagined future scenarios (\"we'll live happily ever after\"). Positive reappraisal increases attachment and can increase relationship satisfaction. In negative reappraisal, one focuses on negative qualities of the beloved (\"he's lazy\", \"she's always late\"), the relationship (\"we fight a lot\") or imagined future scenarios (\"he'll cheat on me\"). Negative reappraisal decreases feelings of infatuation and attachment, but decreases mood in the short term. Langeslag has recommended distraction as an antidote to the short-term decrease in mood. Preliminary results from a 2024 study of online limerence communities conducted by Langeslag found that negative reappraisal decreased limerence for the study participants. A therapist named Brandy Wyant has also had her limerent clients list reasons their LO is not perfect, or reasons they and their LO are not compatible. Love regulation doesn't switch feelings on or off immediately, so Langeslag recommends writing a list of things once a day as an example. The neuroscientist Tom Bellamy is recommending what he calls the \"daymare\" strategy: if a person in limerence finds themselves lost in a romantic daydream, they should \"spoil the rewards\" by changing the end of the story into a nightmare. Ruining reward-seeking habits like",
    "label": 0
  },
  {
    "text": "Bellamy is recommending what he calls the \"daymare\" strategy: if a person in limerence finds themselves lost in a romantic daydream, they should \"spoil the rewards\" by changing the end of the story into a nightmare. Ruining reward-seeking habits like this is recommended as a kind of \"deprogramming\" to \"accelerate the overwriting of memories linking LO to reward\". Based on the addiction theory of romantic love, Helen Fisher and colleagues recommend that rejected lovers remove all reminders of their beloved, such as letters or photos, and avoid contact with the rejecting partner. Reminders can cause cravings which prolong recovery. They also suggest that positive contact with friends could reduce cravings. Rejected lovers should stay busy to distract themselves, and engage in self-expanding activities. Setting a \"no contact\" rule during recovery can facilitate self-care and time to reflect on the situation. Controversy In 2008, Albert Wakin, a professor who knew Tennov at the University of Bridgeport but did not assist in her research, and Duyen Vo, a graduate student, suggested that limerence is similar to obsessive–compulsive disorder (OCD) and substance use disorder (SUD). They presented work to an American Association of Behavioral and Social Sciences conference, but suggested that much more research is needed before it could be proposed to the APA that limerence be included in the DSM. They began conducting an unpublished study and reported to USA Today that about 25% or 30% of their participants had experienced a limerent relationship as they defined it. Wakin has stated that his concept involves people in relationships, where a person is obsessed with their partner to the detriment of the relationship, even to the point of a breakup. Limerence and romantic love (also called passionate love) have been compared to OCD in general since 1998, according to a theory invented by",
    "label": 0
  },
  {
    "text": "obsessed with their partner to the detriment of the relationship, even to the point of a breakup. Limerence and romantic love (also called passionate love) have been compared to OCD in general since 1998, according to a theory invented by other authors. This was partly based on a theoretical comparison between preoccupation features, like worries about a family member being harmed and a need for things to be \"just right\". This is also sometimes paired with a theory involving the neurotransmitter serotonin, but experimental evidence for that is ambiguous. A 2025 study found no association between SSRI use and obsessive thinking about a loved one or the intensity of romantic love. Neuroscientist blogger Tom Bellamy has argued that limerence is distinct from OCD on the basis of psychological and neurobiological differences. OCD is characterized by compulsions to perform rituals that ease some type of fear, whereas limerence initially starts with a period of joy and only reaches a stage of anxiety when a pair bond cannot be formed. Helen Fisher has commented on Wakin & Vo in 2008, stating that limerence is romantic love and that \"They are associating the negative aspects of it with the term, and that can be a disorder.\" Fisher is one of the original authors to compare limerence to OCD, and has proposed that romantic love is a \"natural addiction\" which can be either positive or negative depending on the situation. Fisher stated again in 2024 that she does not think there is any difference between limerence and romantic love. In 2017, Wakin has stated that he feels that brain scans of limerence would help establish it as \"something unlike everything that has been diagnosed already\", but brain scans have been described by Fisher's team since as far back as 2002. In Fisher et al.'s",
    "label": 0
  },
  {
    "text": "that he feels that brain scans of limerence would help establish it as \"something unlike everything that has been diagnosed already\", but brain scans have been described by Fisher's team since as far back as 2002. In Fisher et al.'s original brain scan experiments, all participants spent more than 85% of their waking hours thinking about their loved one. Wakin also claims that a person experiencing limerence can never be satiated, even if their feelings are reciprocated. Tennov found many cases of nonlimerent people who described their limerent partners being \"stricken with a kind of insatiability\", and that \"no degree of attentiveness was ever sufficient\". According to Tennov's theory, the intensity of limerence diminishes with reciprocity, and it's prolonged inside a relationship when the LO behaves in a nonlimerent way. Other mainstream authors have stated that obsession inside a relationship when it's a problem could be related to self-esteem and an insecure attachment style. In the 1999 preface to her revised edition of Love and Limerence, Dorothy Tennov describes limerence as an aspect of basic human nature and remarks that \"Reaction to limerence theory depends partly on acquaintance with the evidence for it and partly on personal experience. People who have not experienced limerence are baffled by descriptions of it and are often resistant to the evidence that it exists. To such outside observers, limerence seems pathological.\" Tennov states that her studies suggest limerence is normal, that it's too often interpreted as \"mental illness\", and that even those who experienced limerence of a distressing variety were \"fully functioning, rational, emotionally stable, normal, nonneurotic, nonpathological members of society\", \"characterized as responsible and quite sane\". Tragedies such as violence, she says, involve limerence when it's \"augmented and distorted\" by other conditions. In a 2005 Q&A, Tennov was asked if limerence could ever",
    "label": 0
  },
  {
    "text": "Love addiction is a proposed disorder concept involving love relations characterized by severe distress and problematic passion-seeking despite adverse consequences. Academics do not currently agree on a precise definition of love addiction or when it needs to be treated. Love addiction can be contrasted with passionate love (the early stage of romantic love) which may be intense but still be prosocial and positive when reciprocated. Research on the biology of romantic love indicates that passionate love resembles a behavioral addiction, but it has evolved for the purpose of pair bonding. A 2010 medical inquiry concluded that medical evidence at the time did not have definitions or criteria to classify love addiction as a disorder. Furthermore, the authors state there is a risk of misunderstanding and \"overmedicalizing\" people who experience it. The term \"love addiction\" does not appear in the Diagnostic and Statistical Manual of Mental Disorders (DSM), a compendium of mental disorders and diagnostic criteria published by the American Psychiatric Association. Love addiction is different from codependence, although historically the concepts have been conflated as if synonymous. Love addiction has also been compared to (as analogous to or encompassing) the concepts of obsessive love, lovesickness and limerence. Definition Defining an addictive disorder which revolves around love passion is difficult because passionate love (also called \"infatuation\") normally has features which resemble addiction. People in love experience salience, they yearn for their beloved and the amourous stage resembles \"getting high\". One of the major differences between love and drug addiction is that the addictive aspects of passionate love tend to fade away in a relationship, whereas the condition of a drug addiction tends to worsen over time. A team of bioethicists including Brian Earp and Julian Savulescu have drawn a distinction between two views on how the relationship between love and addiction",
    "label": 0
  },
  {
    "text": "in a relationship, whereas the condition of a drug addiction tends to worsen over time. A team of bioethicists including Brian Earp and Julian Savulescu have drawn a distinction between two views on how the relationship between love and addiction can be conceptualized: In a narrow view, love might be considered an addiction when it's the result of abnormal brain processes. This is similar to an emerging viewpoint on drug addiction that the brain processes which are responsible for addiction do not exist in the brains of non-addicted people. Drugs of abuse artificially 'co-opt' neurotransmitter systems to produce reward signals which are much higher than could be achieved by natural rewards or with normal functioning. There is also some evidence that certain cases like binge eating and gambling addiction may elicit responses similar to drugs in some susceptible people. In the narrow view of love addiction then, \"only extreme, radical brain processes, attachment behaviors, or manifestations of love\" could indicate addiction, and it may be a rare condition. In a broad view, all love might be considered addiction. In this case, addiction may be \"a spectrum of motivation\" for any type of reward, like an appetite one can develop via reward conditioning, which is an evolved mechanism. This includes drugs, but also food for example, given that the human appetite for food can sometimes be contrary to real nutritive needs. In this way, perhaps everyone is \"addicted\" to food, sex, etc., although not to the point of distress or needing treatment. In the broad view of love addiction then, \"to love someone is literally to be addicted to them\". Helen Fisher, Arthur Aron and colleagues have proposed that romantic love is a \"natural\" addiction, evolved for pair bonding, which is a \"positive addiction\" (i.e., not harmful) when requited and a",
    "label": 0
  },
  {
    "text": "love someone is literally to be addicted to them\". Helen Fisher, Arthur Aron and colleagues have proposed that romantic love is a \"natural\" addiction, evolved for pair bonding, which is a \"positive addiction\" (i.e., not harmful) when requited and a \"negative addiction\" when unrequited or inappropriate. In their 2010 proposal, Reynaud et al. defined addiction as \"the stage where desire becomes a compulsive need, when suffering replaces pleasure, when one persists in the relationship despite knowledge of adverse consequences (including humiliation and shame).\" Another group favoring the six components model of addiction by Mark Griffiths (salience, mood modification, tolerance, withdrawal, conflict and relapse) states that \"Individuals addicted to love tend to experience negative moods and affects when away from their partners and have the strong urge and craving to see their partner as a way of coping with stressful situations.\" Types A 2025 review has raised the question of identifying subtypes, because of the heterogeneity of clinical descriptions. For example, the diagnosis might apply either to individuals who fall in love quickly with a different partner as soon as the passionate phase of a relationship ends, or to individuals who are unable to break off maladaptive relationships. Authors such as Helen Fisher include those \"who have been rejected or broken up with\" as love addicts. Dorothy Tennov's concept of limerence (i.e., all-absorbing infatuated love, commonly for an unreachable person) has been likened to love addiction. Tennov's research, however, suggested to her that limerence is normal (although illogical), and a 2025 survey suggested that as many as 50–60% of the population had experienced it. Stanton Peele has commented on Tennov's work, calling limerence a \"clinical condition\" and \"a severe emotional disability, one that leads people (primarily women) desperately to pursue often inappropriate love objects, frequently to fail at relationships, and to",
    "label": 0
  },
  {
    "text": "had experienced it. Stanton Peele has commented on Tennov's work, calling limerence a \"clinical condition\" and \"a severe emotional disability, one that leads people (primarily women) desperately to pursue often inappropriate love objects, frequently to fail at relationships, and to be incapable of learning from such experiences so that their ardor and desperation are often increased by their failures at love\". Limerence has also been called \"romance addiction\", in reference to its relation to the concept of \"romantic love\". Susan Peabody, a co-founder of Love Addicts Anonymous, has defined four types of love addicts: Obsessed Love Addicts, who are always head over heels for the wrong person—somebody unavailable, distant, noncommittal, abusive, and so on. Codependent Love Addicts, who tend to suffer from low self-esteem and desperately hold onto relationships by taking care of a partner with codependent behavior. Relationship Addicts, who are no longer in love with their partners, but still cannot let go despite being unhappy, because they're afraid of being alone. Ambivalent Love Addicts, who crave love but fear intimacy, so they become addicted to romantic fantasies about unavailable people, or prefer romantic affairs over committed relationships. History It was Stanton Peele & Archie Brodsky who brought love addiction into the literature with their 1975 book Love and Addiction, arguably being the most cited work, although precursors to the concept had been described by earlier authors. The term \"love addict\" is believed to have been first coined in 1928 by the Hungarian psychoanalyst Sandor Rado. However, even earlier, Sigmund Freud had observed a similarity between love passion and drug addiction, also having published his case study of Sergei Pankejeff (the \"Wolf Man\") whose \"compulsive attacks\" of falling in love \"came on and passed off by sudden fits\". In 1945, the psychoanalyst Otto Fenichel defined \"love addicts\" as \"persons",
    "label": 0
  },
  {
    "text": "drug addiction, also having published his case study of Sergei Pankejeff (the \"Wolf Man\") whose \"compulsive attacks\" of falling in love \"came on and passed off by sudden fits\". In 1945, the psychoanalyst Otto Fenichel defined \"love addicts\" as \"persons in whom the affection or the confirmation they receive from external objects plays the same role as food in the case of food addicts. Although they are unable to return love, they absolutely need an object by whom they feel loved, but only as an instrument to procure the condensed oral gratification.\" Most relevant research began after the publication of Peele & Brodsky's Love and Addiction, in 1975. The book sought to show how drugs are not uniquely addictive, and that people can become addicted to any type of sufficiently rewarding activity, an idea which would have been met with skepticism in its time. Through the book's success, that idea is now commonplace. The authors, however, have called that the \"less important half\" of their argument, which more importantly (in their view) also criticized the notion that addiction is a \"disease\". The disease model of addiction was popularized from the 1970s and into the '80s as a facet of Alcoholics Anonymous and other twelve-step programs, then found its way into other self-help material, and this is how \"love addiction\" entered popular vocabulary. According to Peele, his main points became sidelined then, as Love and Addiction was actually \"a social commentary on how our society defines and patterns intimate relationships...all of this social dimension has been removed, and the attention to love addiction has been channeled in the direction of regarding it as an individual, treatable psychopathology.\" The twelve-step program Sex and Love Addicts Anonymous (SLAA) was founded in 1976, by a Boston musician who had been a member of Alcoholics",
    "label": 0
  },
  {
    "text": "love addiction has been channeled in the direction of regarding it as an individual, treatable psychopathology.\" The twelve-step program Sex and Love Addicts Anonymous (SLAA) was founded in 1976, by a Boston musician who had been a member of Alcoholics Anonymous, but additionally struggled with out of control sexual behaviors of extramarital affairs and masturbation. This led him to seek out others who also believed they were suffering from sex and love addiction, and they began holding meetings to try to stop their obsessive and compulsive behaviors. SLAA has been estimated to hold over 1,200 meetings, with 16,000 members in over 42 countries. However, according to a 2010 review of the topic, most people who attend SLAA meetings may actually be there for sexual dependence (for men) or relationship dependence (for women), rather than for love addiction. Another group is Love Addicts Anonymous (LAA), which focuses on love addiction specifically. LAA was founded in 2004 by Susan Peabody and Howard Gold, as a safe place for recovery from \"unhealthy dependency on love as it plays out in our fantasies and relationships\". LAA believes that love addiction and sex addiction are different. Neuroscience Reward & motivation Endogenous opioids Obsessive thinking Cultural references In A Spy in the House of Love, the heroine Sabina is said to have seen her 'love anxieties as resembling those of a drug addict, of alcoholics, of gamblers. The same irresistible impulse, tension, compulsion and then depression following the yielding to the impulse, revulsion, bitterness, depression, and the compulsion once more'. As a result, she has been subsequently described as 'feeling like a \"love addict\" enslaved to obsessive-compulsive patterns of behaviour'. St. Augustine, quoted in Confessions as saying \"to Carthage then I came, where a cauldron of unholy loves sang all about my ears\", has been interpreted",
    "label": 0
  },
  {
    "text": "Moral emotions are a variety of social emotions that are involved in forming and communicating moral judgments and decisions, and in motivating behavioral responses to one's own and others' moral behavior. As defined by social psychologist Jonathan Haidt, moral emotions intrinsically \"are linked to the interests or welfare either of a society as a whole or at least of persons other than the judge or agent\". A person may not always have clear words to articulate the reasoning behind their moral position, yet simultaneously knows it to be true. Moral emotions are linked to a person's conscience - these are the emotions that make up a conscience and promote learning the difference between right and wrong, good and bad, virtuous and evil. Moral emotions include anger, disgust, contempt, shame, pride, guilt, compassion, gratitude, and elevation and help to provide people with the power and energy to do good and avoid doing bad. Some emotions, such as anger, can be triggered both in response to moralized and non-moralized stimuli, making them simultaneously moral and non-moral emotions, whereas other emotions, such as guilt and shame, seem to inherently have a moral component. Academic conceptions of moral emotions have changed in recent years. A large part of moral emotions is based on society's interpretation of things. While it is true that many of these emotions are based on the absolute truths of morality, this is only but a part of what moral emotions are about. The full spectrum of what moral emotions entail also includes emotions based on the narratives of what people teach. Much of this leads people to make their own choices in life, through a process formally known as \"moral decision-making\". This is something that influences people every day, without most people ever even realizing it. Background The study of",
    "label": 0
  },
  {
    "text": "teach. Much of this leads people to make their own choices in life, through a process formally known as \"moral decision-making\". This is something that influences people every day, without most people ever even realizing it. Background The study of morality through philosophy dates back at least as far as Plato and Aristotle, who believed morality derived from moral reasoning and focused their studies there at the expense of moral emotions. The emotive side of morality, worked by later philosophers such as Adam Smith in his The Theory of Moral Sentiments and David Hume has been looked upon with disdain, as subservient to the higher, rational, moral reasoning. This continued even when the field of moral psychology evolved from moral philosophy, with scholars like Immanuel Kant, Piaget and Kohlberg touting moral reasoning as the key factor determining morality. Kohlberg's work on the cognition underlying morality especially was part of the larger \"cognitive revolution\" movement within psychology. However, beginning in the 1970s there has been a rise in a new front of research: moral emotions as the basis for moral behavior. This development began with a focus on empathy and guilt, but has since moved on to encompass new emotional scholarship on emotions such as anger, shame, disgust, awe, and elevation. With the new research, theorists have begun to question whether or not moral emotions might hold a larger role in determining morality, one that might even surpass that of moral reasoning. Definitions There have generally been two approaches taken by philosophers to define moral emotion, with the first approach being \"to specify the formal conditions that make a moral statement (e.g., that is prescriptive, that it is universalizeable, and that it overrides nonmoral concerns, such as expedience)\". This first approach is more tied to language and the definitions given to",
    "label": 0
  },
  {
    "text": "\"to specify the formal conditions that make a moral statement (e.g., that is prescriptive, that it is universalizeable, and that it overrides nonmoral concerns, such as expedience)\". This first approach is more tied to language and the definitions given to moral emotions. The second approach \"is to specify the material conditions of a moral issue, for example, that moral rules and judgments 'must bear on the interest or welfare either of society as a whole or at least of persons other than the judge or agent'\". This definition seems to be more action-based. It focuses on the outcome of a moral emotion. The second definition is more preferred because it is not tied to language and therefore can be applied to prelinguistic children and animals. Moral emotions are \"emotions that are linked to the interests or welfare either of society as a whole or at least of persons other than the judge or agent.\" Types of moral emotion Moral emotions, like any emotion, fall under categories of positive and negative. With moral emotions, however, there are two types of negative: inner-directed negative emotions (which motivate people to act ethically) and outer-directed negative emotions (which aim to discipline or punish). Within the positive and negative categories, there are specific emotions. Examples of positive moral emotions are gratitude, elevation, and pride in one's beneficial successes. Examples of negative moral emotions include shame, guilt, and embarrassment. There is a debate whether there is a set of basic emotions or if there are \"scripts or set of components that can be mixed and matched, allowing for a very large number of possible emotions\". Even those arguing for a basic set acknowledge that there are variants of each emotion (psychologist Paul Ekman calls these variants \"families\"). According to Jonathan Haidt: The principal moral emotions can",
    "label": 0
  },
  {
    "text": "allowing for a very large number of possible emotions\". Even those arguing for a basic set acknowledge that there are variants of each emotion (psychologist Paul Ekman calls these variants \"families\"). According to Jonathan Haidt: The principal moral emotions can be divided into two large and two small joint families. The large families are the \"other-condemning\" family, in which the three brothers are contempt, anger, and disgust (and their many children, such as indignation and loathing), and the \"self-conscious\" family (shame, embarrassment, and guilt)…[T]he two smaller families the \"other-suffering\" family (compassion) and the \"other-praising\" family (gratitude and elevation). Haidt would suggest that the higher the emotionality of a moral agent, the more likely the agent is to act morally. He uses the term \"disinterested elicitor\" to describe an event or situation that provokes emotions in us, even when these emotions do not have anything to do with our own personal welfare. It is these elicitors that cause people to participate in what he calls \"prosocial action tendencies\" (actions that benefit society). Haidt explains moral emotions as \"emotion families\", in which each family contains emotions that may be similar although not exactly the same. These moral emotions are provoked by eliciting events that often lead to prosocial action tendencies. Each person's likelihood of prosocial action is determined by his or her degree of emotionality. Morality and the emotions that come with it have been categorized into an underlying two-dimensional structure. It consists of two components, the first dimension being valence, which is the help/harm framework. The second dimension is moral type, which is the agent/patient framework. These components correspond to the moral event, whether helping or harming, and the exemplars involved, which would be the agent or the patient. Each quadrant illustrates how moral events can have distinct moral emotions attached",
    "label": 0
  },
  {
    "text": "which is the agent/patient framework. These components correspond to the moral event, whether helping or harming, and the exemplars involved, which would be the agent or the patient. Each quadrant illustrates how moral events can have distinct moral emotions attached to them based on the exemplar involved. In the agent/help quadrant, moral emotions such as inspiration and elevation are elicited by heroes. In the patient/help quadrant, emotions of relief and happiness are elicited by beneficiaries. In the agent/harm quadrant, emotions like anger and disgust are elicited by villains. And in the patient/harm section, emotions of sympathy and sadness are elicited by victims. This structure explains how moral emotions can be shaped by the relationship between the morality of the exemplar and the morality of the act. Moral emotions and behavior Empathy and altruism Empathy also plays a large role in altruism. The empathy-altruism hypothesis states that feelings of empathy for another lead to an altruistic motivation to help that person. In contrast, there may also be an egoistic motivation to help someone in need. This is the Hullian tension-reduction model in which personal distress caused by another in need leads the person to help in order to alleviate their own discomfort. The Altruism Born of Suffering Literature states that individuals who have undergone difficult times and grown from this trauma identify with seeing others in need and respond altruistically by protecting or caring for others. In the context of climate change, it is recognised that for individuals to act altruistically towards their society and environment, they need to learn to increase their capacity to process their emotional experiences as well as increased reflective functioning. Batson, Klein, Highberger, and Shaw conducted experiments where they manipulated people through the use of empathy-induced altruism to make decisions that required them to show partiality",
    "label": 0
  },
  {
    "text": "increase their capacity to process their emotional experiences as well as increased reflective functioning. Batson, Klein, Highberger, and Shaw conducted experiments where they manipulated people through the use of empathy-induced altruism to make decisions that required them to show partiality to one individual over another. The first experiment involved a participant from each group choosing someone to experience a positive or negative task. These groups included a non-communication, communication/low-empathy, and communication/high-empathy. They were asked to make their decisions based on these standards resulting in the communication/high-empathy group showing more partiality in the experiment than the other groups due to being successfully manipulated emotionally. Those individuals who they successfully manipulated reported that despite feeling compelled in the moment to show partiality, they still felt they had made the more \"immoral\" decision since they followed an empathy-based emotion rather than adhering to a justice perspective of morality. Batson, Klein, Highberger, and Shaw conducted two experiments on empathy-induced altruism, proposing that this can lead to actions that violate the justice principle. The second experiment operated similarly to the first, using low-empathy and high-empathy groups. Participants were faced with the decision to move an ostensibly ill child to an \"immediate help\" group versus leaving her on a waiting list after listening to her emotionally-driven interview describing her condition and the life it has left her to lead. Those who were in the high-empathy group were more likely than those in the low-empathy group to move the child higher up the list to receive treatment earlier. When these participants were asked what the more moral choice was, they agreed that the more moral choice would have been to not move this child ahead of the list at the expense of the other children. In this case, it is evident that when empathy-induced altruism is at",
    "label": 0
  },
  {
    "text": "choice was, they agreed that the more moral choice would have been to not move this child ahead of the list at the expense of the other children. In this case, it is evident that when empathy-induced altruism is at odds with what is seen as moral, oftentimes empathy-induced altruism has the ability to win out over morality. Recently neuroscientist Jean Decety, drawing on empirical research in evolutionary theory, developmental psychology, social neuroscience, and psychopathy, argued that empathy and morality are neither systematically opposed to one another, nor inevitably complementary. Research has also investigated the relationship between psychopathy and the presence of moral emotions. Moral emotions in humans and other animals Haidt speaks of \"cognitively simpler forms or precursors that can be seen in other animals,\" but notes that moral emotions seem to predominantly affect humans as compared to other animals. He writes that \"[t]he puzzle of the moral emotions is that Homo sapiens, far more than any other animal, appears to devote a considerable portion of its emotional life to reacting to social events that do not directly affect the self.\"\"Most social animals, however are doomed to size up interaction partners by themselves. If vampire bat A fails to share a blood meal with vampire bat B, after bat B shared with bat A, bat b does not go around to bats C, D, and E to warn them away from future interactions with bat A. Among human beings. however, this is exactly what happens. Language and highly developed social-cognitive abilities allow human beings to keep track of the reputations of hundreds of individuals (Dunbar, 1996). In endless hours of gossip, people work together to catch cheaters. liars. hypocrites. and others who are trying to fake the appearance of being reliable interaction partners. Human beings, then, live in a",
    "label": 0
  },
  {
    "text": "reputations of hundreds of individuals (Dunbar, 1996). In endless hours of gossip, people work together to catch cheaters. liars. hypocrites. and others who are trying to fake the appearance of being reliable interaction partners. Human beings, then, live in a rich moral world of reputations and third-party concerns. We care what people do to each other, and we readily develop negative feelings toward individuals with whom we have never interacted.\" Other behavioral findings Emmons (2009) defines gratitude as a natural emotional reaction and a universal tendency to respond positively to another's benevolence. Gratitude is motivating and leads to what Emmons' describes as \"upstream reciprocity\". This is the passing on of benefits to third parties instead of returning benefits to one's benefactors. In the context of social networking behavior, research from Brady, Wills, Jost, Tucker, and Van Bavel (2017) shows that the expression of moral emotion amplifies the extent to which moral and political ideals are disseminated in social media platforms. Analyzing a large sample of Twitter communications on polarizing issues, such as gun control, same-sex marriage, and climate change, results indicated that the presence of moral-emotional language in messages increased their transmission by approximately 20% per word, compared to purely-moral and purely-emotional language. See also References",
    "label": 0
  },
  {
    "text": "Musical escapism is a psychological phenomenon characterized by the use of music to elicit vivid daydreams and imaginative scenarios, facilitating a temporary disconnection from immediate surroundings. It is characterized by immersive, music-induced daydreaming. This practice, a subset of the broader concept of escapism, involves active cognitive engagement, wherein listeners become participants in self-constructed narratives inspired by and synchronized with musical stimuli. The term was coined by Dorsa Rohani at the University of Toronto. Background Variety of daydreaming Musical escapism exists on a spectrum, ranging from casual, brief episodes to more immersive, prolonged experiences. The phenomenon primarily serves as a medium for entertainment and creative expression or distraction to cope with subconscious stressors and anxieties. However, it is not entirely confined to those seeking solace from adversity; rather, it is a diverse phenomenon with broad appeal, engaging individuals across various life circumstances and psychological states. It involves constructing vivid, imaginative scenarios synchronized with the auditory experience. These mental constructs may include self-projections, original characters, or adaptations of existing fictional entities within various contexts. The scenarios can range from purely fantastical narratives to idealized representations of real-life situations. The Five-Dimensional Model of Musical Escapism Musical escapism is a complex phenomenon that comprises five interrelated dimensions: cognitive, emotional, neurological, contextual, and motivational. This is known as the Five-Dimensional Model of Musical Escapism (FDME). According to Rohani, these five dimensions work in concert with one another. The cognitive dimension includes immersive daydreaming, narrative complexity, and temporal displacement. Catharsis, emotional intensity, and mood regulation are all part of the emotional dimension. The neurological dimension entails the dopaminergic response, neural synchronization, and reward circuit activation. The contextual dimension consists of environmental factors, sociocultural influences, and technological mediation, and the motivational dimension is made up of coping mechanisms, identity expression, and creative outlets. Neurological and psychological foundations",
    "label": 0
  },
  {
    "text": "the dopaminergic response, neural synchronization, and reward circuit activation. The contextual dimension consists of environmental factors, sociocultural influences, and technological mediation, and the motivational dimension is made up of coping mechanisms, identity expression, and creative outlets. Neurological and psychological foundations of musical escapism It has been demonstrated that the psychological phenomena of musical escapism is driven by neurological and psychological factors. By employing neuroimaging methods including positron emission tomography scans and function magnetic resonance, researchers from The Neuro at McGill University were able to discover the neural basis of musical stimulation. The study showed that the music activates the brain's reward system, which is demonstrated by the release of dopamine. Researchers discovered that the dopaminergic response happens both in anticipation of and during the experience of pleasurable music listening. Data shows an increase of up to nine-percent of dopamine release during the anticipation of pleasurable music and up to six-percent at peak emotional arousal during music listening. Researchers say that the neurochemical evidence suggests that music can regulate human emotions and behavior by influencing the release of dopamine, which can be used as a strategy to manage stress, improve mood and potentially be used as therapeutic tools for mental health disorders. Musical escapism as a psychological coping mechanism According to a 2021 International Federation of the Phonographic Industry survey, 68-percent of Generation Z respondents use music for mood enhancement. The American Psychological Association (APA) finds that increasing economic burdens, academic challenges, and evolving social behaviors have created a complex mix of stressors impacting younger generations. Rising anxiety, depression, and stress levels in the early 21st century have contributed to the surge of musical escapism as a coping mechanism, particularly Generation Z and Millenials. These generations have been raised alongside digital technology and music streaming services making musical escapism an accessible",
    "label": 0
  },
  {
    "text": "levels in the early 21st century have contributed to the surge of musical escapism as a coping mechanism, particularly Generation Z and Millenials. These generations have been raised alongside digital technology and music streaming services making musical escapism an accessible and personalized form of stress relief. The 2020 national survey conducted by the APA found that nearly 1 in 5 adults reported their mental health was worse than in the previous year. Gen Z adults reported the worse mental health at 34-percent, followed by Gen X at 21-percent, Millenials at 19-percent, boomers at 12-percent, and older adults at eight-percent. Musical escapism during COVID-19 Musical escapism has served as a coping mechanism amidst the global coronavirus pandemic, worldwide quarantines, and economic uncertainty. Data from the International Federation of the Phonographic Industry found that 87-percent of respondents reported music improved their emotional well-being during the pandemic, while 80-percent revealed music provided enjoyment and happiness. Seventy-five-percent of respondents also said music gave a sense of normality during the COVID-19 pandemic. Musical escapism in media Musical escapism is not officially recognized as a mental disorder, and has attracted significant attention from social media outlets since 2020. Around the same time, a trend known as \"reality shifting\" appeared on TikTok. This trend, similar to intense daydreaming and frequently set to music, became an extremely prevalent internet phenomenon. Musical escapism vs. maladaptive daydreaming Maladaptive daydreaming is a psychological condition marked by excessive and immersive fantasies that interfere with daily functioning, leading to concentration difficulties, social withdrawal, and neglect of responsibilities. In contrast, musical escapism is a controlled psychological response to music driven by emotion and occasionally subconscious stress, typically without impairing daily life. The primary differences between maladaptive daydreaming and musical escapism lie in the nature of the activity, degree of control, and individual impact. Maladaptive",
    "label": 0
  },
  {
    "text": "controlled psychological response to music driven by emotion and occasionally subconscious stress, typically without impairing daily life. The primary differences between maladaptive daydreaming and musical escapism lie in the nature of the activity, degree of control, and individual impact. Maladaptive daydreaming involves uncontrollable and immersive fantasy, leading to significant functional impairment and distress, whereas musical escapism is a controlled activity that generally influences and is influenced by emotional well-being, and can have both positive and negative psychological effects. Limitations to musical escapism While studies have shown than musical escapism can be beneficial to mental health, it is important to note its limitations. Music may play a role in mood regulation and enhance positive emotions but its effects are short-lived. Individuals with mental health conditions such as generalized anxiety disorder or post-traumatic stress disorder should acknowledge that the use of music to escape provides temporary relief and is not a substitute for professional help. Music therapy and other types of mental health treatment are recommended to be taken through an experienced counselor with background and training in the field. See also Absorption (psychology) Dissociation (psychology) Escapism (disambiguation) Fantasy-prone personality Musicology References",
    "label": 0
  },
  {
    "text": "Obsessive love is characterized by obsessive or compulsive attempts to possess or control an individual, especially triggered (or even intensified) by rejection. Obsessive love can also be distinguished from other forms of romantic love by its one-sidedness and repulsed approaches. Rejection is the \"ultimate nightmare\" to an obsessive lover, who can not let go when confronted with disinterest or the loss of a partner. Usually obsessive love leads to feelings of worthlessness, self-destructive behavior and social withdrawal, but in some cases an obsessive lover may monitor or stalk the object of their passion, or commit acts of violence. Most obsessional stalkers who are not delusional had some type of a relationship with their victim (an ex-partner), and have personality disorders. Comparison The term \"obsessive love\" may also be compared to other concepts: The love style called mania (or manic love): possessive, dependent love. A manic lover is insecure, jealous, and needs reassurances of being loved. A manic lover is unsure of who attracts them, so they may fall in love with somebody they don't even like and project unrealistic qualities onto them. Among the other love styles, mania is most closely compared to eros, which is erotic love or love of beauty. An eros lover is also intensely preoccupied with their beloved, but they are self-assured. The eros lover is in search of an ideal, and they tend to fall in love with somebody more appropriate. Passionate love (or infatuation): the kind of love felt in the early stage of a relationship, or for a potential partner before a relationship has occurred (which can be unrequited). Passionate love is a state of intense longing for another, which has an obsessional element characterized by intrusive thinking, uncertainty, and mood swings. Passionate love is commonly contrasted with companionate love, the gentler feelings",
    "label": 0
  },
  {
    "text": "has occurred (which can be unrequited). Passionate love is a state of intense longing for another, which has an obsessional element characterized by intrusive thinking, uncertainty, and mood swings. Passionate love is commonly contrasted with companionate love, the gentler feelings of affection or bonding which take more time to develop. Passionate love is a relatively broad concept which encapsulates and combines aspects of other more precise taxonomies, but while ignoring their finer distinctions (such as the distinction between eros and mania, which roughly correspond to passionate love when taken together). Limerence: love madness or all-absorbing infatuated love (comparable to passionate love), commonly for an unreachable person. Limerence is said to be \"a condition of cognitive obsession\" where the person experiencing it spends much of their time fantasizing about their love object (called the \"limerent object\"), the kind of love Romeo and Juliet felt for each other. Limerence can impact day-to-day functioning and mental health. According to the inventor of the term, Dorothy Tennov, limerence is supposed to be viewed as a normal state, and tragedies (e.g. violence or suicide) only seem to happen when limerence is \"augmented and distorted\" by other conditions. Limerence is usually unrequited (resulting in a lovesickness which can be hard to end), but it can be reciprocated. In cases of mutual limerence (as in Romeo and Juliet), according to Tennov's original theory, there must be obstacles to the relationship for the mutual preoccupation to intensify. Love addiction: a proposed disorder involving love relations characterized by severe distress and problematic passion-seeking despite adverse consequences. Academics do not currently agree on when love is an addiction, or when it needs to be treated. Erotomania: a delusional disorder where the sufferer has a delusional belief that their love is reciprocated when it isn't (also called \"de Clérambault's syndrome\"). A",
    "label": 0
  },
  {
    "text": "currently agree on when love is an addiction, or when it needs to be treated. Erotomania: a delusional disorder where the sufferer has a delusional belief that their love is reciprocated when it isn't (also called \"de Clérambault's syndrome\"). A subject with erotomania will invent reasons to explain or excuse rejections, however extreme, so they continue believing their love is (secretly) reciprocated. A related condition has been termed \"morbid infatuation\", where the sufferer does not believe their love is reciprocated, but still believes with a delusional intensity in the legitimacy and eventual success of their attempts to pursue a relationship (despite repeated rejections). Both kinds of delusions are common among stalkers who did not have any relationship with their target. A classification problem exists where morbid infatuations do not fall under a DSM erotomania diagnosis, because the DSM diagnosis was designed with features of de Clérambault's syndrome in mind. Historically, the term \"erotomania\" also did not have a precise definition. Obsessive love disorder: an unofficial diagnosis commonly found online, but not in the DSM. Psychology The problem with obsessive love is not so much a question of loving too intensely, but rather of anger over rejection, or feelings of abandonment. Susan Forward states that in her practice, she found four conditions which helped clarify when somebody is suffering from obsessive love: John Moore describes the process of those who \"confuse love with obsession\" as a cycle of four phases he calls the \"Obsessive Relational Progression\": Attraction, characterized by an initial overwhelming attraction to a new person, unrealistic fantasies, an immediate urge to rush into a relationship, and the start of controlling behaviors. Anxiety, a turning point which usually occurs after a commitment has been made, where unfounded thoughts of infidelity, betrayal and fear of abandonment occur, resulting in mistrust, depression,",
    "label": 0
  },
  {
    "text": "urge to rush into a relationship, and the start of controlling behaviors. Anxiety, a turning point which usually occurs after a commitment has been made, where unfounded thoughts of infidelity, betrayal and fear of abandonment occur, resulting in mistrust, depression, anxiety, and the escalation of controlling behaviors. Obsession, when extreme behaviors begin to overwhelm, resulting in obsessive surveillance, stalking, and possibly violence. Destruction, where the partner flees the relationship. Obsessive love may be related to the anxious attachment style. The mania love attitude (for obsessive, dependent love) has been correlated with attachment anxiety, and also the personality trait neuroticism. A study using the Passionate Love Scale showed that while passionate love with obsession was associated with relationship satisfaction in short-term relationships, it was associated with slightly decreased satisfaction over time. In the dualistic model of passion, a distinction is made between two types of passion: harmonious passion (where the person experiencing it feels positive and in control) and obsessive passion (where the person experiencing it feels a loss of control, and it interferes with their life). This is reminiscent of the distinction between the love styles eros (harmonious) and mania (obsessive). One study found that harmonious romantic passion was strongly correlated (positively) with secure attachment, and obsessive romantic passion was moderately correlated (positively) with anxious attachment. Obsessive passion has also been associated with maladaptive conflict resolution strategies in relationships (e.g. criticism, contempt, defensiveness). The anthropologist Helen Fisher believed that \"abandonment rage\" (anger after a rejection) can be explained in terms of the frustration–aggression hypothesis, where rage is triggered when an expected reward is in jeopardy. Romantic love and rage are connected in the brain by similar circuitry; both involve arousal and energy production, and both drive obsessive focus and goal-directed behaviors. Fisher believed that ordinarily the evolutionary purpose of abandonment",
    "label": 0
  },
  {
    "text": "expected reward is in jeopardy. Romantic love and rage are connected in the brain by similar circuitry; both involve arousal and energy production, and both drive obsessive focus and goal-directed behaviors. Fisher believed that ordinarily the evolutionary purpose of abandonment rage is to facilitate separation and the search for a new partner, although sometimes abandonment rage erupts into violence instead. Another feature of obsessive love is jealousy, which seems to be related to OCD for its obsessional qualities and compulsive checking (for signs of infidelity). This can also take the form of pathological jealousy (or \"Othello syndrome\") where the sufferer has a delusional or paranoid belief in their partner's infidelity despite actual evidence. Jealousy is strongly associated with irritability, which can erupt into violence: spousal murder is often related to infidelity (real or suspected). It has been suggested that this seemingly self-defeating behavior of murdering one's own partner results from a kind of \"brinkmanship\", where a looming threat of violence is used to control women as sexual partners—a threat which sometimes (paradoxically) escalates into actual violence to be \"credible\". It is argued that non-pathological jealousy evolved as protection against reproductive competition, which is adaptive as long as it serves to maintain a relationship. Stalking Love obsessionality can in some cases lead to stalking, although stalkers also often have other conditions. Like falling in love, stalking is also described as a form of addiction. A 1999 study by Paul Mullen and colleagues classified 145 stalkers according to several kinds of groups: Rejected ex-partners (41 of 145), who mostly had personality disorders, although 9 were delusional, 5 of whom had morbid jealousy. Rejected stalkers often had mixed desire for reconciliation and revenge, as well as frustration, anger, jealousy, vindictiveness, and sadness. Intimacy seekers (49 of 145), who were attempting to establish an",
    "label": 0
  },
  {
    "text": "although 9 were delusional, 5 of whom had morbid jealousy. Rejected stalkers often had mixed desire for reconciliation and revenge, as well as frustration, anger, jealousy, vindictiveness, and sadness. Intimacy seekers (49 of 145), who were attempting to establish an intimate relationship with their \"true love\", most of whom were delusional, and further classified as follows: Those with erotomania (27 of 145), with a delusional belief their love was reciprocated. Those with morbid infatuations (22 of 145), who did not believe their love was reciprocated but still believed a delusion of their eventual success. Those who had personality disorders (7 of 145), but were not delusional. Incompetent stalkers (22 of 145), who regarded their victims as attractive potential partners, but who were intellectually limited and socially incompetent instead of being infatuated. The other 33 stalkers identified by the study had motives other than a relationship (e.g. broken friendships/family, sexual predation, or scaring a victim). Limerence is another type of love obsession which is being compared to the others, but the motives and emotional foundations behind limerence are different from stalking. A person in limerence hopes for mutual feelings, and their mood depends on whether their romantic interest seems to be returned, whereas the motive for stalking is to force contact, control or punish a victim for rejection. Limerence can cause distress if it becomes too intense, but it's also a feature of early-stage romantic love for many people. Neuroscience Cultural references The ancient Greeks called obsessive love \"theia mania\" (the madness from the gods), and Greek mythology depicted it in stories such as Apollo and Daphne. Obsessive love has been depicted in the movies Fatal Attraction and Play Misty for Me, and the novel Wuthering Heights. See also References Bibliography",
    "label": 0
  },
  {
    "text": "In psychology, a distinction is often made between two types of love: Passionate love, also called infatuation, is \"a state of intense longing for union with another. Reciprocated love (union with the other) is associated with fulfillment and ecstasy; unrequited love (separation) is associated with emptiness, anxiety, or despair\", and \"the overwhelming, amorous feeling for one individual that is typically most intense during the early stage of love (i.e., when individuals are not (yet) in a relationship with their beloved or are in a new relationship)\". Companionate love, also called attachment, is \"the affection we feel for those with whom our lives are deeply entwined\", and \"the comforting feeling of emotional bonding with another individual that takes some time to develop, often in the context of a romantic relationship\". Evolutionary theories suggest these two types of love exist for different purposes, and research from psychology and biology suggests they follow somewhat different mechanics. Both passionate and companionate love can contribute to relationship satisfaction. Passionate and companionate love can also be further distinguished from a third important type of love, compassionate love, which is love focused on caring about others. Passionate love is also commonly called \"romantic love\" in some literature, especially fields of biology, but the term \"passionate love\" is most common in psychology. Academic literature on love has never adopted a universal terminology. Other terms compared to passionate love are \"being in love\", having a crush, obsessive love, limerence and eros. Companionate love is commonly called \"attachment\" or compared to strong liking, friendship love or storge. This is usually considered the same as the \"attachment system\" from attachment theory, but not all authors agree. Passionate love Passionate love feelings are most commonly measured by psychologists with a questionnaire called the Passionate Love Scale (PLS). In the PLS form, Elaine",
    "label": 0
  },
  {
    "text": "considered the same as the \"attachment system\" from attachment theory, but not all authors agree. Passionate love Passionate love feelings are most commonly measured by psychologists with a questionnaire called the Passionate Love Scale (PLS). In the PLS form, Elaine Hatfield & Susan Sprecher specify the components of passionate love as: Passionate love is linked to passion, as in intense emotion, for example, joy and fulfillment, but also anguish and agony. Hatfield notes that the original meaning of passion \"was agony—as in Christ's passion.\" Rather than being an emotion itself, passionate love is said to be a motivational state which produces different emotions depending on the situation (e.g. joy when requited, and sadness when unrequited). A 2014 study of Iranian young adults found that the early stage of romantic love was associated with the brighter side of hypomania (elation, mental and physical activity, and positive social interaction) and better sleep quality, but also stronger symptoms of depression and anxiety. The authors conclude that romantic love is \"not entirely a joyful and happy period of life\". Passionate love is said to usually only be present (or the most intense) in the early stage of love, when a relationship is new or before a relationship has started. However, in a rare phenomenon called long-term intense romantic love, intense attraction can remain for much longer than is typical, even for 10 years or more. In contemporary literature, the original characteristics of passionate love are seen to some degree as being a mixture of things. For example, it's been determined that the PLS has questions which measure companionate love, which led Sandra Langeslag and colleagues to develop the Infatuation and Attachment Scales (IAS) as a newer measure of passionate and companionate love. The PLS also measures an obsessional element which is distinguishable in that",
    "label": 0
  },
  {
    "text": "questions which measure companionate love, which led Sandra Langeslag and colleagues to develop the Infatuation and Attachment Scales (IAS) as a newer measure of passionate and companionate love. The PLS also measures an obsessional element which is distinguishable in that it's possible to experience love feelings (and even intense attraction) with lower levels of obsession. Finally, while Elaine Hatfield originally described passionate love as having a component of sexual attraction, contemporary authors generally agree that sexual attraction and romantic attraction are distinct. People are motivated to initiate and maintain a pair bond in a way that's different from the sex drive, and it is possible to fall in love in the absence of sexual desire. Infatuation Langeslag et al.'s Infatuation Scale (analogous to passionate love) has items asking about: Staring into the distance while thinking of the beloved. Getting shaky knees while in the presence of the beloved. Feelings for the beloved reducing one's appetite. Thoughts about the beloved making it difficult to concentrate. Being afraid that one will say something wrong while talking to the beloved. Getting clammy hands while near the beloved. Becoming tense while close to the beloved. Having a hard time sleeping because of thinking about the beloved. Searching for alternate meanings in the beloved's words. Being shy in the presence of the beloved. Langeslag et al. found that infatuation is more associated with negative emotion than attachment, and tends to decrease after entering a relationship. Participants who were not in a relationship scored the highest on infatuation. The word \"infatuation\" is also sometimes used colloquially in contrast with \"love\", but Elaine Hatfield has argued that the only difference between infatuation (in this sense) and passionate love is semantic. Albert Ellis and Robert Harper conducted interviews and concluded that the only difference is that people use",
    "label": 0
  },
  {
    "text": "in contrast with \"love\", but Elaine Hatfield has argued that the only difference between infatuation (in this sense) and passionate love is semantic. Albert Ellis and Robert Harper conducted interviews and concluded that the only difference is that people use the word \"infatuation\" in hindsight to refer to a relationship after it ends and \"love\" to refer to a relationship still in progress. Hatfield suggests that when parents and friends say somebody is \"just infatuated\" they're just saying they don't approve of the relationship. Obsession Passionate love is described as having an obsessional element characterized by intrusive thinking, uncertainty, and mood swings. Intrusive thinking (or obsessive thinking) is a component of early-stage romantic love. One study found that on average people in love spent 65% of their waking hours thinking of their loved one. Studies by Bianca Acevedo & Arthur Aron found that the obsessional component of the PLS can be separated from the non-obsessional component. Items on the PLS measuring obsession are, for example, \"Sometimes I feel I can't control my thoughts; they are obsessively on my partner\", \"I sometimes find it difficult to concentrate on work because thoughts of my partner occupy my mind\" and \"I get extremely depressed when things don't go right in my relationship with my partner.\" Items on the PLS measuring non-obsessional romantic love are, for example, \"I want my partner—physically, emotionally, and mentally\", \"For me, my partner is the perfect romantic partner\", \"I would rather be with my partner than anyone else\" and \"I possess a powerful attraction for my partner\". In Acevedo & Aron's analysis, passionate love with obsession was associated with increased relationship satisfaction only in short-term relationships. Romantic obsession was associated with slightly decreased satisfaction in the long-term. Another meta-analysis by James Graham found a strong association between romantic obsession",
    "label": 0
  },
  {
    "text": "& Aron's analysis, passionate love with obsession was associated with increased relationship satisfaction only in short-term relationships. Romantic obsession was associated with slightly decreased satisfaction in the long-term. Another meta-analysis by James Graham found a strong association between romantic obsession (using the mania love attitude—similar in concept to the PLS obsession factor described by Acevedo & Aron) and decreased satisfaction over time. These authors (Acevedo & Aron, Graham) have speculated that continued romantic obsession within a relationship could be connected to attachment style. Attachment style refers to differences in attachment-related thoughts and behaviors, especially relating to the concept of security vs. insecurity. This can be split into components of anxiety (worrying the partner is available, attentive and responsive) and avoidance (preference not to rely on others or open up emotionally). It has been suggested that attachment style forms during childhood and adolescence, but twin studies have also suggested a heritable component, and attachment anxiety is correlated with the personality trait neuroticism. People can also have different attachment styles with different partners, for example an avoidant partner can cause a secure partner to feel and act anxious. Positive illusions Idealization (perceiving the beloved in the most positive way, or overlooking their faults) is a form of positive illusions. A 1996 study of couples who had been dating for 19 months and couples who had been married for 6.5 years found that \"Individuals were happier in their relationships when they idealized their partners and their partners idealized them.\" A brain scan experiment also found that couples who were still in love after four years (as compared to those who weren't) showed activation in a region associated with suspending negative judgment and over-evaluating a partner. While Elaine Hatfield and others have traditionally associated idealization with passionate love, studies on positive illusions have looked",
    "label": 0
  },
  {
    "text": "four years (as compared to those who weren't) showed activation in a region associated with suspending negative judgment and over-evaluating a partner. While Elaine Hatfield and others have traditionally associated idealization with passionate love, studies on positive illusions have looked at couples in varied stages of their relationships, including long-term couples. Companionate love Companionate love is said to be felt less intensely than passionate love, consisting more of gentle affection which is felt when things are going well. Elaine Hatfield writes that companionate love is \"a steady burning fire, fueled by delightful experiences but extinguished by painful ones\" Companionate love is more about long-term relationships, and Hatfield emphasizes partner compatibility as being important. Ellen Berscheid comments that companionate love \"may be the 'staff of life' for many relationships and a better basis for a satisfying marriage than romantic love.\" Companionate love is linked to intimacy and Hatfield suggests that intimate relationships have these characteristics: Companionate love is usually considered the same as storge, although James Graham has argued on the basis of a meta-analytic factor analysis that the storge love attitude most corresponds to practical friendship which lacks qualities of companionate love (such as intimacy and commitment). Attachment Langeslag et al.'s Attachment Scale (analogous to companionate love) has items asking about: Feeling that one can count on the beloved. Being prepared to share one's possessions with the beloved. Feeling lonely without the beloved. Feeling that the beloved is the one for them. The beloved knowing everything about them. Hoping one's feelings for the beloved never end. Feeling emotionally connected to the beloved. The beloved being able to reassure them when they are upset. The beloved being the person who can make them feel the happiest. The beloved being part of their plans for the future. Relation to attachment theory Companionate",
    "label": 0
  },
  {
    "text": "the beloved. The beloved being able to reassure them when they are upset. The beloved being the person who can make them feel the happiest. The beloved being part of their plans for the future. Relation to attachment theory Companionate love is sometimes considered the same as the \"attachment\" referred to by attachment theory. John Bowlby's original concept of an \"attachment system\" referred to a system evolved to keep infants in proximity of their caregiver (or \"attachment figure\"). The person uses the attachment figure as a \"secure base\" to feel safe exploring the environment, seeks proximity with the attachment figure when threatened, and suffers distress when separated. A prominent theory suggests this system is reused for adult pair bonds, as an exaptation or co-option, whereby a given trait takes on a new purpose. However, companionate love has also been characterized as being more like strong friendship, and Ellen Berscheid suggests that it's unproven whether all adult relationships are attachments in the sense meant by attachment theory. Berscheid writes that the assumption that romantic partners are each other's attachment figures is \"in dire need of empirical scrutiny.\" Timeline While passionate love is sometimes associated with the phenomenon of love at first sight, not everyone falls in love quickly or suddenly. In one study of Chinese and American participants, 38% fell in love fast and 35% fell in love slowly, and in another study of Iranians, 70% fell in love slowly or very slowly. A popular hypothesis suggests that passionate love turns into companionate love over time in a relationship, but other accounts suggest that while companionate love takes longer to develop, it is important at the beginning of a relationship as well. Companionate love might also precede passionate love sometimes. There is some reason to think attachment takes about two years",
    "label": 0
  },
  {
    "text": "accounts suggest that while companionate love takes longer to develop, it is important at the beginning of a relationship as well. Companionate love might also precede passionate love sometimes. There is some reason to think attachment takes about two years to develop, for example one study found that participants who had been in a relationship for about this long named their romantic partner as an attachment figure, while other participants named a parent. One estimate for the duration of passionate love is 18 months to 3 years, which comes from survey data collected by Dorothy Tennov, for her 1979 book Love and Limerence. Another estimate comes from a 1999 experiment performed by Donatelli Marazziti and colleagues which found a difference in blood serotonin levels between newly in love people and controls, and found these levels had returned to normal after 12 to 18 months. Intense attraction can also last much longer in rarer cases, as in the phenomenon of long-term intense romantic love. These lovers tend to show lower levels of obsession than in the early stage, however. Companionate love is thought to build over time as a relationship progresses, but then decrease very slowly over the course of several decades. In the past, some have thought companionate love to be stable after it develops, but for example one study of new marriages found a decline after a 1-year period. Causal conditions A number of theories exist about the causal conditions surrounding these types of love (i.e. who people feel a certain love towards and when), but authors generally agree that passionate and companionate love follow different mechanics. Companionate love generally increases with liking and familiarity, but the circumstances surrounding passionate love are more complicated. Studies show that love and conflict can sit side-by-side in a relationship, and passionate love",
    "label": 0
  },
  {
    "text": "that passionate and companionate love follow different mechanics. Companionate love generally increases with liking and familiarity, but the circumstances surrounding passionate love are more complicated. Studies show that love and conflict can sit side-by-side in a relationship, and passionate love in particular is even said to be amplified by negative emotions. Liking According to Ellen Berscheid, companionate love \"follows the pleasure-pain principle; we like those who reward us and dislike those who punish us.\" Examples of factors include similarity, familiarity, expressions of self-esteem and validation one's self-worth, physical attraction and mutual self-disclosures. Also, while passionate love is often said to come before companionate love, Berscheid suggests that companionate love can also be a component in the development of passionate love. Sexual desire Authors disagree on the role sexual desire plays in the development of romantic love. Passionate love is often associated with sexual desire, for example Ellen Berscheid suggests that one possible account of passionate love is \"a felicitous combination of companionate love and sexual desire.\" However, Lisa Diamond has suggested that while sexual desire is often a causal component, passionate love can occur outside the context of sexual desire. Diamond's argument rests on various reports and historical accounts, as well as an evolutionary argument that the brain systems underlying romantic love evolved independent of sexual orientation. Diamond thinks that time spent together and physical touch can act as a \"stand-in\" for sexual desire and facilitate romantic love between partners regardless of their sexual orientation. Helen Fisher has argued that passionate love is related to the phenomenon of mammalian courtship attraction, or mate choice, and that people have certain preferences for choosing a preferred mating partner that determines who they fall in love with. However, Fisher argues this type of attraction is distinct from the sex drive, although they are",
    "label": 0
  },
  {
    "text": "attraction, or mate choice, and that people have certain preferences for choosing a preferred mating partner that determines who they fall in love with. However, Fisher argues this type of attraction is distinct from the sex drive, although they are interrelated. Emotional arousal Ellen Berscheid writes that emotional arousal, such as happy surprises, contributes to eliciting passionate feelings. Surprise and uncertainty tend to be more of a characteristic of new relationships because more established partners tend to behave as expected, thus rarely generating this sort of arousal. Helen Fisher recommends doing novel and exciting things together to ignite passion. In an experiment by Arthur Aron & Christina Norman, couples doing an exciting task (as opposed to a boring one) experienced increased feelings of relationship satisfaction and romantic love. Elaine Hatfield has even suggested that negative or mixed emotions can amplify feelings of passionate love. In A New Look at Love, she writes \"Passion demands physical arousal and unpleasant experiences are just as arousing as pleasant ones.\" Hatfield cites animal studies, such as one study in which puppies that were inconsistently either rewarded or maltreated were the most attracted to and dependent on their trainer. People who behave consistently generate little emotion, she says, and \"What would generate a spark of interest, however, is if our admiring friend suddenly started treating us with contempt—or if our arch enemy started inundating us with kindness.\" Intimacy Another theory is that passion occurs when a rapid increase in intimacy occurs. A similar theory, by Arthur Aron & Elaine Aron, states that passion occurs in the context of a rapid self-expansion of the self and the inclusion of the qualities of the beloved into one's self-concept. With both of these theories, it's predicted that passion wanes in a relationship as partners get to know each",
    "label": 0
  },
  {
    "text": "the context of a rapid self-expansion of the self and the inclusion of the qualities of the beloved into one's self-concept. With both of these theories, it's predicted that passion wanes in a relationship as partners get to know each other and the increase in intimacy tends to stabilize. Love regulation Love regulation is \"the use of behavioral or cognitive strategies to change the intensity of current feelings of romantic love.\" In some cases, love feelings may be stronger than desired such as after a breakup, or love feelings may be weaker than desired such as when they decline throughout a long-term relationship. Sandra Langeslag notes that it's a common misconception that love feelings are uncontrollable, or even should not be controlled; however studies have shown that love regulation is possible and may be useful. For example, looking at pictures of the beloved has been shown to increase feelings of infatuation (i.e. passionate love) and attachment (i.e. companionate love). In another technique called cognitive reappraisal, one focuses on positive or negative aspects of the beloved, the relationship, or imagined future scenarios: In negative reappraisal, one focuses on negative qualities of the beloved (\"he's lazy\", \"she's always late\"), the relationship (\"we fight a lot\") or imagined future scenarios (\"he'll cheat on me\"). Negative reappraisal decreases feelings of infatuation and attachment, but decreases mood in the short term. Langeslag has recommended distraction as an antidote to the short-term decrease in mood. Negative reappraisal can be useful, for example, to those who want to ameliorate heartbreak or put an end to an abusive relationship. In positive reappraisal, one focuses on positive qualities of the beloved (\"he's kind\", \"she's spontaneous\"), the relationship (\"we have so much fun together\") or imagined future scenarios (\"we'll live happily ever after\"). Positive reappraisal increases attachment and can increase",
    "label": 0
  },
  {
    "text": "relationship. In positive reappraisal, one focuses on positive qualities of the beloved (\"he's kind\", \"she's spontaneous\"), the relationship (\"we have so much fun together\") or imagined future scenarios (\"we'll live happily ever after\"). Positive reappraisal increases attachment and can increase relationship satisfaction, which could, for example, help stabilize a long-term relationship. Love regulation doesn't switch feelings on or off immediately, so Langeslag recommends, for example, writing a list of things once a day to feel a lasting change. Biology Passionate and companionate love are thought to be interrelated but involve different brain systems and serve different purposes. Passionate love is thought to have evolved for mate choice or to initiate a pair bond, while companionate love is for maintaining a pair bond, maintaining close proximity and affiliative behaviors. Passionate love is often associated with the neurotransmitter dopamine. Companionate love is often associated with the neuropeptide oxytocin, and sometimes vasopressin or endogenous opioids. Passionate love is sometimes compared to addiction, although there are differences. People in the early stages of romantic love share similar traits with addicts (for example, feeling rushes of euphoria, or craving for their beloved), but this tends to wear off over time, while the condition of a drug addiction tends to worsen. Helen Fisher has suggested romantic love is a \"positive addiction\" (i.e. not harmful) when reciprocated and a \"negative addiction\" when unrequited or inappropriate. See also References Bibliography External links The Passionate Love Scale at Helen Fisher and Lucy Brown's website.",
    "label": 0
  },
  {
    "text": "Psychiatry is the medical specialty devoted to the diagnosis, treatment, and prevention of deleterious mental conditions. These include matters related to cognition, perceptions, mood, emotion, and behavior. Initial psychiatric assessment begins with taking a case history and conducting a mental status examination. Laboratory tests, physical examinations, and psychological assessments may also be used. On occasion, neuroimaging or neurophysiological studies are performed. Mental disorders are diagnosed in accordance with diagnostic manuals such as the International Classification of Diseases (ICD), edited by the World Health Organization (WHO), and the Diagnostic and Statistical Manual of Mental Disorders (DSM), published by the American Psychiatric Association (APA). The fifth edition of the DSM (DSM-5) was published in May 2013. Treatment may include psychotropics (psychiatric medicines), psychotherapy, substance-abuse treatment, and other modalities such as interventional approaches, assertive community treatment, community reinforcement, and supported employment. Treatment may be delivered on an inpatient or outpatient basis, depending on the severity of functional impairment or risk to the individual or community. Research within psychiatry is conducted by psychiatrists on an interdisciplinary basis with other professionals, including clinical psychologists, epidemiologists, nurses, social workers, and occupational therapists. Psychiatry has been controversial since its inception, facing criticism both internally and externally over its medicalization of mental distress, reliance on pharmaceuticals, use of coercion, influence from the pharmaceutical industry, and its historical role in social control and contentious treatments. Etymology The term psychiatry was first coined by the German physician Johann Christian Reil in 1808 and literally means the 'medical treatment of the soul' (ψυχή psych- 'soul' from Ancient Greek psykhē 'soul'; -iatry 'medical treatment' from Gk. ιατρικός iātrikos 'medical' from ιάσθαι iāsthai 'to heal'). A medical doctor specializing in psychiatry is a psychiatrist (for a historical overview, see: Timeline of psychiatry). Theory and focus Psychiatry refers to a field of medicine focused",
    "label": 0
  },
  {
    "text": "'medical treatment' from Gk. ιατρικός iātrikos 'medical' from ιάσθαι iāsthai 'to heal'). A medical doctor specializing in psychiatry is a psychiatrist (for a historical overview, see: Timeline of psychiatry). Theory and focus Psychiatry refers to a field of medicine focused specifically on the mind, aiming to study, prevent, and treat mental disorders in humans. It has been described as an intermediary between the world from a social context and the world from the perspective of those who are mentally ill. People who specialize in psychiatry often differ from most other mental health professionals and physicians in that they must be familiar with both the social and biological sciences. The discipline studies the operations of different organs and body systems as classified by the patient's subjective experiences and the objective physiology of the patient. Psychiatry treats mental disorders, which are conventionally divided into three general categories: mental illnesses, severe learning disabilities, and personality disorders. Although the focus of psychiatry has changed little over time, the diagnostic and treatment processes have evolved dramatically and continue to do so. Since the late 20th century, the field of psychiatry has continued to become more biological and less conceptually isolated from other medical fields. Scope of practice Though the medical specialty of psychiatry uses research in the field of neuroscience, psychology, medicine, biology, biochemistry, and pharmacology, it has generally been considered a middle ground between neurology and psychology. Because psychiatry and neurology are deeply intertwined medical specialties, all certification in the United States for both specialties and for their subspecialties is offered by a single board, the American Board of Psychiatry and Neurology, one of the member boards of the American Board of Medical Specialties. Unlike other physicians and neurologists, psychiatrists specialize in the doctor–patient relationship and are trained to varying extents in the use",
    "label": 0
  },
  {
    "text": "board, the American Board of Psychiatry and Neurology, one of the member boards of the American Board of Medical Specialties. Unlike other physicians and neurologists, psychiatrists specialize in the doctor–patient relationship and are trained to varying extents in the use of psychotherapy and other therapeutic communication techniques. Psychiatrists also differ from psychologists in that they are physicians and have post-graduate training called residency (usually four to five years) in psychiatry; the quality and thoroughness of their graduate medical training is identical to that of all other physicians. Psychiatrists can therefore counsel patients, prescribe medication, order laboratory tests, order neuroimaging, and conduct physical examinations. As well, some psychiatrists are trained in interventional psychiatry and can deliver interventional treatments such as electroconvulsive therapy, transcranial magnetic stimulation, vagus nerve stimulation and ketamine. Ethics The World Psychiatric Association issues an ethical code to govern the conduct of psychiatrists (like other purveyors of professional ethics). The psychiatric code of ethics, first set forth through the Declaration of Hawaii in 1977 has been expanded through a 1983 Vienna update and in the broader Madrid Declaration in 1996. The code was further revised during the organization's general assemblies in 1999, 2002, 2005, and 2011. The World Psychiatric Association code covers such matters as confidentiality, the death penalty, ethnic or cultural discrimination, euthanasia, genetics, the human dignity of incapacitated patients, media relations, organ transplantation, patient assessment, research ethics, sex selection, coercion, torture, and up-to-date knowledge. In establishing such ethical codes, the profession has responded to a number of controversies about the practice of psychiatry, for example, surrounding the use of lobotomy and electroconvulsive therapy. Discredited psychiatrists who operated outside the norms of medical ethics include Harry Bailey, Donald Ewen Cameron, Samuel A. Cartwright, Henry Cotton, and Andrei Snezhnevsky. Approaches Psychiatric illnesses can be conceptualised in a number of",
    "label": 0
  },
  {
    "text": "use of lobotomy and electroconvulsive therapy. Discredited psychiatrists who operated outside the norms of medical ethics include Harry Bailey, Donald Ewen Cameron, Samuel A. Cartwright, Henry Cotton, and Andrei Snezhnevsky. Approaches Psychiatric illnesses can be conceptualised in a number of different ways. The biomedical approach examines signs and symptoms and compares them with diagnostic criteria. Mental illness can be assessed, conversely, through a narrative which tries to incorporate symptoms into a meaningful life history and to frame them as responses to external conditions. Both approaches are important in the field of psychiatry but have not sufficiently reconciled to settle controversy over either the selection of a psychiatric paradigm or the specification of psychopathology. The notion of a \"biopsychosocial model\" is often used to underline the multifactorial nature of clinical impairment. In this notion the word model is not used in a strictly scientific way though. Alternatively, Niall McLaren acknowledges the physiological basis for the mind's existence but identifies cognition as an irreducible and independent realm in which disorder may occur. The biocognitive approach includes a mentalist etiology and provides a natural dualist (i.e., non-spiritual) revision of the biopsychosocial view, reflecting the efforts of Australian psychiatrist Niall McLaren to bring the discipline into scientific maturity in accordance with the paradigmatic standards of philosopher Thomas Kuhn. Once a medical professional diagnoses a patient there are numerous ways that they could choose to treat the patient. Often psychiatrists will develop a treatment strategy that incorporates different facets of different approaches into one. Drug prescriptions are very commonly written to be regimented to patients along with any therapy they receive. There are three major pillars of psychotherapy that treatment strategies are most regularly drawn from. Humanistic psychology attempts to put the \"whole\" of the patient in perspective; it also focuses on self exploration. Behaviorism",
    "label": 0
  },
  {
    "text": "along with any therapy they receive. There are three major pillars of psychotherapy that treatment strategies are most regularly drawn from. Humanistic psychology attempts to put the \"whole\" of the patient in perspective; it also focuses on self exploration. Behaviorism is a therapeutic school of thought that elects to focus solely on real and observable events, rather than mining the unconscious or subconscious. Psychoanalysis, on the other hand, concentrates its dealings on early childhood, irrational drives, the unconscious, and conflict between conscious and unconscious streams. Practitioners All physicians can diagnose mental disorders and prescribe treatments utilizing principles of psychiatry. Psychiatrists are trained physicians who specialize in psychiatry and are certified to treat mental illness. They may treat outpatients, inpatients, or both; they may practice as solo practitioners or as members of groups; they may be self-employed, be members of partnerships, or be employees of governmental, academic, nonprofit, or for-profit entities; employees of hospitals; they may treat military personnel as civilians or as members of the military; and in any of these settings they may function as clinicians, researchers, teachers, or some combination of these. Although psychiatrists may also go through significant training to conduct psychotherapy, psychoanalysis or cognitive behavioral therapy, it is their training as physicians that differentiates them from other mental health professionals. As a career choice in the US Psychiatry was not a popular career choice among medical students, even though medical school placements are rated favorably. This has resulted in a significant shortage of psychiatrists in the United States and elsewhere. Strategies to address this shortfall have included the use of short 'taster' placements early in the medical school curriculum and attempts to extend psychiatry services further using telemedicine technologies and other methods. Recently, however, there has been an increase in the number of medical students entering",
    "label": 0
  },
  {
    "text": "included the use of short 'taster' placements early in the medical school curriculum and attempts to extend psychiatry services further using telemedicine technologies and other methods. Recently, however, there has been an increase in the number of medical students entering into a psychiatry residency. There are several reasons for this surge, including the intriguing nature of the field, growing interest in genetic biomarkers involved in psychiatric diagnoses, and newer pharmaceuticals on the drug market to treat psychiatric illnesses. Subspecialties The field of psychiatry has many subspecialties that require additional training and certification by the American Board of Psychiatry and Neurology (ABPN). Such subspecialties include: Addiction psychiatry, addiction medicine Brain injury medicine Child and adolescent psychiatry Consultation-liaison psychiatry Forensic psychiatry Geriatric psychiatry Hospice and palliative medicine Sleep medicine Additional psychiatry subspecialties, for which the ABPN does not provide formal certification, include: Biological psychiatry Community psychiatry Cross-cultural psychiatry Emergency psychiatry Evolutionary psychiatry Global mental health Learning disabilities Military psychiatry Neurodevelopmental disorders Neuropsychiatry Interventional Psychiatry Social psychiatry Addiction psychiatry focuses on evaluation and treatment of individuals with alcohol, drug, or other substance-related disorders, and of individuals with dual diagnosis of substance-related and other psychiatric disorders. Biological psychiatry is an approach to psychiatry that aims to understand mental disorders in terms of the biological function of the nervous system. Child and adolescent psychiatry is the branch of psychiatry that specializes in work with children, teenagers, and their families. Community psychiatry is an approach that reflects an inclusive public health perspective and is practiced in community mental health services. Cross-cultural psychiatry is a branch of psychiatry concerned with the cultural and ethnic context of mental disorder and psychiatric services. Emergency psychiatry is the clinical application of psychiatry in emergency settings. Forensic psychiatry utilizes medical science generally, and psychiatric knowledge and assessment methods in particular, to",
    "label": 0
  },
  {
    "text": "psychiatry concerned with the cultural and ethnic context of mental disorder and psychiatric services. Emergency psychiatry is the clinical application of psychiatry in emergency settings. Forensic psychiatry utilizes medical science generally, and psychiatric knowledge and assessment methods in particular, to help answer legal questions. Geriatric psychiatry is a branch of psychiatry dealing with the study, prevention, and treatment of mental disorders in the elderly. Global mental health is an area of study, research and practice that places a priority on improving mental health and achieving equity in mental health for all people worldwide, although some scholars consider it to be a neo-colonial, culturally insensitive project. Liaison psychiatry is the branch of psychiatry that specializes in the interface between other medical specialties and psychiatry. Military psychiatry covers special aspects of psychiatry and mental disorders within the military context. Neuropsychiatry is a branch of medicine dealing with mental disorders attributable to diseases of the nervous system. Social psychiatry is a branch of psychiatry that focuses on the interpersonal and cultural context of mental disorder and mental well-being. Evolutionary psychiatry (or Darwinian psychiatry) is a theoretical orientation within psychiatry that applies principles from evolutionary biology—such as natural selection, adaptation, mismatch, and trade‑offs—to frame hypotheses about the origins, persistence, and variation of mental disorders. In larger healthcare organizations, psychiatrists often serve in senior management roles, where they are responsible for the efficient and effective delivery of mental health services for the organization's constituents. For example, the Chief of Mental Health Services at most VA medical centers is usually a psychiatrist, although psychologists occasionally are selected for the position as well. In the United States, psychiatry is one of the few specialties which qualify for further education and board-certification in pain medicine, palliative medicine, and sleep medicine. Research Psychiatric research is, by its very nature,",
    "label": 0
  },
  {
    "text": "selected for the position as well. In the United States, psychiatry is one of the few specialties which qualify for further education and board-certification in pain medicine, palliative medicine, and sleep medicine. Research Psychiatric research is, by its very nature, interdisciplinary; combining social, biological and psychological perspectives in attempt to understand the nature and treatment of mental disorders. Clinical and research psychiatrists study basic and clinical psychiatric topics at research institutions and publish articles in journals. Under the supervision of institutional review boards, psychiatric clinical researchers look at topics such as neuroimaging, genetics, and psychopharmacology in order to enhance diagnostic validity and reliability, to discover new treatment methods, and to classify new mental disorders. Clinical application Diagnostic systems Psychiatric diagnoses take place in a wide variety of settings and are performed by many different health professionals. Therefore, the diagnostic procedure may vary greatly based upon these factors. Typically, though, a psychiatric diagnosis utilizes a differential diagnosis procedure where a mental status examination and physical examination is conducted, with pathological, psychopathological or psychosocial histories obtained, and sometimes neuroimages or other neurophysiological measurements are taken, or personality tests or cognitive tests administered. In some cases, a brain scan might be used to rule out other medical illnesses, but at this time relying on brain scans alone cannot accurately diagnose a mental illness or tell the risk of getting a mental illness in the future. Some clinicians are beginning to utilize genetics and automated speech assessment during the diagnostic process but on the whole these remain research topics. Potential use of MRI/fMRI in diagnosis In 2018, the American Psychological Association commissioned a review to reach a consensus on whether modern clinical MRI/fMRI will be able to be used in the diagnosis of mental health disorders. The criteria presented by the APA stated that",
    "label": 0
  },
  {
    "text": "diagnosis In 2018, the American Psychological Association commissioned a review to reach a consensus on whether modern clinical MRI/fMRI will be able to be used in the diagnosis of mental health disorders. The criteria presented by the APA stated that the biomarkers used in diagnosis should: \"have a sensitivity of at least 80% for detecting a particular psychiatric disorder\" \"should have a specificity of at least 80% for distinguishing this disorder from other psychiatric or medical disorders\" \"should be reliable, reproducible, and ideally be noninvasive, simple to perform, and inexpensive\" \"proposed biomarkers should be verified by 2 independent studies each by a different investigator and different population samples and published in a peer-reviewed journal\" The review concluded that although neuroimaging diagnosis may technically be feasible, very large studies are needed to evaluate specific biomarkers which were not available. Diagnostic manuals Three main diagnostic manuals used to classify mental health conditions are in use today. The ICD-11 is produced and published by the World Health Organization, includes a section on psychiatric conditions, and is used worldwide. The Diagnostic and Statistical Manual of Mental Disorders, produced and published by the American Psychiatric Association (APA), is primarily focused on mental health conditions and is the main classification tool in the United States. It is currently in its fifth revised edition and is also used worldwide. The Chinese Society of Psychiatry has also produced a diagnostic manual, the Chinese Classification of Mental Disorders. The stated intention of diagnostic manuals is typically to develop replicable and clinically useful categories and criteria, to facilitate consensus and agreed upon standards, whilst being atheoretical as regards etiology. However, the categories are nevertheless based on particular psychiatric theories and data; they are broad and often specified by numerous possible combinations of symptoms, and many of the categories overlap in",
    "label": 0
  },
  {
    "text": "agreed upon standards, whilst being atheoretical as regards etiology. However, the categories are nevertheless based on particular psychiatric theories and data; they are broad and often specified by numerous possible combinations of symptoms, and many of the categories overlap in symptomology or typically occur together. While originally intended only as a guide for experienced clinicians trained in its use, the nomenclature is now widely used by clinicians, administrators and insurance companies in many countries. The DSM has attracted praise for standardizing psychiatric diagnostic categories and criteria. It has also attracted controversy and criticism. Some critics argue that the DSM represents an unscientific system that enshrines the opinions of a few powerful psychiatrists. There are ongoing issues concerning the validity and reliability of the diagnostic categories; the reliance on superficial symptoms; the use of artificial dividing lines between categories and from 'normality'; possible cultural bias; medicalization of human distress and financial conflicts of interest, including with the practice of psychiatrists and with the pharmaceutical industry; political controversies about the inclusion or exclusion of diagnoses from the manual, in general or in regard to specific issues; and the experience of those who are most directly affected by the manual by being diagnosed, including the consumer/survivor movement. Treatment General considerations Individuals receiving psychiatric treatment are commonly referred to as patients but may also be called clients, consumers, or service recipients. They may come under the care of a psychiatric physician or other psychiatric practitioners by various paths, the two most common being self-referral or referral by a primary care physician. Alternatively, a person may be referred by hospital medical staff, by court order, involuntary commitment, or, in countries such as the UK and Australia, by sectioning under a mental health law. A psychiatrist or medical provider evaluates people through a psychiatric assessment for",
    "label": 0
  },
  {
    "text": "may be referred by hospital medical staff, by court order, involuntary commitment, or, in countries such as the UK and Australia, by sectioning under a mental health law. A psychiatrist or medical provider evaluates people through a psychiatric assessment for their mental and physical condition. This usually involves interviewing the person and often obtaining information from other sources such as other health and social care professionals, relatives, associates, law enforcement personnel, emergency medical personnel, and psychiatric rating scales. A mental status examination is carried out, and a physical examination is usually performed to establish or exclude other illnesses that may be contributing to the alleged psychiatric problems. A physical examination may also serve to identify any signs of self-harm; this examination is often performed by someone other than the psychiatrist, especially if blood tests and medical imaging are performed. Like most medications, psychiatric medications can cause adverse effects in patients, and some require ongoing therapeutic drug monitoring, for instance full blood counts, serum drug levels, renal function, liver function or thyroid function. Electroconvulsive therapy (ECT) is sometimes administered for serious conditions, such as those unresponsive to medication. The efficacy and adverse effects of psychiatric drugs may vary from patient to patient. Inpatient treatment Psychiatric treatments have changed over the past several decades. In the past, psychiatric patients were often hospitalized for six months or more, with some cases involving hospitalization for many years. Average inpatient psychiatric treatment stay has decreased significantly since the 1960s, a trend known as deinstitutionalization. Today in most countries, people receiving psychiatric treatment are more likely to be seen as outpatients. If hospitalization is required, the average hospital stay is around one to two weeks, with only a small number receiving long-term hospitalization. However, in Japan psychiatric hospitals continue to keep patients for long periods, sometimes",
    "label": 0
  },
  {
    "text": "to be seen as outpatients. If hospitalization is required, the average hospital stay is around one to two weeks, with only a small number receiving long-term hospitalization. However, in Japan psychiatric hospitals continue to keep patients for long periods, sometimes even keeping them in physical restraints, strapped to their beds for periods of weeks or months. Psychiatric inpatients are people admitted to a hospital or clinic to receive psychiatric care. Some are admitted involuntarily, perhaps committed to a secure hospital, or in some jurisdictions to a facility within the prison system. In many countries including the United States and Canada, the criteria for involuntary admission vary with local jurisdiction. They may be as broad as having a mental health condition, or as narrow as being an immediate danger to themselves or others. Bed availability is often the real determinant of admission decisions to hard pressed public facilities. People may be admitted voluntarily if the treating doctor considers that safety is not compromised by this less restrictive option. For many years, controversy has surrounded the use of involuntary treatment and use of the term \"lack of insight\" in describing patients. Internationally, mental health laws vary significantly but in many cases, involuntary psychiatric treatment is permitted when there is deemed to be a significant risk to the patient or others due to the patient's illness. Involuntary treatment refers to treatment that occurs based on a treating physician's recommendations, without requiring consent from the patient. Inpatient psychiatric wards may be secure (for those thought to have a particular risk of violence or self-harm) or unlocked/open. Some wards are mixed-sex whilst same-sex wards are increasingly favored to protect women inpatients. Once in the care of a hospital, people are assessed, monitored, and often given medication and care from a multidisciplinary team, which may include",
    "label": 0
  },
  {
    "text": "or unlocked/open. Some wards are mixed-sex whilst same-sex wards are increasingly favored to protect women inpatients. Once in the care of a hospital, people are assessed, monitored, and often given medication and care from a multidisciplinary team, which may include physicians, pharmacists, psychiatric nurse practitioners, psychiatric nurses, clinical psychologists, psychotherapists, psychiatric social workers, occupational therapists and social workers. If a person receiving treatment in a psychiatric hospital is assessed as at particular risk of harming themselves or others, they may be put on constant or intermittent one-to-one supervision and may be put in physical restraints or medicated. People on inpatient wards may be allowed leave for periods of time, either accompanied or on their own. In many developed countries there has been a massive reduction in psychiatric beds since the mid 20th century, with the growth of community care. Italy has been a pioneer in psychiatric reform, particularly through the no-restraint initiative that began nearly fifty years ago. The Italian movement, heavily influenced by Franco Basaglia, emphasizes ethical treatment and the elimination of physical restraints in psychiatric care. A study examining the application of these principles in Italy found that 14 general hospital psychiatric units reported zero restraint incidents in 2022. Standards of inpatient care remain a challenge in some public and private facilities, due to levels of funding, and facilities in developing countries are typically grossly inadequate for the same reason. Even in developed countries, programs in public hospitals vary widely. Some may offer structured activities and therapies offered from many perspectives while others may only have the funding for medicating and monitoring patients. This may be problematic in that the maximum amount of therapeutic work might not actually take place in the hospital setting. This is why hospitals are increasingly used in limited situations and moments of crisis",
    "label": 0
  },
  {
    "text": "for medicating and monitoring patients. This may be problematic in that the maximum amount of therapeutic work might not actually take place in the hospital setting. This is why hospitals are increasingly used in limited situations and moments of crisis where patients are a direct threat to themselves or others. Alternatives to psychiatric hospitals that may actively offer more therapeutic approaches include rehabilitation centers or \"rehab\" as popularly termed. Outpatient treatment Outpatient treatment involves periodic visits to a psychiatrist for consultation in his or her office, or at a community-based outpatient clinic. During initial appointments, a psychiatrist generally conducts a psychiatric assessment or evaluation of the patient. Follow-up appointments then focus on making medication adjustments, reviewing potential medication interactions, considering the impact of other medical disorders on the patient's mental and emotional functioning, and counseling patients regarding changes they might make to facilitate healing and remission of symptoms. The frequency with which a psychiatrist sees people in treatment varies widely, from once a week to twice a year, depending on the type, severity and stability of each person's condition, and depending on what the clinician and patient decide would be best. Increasingly, psychiatrists are limiting their practices to psychopharmacology (prescribing medications), as opposed to previous practice in which a psychiatrist would provide traditional 50-minute psychotherapy sessions, of which psychopharmacology would be a part, but most of the consultation sessions consisted of \"talk therapy\". This shift began in the early 1980s and accelerated in the 1990s and 2000s. A major reason for this change was the advent of managed care insurance plans, which began to limit reimbursement for psychotherapy sessions provided by psychiatrists. The underlying assumption was that psychopharmacology was at least as effective as psychotherapy, and it could be delivered more efficiently because less time is required for the appointment.",
    "label": 0
  },
  {
    "text": "plans, which began to limit reimbursement for psychotherapy sessions provided by psychiatrists. The underlying assumption was that psychopharmacology was at least as effective as psychotherapy, and it could be delivered more efficiently because less time is required for the appointment. Because of this shift in practice patterns, psychiatrists often refer patients whom they think would benefit from psychotherapy to other mental health professionals, e.g., clinical social workers and psychologists. Telepsychiatry History Earliest knowledge The earliest known texts on mental disorders are from ancient India and include the Ayurvedic text, Charaka Samhita. The first hospitals for curing mental illness were established in India during the 3rd century BCE. Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise), also addressed the workings of the mind. As early as the 4th century BC, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes. In 387 BCE, Plato suggested that the brain is where mental processes take place. In 4th to 5th century B.C. Greece, Hippocrates wrote that he visited Democritus and found him in his garden cutting open animals. Democritus explained that he was attempting to discover the cause of madness and melancholy. Hippocrates praised his work. Democritus had with him a book on madness and melancholy. During the 5th century BCE, mental disorders, especially those with psychotic traits, were considered supernatural in origin, a view which existed throughout ancient Greece and Rome, as well as Egyptian regions. Alcmaeon, believed the brain, not the heart, was the \"organ of thought\". He tracked the ascending sensory nerves from the body to the brain, theorizing that mental activity originated in the CNS and that the cause of mental illness resided within the brain. He applied this understanding to classify mental diseases and treatments. Religious leaders often turned",
    "label": 0
  },
  {
    "text": "nerves from the body to the brain, theorizing that mental activity originated in the CNS and that the cause of mental illness resided within the brain. He applied this understanding to classify mental diseases and treatments. Religious leaders often turned to versions of exorcism to treat mental disorders often utilizing methods that many consider to be cruel or barbaric methods. Trepanning was one of these methods used throughout history. In the 6th century AD, Lin Xie carried out an early psychological experiment, in which he asked people to draw a square with one hand and at the same time draw a circle with the other (ostensibly to test people's vulnerability to distraction). It has been cited that this was an early psychiatric experiment. The Islamic Golden Age fostered early studies in Islamic psychology and psychiatry, with many scholars writing about mental disorders. The Persian physician Muhammad ibn Zakariya al-Razi, also known as \"Rhazes\", wrote texts about psychiatric conditions in the 9th century. As chief physician of a hospital in Baghdad, he was also the director of one of the first bimaristans in the world. The first bimaristan was founded in Baghdad in the 9th century, and several others of increasing complexity were created throughout the Arab world in the following centuries. Some of the bimaristans contained wards dedicated to the care of mentally ill patients. During the Middle Ages, Psychiatric hospitals and lunatic asylums were built and expanded throughout Europe. Specialist hospitals such as Bethlem Royal Hospital in London were built in medieval Europe from the 13th century to treat mental disorders, but were used only as custodial institutions and did not provide any type of treatment. It is the oldest extant psychiatric hospital in the world. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies",
    "label": 0
  },
  {
    "text": "mental disorders, but were used only as custodial institutions and did not provide any type of treatment. It is the oldest extant psychiatric hospital in the world. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing Dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams, insomnia, psychosis, depression and epilepsy. Medical specialty The beginning of psychiatry as a medical specialty is dated to the middle of the nineteenth century, although its germination can be traced to the late eighteenth century. In the late 17th century, privately run asylums for the insane began to proliferate and expand in size. In 1713, the Bethel Hospital Norwich was opened, the first purpose-built asylum in England. In 1656, Louis XIV of France created a public system of hospitals for those with mental disorders, but as in England, no real treatment was applied. During the Enlightenment, attitudes towards the mentally ill began to change. It came to be viewed as a disorder that required compassionate treatment. In 1758, English physician William Battie wrote his Treatise on Madness on the management of mental disorder. It was a critique aimed particularly at the Bethlem Royal Hospital, where a conservative regime continued to use barbaric custodial treatment. Battie argued for a tailored management of patients entailing cleanliness, good food, fresh air, and distraction from friends and family. He argued that mental disorder",
    "label": 0
  },
  {
    "text": "at the Bethlem Royal Hospital, where a conservative regime continued to use barbaric custodial treatment. Battie argued for a tailored management of patients entailing cleanliness, good food, fresh air, and distraction from friends and family. He argued that mental disorder originated from dysfunction of the material brain and body rather than the internal workings of the mind. The introduction of moral treatment was initiated independently by the French doctor Philippe Pinel and the English Quaker William Tuke. In 1792, Pinel became the chief physician at the Bicêtre Hospital. Patients were allowed to move freely about the hospital grounds, and eventually dark dungeons were replaced with sunny, well-ventilated rooms. Pinel's student and successor, Jean Esquirol (1772–1840), went on to help establish 10 new mental hospitals that operated on the same principles. Although Tuke, Pinel and others had tried to do away with physical restraint, it remained widespread into the 19th century. At the Lincoln Asylum in England, Robert Gardiner Hill, with the support of Edward Parker Charlesworth, pioneered a mode of treatment that suited \"all types\" of patients, so that mechanical restraints and coercion could be dispensed with—a situation he finally achieved in 1838. In 1839, Sergeant John Adams and Dr. John Conolly were impressed by the work of Hill, and introduced the method into their Hanwell Asylum, by then the largest in the country. The modern era of institutionalized provision for the care of the mentally ill, began in the early 19th century with a large state-led effort. In England, the Lunacy Act 1845 was an important landmark in the treatment of the mentally ill, as it explicitly changed the status of mentally ill people to patients who required treatment. All asylums were required to have written regulations and to have a resident qualified physician. In 1838, France enacted a",
    "label": 0
  },
  {
    "text": "treatment of the mentally ill, as it explicitly changed the status of mentally ill people to patients who required treatment. All asylums were required to have written regulations and to have a resident qualified physician. In 1838, France enacted a law to regulate both the admissions into asylums and asylum services across the country. In the United States, the erection of state asylums began with the first law for the creation of one in New York, passed in 1842. The Utica State Hospital was opened around 1850. Many state hospitals in the United States were built in the 1850s and 1860s on the Kirkbride Plan, an architectural style meant to have curative effect. At the turn of the century, England and France combined had only a few hundred individuals in asylums. By the late 1890s and early 1900s, this number had risen to the hundreds of thousands. However, the idea that mental illness could be ameliorated through institutionalization ran into difficulties. Psychiatrists were pressured by an ever-increasing patient population, and asylums again became almost indistinguishable from custodial institutions. In the early 1800s, psychiatry made advances in the diagnosis of mental illness by broadening the category of mental disease to include mood disorders, in addition to disease level delusion or irrationality. The 20th century introduced a new psychiatry into the world, with different perspectives of looking at mental disorders. For Emil Kraepelin, the initial ideas behind biological psychiatry, stating that the different mental disorders are all biological in nature, evolved into a new concept of \"nerves\", and psychiatry became a rough approximation of neurology and neuropsychiatry. Following Sigmund Freud's pioneering work, ideas stemming from psychoanalytic theory also began to take root in psychiatry. The psychoanalytic theory became popular among psychiatrists because it allowed the patients to be treated in private practices",
    "label": 0
  },
  {
    "text": "approximation of neurology and neuropsychiatry. Following Sigmund Freud's pioneering work, ideas stemming from psychoanalytic theory also began to take root in psychiatry. The psychoanalytic theory became popular among psychiatrists because it allowed the patients to be treated in private practices instead of warehoused in asylums. By the 1970s, however, the psychoanalytic school of thought became marginalized within the field. Biological psychiatry reemerged during this time. Psychopharmacology and neurochemistry became the integral parts of psychiatry starting with Otto Loewi's discovery of the neuromodulatory properties of acetylcholine; thus identifying it as the first-known neurotransmitter. Subsequently, it has been shown that different neurotransmitters have different and multiple functions in regulation of behaviour. In a wide range of studies in neurochemistry using human and animal samples, individual differences in neurotransmitters' production, reuptake, receptors' density and locations were linked to differences in dispositions for specific psychiatric disorders. For example, the discovery of chlorpromazine's effectiveness in treating schizophrenia in 1952 revolutionized treatment of the disorder, as did lithium carbonate's ability to stabilize mood highs and lows in bipolar disorder in 1948. Psychotherapy was still utilized, but as a treatment for psychosocial issues. This proved the idea of neurochemical nature of many psychiatric disorders. Another approach to look for biomarkers of psychiatric disorders is neuroimaging that was first utilized as a tool for psychiatry in the 1980s. In 1963, US president John F. Kennedy introduced legislation delegating the National Institute of Mental Health to administer Community Mental Health Centers for those being discharged from state psychiatric hospitals. Later, though, the Community Mental Health Centers focus shifted to providing psychotherapy for those with acute but less serious mental disorders. Ultimately there were no arrangements made for actively following and treating severely mentally ill patients who were being discharged from hospitals, resulting in a large population of chronically homeless",
    "label": 0
  },
  {
    "text": "providing psychotherapy for those with acute but less serious mental disorders. Ultimately there were no arrangements made for actively following and treating severely mentally ill patients who were being discharged from hospitals, resulting in a large population of chronically homeless people with mental illness. Controversy and criticism The institution of psychiatry has attracted controversy since its inception. Scholars including those from social psychiatry, psychoanalysis, psychotherapy, and critical psychiatry have produced critiques. It has been argued that psychiatry confuses disorders of the mind with disorders of the brain that can be treated with drugs; that its use of drugs is in part due to lobbying by drug companies resulting in distortion of research; and that the concept of \"mental illness\" is often used to label and control those with beliefs and behaviours that the majority of people disagree with; and that it is too influenced by ideas from medicine causing it to misunderstand the nature of mental distress. Critique of psychiatry from within the field comes from the critical psychiatry group in the UK. Double argues that most critical psychiatry is anti-reductionist. Rashed argues new mental health science has moved beyond this reductionist critique by seeking integrative and biopsychosocial models for conditions and that much of critical psychiatry now exists with orthodox psychiatry but notes that many critiques remain unaddressed. The term anti-psychiatry was coined by psychiatrist David Cooper in 1967 and was later made popular by Thomas Szasz. The word Antipsychiatrie was already used in Germany in 1904. The basic premise of the anti-psychiatry movement is that psychiatrists attempt to classify \"normal\" people as \"deviant\"; psychiatric treatments are ultimately more damaging than helpful to patients; and psychiatry's history involves (what may now be seen as) dangerous treatments, such as psychosurgery, an example of this being the frontal lobectomy (commonly called",
    "label": 0
  },
  {
    "text": "A psychologist is a professional who practices psychology and studies mental states, perceptual, cognitive, emotional, and social processes and behavior. Their work often involves the experimentation, observation, and interpretation of how individuals relate to each other and to their environments. Psychologists usually acquire a bachelor's degree in psychology, followed by a master's degree or doctorate in psychology. Unlike psychiatrists and psychiatric nurse-practitioners, psychologists usually cannot prescribe medication, but depending on the jurisdiction, some psychologists with additional training can be licensed to prescribe medications; qualification requirements may be different from a bachelor's degree and master's degree. Psychologists receive extensive training in psychological testing, communication techniques, scoring, interpretation, and reporting, while psychiatrists are not usually trained in psychological testing. Psychologists are also trained in, and often specialize in, one or more psychotherapies to improve symptoms of many mental disorders, including but not limited to treatment for anxiety, depression, post-traumatic stress disorder, schizophrenia, bipolar disorder, personality disorders and eating disorders. Treatment from psychologists can be individual or in groups. Cognitive behavioral therapy is a commonly used, well studied and high efficacy psychotherapy practiced by psychologists. Psychologists can work with a range of institutions and people, such as schools, prisons, in a private clinic, in a workplace, or with a sports team. Applied psychology applies theory to solve problems in human and animal behavior. Applied fields include clinical psychology, counseling psychology, sport psychology, forensic psychology, industrial and organizational psychology, health psychology and school psychology. Licensing and regulations can vary by state and profession. Australia In Australia, the psychology profession, and the use of the title \"psychologist\", is regulated by an Act of Parliament, the Health Practitioner Regulation (Administrative Arrangements) National Law Act 2008, following an agreement between state and territorial governments. Under this national law, registration of psychologists is administered by the Psychology Board",
    "label": 0
  },
  {
    "text": "title \"psychologist\", is regulated by an Act of Parliament, the Health Practitioner Regulation (Administrative Arrangements) National Law Act 2008, following an agreement between state and territorial governments. Under this national law, registration of psychologists is administered by the Psychology Board of Australia (PsyBA). Before July 2010, the professional registration of psychologists was governed by various state and territorial Psychology Registration Boards. The Australian Psychology Accreditation Council (APAC) oversees education standards for the profession. The minimum requirements for general registration in psychology, including the right to use the title \"psychologist\", are an APAC approved four-year degree in psychology followed by either a two-year master's program or two years of practice supervised by a registered psychologist. However, the Australian Health Practitioner Regulation Agency (AHPRA) is currently in the process of phasing out the 4 + 2 internship pathway. Once the 4 + 2 pathway is phased out, a master's degree or PhD will be required to become a psychologist in Australia. This is because of concerns about public safety, and to reduce the burden of training on employers. There is also a '5 + 1' registration pathway, including a four-year APAC approved degree followed by one year of postgraduate study and one year of supervised practice. Endorsement within a specific area of practice requires additional qualifications. These notations are not \"specialist\" titles (Western Australian psychologists could use \"specialist\" in their titles during a three-year transitional period from 17 October 2010 to 17 October 2013). Membership with the Australian Psychological Society (APS) differs from registration as a psychologist. The standard route to full membership (MAPS) of the APS usually requires four years of APAC-accredited undergraduate study, plus a master's or doctorate in psychology from an accredited institution. An alternate route is available for academics and practitioners who have gained appropriate experience and made",
    "label": 0
  },
  {
    "text": "membership (MAPS) of the APS usually requires four years of APAC-accredited undergraduate study, plus a master's or doctorate in psychology from an accredited institution. An alternate route is available for academics and practitioners who have gained appropriate experience and made a substantial contribution to the field of psychology. Restrictions apply to all individuals using the title \"psychologist\" in all states and territories of Australia. However, the terms \"psychotherapist\", \"social worker\", and \"counselor\" are currently self-regulated, with several organizations campaigning for government regulation. Belgium Since 1933, the title \"psychologist\" has been protected by law in Belgium. It can only be used by people who are on the National Government Commission list. The minimum requirement is the completion of five years of university training in psychology (master's degree or equivalent). The title of \"psychotherapist\" is not legally protected. As of 2016, Belgian law recognizes the clinical psychologist as an autonomous health profession. It reserves the practice of psychotherapy to medical doctors, clinical psychologists and clinical orthopedagogists. Canada A professional in the U.S. or Canada must hold a graduate degree in psychology (MA, Psy.D., Ed.D., or Ph.D.), or have a provincial license to use the title \"psychologist\". Provincial regulators include: Alberta: College of Alberta Psychologists British Columbia: College of Psychologists of British Columbia Manitoba: Psychological Association of Manitoba Newfoundland and Labrador: Newfoundland and Labrador Psychology Board New Brunswick: College of Psychologists of New Brunswick Northwest Territories: Office of the Registrar, Northwest Territories (NWT) Professional Licensing Nova Scotia: Nova Scotia Board of Examiners in Psychology Nunavut: Registrar, Professional Licensing Kugluktuk Ontario: College of Psychologists of Ontario Prince Edward Island: Prince Edward Island Psychologists Registration Board Quebec: Order of Psychologists of Quebec Saskatchewan: Saskatchewan College of Psychologists Dominican Republic A professional psychologist in the Dominican Republic must have a suitable qualification and be a member",
    "label": 0
  },
  {
    "text": "of Ontario Prince Edward Island: Prince Edward Island Psychologists Registration Board Quebec: Order of Psychologists of Quebec Saskatchewan: Saskatchewan College of Psychologists Dominican Republic A professional psychologist in the Dominican Republic must have a suitable qualification and be a member of the Dominican College of Psychologists. Finland In Finland, the title \"psychologist\" is protected by law. The restriction for psychologists (licensed professionals) is governed by National Supervisory Authority for Welfare and Health (Finland) (Valvira). It takes 330 ECTS-credits (about six years) to complete the university studies (master's degree). There are about 6,200 licensed psychologists in Finland. Germany In Germany, the use of the title Diplom-Psychologe (Dipl.-Psych.) is restricted by law, and a practitioner is legally required to hold the corresponding academic title, which is comparable to a M.Sc. degree and requires at least five years of training at a university. Originally, a diploma degree in psychology awarded in Germany included the subject of clinical psychology. With the Bologna-reform, this degree was replaced by a master's degree. The academic degree of Diplom-Psychologe or M.Sc. (Psychologie) does not include a psychotherapeutic qualification, which requires three to five years of additional training. The psychotherapeutic training combines in-depth theoretical knowledge with supervised patient care and self-reflection units. After having completed the training requirements, psychologists take a state-run exam, which, upon successful completion (Approbation), confers the official title of \"psychological psychotherapist\" (Psychologischer Psychotherapeut). After many years of inter-professional political controversy, non-physician psychotherapy was given an adequate legal foundation through the creation of two new academic healthcare professions. Greece Since 1979, the title \"psychologist\" has been protected by law in Greece. It can only be used by people who hold a relevant license or certificate, which is issued by the Greek authorities, to practice as a psychologist. The minimum requirement is the completion of university training",
    "label": 0
  },
  {
    "text": "protected by law in Greece. It can only be used by people who hold a relevant license or certificate, which is issued by the Greek authorities, to practice as a psychologist. The minimum requirement is the completion of university training in psychology at a Greek university, or at a university recognized by the Greek authorities. Psychologists in Greece are legally required to abide by the Code of Conduct of Psychologists (2019). Psychologists in Greece are not required to register with any psychology body in the country in order to legally practice the profession. Titles such as \"psychotherapist\" or \"counselor\" are not protected by law in Greece and anyone may call themselves a \"psychotherapist\" or \"counselor\" without having earned a graduate degree in psychology. India In India, \"clinical psychologist\" is specifically defined in the Mental Health Act, 2017. An MPhil in Clinical Psychology degree of two years duration recognized by the Rehabilitation Council of India is required to apply for registration as a clinical psychologist. PsyD and Professional diploma in Clinical Psychology is also a less popular way to get license of Clinical Psychologist in India. This procedure has been criticized by some stakeholders since clinical psychology is not limited to the area of rehabilitation. Titles such as \"counselor\", \"psychoanalyst\", \"psychoeducator\" or \"psychotherapist\" are not protected at present. In other words, an individual may call themselves a \"psychotherapist\" or \"counselor\" without having any recognized degree from Rehabilitation council of India and without having to register with the Rehabilitation Council of India. Rehabilitation psychologists also require a license from RCI to practice. Psychologs magazine is the major media, working on mental health awareness. Tele-MANAS is a nationwide governmental program launched by Ministry of Health & Family Welfare in October 2021. New Zealand In New Zealand, the use of the title \"psychologist\" is",
    "label": 0
  },
  {
    "text": "Psychologs magazine is the major media, working on mental health awareness. Tele-MANAS is a nationwide governmental program launched by Ministry of Health & Family Welfare in October 2021. New Zealand In New Zealand, the use of the title \"psychologist\" is restricted by law. Prior to 2004, only the title \"registered psychologist\" was restricted to people qualified and registered as such. However, with the proclamation of the Health Practitioners Competence Assurance Act, in 2003, the use of the title \"psychologist\" was limited to practitioners registered with the New Zealand Psychologists Board. The titles \"clinical psychologist\", \"counseling psychologist\", \"educational psychologist\", \"intern psychologist\", and \"trainee psychologist\" are similarly protected. This is to protect the public by providing assurance that the title-holder is registered and therefore qualified and competent to practice, and can be held accountable. The legislation does not include an exemption clause for any class of practitioner (e.g., academics, or government employees). Norway In Norway, the title \"psychologist\" is restricted by law and can only be obtained by completing a six-year integrated program, leading to the Candidate of Psychology degree. Psychologists are considered health personnel, and their work is regulated through the \"health personnel act\". South Africa In South Africa, psychologists are qualified in either clinical, counseling, educational, organizational, or research psychology. As below, qualification requires at least five years of study, and at least one of internship. To become qualified, one must complete a recognized master's degree in Psychology, an appropriate practicum at a recognized training institution, and take an examination set by the Professional Board for Psychology. Registration with the Health Professions Council of South Africa (HPCSA) is required and includes a Continuing Professional Development component. The practicum usually involves a full year internship, and in some specializations, the HPCSA requires completion of an additional year of community service. The",
    "label": 0
  },
  {
    "text": "Health Professions Council of South Africa (HPCSA) is required and includes a Continuing Professional Development component. The practicum usually involves a full year internship, and in some specializations, the HPCSA requires completion of an additional year of community service. The master's program consists of seminars, coursework-based theoretical and practical training, and a dissertation of limited scope, and is (in most cases) two years in duration. Prior to enrolling in the master's program, the student studies psychology for three years as an undergraduate (B.A. or B.Sc., and, for organizational psychology, also B.Com.), followed by an additional postgraduate honours degree in psychology; see List of universities in South Africa. The undergraduate B.Psyc. is a four-year program integrating theory and practical training, and—with the required examination set by the Professional Board for Psychology—is sufficient for practice as a psychometrist or counselor. United Kingdom In the UK, \"registered psychologist\" and \"practitioner psychologist\" are protected titles. The title of \"neuropsychologist\" is not protected. In addition, the following specialist titles are also protected by law: \"clinical psychologist\", \"counselling psychologist\", \"educational psychologist\", \"forensic psychologist\", \"health psychologist\", \"occupational psychologist\" and \"sport and exercise psychologist\". The Health and Care Professions Council (HCPC) is the statutory regulator for practitioner psychologists in the UK. In the UK, the use of the title \"chartered psychologist\" is also protected by statutory regulation, but that title simply means that the psychologist is a chartered member of the British Psychological Society, but is not necessarily registered with the HCPC. However, it is illegal for someone who is not in the appropriate section of the HCPC register to provide psychological services. The requirement to register as a clinical, counselling, or educational psychologist is a professional doctorate (and in the case of the latter two the British Psychological Society's Professional Qualification, which meets the standards of a",
    "label": 0
  },
  {
    "text": "register to provide psychological services. The requirement to register as a clinical, counselling, or educational psychologist is a professional doctorate (and in the case of the latter two the British Psychological Society's Professional Qualification, which meets the standards of a professional doctorate). The title of \"psychologist\", by itself, is not protected. The British Psychological Society is working with the HCPC to ensure that the title of \"neuropsychologist\" is regulated as a specialist title for practitioner psychologists. Employment As of December 2012, in the United Kingdom, there are 19,000 practitioner psychologists registered across seven categories: clinical psychologist, counselling psychologist, educational psychologist, forensic psychologist, health psychologist, occupational psychologist, sport and exercise psychologist. At least 9,500 of these are clinical psychologists, which is the largest group of psychologists in clinical settings such as the NHS. Around 2,000 are educational psychologists. United States and Canada Education and training In the United States and Canada, full membership in each country's professional association—American Psychological Association (APA) and Canadian Psychological Association (CPA), respectively—requires doctoral training (except in some Canadian provinces, such as Alberta, where a master's degree is sufficient). The minimal requirement for full membership can be waived in circumstances where there is evidence that significant contribution or performance in the field of psychology has been made. Associate membership requires at least two years of postgraduate studies in psychology or an approved related discipline. Some U.S. schools offer accredited programs in clinical psychology resulting in a master's degree. Such programs can range from forty-eight to eighty-four units, most often taking two to three years to complete after the undergraduate degree. Training usually emphasizes theory and treatment over research, quite often with a focus on school or couples and family counseling. Similar to doctoral programs, master's level students usually must complete a clinical practicum under supervision; some programs",
    "label": 0
  },
  {
    "text": "the undergraduate degree. Training usually emphasizes theory and treatment over research, quite often with a focus on school or couples and family counseling. Similar to doctoral programs, master's level students usually must complete a clinical practicum under supervision; some programs also require a minimum amount of personal psychotherapy. While many graduates from master's level training go on to doctoral psychology programs, a large number also go directly into practice—often as a licensed professional counselor (LPC), marriage and family therapist (MFT), or other similar licensed practice, which varies by state. There is stiff competition to gain acceptance into clinical psychology doctoral programs (acceptance rates of 2–5% are not uncommon). Clinical psychologists in the U.S. undergo many years of graduate training—usually five to seven years after the bachelor's degree—to gain demonstrable competence and experience. Licensure as a psychologist may take an additional one to two years post-PhD/PsyD. Some states require a 1-year postdoctoral residency, while others do not require postdoctoral supervised experience and allow psychology graduates to sit for the licensure exam immediately. Some psychology specialties, such as clinical neuropsychology, require a 2-year postdoctoral experience regardless of the state, as set forth in the Houston Conference Guidelines. Today in America, about half of all clinical psychology graduate students are being trained in PhD programs that emphasize melding research with practice and are conducted by universities—with the other half in PsyD programs, which less focus on research (similar to professional degrees for medicine and law). Both types of doctoral programs (PhD and PsyD) envision practicing clinical psychology in a research-based, scientifically valid manner, and most are accredited by the APA. APA accreditation is very important for U.S. clinical, counseling, and school psychology programs because graduating from a non-accredited doctoral program may adversely affect employment prospects and present a hurdle for becoming licensed in",
    "label": 0
  },
  {
    "text": "and most are accredited by the APA. APA accreditation is very important for U.S. clinical, counseling, and school psychology programs because graduating from a non-accredited doctoral program may adversely affect employment prospects and present a hurdle for becoming licensed in some jurisdictions. Doctorate (PhD and PsyD) programs usually involve some variation on the following five to seven year, 90–120 unit curriculum: Bases of behavior—biological, cognitive-affective, and cultural-social Individual differences—personality, lifespan development, psychopathology History and systems—development of psychological theories, practices and scientific knowledge Clinical practice—diagnostics, psychological assessment, psychotherapeutic interventions, psychopharmacology, ethical and legal issues Coursework in statistics and research design Clinical experience Practicum—usually three or four years of working with clients under supervision in a clinical setting. Most practicum placements begin in either the first or second year of doctoral training. Doctoral internship—usually an intensive one or two-year placement in a clinical setting Dissertation—PhD programs usually require original quantitative empirical research, while PsyD dissertations involve original quantitative or qualitative research, theoretical scholarship, program evaluation or development, critical literature analysis or clinical application and analysis. The dissertation typically takes 2–3 years to complete. Specialized electives—many programs offer sets of elective courses for specializations, such as health, child/adolescent, family, community, or neuropsychology. Personal psychotherapy—many programs require students to undertake a certain number of hours of personal psychotherapy (with a non-faculty therapist) although in recent years this requirement has become less frequent. Comprehensive exams or master's thesis: a thesis can involve original data collection and is distinct from a dissertation. Psychologists can be seen as practicing within two general categories of psychology: health service psychology, which includes \"practitioners\" or \"professionals\" and research-oriented psychology which includes \"scientists\" or \"scholars\". The training models (Boulder and Vail models) endorsed by the APA require that health service psychologists be trained as both researchers and practitioners, and that they",
    "label": 0
  },
  {
    "text": "psychology, which includes \"practitioners\" or \"professionals\" and research-oriented psychology which includes \"scientists\" or \"scholars\". The training models (Boulder and Vail models) endorsed by the APA require that health service psychologists be trained as both researchers and practitioners, and that they possess advanced degrees. Psychologists typically have one of two degrees: PsyD or PhD. The PsyD program prepares the student primarily as a practitioner for clinical practice (e.g., testing, psychotherapy), but also as a scholar that consumes research. Depending on the specialty (industrial/organizational, social, clinical, school, etc.), a PhD may be trained in clinical practice as well as in scientific methodology, to prepare for a career in academia or research. Both the PsyD and PhD programs prepare students to take the national psychology licensing exam, the Examination for Professional Practice in Psychology (EPPP). Within the two main categories are many further types of psychologists as reflected by APA's 54 Divisions, which are specialty or subspecialty or topical areas, including clinical, counseling, and school psychologists. Such professionals work with persons in a variety of therapeutic contexts. People often think of the discipline as involving only such clinical or counseling psychologists. While counseling and psychotherapy are common activities for psychologists, these health service psychology fields are just two branches in the larger domain of psychology. There are other classifications such as industrial and organizational and community psychologists, whose professionals mainly apply psychological research, theories, and techniques to \"real-world\" problems of business, industry, social benefit organizations, government, and academia. APA-recognized specialties Clinical psychology Clinical neuropsychology Clinical child and adolescent psychology School psychology Behavioral and cognitive psychology Couple and family psychology Clinical health psychology Geropsychology Police and public safety psychology Sleep psychology Rehabilitation psychology Group psychology and group psychotherapy Forensic psychology Industrial and organizational psychology Psychoanalysis Counseling psychology Serious mental illness psychology Clinical psychopharmacology Clinical",
    "label": 0
  },
  {
    "text": "cognitive psychology Couple and family psychology Clinical health psychology Geropsychology Police and public safety psychology Sleep psychology Rehabilitation psychology Group psychology and group psychotherapy Forensic psychology Industrial and organizational psychology Psychoanalysis Counseling psychology Serious mental illness psychology Clinical psychopharmacology Clinical psychologists receive training in a number of psychological therapies, including behavioral, cognitive, humanistic, existential, psychodynamic, and systemic approaches, as well as in-depth training in psychological testing, and to some extent, neuropsychological testing. Services Clinical psychologists can offer a range of professional services, including: Psychological treatment (psychotherapy) Administering, scoring, and interpreting psychological tests Prescribing medications (in six states) Conducting psychological research Teaching Developing prevention programs Consulting Program administration Expert testimony Supervision of students or other mental health professionals In practice, clinical psychologists might work with individuals, couples, families, or groups in a variety of settings, including private practices, hospitals, community mental health centers, schools, businesses, and non-profit agencies. Most clinical psychologists who engage in research and teaching do so within a college or university setting. Clinical psychologists may also choose to specialize in a particular field. Prescriptive Authority for Psychologists (RxP) Psychologists in the United States campaigned for legislative changes to enable specially-trained psychologists to prescribe psychotropic medications. Legislation in Idaho, Iowa, Louisiana, New Mexico, Illinois, and Colorado has granted those who complete an additional master's degree program in clinical psychopharmacology authority to prescribe medications for mental and emotional disorders. As of 2019, Louisiana is the only state where the licensing and regulation of the practice of medical psychology by medical psychologists (MPs) is regulated by a medical board (the Louisiana State Board of Medical Examiners) rather than a board of psychologists. While other states have pursued prescriptive authority, they have not succeeded. Similar legislation in the states of Hawaii and Oregon passed through their respective legislative bodies, but in each",
    "label": 0
  },
  {
    "text": "State Board of Medical Examiners) rather than a board of psychologists. While other states have pursued prescriptive authority, they have not succeeded. Similar legislation in the states of Hawaii and Oregon passed through their respective legislative bodies, but in each case the legislation was vetoed by the state's governor. In 1989, the U.S Department of Defense was directed to create the Psychopharmacology Demonstration Project (PDP). By 1997, ten psychologists were trained in psychopharmacology and granted the ability to prescribe psychiatric medications. Licensure The practice of clinical psychology requires a license in the United States and Canada. Although each of the U.S. states is different in terms of requirements and licenses (see and for examples), there are three common requirements: Graduation from an accredited school with the appropriate degree Completion of supervised clinical experience Passing a written and/or oral examination All U.S. state and Canada provincial licensing boards are members of the Association of State and Provincial Psychology Boards (ASPPB) which created and maintains the Examination for Professional Practice in Psychology (EPPP). Many states require other examinations in addition to the EPPP, such as a jurisprudence (i.e., mental health law) examination or an oral examination. Nearly all states also require a certain number of continuing education credits per year in order to renew a license. Licensees can obtain this through various means, such as taking audited classes and attending approved workshops. There are professions whose scope of practice overlaps with the practice of psychology (particularly with respect to providing psychotherapy) and for which a license is required. Ambiguity of title To practice with the title of \"psychologist\", in almost all cases a doctoral degree is required (PhD, PsyD, or EdD in the U.S.). Normally, after the degree, the practitioner must fulfill a certain number of supervised postdoctoral hours ranging from 1,500",
    "label": 0
  },
  {
    "text": "practice with the title of \"psychologist\", in almost all cases a doctoral degree is required (PhD, PsyD, or EdD in the U.S.). Normally, after the degree, the practitioner must fulfill a certain number of supervised postdoctoral hours ranging from 1,500 to 3,000 (usually taking one to two years), and pass the EPPP and any other state or provincial exams. By and large, a professional in the U.S. must hold a doctoral degree in psychology (PsyD, EdD, or PhD), and/or have a state license to use the title psychologist. However, regulations vary from state to state. For example, in the states of Michigan, West Virginia, and Vermont, there are psychologists licensed at the master's level. Differences with psychiatrists Although clinical psychologists and psychiatrists share the same fundamental aim—the alleviation of mental distress—their training, outlook, and methodologies are often different. Perhaps the most significant difference is that psychiatrists are licensed physicians, and, as such, psychiatrists are apt to use the medical model to assess mental health problems and to also employ psychotropic medications as a method of addressing mental health problems. Psychologists generally do not prescribe medication, although in some jurisdictions they do have prescription privileges. In five U.S. states (New Mexico, Louisiana, Illinois, Iowa, Idaho, and Colorado), psychologists with clinical psychopharmacology training have been granted prescriptive authority for mental health disorders. Psychologists receive extensive training in psychological test administration, scoring, interpretation, and reporting, while psychiatrists are not trained in psychological testing. In addition, psychologists (particularly those from PhD programs) spend several years in graduate school being trained to conduct behavioral research; their training includes research design and advanced statistical analysis. While this training is available for physicians via dual MD/PhD programs, it is not typically included in standard medical education, although psychiatrists may develop research skills during their residency or a",
    "label": 0
  },
  {
    "text": "their training includes research design and advanced statistical analysis. While this training is available for physicians via dual MD/PhD programs, it is not typically included in standard medical education, although psychiatrists may develop research skills during their residency or a psychiatry fellowship (post-residency). Psychiatrists, as licensed physicians, have been trained more intensively in other areas, such as internal medicine and neurology, and may bring this knowledge to bear in identifying and treating medical or neurological conditions that present with primarily psychological symptoms such as depression, anxiety, or paranoia (e.g., hypothyroidism presenting with depressive symptoms, or pulmonary embolism with significant apprehension and anxiety). Mental health professions Marriage and Family Therapist (MFT). An MFT license requires a doctorate or master's degree. In addition, it usually involves two years of post-degree clinical experience under supervision, and licensure requires passing a written exam, commonly the National Examination for Marriage and Family Therapists, which is maintained by the American Association for Marriage and Family Therapy. In addition, most states require an oral exam. MFTs, as the title implies, work mostly with families and couples, addressing a wide range of common psychological problems. Some jurisdictions have exemptions that let someone practice marriage and family therapy without meeting the requirements for a license. That is, they offer a license but do not require that marriage and family therapists obtain one. Licensed Professional Counselor (LPC). Similar to the MFT, the LPC license requires a master's or doctorate degree, a minimum number of hours of supervised clinical experience in a pre-doc practicum, and the passing of the National Counselor Exam. Similar licenses are the Licensed Mental Health Counselor (LMHC), Licensed Clinical Professional Counselor (LCPC), and Clinical Counselor in Mental Health (CCMH). In some states, after passing the exam, a temporary LPC license is awarded and the clinician may begin",
    "label": 0
  },
  {
    "text": "Exam. Similar licenses are the Licensed Mental Health Counselor (LMHC), Licensed Clinical Professional Counselor (LCPC), and Clinical Counselor in Mental Health (CCMH). In some states, after passing the exam, a temporary LPC license is awarded and the clinician may begin the normal 3000-hour supervised internship leading to the full license allowing to practice as a counselor or psychotherapist, usually under the supervision of a licensed psychologist. Some jurisdictions have exemptions that allow counseling to practice without meeting the requirements for a license. That is, they offer a license but do not require that counselors obtain one. Licensed Psychological Associate (LPA) Twenty-six states offer a master's-only license, a common one being the LPA, which allows for the therapist to either practice independently, or, more commonly, under the supervision of a licensed psychologist, depending on the state. Common requirements are two to four years of post-master's supervised clinical experience and passing a Psychological Associates Examination. Other titles for this level of licensing include psychological technician (Alabama), psychological assistant (California), licensed clinical psychotherapist (Kansas), licensed psychological practitioner (Minnesota), licensed behavioral practitioner (Oklahoma), licensed psychological associate (North Carolina) or psychological examiner (Tennessee). Licensed behavior analysts Licensed behavior analysts are licensed in five states to provide services for clients with substance abuse, developmental disabilities, and mental illness. This profession draws on the evidence base of applied behavior analysis and the philosophy of behaviorism. Behavior analysts have at least a master's degree in behavior analysis or in a mental health related discipline, as well as having taken at least five core courses in applied behavior analysis. Many behavior analysts have a doctorate. Most programs have a formalized internship program, and several programs are offered online. Most practitioners have passed the examination offered by the Behavior Analyst Certification Board The model licensing act for behavior analysts can",
    "label": 0
  },
  {
    "text": "The reward theory of attraction states that people are attracted to individuals exhibiting behaviors that are rewarding to them, whom they associate with rewarding events, or have positive, fulfilling interactions with. Reward theory was originally developed in the research on interpersonal attraction of the 1960s, a precursor to modern romantic love research. In this early context, \"attraction\" was often defined as \"a positive attitude towards a particular person\". Romantic love science was not explicitly studied yet in this period of history; the subject was even considered \"taboo\" for research. Attraction was initially conceived of as a continuum, with liking being a \"mild\" form of attraction at one end, and romantic love being a \"strong\" attraction at the other end. This idea of a continuum started to change in 1970, when Zick Rubin published his distinction between \"liking\" and \"loving\". A later distinction was made by Elaine Hatfield between \"passionate\" and \"companionate\" love. Passionate love is \"a state of intense longing for another\" which involves incentive salience (\"wanting\", or what is attention-grabbing). Companionate love is \"the affection we feel for those with whom our lives are deeply entwined\" (or \"strong liking\"). A successor to reward theory is the self-expansion model by Arthur & Elaine Aron in 1986, which conceptualizes reward as \"whatever creates expansion of the self\". Like reward theory, self-expansion encompasses \"mini-theories\" of falling in love and long-term relationships, and has been used to explain the process behind \"strong attractions\" like passionate love, and Dorothy Tennov's concept of limerence. Another major variant of reward theory was social exchange theory. A separate area of research was impression formation, which studied those impressions based on knowledge (i.e. information), rather than the emotional reactions (i.e. affect) studied by interpersonal attraction. Predictors Early interpersonal attraction research identified five major predictors of \"attraction\" (defined then",
    "label": 0
  },
  {
    "text": "separate area of research was impression formation, which studied those impressions based on knowledge (i.e. information), rather than the emotional reactions (i.e. affect) studied by interpersonal attraction. Predictors Early interpersonal attraction research identified five major predictors of \"attraction\" (defined then as a \"positive attitude\"). In this early paradigm, it was assumed that falling in love occurs with an exceptionally strong instance of one of these. Similarity (as in \"birds of a feather flock together\") is an idea dating as far back as Aristotle. The most successful research of this type showed a direct relationship between liking and one's attitudinal agreement on a variety of issues (social, political, or artistic). Studies also demonstrated personality similarity among husbands, wives and friends, although it was unclear to what extent such similarity actually caused attraction. It might also be the case that marriage makes a couple more similar over time, or that such similarity is \"more perceived than actual\". Propinquity (similarity of location) has been found to result in attraction. People who are seated together or live nearby tend to become friends, and the mere exposure effect (e.g. being able to glimpse a fellow student, regardless of similarity) has been shown to elicit attraction. Being liked by a person tends to cause liking towards that person in return, also known as the reciprocal liking rule. A study on falling in love found that a sudden attraction was most frequently associated with a person's discovery of another's attraction to them. As one participant describes, \"I met [her] in a department store in which she worked. I was looking for sandals and she recognized me and came over [...]. From that moment on, I thought a lot about her; fantasizing relationships, etc.\" Matching of admirable characteristics (e.g. physical attractiveness, good health, youthfulness, intelligence, mental health, general",
    "label": 0
  },
  {
    "text": "I was looking for sandals and she recognized me and came over [...]. From that moment on, I thought a lot about her; fantasizing relationships, etc.\" Matching of admirable characteristics (e.g. physical attractiveness, good health, youthfulness, intelligence, mental health, general competence, and so on) limits one's attraction to those who also have a similar level of attractiveness, according to a \"marketplace of open competition\". The matching hypothesis says that a more attractive person might not like a less attractive person, and they would try to \"do better\" with somebody else. Social and cultural influences determine who people are likely to meet, who they are allowed to associate with, and what characteristics are viewed as attractive and important. Self-expansion A limitation of early reward theory was that it could not predict exactly what is rewarding, only determine it by observation (i.e. with studies). To solve this, Arthur & Elaine Aron developed the self-expansion model, which specifies reward as \"whatever creates expansion of the self\". Self-expansion is the human motivation to expand one's physical influence, cognitive complexity, social or bodily identity, and self-awareness. Relationships are a key area for self-expansion then, via \"inclusion of the other in the self\", where aspects of a partner (e.g. traits, skills, attitudes, resources, abilities, and worldviews) are incorporated into one's own self concept. The Arons revise the definition of \"attraction\" to mean a desire to enter a close relationship, usually reflected in attitudes or behaviors. According to their theory, attraction arises when opportunities for self-expansion are perceived, and so a \"positive attitude\" towards a person (the earlier definition) is only a \"frequent symptom\". Self-expansion is then used to explain the \"strong attraction\" of romantic love, including intense varieties of passionate love or limerence, when the rate of expansion is rapid and approaches the maximum total possible",
    "label": 0
  },
  {
    "text": "earlier definition) is only a \"frequent symptom\". Self-expansion is then used to explain the \"strong attraction\" of romantic love, including intense varieties of passionate love or limerence, when the rate of expansion is rapid and approaches the maximum total possible from all sources. Additionally, self-expansion explains how unrequited love can be a desired experience. Besides romantic love, opportunities for self-expansion include learning, career, family, friendship, athletics, travel, artistic expression, politics, gossip, religion, and the experience of nature. The Arons use a value-expectancy approach to determine attraction as the combination of two factors (desirability and probability): Perceived degree of potential expansion of self that is possible through a close relationship with that particular other. Perceived probability of actually obtaining that expansion with other—that is, probability that one could actually form and maintain a close relationship with this particular other. According to the self-expansion model, attraction would actually seem to result from the opposite of the five predictors (because e.g. similarity would actually seem to minimize self-expansion—resulting in less attraction). Therefore, the Arons propose that these are five preconditions which make a relationship possible, whereas attraction according to self-expansion increases when the opposite conditions are present. For example, a person may be attracted to similarity when it provides the basis for effective communication or predictability, whereas differences provide the basis for self-expansion: new challenges, new experiences, new resources, etc. The Arons interpret study results (some of which did show dissimilarity was attractive) to mean that in their model, similarity is attractive because it increases the probability of a relationship. If a person believes forming a relationship will be easy, then dissimilarity becomes more attractive for self-expansion. Passion seems to decline when interactions with a love object become frequent, showing that both propinquity and distance can facilitate attraction. Accordingly, in the tradition of",
    "label": 0
  },
  {
    "text": "believes forming a relationship will be easy, then dissimilarity becomes more attractive for self-expansion. Passion seems to decline when interactions with a love object become frequent, showing that both propinquity and distance can facilitate attraction. Accordingly, in the tradition of medieval romance, the love object was always inaccessible, and modern people still seem to be \"obsessed with the unknown, mysterious lover\". The violation of social norms could also be an experience for self-expansion \"towards greater autonomy, clearer personal values, new social roles, and the like\"—as in \"the Romeo and Juliet effect\", where parental disapproval seems to enhance romantic love. An fMRI experiment found that neural activity in regions associated with the physical attractiveness of potential alternative romantic partners was diminished when the participants were primed with a recollection of self-expansion in their current relationship. This effect \"may be because the current relationship is bolstered by feelings of self-expansion diminishing the relative attractiveness and therefore the incentive salience of alternative partners\". Reinforcement The mechanics of interpersonal attraction are believed to follow principles of reinforcement and classical conditioning. Reinforcement theory distinguishes between several different paradigms: Positive reinforcement (reward) increases the frequency of a response leading to a desired stimulus. Negative reinforcement also increases the frequency of a response, but with an aversive stimulus which must be removed or avoided. Punishment (different from negative reinforcement) is a painful or unwanted stimulus that decreases the frequency of a response leading to the encounter. Classical (or Pavlovian) conditioning is essentially learning by association: when two things happen together, we come to associate them and expect them together. Ivan Pavlov, who developed the theory, is said to have trained dogs to salivate (having an automatic reflex response) at the sound of a bell by repeatedly ringing it when food was delivered. In this paradigm, a \"neutral",
    "label": 0
  },
  {
    "text": "them together. Ivan Pavlov, who developed the theory, is said to have trained dogs to salivate (having an automatic reflex response) at the sound of a bell by repeatedly ringing it when food was delivered. In this paradigm, a \"neutral stimulus\" is paired along with a biological stimulus (an \"unconditioned stimulus\") which elicits a usually innate reflex response (an \"unconditioned response\") so that when the previously neutral stimulus (now a \"conditioned stimulus\") is presented again by itself it elicits a new reflex response (a \"conditioned response\"). In Pavlovian theory, reinforcement is described as this repeated pairing of an unconditioned (or unlearned) stimulus along with a conditioned (or learned) one, which strengthens the association, until eventually the conditioned stimulus elicits the response on its own. According to a similar mechanic, liking for a person results when an individual experiences reward in the presence of that person, although regardless of the actual relationship between the person and the rewarding event. The liked person then becomes a secondary reinforcer, meaning if their presence is contingent on a particular behavior, that behavior should be strengthened. The \"reinforcement-affect model\", developed by Donn Byrne & Gerald Clore, additionally posits that attraction is based on the positive affect which accompanies reinforcement, and that these feelings spread from one stimulus to another via association: (a) a variety of social communications and other interpersonal events can be classed as either reinforcing or punishing; (b) reinforcing events elicit positive affect, while punishing events generate negative affect; (c) stimuli associated with positive or negative affect develop the capacity to evoke that affect; and (d) stimuli that evoke positive affect are liked, while stimuli that evoke negative affect are disliked. Thus, one likes others who reward him because they are associated with one's own good feelings. The authors also acknowledge a complexity",
    "label": 0
  },
  {
    "text": "affect; and (d) stimuli that evoke positive affect are liked, while stimuli that evoke negative affect are disliked. Thus, one likes others who reward him because they are associated with one's own good feelings. The authors also acknowledge a complexity in how this reinforcement functions in everyday situations: \"Many of the associations made in the process of attraction development are between words, thoughts, images, or collections, rather than between buzzers, electric shocks, or visceral responses.\" A variety of studies have been done which support the idea that people who are associated with reinforcement tend to be liked, even if they are not the source of reinforcement. An experiment by Pawel Lewicki tested \"liking by association\" by giving participants a choice between two pictured women, asking them which looked friendlier, and the regular outcome was nearly 50-50. However, when the participants had a friendly interaction beforehand with an experimenter who merely looked similar to a woman pictured, the similar-looking woman was chosen with a 6-to-1 margin. When the interaction was unfriendly, the similar-looking woman was nearly always avoided. Another experiment found that college students liked a stranger better when evaluating them in a pleasant room as compared to a hot room. Neuroscience Reward, motivation & addiction Brain opioid theory of social attachment See also References Sources",
    "label": 0
  },
  {
    "text": "In social psychology, social buffering is a phenomenon where social connections can alleviate negative consequences of stressful events. Although there are other models and theories to describe how social support can help reduce individuals' stress responses, social buffering hypothesis is one of the dominant ones. According to this idea, social partners, who can be familiar others or conspecifics, act as buffers in the face of stressful events, specifically while the stress is happening. The model further describes that social support is especially beneficial when levels of stress are also high, but buffering effects are not as relevant when levels of stress are low. Social buffering has been explored in humans and other social animals, and is important to questions about physical and mental health. Research has attempted to gain insight about the protective effects of social support in several domains, such as biological, developmental, neurological, and clinical settings. Social buffering is also relevant to other psychological processes, including fear, social bonding, and emotional reactivity. Background Early history Psychological research in the mid-twentieth century began to increasingly reveal the role of stressful life events on psychological well-being. This was also around the time that there was a focus on creating standardized approach to diagnosing mental illnesses, with the first Diagnostic and Statistical Manual of Mental Disorders (commonly referred to as the DSM) being published in 1952. With a honed focus on effective, universal ways to measure mental well-being, and the application of experimental psychology on social issues, a large literature on the effects of social support began to form. This occurred in an effort to fill in the gaps on the specific factors that mediate the relationship between life events and psychological consequences. Specific focus on the attenuation of social support on the negative impacts of stressful events on physical and",
    "label": 0
  },
  {
    "text": "in an effort to fill in the gaps on the specific factors that mediate the relationship between life events and psychological consequences. Specific focus on the attenuation of social support on the negative impacts of stressful events on physical and mental health began in the mid-1970s. This is around the time when the idea of social buffering began to take shape. It is unclear where the phrase social buffering hypothesis was first mentioned, but one of the most credited and cited works on the topic was published by researchers Sheldon Cohen and Thomas A. Wills in 1985. Social buffering has been an important feature in psychology since its early use, specifically relevant to social and health psychology. The framework has been applied to several other areas as well, and methods of measurement and definitions of relevant terminology continue to be refined and built upon. Social support Social buffering is a subset of social support, and not all occurrences of social support are social buffering. Social support encompasses both the expectation and actual act of being assisted, nurtured, attended to, or participation in a social network. Research on social support makes the distinction between perceived support and received social support. Perceived social support refers to the amount of support people believe that they could potentially receive from their available support system, while received social support is the level of support that people have received. Some studies have shown that perceived social support may be better for mitigating the negative effects of stress on health and psychological well-being, suggesting there is a measurable difference between the types of social support, and proper definition of variables is important to research in the area. Social support is robustly associated with positive health outcomes. Research has shown that people with larger social networks, higher-quality relationships,",
    "label": 0
  },
  {
    "text": "measurable difference between the types of social support, and proper definition of variables is important to research in the area. Social support is robustly associated with positive health outcomes. Research has shown that people with larger social networks, higher-quality relationships, and greater access to social support resources have better mental and physical health. The effects of social support have been studied in many different domains of psychology, such as social, developmental, clinical, and health psychology, as well as neuroscience. Competing model: direct effects hypothesis The social buffering hypothesis is often compared to or evaluated with the direct effect hypothesis. This hypothesis differs from social buffering in that it holds that social support enhances physical and psychological well-being in general, regardless of the presence of stressors. This model says that social support is beneficial all the time, and that people with high social support have overall better health than those without it. The two models tend to deal with different measures of social support. The direct effects hypothesis measures the level at which a person is integrated into a social network, while the social buffering hypothesis assesses how available the social resources are that help people respond to stressful events. The language around both hypotheses also tends to be different, with the direct effects hypothesis often looking at the enhancement of health and well-being as a result of the perception of support and integration in a network, whereas the buffering hypothesis is more concerned with protection (or prevention), especially in times of need. Statistically, the direct effect hypothesis holds that there is no interaction between stress and social support, meaning the same beneficial effects will be observed notwithstanding of the level of stress. Conversely, according to the social buffering hypothesis, the magnitude of the beneficial effect from social support is larger",
    "label": 0
  },
  {
    "text": "is no interaction between stress and social support, meaning the same beneficial effects will be observed notwithstanding of the level of stress. Conversely, according to the social buffering hypothesis, the magnitude of the beneficial effect from social support is larger when stress is present, which is reflected in a statistically significant observable interaction when the two effects are studied experimentally. This also means that knowledge of the degree of stress is required for the social buffering hypothesis, where this level may not be as relevant in the context of the direct effects hypothesis. Despite these models providing somewhat separate explanations, research has found support for both hypotheses, and some work even suggests that both processes happen simultaneously. Researchers have suggested that work directed at critically comparing the two hypotheses may not actually benefit the field studying social support. Instead, it may be more beneficial to use either one as a model that aims to explain specific questions about how social support relates to mediators of health that can be behavioral, psychological, emotional, or biological. Biology Research studies done on social buffering and health consequences consistently show that the HPA axis plays a central role in the link between the two. The hypothalamic–pituitary–adrenal (HPA) axis is a crucial regulator of neuroendocrine responses in the body. The HPA axis is made up of a series of pathways and feedback loops that involve the hypothalamus, anterior pituitary gland, and adrenal gland. It modulates several physiological processes, including the autonomic nervous system, immune system reactions, metabolism, and several other processes that are active during short-term physiological responses to stress. The HPA axis also plays a major role in bodily homeostasis, which includes regulating the cardiovascular system, reproductive system, and central nervous system in addition to those previously mentioned. Proper functioning of the HPA axis",
    "label": 0
  },
  {
    "text": "short-term physiological responses to stress. The HPA axis also plays a major role in bodily homeostasis, which includes regulating the cardiovascular system, reproductive system, and central nervous system in addition to those previously mentioned. Proper functioning of the HPA axis is very important for adaptation and development, and both over- and under-reactivity can lead to a series of consequences. It is important for humans to experience high levels of circulating stress hormones early in life so that they can learn to effectively respond to threat and adapt to their environments. However, too much stress in childhood can lead to long-term and often irreversible dysfunction of the HPA axis. HPA axis activity goes up during aversive or arousing situations, which can be induced by physical or psychosocial events. The HPA axis is particularly sensitive to psychological stressors, including uncertainty, novelty, and the feeling of being out of control. In addition to being influenced by psychological stressors, one of the most powerful and widely studied moderators of HPA axis activity induced by stressful events is social support. This is why the HPA axis is often a focal point in physiological research examining social buffering effects. The biological mechanisms of social buffering have been explored, and several components have been identified as relevant: Endocrine There are several hormones involved in HPA axis endocrine regulation. One is corticotrophin-releasing factor (CRF). CRF stimulates the release of adrenocorticotropic hormone (ACTH), which is the crucial physiological regulator of increased activity of the HPA axis. Another is vasopressin (AVP), and glucocorticoids are the final step in the process. Social buffering is observed when high levels of social support are correlated with lower levels of stress hormone and HPA axis activation. Oxytocin Oxytocin has been recognized as an important hormone involved in the mechanism of social buffering. Oxytocin is",
    "label": 0
  },
  {
    "text": "Social buffering is observed when high levels of social support are correlated with lower levels of stress hormone and HPA axis activation. Oxytocin Oxytocin has been recognized as an important hormone involved in the mechanism of social buffering. Oxytocin is a molecule that is often called the \"love hormone\". It is released into the blood in response to physiological and psychological stress, and increased oxytocin release from the hypothalamus inhibits activation of the HPA axis. Oxytocin is involved in stress processes by inhibiting the release of corticosteroids, ACTH release from the pituitary gland, and release of CRF from the hypothalamus. The central nervous system also meditates the inhibitory effect of oxytocin on the HPA axis. Oxytocin is also involved in the behavioral side of social buffering. Oxytocin's role in bonding means that it is related to the process of social support, in addition to being released in response to a stressor. Opioids Endogenous opioids, or endorphins, also appear to play a major role in social buffering, via a reinforcing effect that is active in social attachment. Opioid release is also observed when animals recognize each other, which supports the idea that animals find social support in others of the same species. This is necessary to see social buffering effects in animals, and opioid release also helps explain the seeking of affiliation in animals as well. Opioid release as a result of social stimulation has also been found to reduce corticotrophin-releasing factor activity in the brain and body. Neural circuitry Research on the brain regions involved in social buffering is less extensive compared to the role of the HPA axis and cortisol. However, the prefrontal cortex has been identified as a region involved in the social buffering and stress responses. Higher activity in the prefrontal cortex has been found to be",
    "label": 0
  },
  {
    "text": "extensive compared to the role of the HPA axis and cortisol. However, the prefrontal cortex has been identified as a region involved in the social buffering and stress responses. Higher activity in the prefrontal cortex has been found to be correlated with higher degrees of cortisol responses to stress. A similar relationship has been observed in the anterior cingulate cortex. Both the prefrontal cortex and anterior cingulate cortex have been shown to be involved in emotional responses and reactivity to stressful situations, and activity in these areas is closely related with HPA activation. The presence of social support causes cortisol levels to go down as well as decreased activity in these regions. Social buffering has also been observed in regions that are thought to be involved in responding to threats to safety, which include the ventromedial prefrontal cortex, anterior cingulate cortex, posterior insula, and posterior cingulate cortex. Clinical applications Social support has been historically identified as very important for people's well-being, and it can be even more important for populations that are vulnerable to high stress and loneliness. Work on the social buffering hypothesis has been done on these populations, which include racial and ethnic minorities, sexual minorities, middle-aged and elderly, impoverished individuals, and other adversely affected demographic groups. This type of work aims to find specific applications of social buffering, often to provide frameworks for developing or assessing effectiveness of treatments or to build an understanding for prevention of the negative consequences of stressful life events. A social buffering effect was observed in work done on suicidality, and findings indicate that focusing on buffering has the potential of being an effective area in developing interventions. The buffering effect has also been found to be strong in individuals with depression, meaning that social support can reduce symptoms of depression during",
    "label": 0
  },
  {
    "text": "indicate that focusing on buffering has the potential of being an effective area in developing interventions. The buffering effect has also been found to be strong in individuals with depression, meaning that social support can reduce symptoms of depression during times of stress. A relationship has also been drawn between social buffering and drug and alcohol use disorders, which lower likelihood of developing substance abuse disorders with higher social support. Another issue that social buffering relates to is loneliness. The United States has a growing rate of loneliness, considered by some to be a \"loneliness epidemic\", and there is a documented rise in the number of people living alone in many cultures globally. Loneliness is strongly linked with many psychiatric disorders, including depression and anxiety, as well as several physical disorders, such as cardiovascular diseases and hypertension. Additionally, the rate of loneliness increases with age and has serious health consequences particularly in older populations. Research shows that high levels of social connectedness can help alleviate negative effects of loneliness that frequently accompany getting older. The robust connection between loneliness and poor mental and physical health is difficult to debate, and social buffering research can highlight the specific aspects of loneliness that are most damaging. Social buffering is also relevant to the process of acculturation. Immigrants, guest workers, and international students may experience increased likelihood of isolation and psychological difficulties such as depressive symptoms. Research shows that those who have more interpersonal connections and participate in acculturation at higher degrees, benefit from the effects of social buffering. Increasing the size of one's social network has been shown to have salient buffering effects, particularly in older immigrants. It Is important to note that social buffering works differently in different groups in society. There are gender, age, and cultural differences. Additionally, it can",
    "label": 0
  },
  {
    "text": "one's social network has been shown to have salient buffering effects, particularly in older immigrants. It Is important to note that social buffering works differently in different groups in society. There are gender, age, and cultural differences. Additionally, it can be difficult to study the effects of social support on stress in individuals who have impaired social functioning. These include individuals with social phobias and social anxiety disorder as well as autistic people. Development Social buffering is recognized as an essential way through which the nature of experiences in childhood affect development and subsequent health. During infancy, parents play a large role in regulating the negative consequences of childhood, especially regarding fear or pain responses. Attachment is also very relevant to studies on development and stress reduction. Attachment theory posits that the type of attachment relationship a child forms with their parents influences their ability to regulate emotional states whether or not the parents are immediately present. Stress buffering effects are often seen with securely attached children, indicating that the type and stability of relationships is crucial to how well a child recovers from stressful events. Some research shows that parents are especially important for social buffering up until around puberty or late childhood, when primary caregivers tend to become less influential than peers in social settings. This does not take away from the role that parents play in social buffering as a whole, and instead reflects how they are replaced by friends who become a major source of social support with buffering. Furthermore, major stressors during adolescence and puberty tend to be peers and social standing, so the social buffering provided by friends during this time is heavily interwoven with social networks and relates to an idea called the friendship protection hypothesis. The friendship protection hypothesis reflects the idea",
    "label": 0
  },
  {
    "text": "tend to be peers and social standing, so the social buffering provided by friends during this time is heavily interwoven with social networks and relates to an idea called the friendship protection hypothesis. The friendship protection hypothesis reflects the idea of the social buffering hypothesis and explains how children with at least one supportive friend are less negatively affected by bullying and peer rejection. When looking at social buffering and development, a common approach to measuring stress responses involves looking at the HPA axis. The HPA axis is referred to as one of the primary hormonal stress systems. Research looking at stress and social buffering in development consistently shows that parents play a role in shaping HPA axis function, which is evidenced in part by how early social deprivation may later result in long-term dysfunction of stress reactivity. Social buffering effects have also been observed when a child is exposed to a threatening event. The presence of a parent during such a time can lower or completely block the activation of the HPA axis. Additional support for the social buffering hypothesis and social neuroscience involves fear conditioning. The amygdala has been recognized as an important part in the process of fear learning, and research has shown that children have reduced amygdala activity when around a parent. In addition to this, greater connectivity between the prefrontal cortex and amygdala was observed when a child viewed their mother compared to a stranger. The prefrontal cortex is involved in emotional processes, so communication between the two brain areas indicates that parents play a large role in emotional regulation and provide neurological support for social buffering. Although the functioning of stress processes and fear learning are evolutionarily crucial, research on social buffering shows that social support can reduce the negative developmental consequences of too",
    "label": 0
  },
  {
    "text": "large role in emotional regulation and provide neurological support for social buffering. Although the functioning of stress processes and fear learning are evolutionarily crucial, research on social buffering shows that social support can reduce the negative developmental consequences of too high stress and potentially aid with proper biological functioning. Animal research Phenomena consistent with the social buffering hypothesis have been widely observed in non-human animals. The literature on animals uses the word conspecifics to refer to animals of the same species, and in each case it is clear that the effects are seen between like-animals and not due to the presence of other beings in general. Social interaction and communication are very important for survival in many species of animals, aiding with cooperation and essential for protection from threats. Highly social animals, such as non-human primates, rodents, and birds, show positive physiological and psychological effects when they are together with animals of the same species. This was an idea that gained more attention in the early 2000s, while many of the experiments conducted in earlier research focused on the stress-inducing factors of social interaction. Social buffering has been observed in a wide range of animals, including guinea pigs, horses, rhesus monkeys, and pigs. Studies have found support for social buffering both from a physiological, in the form of reduced HPA activity or lower cortisol levels, and behavioral perspective. Behavioral observations in rats, for example, include increased locomotor activity, indicating less fear, in a stress inducing open space in the presence of other rats, and an increase in the seeking of the proximity of other rats when they were exposed to stress. Research on animals has also revealed several sensory cues for social buffering: Olfactory cues – Highly social animals use chemical communication to transmit certain information, such as emotional status",
    "label": 0
  },
  {
    "text": "the proximity of other rats when they were exposed to stress. Research on animals has also revealed several sensory cues for social buffering: Olfactory cues – Highly social animals use chemical communication to transmit certain information, such as emotional status in times of stress that alters both the physiological responses and behavior of animals. Animals may also use appeasing pheromones to calm others down and reduce fighting behavior. An example of this in humans is that maternal breast odor is both attractive to infants and has positive emotional effects. Vocal cues – Vocal communication can be used to indicate familiarity, emotions, and help with attachment formation. In marmosets, a species that use vocal communication, stress hormone levels decrease more when hearing a familiar voice compared to hearing an unfamiliar one. Visual cues – In certain animals, visual cues may be enough to evoke the effects of social buffering. These effects were shown in sheep, where seeing pictures of other sheep's faces reduced behavioral and physiological stress responses. Zebrafish have been observed to communicate about danger with visual cues, leading to social buffering from conspecifics in response to the threat of predators. References",
    "label": 0
  },
  {
    "text": "Ultra-realism is a school of thought within the discipline of criminology and the sub-discipline of zemiology. Ultra-realists revisit the fundamental question that underpins both disciplines - why, rather than seek solidarity and cooperation, do specific individuals, groups or institutions choose to risk harm to others as they pursue their own interests? Early proponents of the ultra-realist perspective are Steve Hall and Simon Winlow. The original ultra-realist concepts first emerged from the mid-1990s onwards in a series of articles and books. The ultra-realist theoretical framework and research-programme began to take a clearly defined shape in three later works. Theoretical framework The need for revision Ultra-realism is a new school of thought in western social science. Ultra-realists focus on the western way of life and its zemiological outcomes. Ultra-realism is a revisionist position constructed for the purposes of revisiting the research findings and fundamental domain assumptions of existing schools of thought to evaluate their validity and, where necessary, suggest further investigation using ethnographic research methods and advanced concepts. Basic components of the framework Ultra-realism's initial empirical focus is on broad crime trends that show discernable shifts over time in various locales, regions and nations. Drawing upon the work of criminologists such as Robert K. Merton, Jock Young and Robert Reiner, they connect these trends to shifts in political economy, culture and subjectivity, investigating their forms and dynamic relations, which together create probabilistic contexts at the local, regional and national levels. Early preparatory work undertaken before the establishment of the ultra-realist name in 2015 drew evidence from the New Deal period in the US, which between 1933 and 1937 correlated with a significant reduction of crime, violence and homicide. This contrasts with the crime explosions in the US and UK during the deindustrialization process of the 1980s. Ultra-realists analysed the decline in traditional",
    "label": 0
  },
  {
    "text": "US, which between 1933 and 1937 correlated with a significant reduction of crime, violence and homicide. This contrasts with the crime explosions in the US and UK during the deindustrialization process of the 1980s. Ultra-realists analysed the decline in traditional crime from the mid-1990s in the context of advances in detection technology and surveillance and increased incarceration rates. They also researched the mutation of criminal markets, a process in which traditional crimes declined as detection and surveillance advanced while criminal markets expanded silently in the background and new forms of minimally reported and recorded crime appeared, much of it online. Winlow's sociological work on the displacement of traditional working-class identities and cultural pursuits by those promoted by mass-mediated consumer culture indicated that changes were taking place. His research on the drift into criminality added empirical and theoretical weight to Hall's claim that the cultural context of criminality is the combined pacification and intensification of a competitive-individualist norm. Further research added detail to the historical intensification of the practices around this norm and explained how an associated subjectivity was normalised and pacified, leading to the emergence of foundational ultra-realist concepts such as the pseudo-pacification process, objectless anxiety and special liberty. Further ethnographic and theoretical work associated these socioeconomic, cultural and subjective shifts with corresponding changes in crime trends and the expansion of criminal markets. The construction of a theoretical framework for a renewed realist analysis of trends in crime and harm led to the establishment of the ultra-realist project in 2015. Ultra-realists agreed with the left realist and early feminist position that critical criminology must take crime, harm and victims more seriously. They borrowed selected concepts from other realist schools of thought - for instance critical realism's notions of depth structures and the causative impact of absence; speculative realism's critique of",
    "label": 0
  },
  {
    "text": "that critical criminology must take crime, harm and victims more seriously. They borrowed selected concepts from other realist schools of thought - for instance critical realism's notions of depth structures and the causative impact of absence; speculative realism's critique of correlationism and its emphasis on realist contingency and agency in the anthropecene era; Mark Fisher's notion of 'capitalist realism', a culture that normalises and reproduces the cynical, 'zero-sum' subject frequently encountered by ultra-realist ethnographers in their research. The ubiquity of an indifferent attitude towards risking harm to others amongst respondents involved in crime and criminal markets prompted ultra-realism's first move towards zemiology. Ultra-realists drew upon Slavoj Žižek's notion of fetishistic disavowal to formulate their concept of the 'chosen unconscious', which led to the early adoption of Adrian Johnston's transcendental materialism as a basis for the ultra-realist framework. Further theoretical development was prompted by researchers' empirical findings relating to subjectivity, particularly the clear distinction between expressive and instrumental forms of special liberty. The recognition of the use of criminal or harmful means of either remaining in or leaving specific situations led to the current reformulation of the ultra-realist framework, drawing upon the concepts of homeostasis, non-suffering, sacrifice, and existential difficulties in post-normal times. Ultra-realists replaced the Lacanian notion of 'the void' in the psyche with the fundamental 'molecular question' - to stay or go - at the root of all organisms. This is a bodily demand for a primordial action-oriented metaphysics that only the affective system's deep 'mood' emotions can translate and mediate. Because the molecular body understands only 'non-suffering' as its initial objective, the stay-go decision involves a calculation of sacrifice, which becomes mythologised to dramatize narratives and enhance the emotional credibility of any answers to the molecular question provided by experience, culture and ideology. The more persistent and committed",
    "label": 0
  },
  {
    "text": "its initial objective, the stay-go decision involves a calculation of sacrifice, which becomes mythologised to dramatize narratives and enhance the emotional credibility of any answers to the molecular question provided by experience, culture and ideology. The more persistent and committed criminals encountered by ultra-realist researchers throughout the social structure had accepted the cyncial belief that the 'zero-sum' world is natural and timeless. The onus is on individuals to stay with it and actively compete against others but avoid sacrifice at all costs by levering themselves up the hierarchy of non-suffering by any means possible, legal or otherwise. Ultra-realists argue that criminology can make firmer contributions to the pragmatic political interventions required to stabilise economies, reduce sociosymbolic competition, transcend zero-sum subjectivity and restore ethics to the heart of economy, society, culture. In an attempt to lay the foundations for a theoretical framework that encompasses these issues, ultra-realists constructed the following new concepts specifically for criminological and zemiological research: The pseudo-pacification process. Hall's concept, developed since the mid-1990s in theoretical works is an alternative to Norbert Elias's concept of the 'civilizing process'. Pseudo-pacification is a process that emerged in 12th century England in the wake of major cultural, legal and socioeconomic changes. The fundamental claim is that what is mistaken for 'civilizing' momentum is a fragile by-product of the displacement of physical violence as a normative means of ordering and disrupting social systems with rule-bound sociosymbolic competition acted out in commercial life and consumer culture. The relatively early replacement of violence with pseudo-pacified competitive individualism changed trends in crime and harm, accelerated urbanisation, social atomism and marketisation, increased consumer desires and helped Britain become the first fully industrialised nation. Special liberty. Hall's concept of 'special liberty' is a socially unstructured sense of entitlement that defies the prohibition Kant placed on justifying means",
    "label": 0
  },
  {
    "text": "urbanisation, social atomism and marketisation, increased consumer desires and helped Britain become the first fully industrialised nation. Special liberty. Hall's concept of 'special liberty' is a socially unstructured sense of entitlement that defies the prohibition Kant placed on justifying means by ends. It is the culmination of ethically over-inflated but fundamentally cynical motivations and justifications constructed in the minds of those who are determined to achieve personal ends, whether instrumental or expressive, and have fetishistically disavowed the likelihood that their actions will cause harm to others and their environments. Special liberty operates at the boundary of the pseudo-pacification process's normative structure and threatens its stability. Loss, trauma and nostalgia. Winlow's work on the impact of the loss of identities during the process of rapid deindustrialization in the UK demonstrated how this existential trauma added another dimension to material, communal and social loss. As new identities were forged in consumer culture's symbolic order, many respondents were also moved by a deep nostalgia that combines cynicism with a sense of marginalisation and loss of hope. Some individuals immersed themselves in criminal markets to compensate for this loss, achieve status and forge a simulated autonomous identity. Others also drifted into far-right politics manifested in the UK riots of 2024. Objectless anxiety. Hall's concept denotes the psychocultural end result of the operation of mass-mediated ideology in support of an existing way of life. Anxiety naturally lacks an object. By constantly ignoring, denying or reframing the real systemic causes of problems and preventing the ensuing anxiety from locating and understanding the appropriate object of fear on which people can act, the natural condition is artificially, systematically and indefinitely sustained. The concept has been deployed by criminologists researching deviant leisure, drug culture and policing. The assumption of harmlessness. Raymen's concept was the product of a search",
    "label": 0
  },
  {
    "text": "on which people can act, the natural condition is artificially, systematically and indefinitely sustained. The concept has been deployed by criminologists researching deviant leisure, drug culture and policing. The assumption of harmlessness. Raymen's concept was the product of a search into the historical background of special liberty in an attempt to explore the cultural context that underlies Western liberal societies' understanding of harm. An unwritten rule in liberal thought insists that, where there is initial doubt, an activity should be assumed to be sufficiently harmless to practice before anyone should suspect it might be harmful. Therefore the onus is always on those who wish to prove it is harmful. This concept is one of ultra-realism's theoretical contributions to zemiology. Methods – ethnographic networks Drawing upon earlier ethnographic work from Winlow, ultra-realists argue that criminological researchers are capable of capturing the details of the drivers behind crime and criminal markets by networking and generating qualitative data from different geographical locations and sections of the population. For example, projects undertaken to examine rioting in various locations of the UK and political protests around Europe revealed both differences and similarities in experiences, interpretations, motivations and outcomes. Networked ethnographers working in various spaces gathered data from observations and interviews to be analysed in the ultra-realist framework. . Research projects A number of researchers have used ultra-realist concepts as they construct their research projects and analyze their findings. Ultra-realists criticize 'backwards research' and insist on 'forwards research'. In the period 2015 to 2025 ultra-realism has provided a platform for research in the following areas: Crime, deviant leisure and consumer culture Hall, Steve; Kuldova, Tereza; Horsley, Mark (2020). Crime, Harm and Consumerism. Routledge Studies in Crime and Society. Abingdon, Oxon: Routledge. ISBN 9781138388628. McRae, Leanne (2020). Terror, Leisure and Consumption: Spaces for Harm in a Post-Crash",
    "label": 0
  },
  {
    "text": "deviant leisure and consumer culture Hall, Steve; Kuldova, Tereza; Horsley, Mark (2020). Crime, Harm and Consumerism. Routledge Studies in Crime and Society. Abingdon, Oxon: Routledge. ISBN 9781138388628. McRae, Leanne (2020). Terror, Leisure and Consumption: Spaces for Harm in a Post-Crash Era. Emerald studies in deviant leisure. Bingley: Emerald. ISBN 9781787565234. Lloyd, Anthony; Horsley, Mark (2022). \"Consumer culture, precarious incomes and mass indebtedness: Borrowing from uncertain futures, consuming in precarious times\". Thesis Eleven. 168 (1): 55–71. doi:10.1177/07255136211053421. ISSN 0725-5136. Hall, Steve; Winlow, Simon (2007). \"Cultural criminology and primitive accumulation: A formal introduction for two strangers who should really become more intimate\". Crime, Media, Culture. 3 (1): 82–90. doi:10.1177/1741659007074451. ISSN 1741-6590. Winlow, Simon; Hall, Steve (2018). \"Criminology and Consumerism\". In Carlen, Pat; França, Leandro Ayres (eds.). Alternative criminologies. Abingdon, Oxon: Routledge. ISBN 978-1-315-15866-2. Ellis, Anthony; Winlow, Simon; Briggs, Daniel; Esquinas, Antonio Silva; Verdugo, Rebeca Cordero; Suárez, Jorge Ramiro Pérez (2018). \"Liberalism, Lack and 'Living the Dream': Reconsidering the attractions of alcohol-based leisure for young tourists in Magaluf, Majorca\". Journal of Extreme Anthropology. 2 (2): 22–41. doi:10.5617/jea.6446. ISSN 2535-3241. Lynes, Adam; Kelly, Craig; Kelly, Emma (2020). \"THUG LIFE: Drill music as a periscope into urban violence in the consumer age\". The British Journal of Criminology. 60 (5): 1201–1219. doi:10.1093/bjc/azaa011. ISSN 0007-0955. Raymen, Thomas; Smith, Oliver (2020). \"Lifestyle gambling, indebtedness and anxiety: A deviant leisure perspective\". Journal of Consumer Culture. 20 (4): 381–399. doi:10.1177/1469540517736559. ISSN 1469-5405. Raymen, Thomas; Smith, Oliver, eds. (2019). Deviant Leisure: Criminological Perspectives on Leisure and Harm. Palgrave Studies in Crime, Media and Culture. Basingstoke: Palgrave Macmillan. ISBN 9783030177355. Raymen, Thomas (2019). Parkour, Deviance and Leisure in the Late-Capitalist City: An Ethnography. Emerald studies in deviant leisure. Bingley: Emerald. ISBN 9781787438125. Winlow, Simon (2019), \"What Lies Beneath? Some Notes on Ultra-realism, and the Intellectual Foundations of the 'Deviant Leisure' Perspective\", in",
    "label": 0
  },
  {
    "text": "(2019). Parkour, Deviance and Leisure in the Late-Capitalist City: An Ethnography. Emerald studies in deviant leisure. Bingley: Emerald. ISBN 9781787438125. Winlow, Simon (2019), \"What Lies Beneath? Some Notes on Ultra-realism, and the Intellectual Foundations of the 'Deviant Leisure' Perspective\", in Raymen, Thomas; Smith, Oliver (eds.), Deviant Leisure, Palgrave Studies in Crime, Media and Culture, Basingstoke: Palgrave Macmillan, pp. 45–65, doi:10.1007/978-3-030-17736-2_3, ISBN 9783030177355 Raymen, Thomas; Smith, Oliver (2019). \"Deviant Leisure: A Critical Criminological Perspective for the Twenty-First Century\". Critical Criminology. 27 (1): 115–130. doi:10.1007/s10612-019-09435-x. ISSN 1205-8629. Raymen, Thomas (2019), \"The Paradox of Parkour: Conformity, Resistance and Spatial Exclusion\", in Raymen, Thomas; Smith, Oliver (eds.), Deviant Leisure, Palgrave Studies in Crime, Media and Culture, Basingstoke: Palgrave Macmillan, pp. 349–377, doi:10.1007/978-3-030-17736-2_16, ISBN 9783030177355 Briggs, Daniel; Ellis, Anthony (2017). \"The Last Night of Freedom: Consumerism, Deviance and the \"Stag Party\"\". Deviant Behavior. 38 (7): 756–767. doi:10.1080/01639625.2016.1197678. ISSN 0163-9625. Crime, harm and place Briggs, Daniel; Monge Gamero, Rubén (2017). Dead-End Lives: Drugs and Violence in the City Shadows. Bristol: Policy. ISBN 9781447341680. Telford, Luke; Wistow, Jonathan (2022). Levelling Up the UK Economy: The Need for Transformative Change. Basingstoke: Palgrave Macmillan. ISBN 9783031175060. Telford, Luke (2022). English Nationalism and its Ghost Towns. Abingdon, Oxon: Routledge. ISBN 9781032056715. Briggs, Daniel; Gavrielides, Theo (2022). Hotel Puta: A Hardcore Ethnography of a Luxury Brothel. Restorative Justice Series. London: RJ4All Publications. ISBN 9781911634645. Kuldova, Tereza; Varghese, Mathew A., eds. (2022). Urban Utopias: Excess and Expulsion in Neoliberal South Asia. Palgrave Studies in Urban Anthropology. Basingstoke: Palgrave Macmillan. ISBN 9783319476223. Lynes, Adam; Kelly, Craig; Treadwell, James, eds. (2023). 50 Dark Destinations: Crime and Contemporary Tourism. Bristol: Policy. ISBN 9781447362197. Telford, Luke; Lloyd, Anthony (2020). \"From \"Infant Hercules\" to \"Ghost Town\": Industrial Collapse and Social Harm in Teesside\". Critical Criminology. 28 (4): 595–611. doi:10.1007/s10612-020-09523-3. ISSN 1572-9877. Crime, harm, work and employment Lloyd,",
    "label": 0
  },
  {
    "text": "Crime and Contemporary Tourism. Bristol: Policy. ISBN 9781447362197. Telford, Luke; Lloyd, Anthony (2020). \"From \"Infant Hercules\" to \"Ghost Town\": Industrial Collapse and Social Harm in Teesside\". Critical Criminology. 28 (4): 595–611. doi:10.1007/s10612-020-09523-3. ISSN 1572-9877. Crime, harm, work and employment Lloyd, Anthony (2019). The Harms of Work: An Ultra-Realist Account of the Service Economy. Studies in Social Harm. Bristol: Bristol University Press. ISBN 9781529204032. Raymen, Thomas; Smith, Oliver (2016). \"What's Deviance Got to Do With It? Black Friday Sales, Violence and Hyper-conformity\". British Journal of Criminology. 56 (2): 389–405. doi:10.1093/bjc/azv051. ISSN 0007-0955. Smith, Oliver; Raymen, Thomas (2017). \"Shopping with violence: Black Friday sales in the British context\". Journal of Consumer Culture. 17 (3): 677–694. doi:10.1177/1469540515611204. ISSN 1469-5405. Bushell, Mark; Braithwaite, Chelsea (2024). \"Barbarians at the Tills? Post-pandemic reflections on violence and abuse against workers in the retail industry\". Journal of Consumer Culture. 24 (4): 441–458. doi:10.1177/14695405241290934. ISSN 1469-5405. Bushell, Mark G. (2023). \"No Time for Rest: An Exploration of Sleep and Social Harm in the North East Night-Time Economy (NTE)\". Critical Criminology. 31 (1): 145–160. doi:10.1007/s10612-022-09655-8. ISSN 1572-9877. PMC 9419915. PMID 36061069. Lloyd, Anthony (2023). \"Harm at Work: Bullying and Special Liberty in the Retail Sector\". Critical Criminology. 28 (4): 669–683. doi:10.1007/s10612-019-09445-9. ISSN 1205-8629. Telford, Luke; Briggs, Daniel (2022). \"Targets and overwork: Neoliberalism and the maximisation of profitability from the workplace\". Capital & Class. 46 (1): 59–76. doi:10.1177/03098168211022208. ISSN 0309-8168. Covid, lockdown and social harm Briggs, Daniel; Telford, Luke; Lloyd, Anthony; Ellis, Anthony; Kotzé, Justin (2021). Lockdown: Social Harm in the Covid-19 Era. Cham: Springer. ISBN 9783030888244. Briggs, Daniel; Telford, Luke; Lloyd, Anthony; Ellis, Anthony (2023). The New Futures of Exclusion: Life in the Covid-19 Aftermath (1st ed.). Cham: Springer. ISBN 9783031418655. Lloyd, Anthony; Briggs, Daniel; Ellis, Anthony; Telford, Luke (2024). \"Critical Reflections on the COVID-19 Pandemic from the NHS Frontline\".",
    "label": 0
  },
  {
    "text": "Lloyd, Anthony; Ellis, Anthony (2023). The New Futures of Exclusion: Life in the Covid-19 Aftermath (1st ed.). Cham: Springer. ISBN 9783031418655. Lloyd, Anthony; Briggs, Daniel; Ellis, Anthony; Telford, Luke (2024). \"Critical Reflections on the COVID-19 Pandemic from the NHS Frontline\". Sociological Research Online. 29 (1): 83–100. doi:10.1177/13607804231156293. ISSN 1360-7804. PMC 9950816. Lloyd, Anthony (2017). \"Ideology at work: reconsidering ideology, the labour process and workplace resistance\". International Journal of Sociology and Social Policy. 37 (5/6): 266–279. doi:10.1108/IJSSP-02-2016-0019. ISSN 0144-333X. Briggs, Daniel; Telford, Luke; Lloyd, Anthony; Ellis, Anthony (2021). \"Working, living and dying in COVID times: perspectives from frontline adult social care workers in the UK\". Safer Communities. 20 (3): 208–222. doi:10.1108/SC-04-2021-0013. ISSN 1757-8043. Riots and far-right politic Winlow, Simon; Hall, Steve; Treadwell, James (2017). The Rise of the Right: English Nationalism and the Transformation of Working-Class Politics. Bristol: Policy. ISBN 9781447328483. Winlow, Simon; Hall, Steve; Treadwell, James (2019-03-01), \"Why the Left Must Change\", Progressive Justice in an Age of Repression, Abingdon, Oxon: Routledge, pp. 26–41, doi:10.4324/9781351242059-3, ISBN 9781351242059 Kelly, Emma; Winlow, Simon (2022). \"Traversing the Fantasy: Why Leftist Academics Must Abandon the Myth of Organic Resistance and Think Again About the Problems We Face\". Critical Criminology. 30 (2): 237–244. doi:10.1007/s10612-021-09573-1. ISSN 1205-8629. Winlow, Simon (2025). The Politics of Nostalgia: Class, Rootlessness and Decline. Society Now. Bingley: Emerald. ISBN 9781837535514. Research methods Treadwell, James (2019). Criminological Ethnography: An Introduction. London: Sage. ISBN 9781473975712. Winlow, Simon; Measham, Fiona (2016). \"Doing the Right Thing: Some notes on the control of research in British criminology\". In Cowburn, Malcolm; Gelsthorpe, Loraine; Wahidin, Azrini (eds.). Research Ethics in Criminology: Dilemmas, Issues and Solutions. Abingdon, Oxon: Routledge. doi:10.4324/9781315753553. ISBN 9781315753553. Hall, Steve (2018). \"Criminological Ethnography\". In Davies, Pamela; Francis, Peter (eds.). Doing Criminological Research (3rd ed.). London: Sage. ISBN 9781473902725. Winlow, Simon (2022). \"Beyond Measure: On the Marketization",
    "label": 0
  },
  {
    "text": "in Criminology: Dilemmas, Issues and Solutions. Abingdon, Oxon: Routledge. doi:10.4324/9781315753553. ISBN 9781315753553. Hall, Steve (2018). \"Criminological Ethnography\". In Davies, Pamela; Francis, Peter (eds.). Doing Criminological Research (3rd ed.). London: Sage. ISBN 9781473902725. Winlow, Simon (2022). \"Beyond Measure: On the Marketization of British Universities, and the Domestication of Academic Criminology\". Critical Criminology. 30 (3): 479–494. doi:10.1007/s10612-022-09643-y. ISSN 1572-9877. Armstrong, Emma Katie (2020). \"Political Ideology and Research: How Neoliberalism Can Explain the Paucity of Qualitative Criminological Research\". Alternatives. 45 (1): 20–32. doi:10.1177/0304375419899832. ISSN 0304-3754. Rios, Gino; Silva, Antonio (2020). Nuevos Horizontes en la Investigacion Criminlogica Ultra-Realismo (in Spanish). Lima, Peru: Universidad de San Martín de Porres. ISBN 9786124460234. Silva, Antonio; Pérez, Jorge; Cordero, Raquel; Díaz, Julio (2025). Researching Social Media with Children. Abingdon, Oxon: Routledge. ISBN 9781032506173. Violence and masculinity Winlow, Simon (2001). Badfellas: Crime, Tradition and New Masculinities. Oxford: Berg. ISBN 1859734146. Ellis, Anthony (2017). Men, Masculinities and Violence: An Ethnographic Study. Routledge studies in crime and society. Abingdon, Oxon: Routledge. ISBN 9781138040274. Ellis, Anthony; Winlow, Simon; Hall, Steve (2017). \"'Throughout my life I've had people walk all over me': Trauma in the lives of violent men\". The Sociological Review. 65 (4): 699–713. doi:10.1177/0038026117695486. ISSN 0038-0261. Kotzé, Justin; Antonopoulos, Georgios A. (2021). \"Boosting bodily capital: Maintaining masculinity, aesthetic pleasure and instrumental utility through the consumption of steroids\". Journal of Consumer Culture. 21 (3): 683–700. doi:10.1177/1469540519846196. ISSN 1469-5405. Gibbs, Nick (2021). The Muscle Trade: The Use and Supply of Image and Performance Enhancing Drugs. Bristol: Bristol University Press. ISBN 9781529227949. Crime, harm and glocal markets Hall, Alexandra; Antonopoulos, Giorgios (2016). Fake Meds Online: The Internet and the Transnational Market in Illicit Pharmaceuticals. Basingstoke: Palgrave Macmillan. ISBN 9781137570888. Kotzé, Justin; Antonopoulos, Giorgios A (2023). \"Con Air: exploring the trade in counterfeit and unapproved aircraft parts\". The British Journal of Criminology. 63 (5): 1293–1308.",
    "label": 0
  },
  {
    "text": "Online: The Internet and the Transnational Market in Illicit Pharmaceuticals. Basingstoke: Palgrave Macmillan. ISBN 9781137570888. Kotzé, Justin; Antonopoulos, Giorgios A (2023). \"Con Air: exploring the trade in counterfeit and unapproved aircraft parts\". The British Journal of Criminology. 63 (5): 1293–1308. doi:10.1093/bjc/azac089. ISSN 0007-0955. Bural, Dilara; Lloyd, Anthony; Antonopoulos, Georgios A.; Kotzé, Justin (2023). \"\"Fake it to make it\": exploring product counterfeiting in Türkiye\". Journal of Financial Crime. 31 (6): 1451–1466. doi:10.1108/JFC-10-2023-0252. ISSN 1758-7239. Policing and corruption Brookshaw, Brendan (2024). Addressing Corruption in the Police Service: The Thick Blue Line. Palgrave critical policing studies. Basingstoke: Palgrave Macmillan. ISBN 9783031750670. Kuldova, Tereza; Gundhus, Helene Oppen Ingebrigtsen; Wathne, Christin Thea, eds. (2024). Policing and Intelligence in the Global Big Data Era, Volume I: New Global Perspectives on Algorithmic Governance. Palgrave Critical Policing Studies. Basingstoke: Palgrave Macmillan. ISBN 9783031683251. Kuldova, Tereza; Gundhus, Helene Oppen Ingebrigtsen; Wathne, Christin Thea, eds. (2024). Policing and Intelligence in the Global Big Data Era, Volume II: New Global Perspectives on the Politics and Ethics of Knowledge. Palgrave Critical Policing Studies. Basingstoke: Palgrave Macmillan. ISBN 9783031682971. Homicide and serial murder Yardley, Elizabeth; Wilson, David (2015). Female Serial Killers in Social Context: Criminological Institutionalism and the Case of Mary Ann Cotton. Shorts Research. Bristol: Policy. ISBN 9781447326458. Hall, Steve; Wilson, David (2014). \"New foundations: Pseudo-pacification and special liberty as potential cornerstones for a multi-level theory of homicide and serial murder\". European Journal of Criminology. 11 (5): 635–655. doi:10.1177/1477370814536831. ISSN 1477-3708. Lynes, Adam; Kelly, Craig; Uppal, Pravanjot Kapil Singh (2018). \"Benjamin's 'flâneur' and serial murder: An ultra-realist literary case study of Levi Bellfield\". Crime, Media, Culture. 15 (3): 523–543. doi:10.1177/1741659018815934. ISSN 1741-6590 Crime, harm and mass media Hayward, Keith J; Hall, Steve (2021). \"Through Scandinavia, Darkly: A Criminological Critique of Nordic Noir\". The British Journal of Criminology. 61 (1): 1–21. doi:10.1093/bjc/azaa044. ISSN",
    "label": 0
  },
  {
    "text": "Bellfield\". Crime, Media, Culture. 15 (3): 523–543. doi:10.1177/1741659018815934. ISSN 1741-6590 Crime, harm and mass media Hayward, Keith J; Hall, Steve (2021). \"Through Scandinavia, Darkly: A Criminological Critique of Nordic Noir\". The British Journal of Criminology. 61 (1): 1–21. doi:10.1093/bjc/azaa044. ISSN 0007-0955. Wilson, David (2011). Looking for Laura: Public Criminology and Hot News. Hook: Waterside Press. ISBN 9781904380702. OCLC 727021849. Yardley, Elizabeth; Kelly, Emma; Robinson-Edwards, Shona (2019). \"Forever trapped in the imaginary of late capitalism? The serialized true crime podcast as a wake-up call in times of criminological slumber\". Crime, Media, Culture. 15 (3): 503–521. doi:10.1177/1741659018799375. ISSN 1741-6590 Crime, corruption and compliance Kuldova, Tereza; Østbø, Jardar; Raymen, Thomas (2024). Luxury and Corruption: Challenging the Anti-Corruption Consensus. Bristol: Bristol University Press. ISBN 9781529236330. Kuldova, Tereza; Østbø, Jardar; Shore, Cris, eds. (2024). Compliance, Defiance, and 'Dirty' Luxury: New Perspectives on Anti-Corruption in Elite Contexts. Basingstoke: Palgrave Macmillan. ISBN 9783031571398. Lynes, Adam; Treadwell, James; Bavin, Kyla (2024). Crimes of the Powerful and the Contemporary Condition: The Democratic Republic of Capitalism. Bristol: Bristol University Press. ISBN 978-1-5292-2829-8. Technology, harm and crime Yardley, Elizabeth (2020). \"Technology-Facilitated Domestic Abuse in Political Economy: A New Theoretical Framework\". Violence Against Women. 27 (10): 1479–1498. doi:10.1177/1077801220947172. ISSN 1077-8012. PMID 32757887.The crime decline Kotzé, Justin (2019). The Myth of the Crime Decline: Exploring Change and Continuity in Crime and Harm. Routledge Studies in Crime and Society. Abingdon, Oxon: Routledge. ISBN 9780815353935.Child abuse Ellis, Anthony; Briggs, Daniel; Lloyd, Anthony (2021). \"A ticking time bomb of future harm: Lockdown, child abuse and future violence\". Abuse: An International Impact Journal. 2 (1): 37–48. doi:10.37576/abuse.2021.017. hdl:11268/11957 History and violence Ellis, Anthony (2019). \"A De-Civilizing Reversal or System Normal? Rising Lethal Violence in Post-Recession Austerity United Kingdom\". The British Journal of Criminology. 59 (4): 862–878. doi:10.1093/bjc/azz001. ISSN 0007-0955. Horsley, Mark; Kotze, Justin; Hall, Stephen (2015). \"The",
    "label": 0
  },
  {
    "text": "hdl:11268/11957 History and violence Ellis, Anthony (2019). \"A De-Civilizing Reversal or System Normal? Rising Lethal Violence in Post-Recession Austerity United Kingdom\". The British Journal of Criminology. 59 (4): 862–878. doi:10.1093/bjc/azz001. ISSN 0007-0955. Horsley, Mark; Kotze, Justin; Hall, Stephen (2015). \"The Maintenance of Orderly Disorder: Law, markets and the pseudo-pacification process\". Journal on European History of Law. 6 (1). ISSN 2042-6402. Crime, corruption and sport Jump, Deborah (2020). The Criminology of Boxing, Violence and Desistance. Bristol: Bristol University Press. ISBN 9781529203295. Gallacher, Grace (2022), Silva, Derek; Kennedy, Liam (eds.), \"The (De)Civilizing Process: An Ultra-Realist Examination of Sport\", Power Played: A critical criminology of sport, Vancouver: University of British Columbia Press, pp. 100–116, doi:10.59962/9780774867818-004, ISBN 9780774867818 Harm and hate crime James, Zoë (2020). The Harms of Hate for Gypsies and Travellers: A Critical Hate Studies Perspective. Palgrave Hate Studies. Basingstoke: Palgrave Macmillan. ISBN 9781137518286. James, Zoë (2020). \"Gypsies' and Travellers' lived experience of harm: A critical hate studies perspective\". Theoretical Criminology. 24 (3): 502–520. doi:10.1177/1362480620911914. ISSN 1362-4806. Other Projects Military Studies: Armstrong, Emma (2025). British Army Veterans' Experiences of the Transition into Civilian Life: An Ultra-Realist Perspective. Abingdon, Oxon: Routledge. ISBN 9781032797113. Subjectivity and Investment Fraud: Tudor, Kate (2018). \"Toxic Sovereignty:Understanding Fraud as the Expression of Special Liberty within Late Capitalism\". Journal of Extreme Anthropology. 2 (2): 7–21. doi:10.5617/jea.6476. ISSN 2535-3241. Crime, Harm and Drugs: Ayres, Tammy; Ancrum, Craig (2023). Understanding Drug Dealing and Illicit Drug Markets: National and International Perspectives. Abingdon, Oxon: Routledge. ISBN 9781138541825. Criminology and Borders: Briggs, Daniel (2021). Climate Changed: Refugee Border Stories and the Business of Misery. Abingdon, Oxon: Routledge. ISBN 9781003004929. Critique and responses Early criticism from reviewers focused on ultra-realism's complexity, opacity and lack of diversity. Ultra-realists responded by arguing that students and early career researchers experience little trouble in dealing with the complexity, that",
    "label": 0
  },
  {
    "text": "Abingdon, Oxon: Routledge. ISBN 9781003004929. Critique and responses Early criticism from reviewers focused on ultra-realism's complexity, opacity and lack of diversity. Ultra-realists responded by arguing that students and early career researchers experience little trouble in dealing with the complexity, that the ethnographic networking method enables and highlights diversity, and that ultra-realism's theoretical concepts can be adopted by any researcher in any part of the world. Debates followed around the relative conceptual value of the civilizing process and the pseudo-pacification process. Ultra-realists regard the latter as more representative of reality while defenders of Elias's concept argue that the nuances of his thesis have been ignored or misunderstood. Ultra-realists responded by pointing out that the roots of the pseudo-pacification process were in 12th-century England. It is an alternative theory rooted in a time that predates the roots of the civilizing process in 17th century Europe. Further criticisms focused on ultra-realism's naturalization of violent drives, the incoherence of the connection between the pseudo-pacification process and special liberty, a tendency towards monocausality, essentialism and reductionism associated with the 'direct causality' of the economic context, the denial of human agency, and neglect of gender relations as epiphenomenal. Ultra-realists responded by pointing out that special liberty motivates and permits individuals to move beyond the legal and normative boundaries of pseudo-pacification, therefore they are integrated parts of the same process. The ultra realist concept of the 'chosen unconscious' suggests that the thesis is grounded in agency and choice. The economic context is probabilistic, not directly causal, because ultra-realists use crime trends as initial indicators of the need for further investigation using ethnographic methods to investigate the complex relations between economy, culture, and individual situations. Gender relations have featured centrally in ultra-realist research. More recent debates have discussed the possibility that ultra-realism doesn't offer anything new, its causal",
    "label": 0
  },
  {
    "text": "for further investigation using ethnographic methods to investigate the complex relations between economy, culture, and individual situations. Gender relations have featured centrally in ultra-realist research. More recent debates have discussed the possibility that ultra-realism doesn't offer anything new, its causal logic is tautological and can't be falsified, and the transcendental materialist framework is unverifiable. Ultra-realists responded by arguing that concepts such as the pseudo-pacification process and special liberty are unique to ultra-realism. Whereas ultra-realists agreed that they require more research to address the issue of falsification, Hall and Winlow also agreed with Lakatos's claim that Popper's notion is flawed because the falsifying premises themselves are often preferences that have evaded falsification. The criticism that the transcendental materialist framework is currently unverified has been accepted. Hall and Winlow addressed this issue by integrating recent neuroscientific and neuropsychoanalytic work into their theoretical framework. This produced the concept of 'the molecular question', the combined biological, affective and metaphysical process that seems to underlie what Lacan proposed as a 'void'. Further reading Hall, S. and Winlow, S. (2025) Revitalizing Criminological Theory: Advances in Ultra-Realism. Abingdon: Routledge. ISBN 9781041034933 Kotzé, J. and Lloyd, A. (2022) Making Sense of Ultra-Realism. Bingley: Emerald. ISBN 978-1-80117-170-0. Hall, S. and Winlow, S. (2025) Ultra-Realism: A Study Guide. UR Publications. ISBN 979-8283050799 Hopkins-Burke, R. (2021) 'Ultra-Realist Criminology', in Hopkins-Burke, R. Contemporary Criminological Theory: Crime and Criminal Behaviour in the Age of Moral Uncertainty. Abingdon: Routledge. ISBN 9780815374473 Hall, S. and Winlow, S. (2018) 'Ultra-Realism', in DeKeseredy, W. and Dragiewicz, M. (eds.) The Routledge Handbook of Critical Criminology, Abingdon: Routledge. ISBN 9781138656192 Hall, S. and Winlow, S. (2017) 'Ultra-Realism', in Brisman, A., Carrabine, E. and South, N. (eds.) The Routledge Companion to Criminological Theory and Concepts, Abingdon: Routledge. ISBN 9781138819009 References",
    "label": 0
  },
  {
    "text": "Western esotericism and psychology surveys the documented exchanges between Western esotericism—including Westernized hybrids of Asian traditions—and selected areas of psychology, psychotherapy, and popular psychology. From the late eighteenth century onward, conduits such as animal magnetism and early hypnosis (reinterpreted from mesmeric “somnambulism”), Spiritualism/psychical research, and fin de siècle occultism and comparative projects created channels by which esoteric repertoires (e.g., alchemy, astrology, and subtle body schemes) were translated into psychological idioms or embedded in therapeutic and self-development techniques. In the twentieth century, these exchanges were variously articulated in analytical psychology (including Jung’s alchemical hermeneutics), humanistic workshop cultures and the human potential movement, transpersonal psychology, and symbolic counselling that repurposed oracular media (e.g., tarot, astrology, or the I Ching). Rather than a single genealogy, historians emphasise plural processes of transmission, translation, and hybridisation across specific networks and publics—among them Theosophy (with codified chakras and subtle bodies), Anthroposophy (linking esoteric doctrines to pedagogical and para-clinical projects), the Eranos circle (mediating Jungian hermeneutics and history-of-religions), and late-modern markets often labelled “New Age”. Sociological accounts frame the broader diffusion via the late-modern “cultic milieu” and “occulture”, which describe how esoteric symbols and narratives circulate beyond formal religion through publishing, workshops, retreats and wellness/coaching niches, where psychologised self-work became a prominent vector of reception. Scope and definitions Definitions In the academic literature, Western esotericism is approached not as a single doctrine but as a modern Western form of thought. A widely used componential definition was formulated by the historian of religion Antoine Faivre, who identified four intrinsic features—(1) correspondences linking microcosm and macrocosm, (2) a living nature open to hidden forces, (3) the primacy of imagination and mediations (e.g., images, symbols, rites), and (4) transmutation (psychic or spiritual transformation)—together with two relative traits, (5) concordance among traditions and (6) transmission (initiatory or doctrinal). Hanegraaff’s overviews present",
    "label": 0
  },
  {
    "text": "nature open to hidden forces, (3) the primacy of imagination and mediations (e.g., images, symbols, rites), and (4) transmutation (psychic or spiritual transformation)—together with two relative traits, (5) concordance among traditions and (6) transmission (initiatory or doctrinal). Hanegraaff’s overviews present this scheme as a heuristic for describing modern formations rather than as an ontology. In this article, psychology is used broadly to cover three overlapping domains: (a) theoretical systems (e.g., Jungian and archetypal psychology), (b) clinical and para-clinical modalities (e.g., hypnosis, psychotherapy, arts-based and anthroposophic therapies), and (c) popular psychological culture and coaching in which esoteric repertoires are translated into symbolic–psychological idioms or embedded as operative frameworks. Extent and limits The temporal focus is modern–contemporary, from late eighteenth-century mesmerism to the present. Medieval and Renaissance entanglements (e.g., Hermeticism, Neoplatonism, Christian Kabbalah) provide background but are not the primary focus. The emphasis falls on channels by which esoteric repertoires (alchemy, astrology, and subtle-body schemes such as “chakras”) entered psychological theory, therapy, and self-work. Purely devotional mysticism (union without operative correspondences) is distinguished from esotericism; by contrast, secular mind–body programmes without esoteric genealogy (e.g., standardized MBSR) are treated as contrasts rather than instances. Methodological cautions Consistent with the academic study of Western esotericism, the article distinguishes actors’ emic self-descriptions from scholarly etic analysis. To avoid false positives, resemblance alone does not warrant claims of influence: at least one of the following is required—(a) explicit self-attribution by the actors, (b) operational adoption of symbols/diagrams/rites as method, or (c) a traceable chain of transmission (people, texts, institutions) consistent with Faivre’s components. In addition, the article separates realist–occult claims from a symbolic/interpretive framing—sometimes glossed informally, once here, as “as-if real”—in which images, diagrams, rites or oracles function as structured prompts for meaning-making rather than as vehicles of literal knowledge. Analytical framework used in this article",
    "label": 0
  },
  {
    "text": "article separates realist–occult claims from a symbolic/interpretive framing—sometimes glossed informally, once here, as “as-if real”—in which images, diagrams, rites or oracles function as structured prompts for meaning-making rather than as vehicles of literal knowledge. Analytical framework used in this article For descriptive clarity, each case is characterised along four axes: (1) episteme (realist–occult; symbolic/interpretive; secular–therapeutic), (2) frame (ritual/doctrinal or clinical/educational), (3) purpose (gnosis/unio; psychic integration; symptom relief; optimization), and (4) device (symbolic tools such as tarot, astrology or I Ching; trance/hypnosis/breathwork; somatic work; ritual/drama; textual formulas). Statements about efficacy are reported separately using broad bands (A = controlled trials or systematic reviews; B = observational or case-series evidence; C = testimonials or mixed/weak findings). These tags are heuristic and remain distinct from genealogical claims or value judgements. Historiography and frameworks Modern scholarship generally treats Faivre’s componential definition as a heuristic grid rather than as an ontology for what esotericism “is”. In this perspective, the grid helps historians describe how symbolic mediations and techniques of transformation were translated into psychological idioms in specific periods and publics, while avoiding essentialist claims. Placed within nineteenth- and twentieth-century contexts, Wouter J. Hanegraaff proposes an etic use of “occultism” for modern currents that explored interfaces between science, comparative religion and esoteric practice, and describes a two-way traffic between religious and psychological languages—“psychologization of religion” and “sacralization of psychology”—to account for the reception of alchemy, astrology and subtle-body maps in theory, therapy and self-work. For diffusion beyond academic or ecclesiastical institutions, historians of contemporary religion draw on sociological models. The notion of the cultic milieu proposed by Colin Campbell designates an environment in which heterodox repertoires (e.g., astrology, trance, subtle bodies) persist, recirculate and recombine; Christopher Partridge’s “occulture” points to a cultural reservoir mediated by publishing, media and popular culture, through which psychologised self-work, workshops and",
    "label": 0
  },
  {
    "text": "by Colin Campbell designates an environment in which heterodox repertoires (e.g., astrology, trance, subtle bodies) persist, recirculate and recombine; Christopher Partridge’s “occulture” points to a cultural reservoir mediated by publishing, media and popular culture, through which psychologised self-work, workshops and wellness/coaching markets became prominent vectors of reception. Within this historiographic framing, hubs such as the Eranos meetings in Switzerland functioned as interfaces between Jungian hermeneutics and history-of-religions approaches, helping to normalise psychological readings of premodern symbol-systems (including Asian repertoires) in interpretive, non-realist terms. The article therefore combines componential description with diffusion models to track how esoteric repertoires travelled into psychological theory, clinical/para-clinical settings, and popular counselling cultures. Jung’s theorisation of synchronicity is often cited as part of the modern “psychologization of religion”, supplying a vocabulary by which divinatory repertoires could be recoded for counselling without realist–occult commitments. Rejected knowledge by academic psychology In this line of analysis, Wouter J. Hanegraaff introduced the notion of rejected knowledge to describe bodies of thought and practice that once claimed rational or scientific legitimacy but were later excluded from academic discourse. The concept provides historians with a tool for understanding why movements such as mesmerism, psychical research, or vitalist biophysics—though not self-identified as “esoteric”—nonetheless display structural affinities with the esoteric field through their use of subtle-force cosmologies, analogical reasoning, and transformative techniques. Psychologization of esotericism (historiographical models) Scholarship has used “psychologization” to describe how esoteric repertoires were reframed in psychological idioms or channelled through therapeutic formats. Wouter J. Hanegraaff proposed a bidirectional traffic—“psychologization of religion” and “sacralization of psychology”—as a lens for modern encounters between esotericism and psychology. Complementarily, long-duration histories of dynamic psychiatry (as theorized by the historiographer of psychiatry Henri Ellenberger) trace continuities from exorcism to mesmerism, hypnosis and the emergence of clinical psychologies, providing a background against which such reframings occurred.",
    "label": 0
  },
  {
    "text": "between esotericism and psychology. Complementarily, long-duration histories of dynamic psychiatry (as theorized by the historiographer of psychiatry Henri Ellenberger) trace continuities from exorcism to mesmerism, hypnosis and the emergence of clinical psychologies, providing a background against which such reframings occurred. Recent work has refined “psychologization” by distinguishing several modes (e.g., complementary, terminological, reductive, idealist) and by analysing the appeal to a distinct “magical plane” (a separate-but-connected psychic locus) that insulates ritual practice from empirical critique while aligning it with psychoanalytic or depth-psychological vocabularies. These models help to map how actors translate ritual devices into symbolic or therapeutic procedures without presupposing realist–occult claims. Proto-psychologization in harmonial religion For clarity, modern scholarship distinguishes between actors’ metaphysical vocabularies and historians’ analytic categories. In this regard, the article follows the usage of harmonial religion (also called harmonialism) as a historiographical label (etic) designating a North American metaphysical ethos (19th–20th centuries) that links mind–body–spirit harmony to wellbeing, encompassing mind-cure/New Thought, breath culture and related esoteric milieux. The label harmonial religion—part of what Catherine L. Albanese terms the broader American metaphysical religion—was introduced in her study A Republic of Mind and Spirit (2007), while harmonialism is used by Anya P. Foxen to analyse Western reimaginings of yoga, respiration, and their orientalisation. Placed within the wider narrative of Western esotericism and psychology, what Albanese terms harmonial religion (and Foxen discusses as harmonialism)—ranging from Swedenborgian metaphysics through mesmerism, mind-cure and breath culture—constitutes a proto-psychological strand of modern esoteric thought. These formations prefigure the twentieth-century “psychologization of religion” analysed by Wouter J. Hanegraaff, wherein spiritual transformation and mental hygiene converge as parallel idioms of self-regulation and correspondence. Periodisation of the intersection Enlightenment to nineteenth century From late Enlightenment experiments in animal magnetism and guided “somnambulism” to fin-de-siècle debates on suggestion and trance, early interfaces between esoteric repertoires and psychological",
    "label": 0
  },
  {
    "text": "converge as parallel idioms of self-regulation and correspondence. Periodisation of the intersection Enlightenment to nineteenth century From late Enlightenment experiments in animal magnetism and guided “somnambulism” to fin-de-siècle debates on suggestion and trance, early interfaces between esoteric repertoires and psychological inquiry unfolded across salons, clinics and séance rooms. Mesmeric practices circulated the idea of an invisible influence acting upon body and mind; controlled inductions and narrative management of altered states provided techniques and cases that would be reframed in medical and psychological vocabularies. During the early nineteenth century, the Transcendentalist movement in the United States—particularly the writings of Ralph Waldo Emerson—absorbed elements of Western esotericism, including Swedenborgian doctrines of correspondences—which interpreted the visible world as an image of inner and spiritual processes—and Neoplatonic conceptions of the \"Over-Soul\". Scholars of American religion such as Catherine L. Albanese interpret this Transcendentalist synthesis as part of the broader harmonial or metaphysical current that linked moral and mental harmony with spiritual vitality. This intellectual milieu provided a cultural background for later mind-cure and New Thought movements, in which esoteric motifs of correspondence and transformation were increasingly psychologized and reframed as methods of self-regulation and healing. Within the social worlds of spiritualism and early psychical research, clinicians and philosophers used trance cases to theorise divided consciousness. Pierre Janet’s work on dissociation systematised observations of automatisms (automatic writing, trance speech) as expressions of partitioned awareness, while discussions by William James and Sigmund Freud—both attentive to hypnosis and to hysteria debates—helped normalise psychological framings that did not rely on occult fluids or literal spirit agencies. Case-rich monographs such as Théodore Flournoy’s study of glossolalia and secondary personalities supplied detailed documentation without doctrinal commitments, furnishing materials for theorising imagination, suggestion and divided consciousness in secular terms. By the century’s end, comparative and indological presentations had begun to recast",
    "label": 0
  },
  {
    "text": "such as Théodore Flournoy’s study of glossolalia and secondary personalities supplied detailed documentation without doctrinal commitments, furnishing materials for theorising imagination, suggestion and divided consciousness in secular terms. By the century’s end, comparative and indological presentations had begun to recast selected Asian repertoires in psychological or mentalist terms for Western audiences—for example, Neo-Vedantic readings of Raja Yoga within modern interpretations of Yoga—foreshadowing twentieth-century migrations of somatic–attentional techniques into wellness and para-clinical niches. Tradition interfaces in this period Mesmerism and induced “somnambulism” popularised controlled trance and supplied early conduits from esoteric milieus to psychological technique (later recoded as hypnosis and suggestion). spiritualism and early psychical research generated case-materials—automatic writing, trance communication, secondary personalities—that fed debates on imagination and dissociation; Flournoy’s monograph is paradigmatic for this descriptive turn. Late-nineteenth-century indological receptions (e.g., Neo-Vedantic, mentalist readings of Raja Yoga) prefigured the twentieth-century translation of body–attention repertoires into secular or symbolic frames. Institutional and editorial hubs in this period Venues ranged from salons to clinical theatres (e.g., the Salpêtrière in Paris), while the Society for Psychical Research of London (SPR, 1882–) professionalised report genres on automatisms and trance that fed psychological debates on imagination and dissociation. Book-length case studies—such as Flournoy’s—circulated between séance rooms and clinics, helping to normalise descriptive vocabularies for trance phenomena without endorsing occult realism; sociological models later describe such circulations within a broader late-modern “cultic milieu”. Occultism and comparative religion (late 19th–early 20th centuries) In the milieu of the Occult Revival of the 19th century in Western societies the Society for Psychical Research (SPR) became a key contact zone between séance cultures and emerging psychological inquiry. Its programmes covered hypnotism, dissociation, mediumship and “thought-transference” (later standardised as telepathy by Frederic W. H. Myers), with figures such as William James engaging controversy around mediums like Eusapia Palladino and critics like Hugo",
    "label": 0
  },
  {
    "text": "séance cultures and emerging psychological inquiry. Its programmes covered hypnotism, dissociation, mediumship and “thought-transference” (later standardised as telepathy by Frederic W. H. Myers), with figures such as William James engaging controversy around mediums like Eusapia Palladino and critics like Hugo Münsterberg. Historians read these investigations as part of the broader traffic in which esoteric repertoires (trance techniques, oracular devices) and laboratory or literary methods cross-pollinated, even as medical framings recoded practice in secular psychological terms. Around 1900, currents grouped under modern occultism and comparative projects systematised repertoires that would become salient in psychological translation. Theosophy articulated a modern discourse on subtle bodies and diffused codified “chakra” schemes through handbooks and visual plates that circulated well beyond initiatory settings; these maps furnished a flexible idiom for inner states and “energy” later reframed in counselling and self-development. At the same time, Anthroposophy extended esoteric doctrines into education and para-clinical practice, associating imagination and development with disciplined movement and arts-based therapies (e.g., Eurythmy). Organisationally, anthroposophic centres linked doctrine, pedagogy and therapeutic experiments, creating channels by which symbolic repertoires reached classrooms, clinics and workshops. In North Atlantic contexts, an American strand labelled New Thought (or “mental healing”) supplied vocabularies of mental causation and affirmation that later fed self-help and coaching cultures. Historians read these idioms as part of a broader late-modern psychologization of religious repertoires that facilitated their translation into popular psychological frames. One of the main intellectual bridges of mind-cure/New Thought with earlier esotericisms and scholar philosophy of mind was the swedenborgian minister Warren Felt Evans, who articulated a doctrine of “mental therapeutics” by combining mesmeric suggestion with Emanuel Swedenborg’s mysticism, the American mind-cure tradition, and the subjective idealism of George Berkeley. Alongside occultist codifications, late nineteenth- and early twentieth-century comparative religion and history of religions projects created interpretive frames for reading premodern",
    "label": 0
  },
  {
    "text": "by combining mesmeric suggestion with Emanuel Swedenborg’s mysticism, the American mind-cure tradition, and the subjective idealism of George Berkeley. Alongside occultist codifications, late nineteenth- and early twentieth-century comparative religion and history of religions projects created interpretive frames for reading premodern symbol-systems (e.g., alchemical, visionary or divinatory corpora) in dialogue with emerging psychological languages. Philological and comparative syntheses furnished taxonomies and narratives that could later be reframed in counselling and self-work, a trajectory consolidated in the 1930s around forums integrating Jungian hermeneutics and history-of-religions approaches, including figures later associated with the Eranos circle such as Mircea Eliade and Henry Corbin. Within twentieth-century ceremonial magic, Israel Regardie framed Golden Dawn techniques—especially the “Middle Pillar”—as psychological procedures for enhancing or “exalting” consciousness, and published core ritual materials (1937–40), easing their circulation into occulture and self-work milieus. Historiographically, Regardie functions as a case for analysing distinct modes of \"psychologization of esotericism\" (e.g., terminological and complementary) and the appeal to a “magical plane” that relocates efficacy claims to a psychic register compatible with depth-psychological idioms. Tradition interfaces in this period Theosophy codified subtle-body and chakra maps for Western publics, providing portable visual grammars subsequently re-described in psychological and wellness idioms. Anthroposophy linked esoteric doctrines to pedagogical and para-clinical projects (e.g., Eurythmy), where movement, colour and form were framed as instruments of development and regulation. A concurrent American strand, New Thought, articulated mental-causation vocabularies that later informed self-help and coaching; historians situate this diffusion within late-modern occulture and the psychologization of religion. Institutional and editorial hubs in this period The Theosophical Society and its publishing arms (e.g., Theosophical Publishing House, Adyar) disseminated handbooks and plates of subtle-body/chakra schemes for non-initiatory audiences, while anthroposophic centres around the Goetheanum (Dornach) developed pedagogical and para-clinical programmes that integrated symbolic movement and arts therapies. Lecture circuits and comparative-religion forums linked",
    "label": 0
  },
  {
    "text": "(e.g., Theosophical Publishing House, Adyar) disseminated handbooks and plates of subtle-body/chakra schemes for non-initiatory audiences, while anthroposophic centres around the Goetheanum (Dornach) developed pedagogical and para-clinical programmes that integrated symbolic movement and arts therapies. Lecture circuits and comparative-religion forums linked erudite synthesis with psychological hermeneutics, helping to normalise interpretive (non-realist) uses of esoteric materials in counselling and education. Twentieth century: analytical, humanistic and transpersonal In the early to mid-twentieth century, approaches associated with Analytical psychology developed sustained hermeneutics of premodern symbol-systems—most prominently Alchemy—as resources for psychological theory and practice, with Active imagination and mandala work translating symbolic repertoires into models of psychic transformation and individuation. Comparative engagements (including dialogues around classical divination texts such as the I Ching) encouraged a broader reception of esoteric imagery in counselling milieus. Within this framework, Carl Jung’s notion of synchronicity (“an acausal connecting principle”) underwrote interpretive uses of divinatory media—such as the I Ching—in counselling settings, framing them as structured prompts for meaning-making rather than as vehicles of predictive knowledge. From the 1960s, humanistic workshop cultures and venues such as the Esalen Institute functioned as hubs for experiential methods that framed inner change in strongly symbolic terms. Within these circuits, oracular media and ritualised settings were re-described as vehicles for self-exploration and interpersonal learning, a move that normalised the assimilation of esoteric motifs into therapeutic language without requiring adherence to realist–occult claims. Transpersonal psychology systematised these developments by theorising non-ordinary states and by integrating selected esoteric vocabularies—subtle-body maps, archetypal narratives, and ritualised techniques—into psychological models of development, crisis and integration. Reviews in the field highlight both historical debts to Western esotericism and the need for careful source-tracing to distinguish symbolic/interpretive framings from realist assertions, and to separate genealogy from questions of efficacy. Tradition interfaces in this period Within Analytical psychology, Jungian hermeneutics re-read premodern",
    "label": 0
  },
  {
    "text": "highlight both historical debts to Western esotericism and the need for careful source-tracing to distinguish symbolic/interpretive framings from realist assertions, and to separate genealogy from questions of efficacy. Tradition interfaces in this period Within Analytical psychology, Jungian hermeneutics re-read premodern symbol-systems (notably Alchemy) for psychological theory and practice, while Jungian and allied circles engaged divination corpora (e.g., the I Ching) as symbolic prompts in counselling. Humanistic psychology and workshop cultures (e.g., Esalen Institute) provided experiential containers in which oracular devices and ritualised sequences circulated in interpretive frames. Transpersonal psychology theorised non-ordinary states and integrated selected esoteric vocabularies (subtle-body maps, archetypal language, ritual techniques) into developmental models. In parallel, Fourth Way circles associated with G. I. Gurdjieff and P. D. Ouspensky supplied movement and dramaturgic formats later adapted in self-development workshops. Transpersonal circles also developed experiential cartographies through psychedelic-assisted psychotherapy and, later, breathwork protocols (e.g., holotropic breathwork), integrating archetypal and symbolic vocabularies while bracketing realist–occult claims. Institutional and editorial hubs in this period The Eranos meetings (Ascona) interfaced Jungian circles with history-of-religions scholarship, stabilising psychological readings of premodern symbol-systems (including Asian corpora) in interpretive, non-realist frames. Mid-century workshop venues such as the Esalen Institute (Big Sur) structured experiential formats—encounter, movement, guided imagination—in which symbolic and ritual devices circulated alongside clinical and educational experimentation; associations and journals consolidated Transpersonal psychology as a field. At the boundary of psychology proper, parapsychology laboratories and surveys extended the nineteenth-century psychical-research agenda (e.g., studies of telepathy/clairvoyance) into experimental and questionnaire-based programmes. Late 20th century to present: hybrids, wellness and coaching From the late twentieth century, repertoires previously clustered in specialist milieus migrated into wider self-help, wellness and coaching cultures. Modern transpositions of yoga supplied somatic and attentional techniques that could be framed in secular or symbolic terms, sometimes retaining subtle-body vocabularies and sometimes recoded physiologically; posture-based",
    "label": 0
  },
  {
    "text": "previously clustered in specialist milieus migrated into wider self-help, wellness and coaching cultures. Modern transpositions of yoga supplied somatic and attentional techniques that could be framed in secular or symbolic terms, sometimes retaining subtle-body vocabularies and sometimes recoded physiologically; posture-based systems and allied practices entered therapeutic and educational niches where they functioned as vehicles for self-regulation without requiring adherence to esoteric cosmologies. In counselling and workshop circuits, oracular media such as tarot, astrology and the I Ching were repurposed as projective tools within interpretive frames, foregrounding narrative construction and reflective dialogue rather than divinatory truth-claims. Parallel developments recoded esoteric diagrammatics (e.g., Kabbalah’s Tree of Life) as grammars of self-work for lay audiences, often distributed through publishing, courses and retreat formats. Sociological accounts describe these circulations through late-modern occulture and workshop economies that sit largely outside academic psychology but help explain the cultural vigency of hybrid vocabularies of “energy”, archetypes and transformation. Tradition interfaces in this period Within the post-1960s expansion of experiential psychologies, the Gestalt movement (Fritz and Laura Perls; Paul Goodman) became a principal model at Esalen and allied centres. Although not an esoteric tradition, historians note that its here-and-now expressiveness and holistic awareness coexisted with—and helped normalise—a milieu in which esoteric repertoires were reframed in psychological terms, characteristic of the broader human potential and transpersonal currents. Modern yoga supplied body–attention techniques translated into secular or symbolic-psychological idioms in wellness and para-clinical niches. Oracular media (tarot, astrology, I Ching) were redeployed as projective devices in symbolic counselling, while psychological receptions of astrology articulated a counselling idiom distinct from divinatory epistemologies. Contemporary treatments of (Hermetic) Kabbalah adapted diagrammatic grammars (e.g., the Tree of Life) to popular self-development, illustrating how esoteric repertoires circulate through publishing, courses and retreat cultures. Institutional and editorial hubs in this period Degree and certificate programmes under",
    "label": 0
  },
  {
    "text": "epistemologies. Contemporary treatments of (Hermetic) Kabbalah adapted diagrammatic grammars (e.g., the Tree of Life) to popular self-development, illustrating how esoteric repertoires circulate through publishing, courses and retreat cultures. Institutional and editorial hubs in this period Degree and certificate programmes under “contemplative” or “integrative” banners—such as those at Naropa University and the California Institute of Integral Studies—institutionalised translations of Asian and esoteric repertoires into educational and para-clinical formats alongside humanistic and transpersonal frameworks. Retreat centres and training networks scaled delivery through workshops and certifications, while publishing series oriented to spirituality–psychology and practical symbolism supported diffusion into counselling, education and coaching. Historians analyse these developments under late-modern occulture and market logics rather than as extensions of academic psychology proper. New religious movements and diffusion Scholars of new religious movements (NRMs) have noted that several conduits linking esoteric repertoires to popular psychological culture ran through movement contexts—e.g., Theosophy, Anthroposophy, Spiritism and, in late modernity, the heterogeneous networks often labelled “New Age” (a label related with the esoteric belief in an astrological Age of Aquarius, although not every idea or movement labeled as New Age actually holds this belief). Rather than a single denomination, the New Age milieu functioned as a loose market of workshops, retreats and publishing that recycled and recombined esoteric materials with psychologized self-cultivation and therapeutic language. Sociological models describe this circulation in terms of a “cultic milieu”, in which heterodox ideas—astrology, subtle-body schemes, trance techniques—persist and periodically reappear in new guises; Partridge’s notion of “occulture” further captures how such repertoires permeate popular culture beyond formal religion. In this environment, esoteric content was often translated into symbolic-psychological frames (e.g., projective counselling, human potential workshops) or packaged for wellness and coaching, while academic psychology largely followed separate methodological trajectories. Although the New Age as a self-identified movement declined as an organizational label",
    "label": 0
  },
  {
    "text": "content was often translated into symbolic-psychological frames (e.g., projective counselling, human potential workshops) or packaged for wellness and coaching, while academic psychology largely followed separate methodological trajectories. Although the New Age as a self-identified movement declined as an organizational label before the end of the 20th century, scholars argue that many of its psychologized practices and narratives dispersed into broader wellness, self-help, and 'spiritual but not religious' cultures where they continue to provide idioms for meaning-making and self-work. Schools and modalities in psychology, psychotherapy, and counselling Analytical psychology (Jung) Within analytical psychology, Carl Jung developed a sustained hermeneutics of premodern symbol-systems—most prominently alchemy—as resources for theorising psychic structure, transformation and individuation. Techniques such as active imagination, dream work and the use of mandalas exemplified a translation of esoteric repertoires into psychological practice under a symbolic/interpretive framing. Comparative engagements in Jungian circles, including dialogues around classical divination corpora (notably the I Ching), normalised the use of oracular media as projective prompts in counselling and self-exploration. In this milieu, the notion of synchronicity (“an acausal connecting principle”) offered an interpretive rationale for treating draws and charts as meaningful coincidences to be worked hermeneutically rather than as divinatory proofs. Subsequent developments associated with archetypal psychology started by James Hillman emphasised imaginal and mythopoetic registers for clinical reflection, extending the Jungian repertoire of symbols and narratives while maintaining an interpretive (non-realist) stance toward premodern materials. In the broader history of ideas, historians treat these currents as instances of the “psychologization of religion” and “sacralization of psychology” in the twentieth century. Psychosynthesis Founded by Roberto Assagioli, psychosynthesis articulated a developmental model integrating will, imagination and symbolic techniques (guided imagery, reflective exercises), framed for psychotherapy and education. While drawing selectively on religious and esoteric vocabularies (e.g., stages, higher/unifying self), psychosynthesis presented these as symbolic devices for",
    "label": 0
  },
  {
    "text": "Assagioli, psychosynthesis articulated a developmental model integrating will, imagination and symbolic techniques (guided imagery, reflective exercises), framed for psychotherapy and education. While drawing selectively on religious and esoteric vocabularies (e.g., stages, higher/unifying self), psychosynthesis presented these as symbolic devices for integration and meaning-making rather than as doctrinal commitments, and established dedicated training and counselling contexts distinct from Jungian institutions. In workshop and clinical formats, psychosynthesis employed structured sequences (evocation of imagery, reflective processing, value clarification) that align with humanistic and transpersonal milieus in which ritualised settings and symbolic repertoires were redeployed for experiential learning. Reviews at the psychology–religion interface underline the need to separate genealogical description from empirical appraisal and to keep realist–occult claims distinct from interpretive, “as-if real” framings. Somatic and bioenergetic psychology Wilhelm Reich’s somatic psychotherapy, developed as vegetotherapy and later orgonomy, sought to integrate emotional release with muscular and respiratory dynamics. Although Reich presented his model as biophysics rather than spirituality, his concept of orgone—a universal life energy said to permeate both psyche and cosmos—echoed older vitalist and mesmerist cosmologies. Scholars such as Christopher Partridge and Egil Asprem interpret Reich’s system as part of the modern afterlife of esoteric vitalism, where notions of subtle energy were reformulated within psychological and quasi-scientific vocabularies. In historiographical perspective, Wouter J. Hanegraaff classifies this type of “biophysical occultism” under the rubric of “rejected knowledge”: claims framed as science but conceptually aligned with esoteric models of energy and self-transformation. In the postwar period, “neo-Reichian” movements extended these ideas into expressive and group-based psychotherapies that combined breathing, grounding, and cathartic movement. Among them, Alexander Lowen’s bioenergetic analysis, and related body-oriented approaches within the human potential movement integrated performative and affective techniques while recasting them in psychodynamic and secular terms. These modalities demonstrate how vitalist and paraphysical conceptions of body–mind energy migrated from Western",
    "label": 0
  },
  {
    "text": "Alexander Lowen’s bioenergetic analysis, and related body-oriented approaches within the human potential movement integrated performative and affective techniques while recasting them in psychodynamic and secular terms. These modalities demonstrate how vitalist and paraphysical conceptions of body–mind energy migrated from Western esotericism into somatic and transpersonal psychology, bridging the genealogies of dynamic psychiatry and the modern culture of self-regulation. Transpersonal psychology Transpersonal psychology systematised mid-twentieth-century developments by theorising non-ordinary states and integrating selected esoteric vocabularies—subtle-body maps, archetypal narratives and ritualised techniques—into psychological models of development, crisis and integration. In practice and training milieus influenced by the Human Potential Movement and venues such as the Esalen Institute, experiential formats (e.g., guided imagery, breathwork, ritualised settings) supplied containers within which symbolic repertoires could be framed for self-exploration and clinical reflection. Within the field’s literature, commentators emphasise both genealogical debts to Western esotericism (e.g., Jungian hermeneutics, occultist codifications of subtle bodies) and the need for source-critical distinctions between symbolic/interpretive framings and realist–occult claims. Reviews at the psychology–religion interface also recommend keeping genealogical description analytically separate from empirical appraisal of outcomes in clinical or para-clinical contexts. In boundary zones adjacent to transpersonal studies, parapsychology continued the nineteenth-century psychical-research agenda in laboratory and survey settings (e.g., studies of telepathy or clairvoyance); in this article such intersections are treated as historical interfaces with psychology rather than as evidential endorsements. In this milieu, psychedelic-assisted psychotherapy and adjacent “psychonautics”—most prominently in the early work of Stanislav Grof (e.g., LSD psychotherapy and later holotropic breathwork)—were framed as laboratories for mapping non-ordinary states with archetypal and symbolic vocabularies. Such programmes illustrate the field’s dual emphasis on experiential method and interpretive framing, while reviews at the psychology–religion interface caution to separate genealogical links to esoteric repertoires from evidential claims and to distinguish clinical protocols from neoshamanic or ritual translations. Anthroposophic clinical and",
    "label": 0
  },
  {
    "text": "the field’s dual emphasis on experiential method and interpretive framing, while reviews at the psychology–religion interface caution to separate genealogical links to esoteric repertoires from evidential claims and to distinguish clinical protocols from neoshamanic or ritual translations. Anthroposophic clinical and pedagogical therapies In Anthroposophy, esoteric doctrines were extended into pedagogy and para-clinical practice, associating imagination and development with disciplined movement, colour and form. Under banners such as Eurythmy and arts-based therapeutic work, anthroposophic centres and clinics—organised around hubs like the Goetheanum—framed symbolic movement and creative exercises as instruments for well-being, rehabilitation or education, sometimes alongside subtle-body explanations and sometimes within secular pedagogical vocabularies. These programmes illustrate a pathway by which modern occultist repertoires entered educational and therapeutic niches: ritualised movement, diagrammatic grammars and guided imagination were re-coded as techniques for development and regulation in schoolrooms, clinics and workshops. Historians treat the linkage between doctrine, pedagogy and para-clinical arts as a key channel of diffusion from esoteric milieus to psychological language, while maintaining a clear analytic separation between provenance and empirical claims about efficacy or safety. Hypnosis and brief therapies From nineteenth-century mesmerism and guided somnambulism to medical hypnosis and later brief, strategic or indirect approaches, trance work provides a through-line connecting esoteric repertoires and psychological technique. What began as claims about invisible “fluids” and mediumistic channels was re-coded in medical and psychological vocabularies as suggestion, attentional focusing and communicative framing; staged demonstrations and clinical case series helped normalise descriptive languages for trance phenomena and dissociation. In contemporary practice, indirect and narrative methods (often associated with Ericksonian influence) treat images, metaphors and ritualised sequences as devices for re-patterning experience without realist–occult commitments. Neoshamanisms and related practices Late twentieth-century neoshamanic currents adapted drumming, “journeying” and small-scale ritual dramaturgy into workshop formats oriented toward self-knowledge, counselling or group learning. In many cases—e.g., the",
    "label": 0
  },
  {
    "text": "and ritualised sequences as devices for re-patterning experience without realist–occult commitments. Neoshamanisms and related practices Late twentieth-century neoshamanic currents adapted drumming, “journeying” and small-scale ritual dramaturgy into workshop formats oriented toward self-knowledge, counselling or group learning. In many cases—e.g., the approach popularised by Michael Harner—realist–occult claims were downplayed in favour of symbolic or psychagogic framings, yet the borrowing of esoteric vocabularies and techniques remained evident. These translations illustrate how ritual devices (rhythm, guided imagination, role-work, structured closure) can migrate into psychological idioms while cosmological truth-claims are bracketed. Communication-based approaches (NLP and related packages) Communication-centred trainings—often grouped under NLP and allied packages—combined elements from humanistic psychotherapy, Ericksonian communication and popular linguistics to offer pragmatic toolkits for persuasion, reframing and self-change. While not esoteric in doctrine, these programmes circulated in the same workshop economies as symbolic counselling and energy-themed practices, and at times repackaged esoteric maps in personal optimisation language (archetypes, “energy”, transformation). Scholarly treatments emphasise the distinction between genealogical proximity to esoteric repertoires and empirical evaluation of outcomes, urging clear separation between provenance and evidential claims. Parapsychology (boundary-field interface) Nineteenth-century psychical research evolved into twentieth-century parapsychology, which pursued experimental and survey studies of anomalous cognition (e.g., telepathy, clairvoyance) at the boundary of psychology proper. In this article parapsychology is treated as a historical interface with psychology—linked to case-genres on automatism and dissociation and intersecting selectively with transpersonal psychology debates on non-ordinary states—rather than as a psychological school; genealogical description is kept distinct from evidential claims. Popular psychology and coaching From the late twentieth century, repertoires associated with Western esotericism circulated widely through self-help and coaching markets. An American strand often labelled New Thought—later echoed in formulations such as the law of attraction—provided vocabularies of mental causation and affirmation that were readily repackaged as techniques for self-knowledge, motivation and optimisation. Historians read",
    "label": 0
  },
  {
    "text": "through self-help and coaching markets. An American strand often labelled New Thought—later echoed in formulations such as the law of attraction—provided vocabularies of mental causation and affirmation that were readily repackaged as techniques for self-knowledge, motivation and optimisation. Historians read these idioms within the broader late-modern dynamics of occulture and the cultic milieu, which situate diffusion through publishing, workshops, retreats and media rather than through academic psychology. Early twentieth-century popular psychology also popularised relaxation-through-attention methods. Writers such as Annie Payson Call taught guided relaxation and posture–breath discipline for “nerves” and self-regulation in non-clinical settings, combining moral reform with bodily awareness (see relaxation techniques, guided imagery). Her work, including Power Through Repose (1891), translated the harmonial and mind-cure milieux—shaped in part by Swedenborgianism, mesmerism and American metaphysical religion—into a psychophysical programme of self-culture. Historian Mark Singleton characterises her method (analytically) as “salvation through relaxation”, and argues that its emphasis on proprioceptive awareness and repose prefigured twentieth-century relaxation therapies and fed into Western receptions of “yoga-like” breath and relaxation practices. In this sense, Call exemplifies how harmonial religion prefigured the modern psychologization of esoteric ideas about inner harmony and divine influx. Historians also note that the “breath culture” repertoire systematized by William Walker Atkinson (as “Yogi Ramacharaka”) drew largely on Western hygienic and psychological currents subsequently reframed in Indic vocabulary; scholarship interprets this as a case of Western re-signification rather than direct transmission from South Asian sources. In parallel, the Mazdaznan movement propagated a “religion of breathing” that blended Theosophical, neo-Zoroastrian and popular psychophysiological motifs, illustrating early esoteric reinterpretations of respiratory discipline within the American harmonial field. Among late-1960s self-help and human potential offerings, José Silva’s Silva Method (originally “Silva Mind Control”) packaged relaxation/auto-hypnosis and guided imagery— including exercises with “inner guides” (imaginal counselors)—together with the era’s alpha-training discourse and New Thought-style",
    "label": 0
  },
  {
    "text": "within the American harmonial field. Among late-1960s self-help and human potential offerings, José Silva’s Silva Method (originally “Silva Mind Control”) packaged relaxation/auto-hypnosis and guided imagery— including exercises with “inner guides” (imaginal counselors)—together with the era’s alpha-training discourse and New Thought-style autosuggestion, while also invoking elements of ESP in a parapsychological key. Reference works place the program in the self-religion/auto-help milieu rather than clinical psychotherapy and note Silva’s autodidactic background (psychology, yoga, modern Rosicrucian doctrines) and the model’s international diffusion via seminars and books. A second conduit was personality typing. The Myers–Briggs Type Indicator (MBTI), inspired by Jungian typology, achieved wide uptake in education, management and pastoral guidance despite sustained methodological criticism concerning reliability, temporal stability of “types” and predictive validity; overviews recommend caution regarding its use for selection or diagnosis. By contrast, the modern Enneagram took shape in late twentieth-century esoteric–therapeutic milieus (e.g., Oscar Ichazo's system and later adaptations by Claudio Naranjo), before diffusing into counselling and coaching; academic surveys emphasise its recent genealogy, heterogeneous evidence base and cultural vigency rather than established efficacy. Communication-focused packages—commonly grouped under NLP—combined elements from humanistic psychotherapy, Ericksonian communication and popular linguistics to offer toolkits for persuasion, reframing and self-change. While not esoteric in doctrine, such programmes circulated in the same workshop economies as symbolic counselling and energy-themed practices, and at times repackaged esoteric maps (archetypes, “energy”, transformation) in optimisation language. Systematic reviews report insufficient evidence for robust effects and note pervasive methodological limitations. Educational and pastoral counselling contexts also served as diffusion channels: personality typologies (MBTI; the modern Enneagram) and symbolic counselling devices were adopted for guidance and self-exploration, often framed under values education or spiritual accompaniment. Within this translation, symbolic/interpretive uses were normalised over realist–occult claims. At the edges of popular psychology, ritualised or dramaturgic formats were marketed for personal change. The",
    "label": 0
  },
  {
    "text": "were adopted for guidance and self-exploration, often framed under values education or spiritual accompaniment. Within this translation, symbolic/interpretive uses were normalised over realist–occult claims. At the edges of popular psychology, ritualised or dramaturgic formats were marketed for personal change. The term “psychomagic” has been used for symbolic–ritual interventions in advice literature and workshops, with genealogical ties to esoteric repertoires and mythopoetic narratives; scholarly treatments read these as cultural phenomena within contemporary occulture rather than as established clinical modalities. Likewise, family constellations circulate in coaching and group-work settings with strong dramaturgic elements; reviews highlight mixed evidence and methodological concerns, advising caution and clear framing when used in para-clinical contexts. Some masculinity coaching and men’s-group ecologies drew selectively on Jungian archetypal vocabularies and, in some milieus, on neo-tantric, neidan, or polarity of “energy” (Sulphur or Sun-masculine/Mercury or Moon-femenine from alchemy, Shiva/Shakti, yin and yang) rhetorics, illustrating how symbolic repertoires are operationalised in advice genres outside formal clinical frameworks. Across these formats, the article maintains an analytic distinction between provenance (genealogical links to esoteric repertoires) and evidence (empirical appraisal of outcomes). Symbolic/interpretive framings—sometimes glossed as “as-if real”—are reported with attribution and kept distinct from realist–occult assertions; cultural visibility in wellness and coaching markets does not constitute clinical validation. Energy-labelled offerings (e.g., reiki-style sessions in coaching or as complementary add-ons) likewise illustrate how subtle-body vocabularies circulate in para-clinical niches; historians frame these under occulture/market diffusion rather than academic psychology. Esoteric-linked mental health hospitals In Brazil, Kardecist spiritism (a French origins branch of Spiritualism) developed durable interfaces with mental-health provision through Spiritist psychiatric hospitals that combine conventional psychiatric care with complementary Spiritist practices (e.g., “passes”, mediumistic counselling). Historical and clinical overviews document this model as emerging in the early–mid twentieth century and concentrated in Brazil; surveys describe operating structures, staffing and modalities of care across",
    "label": 0
  },
  {
    "text": "combine conventional psychiatric care with complementary Spiritist practices (e.g., “passes”, mediumistic counselling). Historical and clinical overviews document this model as emerging in the early–mid twentieth century and concentrated in Brazil; surveys describe operating structures, staffing and modalities of care across leading centres. In this article such institutions are treated as regional institutionalizations of an esoteric repertoire within modern healthcare systems; genealogical description is kept distinct from evidential claims about clinical outcomes. A different pathway of institutionalization appears in anthroposophic hospitals in Germany to Anthroposophy—where integrative frameworks include psychiatry/psychosomatics alongside arts-based and movement therapies (e.g., Eurythmy). Reviews of anthroposophic medicine situate these hospitals within an integrative multimodal system, while institutional profiles list dedicated departments (e.g., Gemeinschaftskrankenhaus Herdecke). As elsewhere in the article, institutional presence is reported separately from clinical validation, and interpretive/symbolic framings are distinguished from realist–occult claims. Comparative techniques and devices This section surveys devices that operate across otherwise distinct schools—symbolic tools, altered-state protocols, ritual/dramatic/artistic expression formats, body–attention practices, and sound-based procedures—and documents how they were translated into psychological frames (interpretive, non-realist) and adopted in clinical, para-clinical and coaching settings without necessitating doctrinal buy-in from their esoteric or religious sources. Provenance (channels of transmission) is kept analytically distinct from evidential claims, with outcomes often interpreted through common factors and meaning responses in structured settings. Symbol work Symbol-oriented work includes the redeployment of oracular media—e.g., tarot, astrology and the I Ching—as projective prompts in counselling and self-exploration. In Jungian and humanistic milieus, practice centres on image, narrative construction and reflective dialogue rather than on divinatory epistemologies; references to synchronicity provided an interpretive rationale for treating draws, charts and hexagrams as meaningful coincidences to be worked hermeneutically, not as predictive proofs. In educational and pastoral contexts, symbolic devices have similarly been used for guidance and values-clarification, further decoupling symbolic efficacy from realist–occult",
    "label": 0
  },
  {
    "text": "interpretive rationale for treating draws, charts and hexagrams as meaningful coincidences to be worked hermeneutically, not as predictive proofs. In educational and pastoral contexts, symbolic devices have similarly been used for guidance and values-clarification, further decoupling symbolic efficacy from realist–occult claims. Trance, breathing and psychedelics Although often framed in physiological or psychological terms, relaxation, guided imagery, and breath-culture systems developed within harmonial and New Thought milieus (see also Body and attention for later somatic adaptations) functioned as practical psychologizations (popular psychology) of esoteric ideas—linking inner discipline, divine influx, and mental causation. Scholars of Western esotericism interpret these as early instances of “psychologized esotericism” preceding formal schools of psychotherapy. Beyond explicitly mystical or trance-based techniques, late nineteenth-century harmonial and New Thought movements developed gentler forms of psychophysical practice—relaxation, guided imagery, and breath-culture systems—that aimed at balancing mind and body without inducing altered states. Decades later, in transpersonal milieus, there were developed altered state of consciousness techniques that range from clinical hypnosis and guided breathing to psychedelic-assisted sessions. Across otherwise divergent framings, practice typically combines induction (or pharmacological set/setting), focused attention and post-experience integration, with symbolic material (imagery, archetypal language, ritual scripts) recruited as aids to meaning-making and restructuring. Scholarship in transpersonal studies stresses separating genealogy from efficacy and distinguishing clinical protocols (screening, preparation, integration) from neoshamanic or workshop translations. A related practice are late–twentieth-century “active/dynamic meditations” that blend breathwork, rhythmic movement and brief catharsis with Eastern symbolic vocabularies (e.g. \"kundalini activation\" with neotantric or yogic language); in practice they circulated through humanistic and transpersonal workshop markets as psychologized self-regulation techniques. Arts-based psychotherapies and counselling Ritual movement and dramatization Ritualised and dramatic formats—ranging from structured openings and closures to role-work, gesture, and symbolic sequencing—translate esoteric dramaturgies into educational, humanistic, and para-clinical settings oriented toward self-insight and emotional regulation. In workshop and counselling",
    "label": 0
  },
  {
    "text": "psychotherapies and counselling Ritual movement and dramatization Ritualised and dramatic formats—ranging from structured openings and closures to role-work, gesture, and symbolic sequencing—translate esoteric dramaturgies into educational, humanistic, and para-clinical settings oriented toward self-insight and emotional regulation. In workshop and counselling circuits influenced by humanistic and transpersonal psychology, the emphasis falls on containment, process, and symbolic rehearsal rather than on doctrinal belief, illustrating how ritual and theatrical devices were reframed as psychological techniques. Early twentieth-century reformers such as George Gurdjieff and Rudolf Steiner developed choreographed repertoires—Gurdjieff’s \"Movements\" and Steiner’s Eurythmy—that integrated posture, rhythm, and spatial design to enact metaphysical correspondences and cultivate heightened awareness. These practices functioned as embodied exercises of esoteric self-transformation, combining ritual, theatre, and psychophysical discipline. Later adaptations recontextualised their techniques for pedagogical and therapeutic use under symbolic (non-realist) descriptions, creating points of translation between spiritual choreography and psychological process work. From the mid-twentieth century onward, a constellation of expressive and creative psychotherapies—such as psychodrama, dance/movement therapy, and related experiential group practices—expanded this performative lineage. Drawing variously on Jungian symbolism, Reichian somatic therapy, and the holistic philosophy of humanistic and transpersonal psychology, their founders framed movement, voice, and enactment as means of catharsis, integration, and energetic balance. Although later professionalised in clinical and educational contexts, historians of psychology trace their conceptual origins to the spiritual humanism and vitalist cosmologies that characterised the broader Western esoteric revival. Within these hybrid workshop cultures, ritualised movement and dramatic role-play were interpreted through the languages of affect regulation, meaning-making, and embodied insight—continuing the psychologisation of esoteric repertoires within the broader human potential and transpersonal milieu. In parallel, other twentieth-century movements systematised body-based psychotherapies into formal schools, bridging somatic, energetic, and expressive paradigms. Visual and imaginal practices (colour/diagram work) Beyond formal art therapies, workshop and counselling milieus influenced by Jungian and esoteric repertoires",
    "label": 0
  },
  {
    "text": "and transpersonal milieu. In parallel, other twentieth-century movements systematised body-based psychotherapies into formal schools, bridging somatic, energetic, and expressive paradigms. Visual and imaginal practices (colour/diagram work) Beyond formal art therapies, workshop and counselling milieus influenced by Jungian and esoteric repertoires have redeployed imaginal and diagrammatic devices—e.g., drawing or colouring mandalas; colour sequences mapped to subtle-body grammars; reflective composition around emblematic images—as structured prompts for narrative, value-clarification and self-regulation. In these uses, premodern diagrams and colour codings function as symbolic scaffolds rather than doctrinal claims, illustrating how arts-based exercises circulate across clinical, para-clinical and coaching settings under interpretive (non-realist) frames. Sound-based practices Sound-focused sessions—e.g., singing bowls and gong baths—are often framed through Pythagorean and (neo)hermetic (The Kybalion) narratives of hidden vibration and cosmic harmony (a Western esoteric lineage) while being presented with Asian aesthetics or branding (“Tibetan bowls”) in late-modern wellness markets. A parallel, secular rationale describes these formats as aids to relaxation, attentional focus and group containment. In either case, practice emphasises patterned sound, pacing and bounded ritual to scaffold meaning-making and regulation, with symbolic/interpretive efficacy foregrounded over realist claims. Body and attention Somatic and attentional practices—postures, movement sequences and contemplative exercises—were adopted in modern transpositions of yoga, Buddhist sati, and related repertoires, sometimes retaining Theosophical subtle body vocabularies and sometimes reframed in secular or physiological terms. Historical studies trace how posture-based systems and allied techniques entered wellness and therapeutic niches (e.g. relaxation techniques), where they could function as vehicles for self-regulation without adherence to esoteric or religious cosmologies. Earlier Western harmonial and breath-culture traditions (see Popular psychology and coaching) already framed breathing and relaxation as psychophysical techniques linking mind–body regulation and esoteric ideas of vital influx. Energy-themed practices (subtle-body framings) Under “energy” labels (e.g., reiki, “healing touch”, pranic variants), late-modern counselling and coaching milieus sometimes redeploy subtle-body vocabularies from",
    "label": 0
  },
  {
    "text": "already framed breathing and relaxation as psychophysical techniques linking mind–body regulation and esoteric ideas of vital influx. Energy-themed practices (subtle-body framings) Under “energy” labels (e.g., reiki, “healing touch”, pranic variants), late-modern counselling and coaching milieus sometimes redeploy subtle-body vocabularies from Western esoteric and hybrid Asian-Western repertoires. In such settings, techniques are framed interpretively—as symbolic scaffolds for attention, meaning-making and regulation—or, in some complementary/alternative medicine contexts, as putative causal interventions. Scholarly accounts treat these as boundary phenomena of diffusion (occulture, workshop economies) and urge analytic separation between provenance (subtle-body genealogies) and evidential claims about outcomes. In practice these offerings often co-occur with body–attention formats (e.g., yoga-based sessions); here they are treated separately because “energy” labels involve subtle-body vocabularies and, in some contexts, putative causal claims that go beyond symbolic/physiological framings. Past-life regressions and “Akashic” narrative work (boundary uses) Some counselling and coaching niches redeploy theosophical vocabularies—e.g., Akashic records channeling or use past life regressions and related narratives as frames for meaning-making and personal storytelling. In such settings, practitioners may bracket cosmological truth-claims and present sessions as symbolic prompts; in others, realist assertions are made. Historians treat these as cases of late-modern diffusion (occulture) of esoteric repertoires into self-work, while transpersonal commentators emphasise keeping genealogical description distinct from evidential claims and separating workshop formats from clinical protocols. Cross-cutting uptake in clinical and coaching settings Across these devices, adoption in psychotherapy, counselling and coaching often proceeds under interpretive (symbolic) framings that bracket cosmological truth-claims. Reported benefits—when present—are frequently interpreted in terms of common factors (working alliance, expectancy, meaning response) and the structuring of attention and emotion by bounded settings (ritual sequence, symbolic media, group containment). Cultural visibility in workshop and wellness markets is documented by sociological accounts of late-modern occulture and the cultic milieu, but such visibility is kept analytically distinct from clinical",
    "label": 0
  },
  {
    "text": "attention and emotion by bounded settings (ritual sequence, symbolic media, group containment). Cultural visibility in workshop and wellness markets is documented by sociological accounts of late-modern occulture and the cultic milieu, but such visibility is kept analytically distinct from clinical validation. Evidence, risks, and debates This article separates questions of provenance from questions of efficacy. Genealogical links to esoteric repertoires do not in themselves predict outcomes; conversely, cultural visibility in wellness or coaching markets does not constitute clinical validation. Reviews at the psychology–religion interface therefore recommend keeping source-tracing distinct from empirical appraisal and maintaining clarity between symbolic/interpretive framings and realist–occult assertions. Reported benefits—when present—are frequently interpreted in terms of common factors and meaning responses in structured settings (working alliance, expectancy, ritual sequence, symbolic media, group containment). In personality typing, the Myers–Briggs Type Indicator (MBTI)—inspired by Jungian typology—has seen widespread uptake in education and management while attracting sustained criticism concerning reliability, temporal stability of “types” and predictive validity; overviews recommend caution regarding its use for selection or diagnosis. By contrast, the modern Enneagram emerged from late twentieth-century esoteric–therapeutic milieus (e.g., Óscar Ichazo; later adaptations by Claudio Naranjo) before diffusing into counselling and coaching; academic surveys emphasise its recent provenance, heterogeneous evidence base and cultural vigency rather than established efficacy. Communication-focused packages commonly grouped under NLP circulate in coaching and health-adjacent contexts; systematic reviews have reported insufficient evidence for robust effects and pervasive methodological limitations. Energy-labelled offerings (reiki-style sessions, “healing touch”, pranic variants) sit at the boundary of the field: critics question proposed subtle-energy mechanisms and call for higher-quality trials, while proponents and detractors dispute both mechanisms and effect sizes; the article therefore reports such practices as diffusion phenomena (occulture/workshop economies) and urges explicit separation between complementary uses and clinical claims. Boundary-field literatures such as parapsychology—evolving from nineteenth-century psychical research into laboratory",
    "label": 0
  },
  {
    "text": "and detractors dispute both mechanisms and effect sizes; the article therefore reports such practices as diffusion phenomena (occulture/workshop economies) and urges explicit separation between complementary uses and clinical claims. Boundary-field literatures such as parapsychology—evolving from nineteenth-century psychical research into laboratory and survey programmes on anomalous cognition (e.g., telepathy, clairvoyance)—are treated here as historical interfaces with psychology rather than as psychological schools; claims are reported with attribution and kept analytically distinct from genealogical description and from clinical efficacy appraisals. Likewise, narrative formats such as past-life or “Akashic” consultations are presented as boundary phenomena of diffusion; symbolic/interpretive framings are distinguished from realist claims. Safety and scope-of-practice concerns are most salient where non-ordinary states or strong catharsis are involved. Scholarly and clinical discussions converge on screening for vulnerability; clear preparation and integration phases; monitoring for dissociation or destabilisation; explicit consent about symbolic versus doctrinal framings; and robust referral pathways when indicated. In symbolic counselling and oracular work, ethical guidance stresses non-directive framing and avoidance of deterministic claims. More generally, hybrid settings benefit from transparent consent about interpretive status, clear separation between educational/experiential workshops and clinical treatment, and caution against conflating cultural vigency with evidential support. See also Western esotericism and science Western esotericism and the arts Analytical psychology Transpersonal psychology Cognitive science of religion Neuroscience of religion Scholarly approaches to mysticism References Bibliography Faivre, Antoine; Needleman, Jacob, eds. (1992). Modern Esoteric Spirituality. World Spirituality: An Encyclopedic History of the Religious Quest, 21. New York: Crossroad. ISBN 978-0-8245-1145-6. Flournoy, Théodore (1900). From India to the Planet Mars: A Study of a Case of Somnambulism with Glossolalia. New York: Harper & Brothers. Hanegraaff, Wouter J. (1998). New Age Religion and Western Culture: Esotericism in the Mirror of Secular Thought. Leiden: Brill. ISBN 978-90-04-11035-9. Hanegraaff, Wouter J. (2012). Esotericism and the Academy: Rejected Knowledge in Western",
    "label": 0
  },
  {
    "text": "with Glossolalia. New York: Harper & Brothers. Hanegraaff, Wouter J. (1998). New Age Religion and Western Culture: Esotericism in the Mirror of Secular Thought. Leiden: Brill. ISBN 978-90-04-11035-9. Hanegraaff, Wouter J. (2012). Esotericism and the Academy: Rejected Knowledge in Western Culture. Cambridge: Cambridge University Press. ISBN 978-1-107-02112-9. Leadbeater, C. W. (1927). The Chakras (PDF). Adyar: Theosophical Publishing House. Partridge, Christopher (2004). The Re-Enchantment of the West, Vol. 1: Alternative Spiritualities, Sacralization, Popular Culture and Occulture. London: T&T Clark. ISBN 978-0-567-08408-8. Campbell, Colin (1972). \"The Cult, the Cultic Milieu and Secularization\". A Sociological Yearbook of Religion in Britain. 5: 119–136. Rush, M. J. (2016). \"The Contribution of Western Esotericism to Transpersonal Psychology\". Transpersonal Psychology Review. 18 (1): 32–43. doi:10.53841/bpstran.2016.18.1.32. Singleton, Mark (2010). Yoga Body: The Origins of Modern Posture Practice. Oxford: Oxford University Press. ISBN 978-0-19-539535-8. Pittenger, David J. (1993). \"Measuring the MBTI and Coming Up Short\". Review of Educational Research. 63 (4): 467–488. doi:10.3102/00346543063004467. Moreira-Almeida, Alexander (2005). \"History of 'Spiritist madness' in Brazil (1900–1950)\". History of Psychiatry. 16 (3): 5–25. doi:10.1177/0957154X05044602. PMID 15981363. Lucchetti, Giancarlo; Lucchetti, Alessandra G.; Bassi, Roberta M.; Nasri, Fatima; Neto, José T. (2012). \"Spiritist psychiatric hospitals in Brazil: integration of conventional psychiatric treatment and spiritual complementary therapy\". Journal of Religion and Health. 51 (3): 931–944. doi:10.1007/s11013-011-9239-6. PMID 22052248. Kienle, G. S. (2013). \"Anthroposophic Medicine: An Integrative Medical System Originating in Europe\". Global Advances in Health and Medicine. 2 (4): 20–31. doi:10.7453/gahmj.2012.087. PMC 3865373. PMID 24416705. Sturt, Jackie; Ali, Saima; Robertson, Wendy; Metcalfe, David; Grove, Amy; Bourne, Claire; Bridle, Chris (2012). \"Neurolinguistic programming: a systematic review of the effects on health outcomes\". British Journal of General Practice. 62 (604): e757 – e764. doi:10.3399/bjgp12X658287. PMC 3481516. PMID 23211179. Alexander, Morgan; Schnipke, Brent (2020). \"The Enneagram: A Primer for Psychiatry Residents\". American Journal of Psychiatry Residents' Journal. 15 (3):",
    "label": 0
  },
  {
    "text": "the effects on health outcomes\". British Journal of General Practice. 62 (604): e757 – e764. doi:10.3399/bjgp12X658287. PMC 3481516. PMID 23211179. Alexander, Morgan; Schnipke, Brent (2020). \"The Enneagram: A Primer for Psychiatry Residents\". American Journal of Psychiatry Residents' Journal. 15 (3): 2–5. doi:10.1176/appi.ajp-rj.2020.150301. Lederer, Peter Scott (2023). \"H. P. Blavatsky and Alejandro Jodorowsky: The Influence of the Russian Orthodox Church and Theosophy on Psychomagic in El Topo and The Holy Mountain\". In Witte, Michael (ed.). ReFocus: The Films of Alejandro Jodorowsky. Edinburgh: Edinburgh University Press. pp. 73–91. doi:10.1515/9781399505963-008. ISBN 978-1-3995-0596-3. Konkolÿ Thege, Barna; Petroll, Carla; Rivas, Carlos; Scholtens, Salome (2021). \"The Effectiveness of Family Constellation Therapy in Improving Mental Health: A Systematic Review\". Family Process. 60 (2): 409–423. doi:10.1111/famp.12636. PMID 33528854. Wegner, Daniel M. (2017). The Illusion of Conscious Will. Cambridge, MA: MIT Press. ISBN 978-0-262-53492-5. Sommer, Andreas (2012). \"Psychical research and the origins of American psychology: Hugo Münsterberg, William James and Eusapia Palladino\". History of the Human Sciences. 25 (2): 23–44. doi:10.1177/0952695112439376. PMC 3552602. PMID 23355763. Ellenberger, Henri F. (1970). The Discovery of the Unconscious: The History and Evolution of Dynamic Psychiatry. New York: Basic Books. ISBN 978-0-465-01694-5. Plaisance, Christopher A. (2015). \"Israel Regardie and the Psychologization of Esoteric Discourse\". Correspondences: Journal for the Study of Esotericism. 3: 5–54. Albanese, Catherine L. (2007). A Republic of Mind and Spirit: A Cultural History of American Metaphysical Religion. New Haven: Yale University Press. ISBN 978-0-300-11089-0. Foxen, Anya P. (2020). Inhaling Spirit: Harmonialism, Orientalism, and the Western Imagination of Yoga. Oxford: Oxford University Press. ISBN 978-0-19-008220-8. Singleton, Mark (2005). \"Salvation through Relaxation: Proprioceptive Therapy and its Relationship to Yoga\". Journal of Contemporary Religion. 20 (3): 289–304. doi:10.1080/13537900500249780.",
    "label": 0
  },
  {
    "text": "The scientific community has been investigating the causes of current climate change for decades. After thousands of studies, the scientific consensus is that it is \"unequivocal that human influence has warmed the atmosphere, ocean and land since pre-industrial times.\" This consensus is supported by around 200 scientific organizations worldwide. The scientific principle underlying current climate change is the greenhouse effect, which provides that greenhouse gases pass sunlight that heats the earth, but trap some of the resulting heat that radiates from the planet's surface. Large amounts of greenhouse gases such as carbon dioxide and methane have been released into the atmosphere through burning of fossil fuels since the industrial revolution. Indirect emissions from land use change, emissions of other greenhouse gases such as nitrous oxide, and increased concentrations of water vapor in the atmosphere, also contribute to climate change. The warming from the greenhouse effect has a logarithmic relationship with the concentration of greenhouse gases. This means that every additional fraction of CO2 and the other greenhouse gases in the atmosphere has a slightly smaller warming effect than the fractions before it as the total concentration increases. However, only around half of CO2 emissions continually reside in the atmosphere in the first place, as the other half is quickly absorbed by carbon sinks in the land and oceans. Further, the warming per unit of greenhouse gases is also affected by feedbacks, such as the changes in water vapor concentrations or Earth's albedo (reflectivity). As the warming from CO2 increases, carbon sinks absorb a smaller fraction of total emissions, while the \"fast\" climate change feedbacks amplify greenhouse gas warming. Thus, the effects counteract one another, and the warming from each unit of CO2 emitted by humans increases temperature in linear proportion to the total amount of emissions. Further, some fraction of",
    "label": 0
  },
  {
    "text": "\"fast\" climate change feedbacks amplify greenhouse gas warming. Thus, the effects counteract one another, and the warming from each unit of CO2 emitted by humans increases temperature in linear proportion to the total amount of emissions. Further, some fraction of the greenhouse warming has been \"masked\" by the human-caused emissions of sulfur dioxide, which forms aerosols that have a cooling effect. However, this masking has been receding in the recent years, due to measures to combat acid rain and air pollution caused by sulfates. Factors affecting Earth's climate A forcing is something that is imposed externally on the climate system. External forcings include natural phenomena such as volcanic eruptions and variations in the sun's output. Human activities can also impose forcings, for example, through changing the composition of Earth's atmosphere. Radiative forcing is a measure of how various factors alter the energy balance of planet Earth. A positive radiative forcing will lead towards a warming of the surface and, over time, the climate system. Between the start of the Industrial Revolution in 1750, and the year 2005, the increase in the atmospheric concentration of carbon dioxide (chemical formula: CO2) led to a positive radiative forcing, averaged over the Earth's surface area, of about 1.66 watts per square metre (abbreviated W m−2). Climate feedbacks can either amplify or dampen the response of the climate to a given forcing. There are many feedback mechanisms in the climate system that can either amplify (a positive feedback) or diminish (a negative feedback) the effects of a change in climate forcing. The climate system varies in response to changes in external forcings. The climate system also has internal variability both in the presence and absence of external forcings. This internal variability is a result of complex interactions between components within the climate system, such as",
    "label": 0
  },
  {
    "text": "in response to changes in external forcings. The climate system also has internal variability both in the presence and absence of external forcings. This internal variability is a result of complex interactions between components within the climate system, such as the coupling between the atmosphere and ocean. An example of internal variability is the El Niño–Southern Oscillation. Human-caused influences Factors affecting Earth's climate can be broken down into forcings, feedbacks and internal variations. Four main lines of evidence support the dominant role of human activities in recent climate change: A physical understanding of the climate system: greenhouse gas concentrations have increased and their warming properties are well-established. There are historical estimates of past climate changes suggest that the recent changes in global surface temperature are unusual. Advanced climate models are unable to replicate the observed warming unless human greenhouse gas emissions are included. Observations of natural forces, such as solar and volcanic activity, show that solar activity cannot explain the observed warming. For example, an increase in solar activity would have warmed the entire atmosphere, yet only the lower atmosphere has warmed. Observations from space show that Earth's energy imbalance—a measure of how much more energy Earth absorbs than it radiates into space—reached values in 2023 that were twice that of the best estimate from the IPCC. Greenhouse gases Greenhouse gases are transparent to sunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time. While water vapour and clouds are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature. Therefore, they are considered",
    "label": 0
  },
  {
    "text": "heat escapes into space, trapping heat near the Earth's surface and warming it over time. While water vapour and clouds are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature. Therefore, they are considered to be feedbacks that change climate sensitivity. On the other hand, gases such as CO2, tropospheric ozone, CFCs and nitrous oxide are added or removed independently from temperature. Hence, they are considered to be external forcings that change global temperatures. Human activity since the Industrial Revolution (about 1750), mainly extracting and burning fossil fuels (coal, oil, and natural gas), has increased the amount of greenhouse gases in the atmosphere, resulting in a radiative imbalance. Over the past 150 years human activities have released increasing quantities of greenhouse gases into the atmosphere. By 2019, the concentrations of CO2 and methane had increased by about 48% and 160%, respectively, since 1750. These CO2 levels are higher than they have been at any time during the last 2 million years. Concentrations of methane are far higher than they were over the last 800,000 years. This has led to increases in mean global temperature, or global warming. The likely range of human-induced surface-level air warming by 2010–2019 compared to levels in 1850–1900 is 0.8 °C to 1.3 °C, with a best estimate of 1.07 °C. This is close to the observed overall warming during that time of 0.9 °C to 1.2 °C. Temperature changes during that time were likely only ±0.1 °C due to natural forcings and ±0.2 °C due to variability in the climate. Global anthropogenic greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases. Carbon dioxide CO2 emissions primarily come",
    "label": 0
  },
  {
    "text": "climate. Global anthropogenic greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases. Carbon dioxide CO2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity. Additional CO2 emissions come from deforestation and industrial processes, which include the CO2 released by the chemical reactions for making cement, steel, aluminum, and fertiliser. CO2 is absorbed and emitted naturally as part of the carbon cycle, through animal and plant respiration, volcanic eruptions, and ocean-atmosphere exchange. Human activities, such as the burning of fossil fuels and changes in land use (see below), release large amounts of carbon to the atmosphere, causing CO2 concentrations in the atmosphere to rise. The high-accuracy measurements of atmospheric CO2 concentration, initiated by Charles David Keeling in 1958, constitute the master time series documenting the changing composition of the atmosphere. These data, known as the Keeling Curve, have iconic status in climate change science as evidence of the effect of human activities on the chemical composition of the global atmosphere. Keeling's initial 1958 measurements showed 313 parts per million by volume (ppm). Atmospheric CO2 concentrations, commonly written \"ppm\", are measured in parts-per-million by volume (ppmv). In May 2019, the concentration of CO2 in the atmosphere reached 415 ppm. The last time when it reached this level was 2.6–5.3 million years ago. Without human intervention, it would be 280 ppm. In 2022–2024, the concentration of CO2 in the atmosphere increased faster than ever before according to National Oceanic and Atmospheric Administration, as a result of sustained emissions and El Niño conditions. In November, 2025 Global Carbon Budget predicted CO2 emissions from burning coal, oil and gas would be a record 38.1 billion tonnes",
    "label": 0
  },
  {
    "text": "before according to National Oceanic and Atmospheric Administration, as a result of sustained emissions and El Niño conditions. In November, 2025 Global Carbon Budget predicted CO2 emissions from burning coal, oil and gas would be a record 38.1 billion tonnes in 2025, up 1.1 percent from the prior year. Methane and nitrous oxide Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, and coal mining, as well as oil and gas extraction. Nitrous oxide emissions largely come from the microbial decomposition of fertiliser. Methane and to a lesser extent nitrous oxide are also major forcing contributors to the greenhouse effect. The Kyoto Protocol lists these together with hydrofluorocarbon (HFCs), perfluorocarbons (PFCs), and sulfur hexafluoride (SF6), which are entirely artificial gases, as contributors to radiative forcing. The chart at right attributes anthropogenic greenhouse gas emissions to eight main economic sectors, of which the largest contributors are power stations (many of which burn coal or other fossil fuels), industrial processes, transportation fuels (generally fossil fuels), and agricultural by-products (mainly methane from enteric fermentation and nitrous oxide from fertilizer use). Aerosols Air pollution, in the form of aerosols, affects the climate on a large scale. Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming, and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel. Smaller contributions come from black carbon, organic carbon from combustion of fossil fuels and biofuels, and from anthropogenic dust. Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much. Aerosols also have indirect effects on the Earth's energy budget. Sulfate",
    "label": 0
  },
  {
    "text": "fuels and biofuels, and from anthropogenic dust. Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much. Aerosols also have indirect effects on the Earth's energy budget. Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. They also reduce the growth of raindrops, which makes clouds more reflective to incoming sunlight. Indirect effects of aerosols are the largest uncertainty in radiative forcing. While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise. Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050. Land surface changes According to Food and Agriculture Organization, around 30% of Earth's land area is largely unusable for humans (glaciers, deserts, etc.), 26% is forests, 10% is shrubland and 34% is agricultural land. Deforestation is the main land use change contributor to global warming, Between 1750 and 2007, about one-third of anthropogenic CO2 emissions were from changes in land use - primarily from the decline in forest area and the growth in agricultural land. primarily deforestation. as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink. Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%. Some",
    "label": 0
  },
  {
    "text": "enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%. Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink. Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns. In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler. At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains. Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect. Livestock-associated emissions More than 18% of anthropogenic greenhouse gas emissions are attributed to livestock and livestock-related activities such as deforestation and increasingly fuel-intensive farming practices. Specific attributions to the livestock sector include: 9% of global anthropogenic carbon dioxide emissions 35–40% of global anthropogenic methane emissions (chiefly due to enteric fermentation and manure) 64% of global anthropogenic nitrous oxide emissions, chiefly due to fertilizer use. Methods for attribution \"Fingerprint\" studies To determine the human contribution to climate change, unique \"fingerprints\" for all potential causes are developed and compared with both observed patterns and known internal climate variability. For example, solar forcing—whose fingerprint involves",
    "label": 0
  },
  {
    "text": "fertilizer use. Methods for attribution \"Fingerprint\" studies To determine the human contribution to climate change, unique \"fingerprints\" for all potential causes are developed and compared with both observed patterns and known internal climate variability. For example, solar forcing—whose fingerprint involves warming the entire atmosphere—is ruled out because only the lower atmosphere has warmed. Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo, are less impactful. Fingerprint studies exploit these unique signatures, and allow detailed comparisons of modelled and observed climate change patterns. Scientists rely on such studies to attribute observed changes in climate to a particular cause or set of causes. In the real world, the climate changes that have occurred since the start of the Industrial Revolution are due to a complex mixture of human and natural causes. The importance of each individual influence in this mixture changes over time. Therefore, climate models are used to study how individual factors affect climate. For example, a single factor (like greenhouse gases) or a set of factors can be varied, and the response of the modelled climate system to these individual or combined changes can thus be studied. These projections have been confirmed by observations (shown above). For example, when climate model simulations of the last century include all of the major influences on climate, both human-induced and natural, they can reproduce many important features of observed climate change patterns. When human influences are removed from the model experiments, results suggest that the surface of the Earth would actually have cooled slightly over the last 50 years. The clear message from fingerprint studies is that the observed warming over the last half-century cannot be explained by natural factors, and is instead caused primarily by human factors. Atmospheric fingerprints Another fingerprint of human effects on climate has",
    "label": 0
  },
  {
    "text": "years. The clear message from fingerprint studies is that the observed warming over the last half-century cannot be explained by natural factors, and is instead caused primarily by human factors. Atmospheric fingerprints Another fingerprint of human effects on climate has been identified by looking at a slice through the layers of the atmosphere, and studying the pattern of temperature changes from the surface up through the stratosphere (see the section on solar activity). The earliest fingerprint work focused on changes in surface and atmospheric temperature. Scientists then applied fingerprint methods to a whole range of climate variables, identifying human-caused climate signals in the heat content of the oceans, the height of the tropopause (the boundary between the troposphere and stratosphere, which has shifted upward by hundreds of feet in recent decades), the geographical patterns of precipitation, drought, surface pressure, and the runoff from major river basins. Studies published after the appearance of the IPCC Fourth Assessment Report in 2007 have also found human fingerprints in the increased levels of atmospheric moisture (both close to the surface and over the full extent of the atmosphere), in the decline of Arctic sea ice extent, and in the patterns of changes in Arctic and Antarctic surface temperatures. Ripple effects Carbon sinks The Earth's surface absorbs CO2 as part of the carbon cycle. Despite the contribution of deforestation to greenhouse gas emissions, the Earth's land surface, particularly its forests, remain a significant carbon sink for CO2. Land-surface sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions. The ocean also serves as a significant carbon sink via a two-step process. First, CO2 dissolves in the surface water. Afterwards, the ocean's overturning circulation distributes it deep into the ocean's interior, where it accumulates over time as",
    "label": 0
  },
  {
    "text": "CO2 emissions. The ocean also serves as a significant carbon sink via a two-step process. First, CO2 dissolves in the surface water. Afterwards, the ocean's overturning circulation distributes it deep into the ocean's interior, where it accumulates over time as part of the carbon cycle. Over the last two decades, the world's oceans have absorbed 20 to 30% of emitted CO2. Thus, around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans. This fraction of absorbed emissions is not static. If future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%. This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer. The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution. Climate change feedbacks The response of the climate system to an initial forcing is modified by feedbacks: increased by \"self-reinforcing\" or \"positive\" feedbacks and reduced by \"balancing\" or \"negative\" feedbacks. The main reinforcing feedbacks are the water-vapour feedback, the ice–albedo feedback, and the net effect of clouds. The primary balancing mechanism is radiative cooling, as Earth's surface gives off more heat to space in response to rising temperature. In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO2 on plant growth. Uncertainty over feedbacks, particularly cloud cover, is the major reason why different climate models project different magnitudes of warming for a given amount of emissions. As air warms, it can hold more moisture.",
    "label": 0
  },
  {
    "text": "fertilizing effect of CO2 on plant growth. Uncertainty over feedbacks, particularly cloud cover, is the major reason why different climate models project different magnitudes of warming for a given amount of emissions. As air warms, it can hold more moisture. Water vapour, as a potent greenhouse gas, holds heat in the atmosphere. If cloud cover increases, more sunlight will be reflected back into space, cooling the planet. If clouds become higher and thinner, they act as an insulator, reflecting heat from below back downwards and warming the planet. Another major feedback is the reduction of snow cover and sea ice in the Arctic, which reduces the reflectivity of the Earth's surface. More of the Sun's energy is now absorbed in these regions, contributing to amplification of Arctic temperature changes. Arctic amplification is also thawing permafrost, which releases methane and CO2 into the atmosphere. Climate change can also cause methane releases from wetlands, marine systems, and freshwater systems. Overall, climate feedbacks are expected to become increasingly positive. Natural variability Already in 2001, the IPCC Third Assessment Report had found that, \"The combined change in radiative forcing of the two major natural factors (solar variation and volcanic aerosols) is estimated to be negative for the past two, and possibly the past four, decades.\" Solar irradiance has been measured directly by satellites, and indirect measurements are available from the early 1600s onwards. Yet, since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere). Similarly, volcanic activity has the single largest natural impact (forcing) on temperature, yet it is equivalent to less than 1% of current human-caused CO2 emissions. Volcanic activity as a whole has had negligible impacts on global temperature trends since the Industrial",
    "label": 0
  },
  {
    "text": "volcanic activity has the single largest natural impact (forcing) on temperature, yet it is equivalent to less than 1% of current human-caused CO2 emissions. Volcanic activity as a whole has had negligible impacts on global temperature trends since the Industrial Revolution. Between 1750 and 2007, solar radiation may have at most increased by 0.12 W/m2, compared to 1.6 W/m2 for the net anthropogenic forcing. Consequently, the observed rapid rise in global mean temperatures seen after 1985 cannot be ascribed to solar variability.\" Further, the upper atmosphere (the stratosphere) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling. This is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere. Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapor into the atmosphere, which adds to greenhouse gases and increases temperatures. Because both water vapor and volcanic material have low persistence in the atmosphere, even the largest eruptions only have an effect for several years. See also Climate change adaptation Climate change denial Climate change mitigation Climate resilience References Sources Albrecht, Bruce A. (1989). \"Aerosols, Cloud Microphysics, and Fractional Cloudiness\". Science. 245 (4923): 1227–1239. Bibcode:1989Sci...245.1227A. doi:10.1126/science.245.4923.1227. PMID 17747885. S2CID 46152332. Davidson, Eric (2009). \"The contribution of manure and fertilizer nitrogen to atmospheric nitrous oxide since 1860\". Nature Geoscience. 2: 659–662. doi:10.1016/j.chemer.2016.04.002. Dean, Joshua F.; Middelburg, Jack J.; Röckmann, Thomas; Aerts, Rien; et al. (2018). \"Methane Feedbacks to the Global Climate System in a Warmer World\". Reviews of Geophysics. 56 (1): 207–250. Bibcode:2018RvGeo..56..207D. doi:10.1002/2017RG000559. hdl:2066/195183. ISSN 1944-9208. US EPA (13 September 2019). \"Global Greenhouse Gas Emissions Data\". Archived from the original on 18 February 2020. Retrieved 8 August 2020. US EPA (15 September 2020). \"Overview of Greenhouse Gases\". Retrieved",
    "label": 0
  },
  {
    "text": "56 (1): 207–250. Bibcode:2018RvGeo..56..207D. doi:10.1002/2017RG000559. hdl:2066/195183. ISSN 1944-9208. US EPA (13 September 2019). \"Global Greenhouse Gas Emissions Data\". Archived from the original on 18 February 2020. Retrieved 8 August 2020. US EPA (15 September 2020). \"Overview of Greenhouse Gases\". Retrieved 15 September 2020. He, Yanyi; Wang, Kaicun; Zhou, Chunlüe; Wild, Martin (2018). \"A Revisit of Global Dimming and Brightening Based on the Sunshine Duration\". Geophysical Research Letters. 45 (9): 4281–4289. Bibcode:2018GeoRL..45.4281H. doi:10.1029/2018GL077424. hdl:20.500.11850/268470. ISSN 1944-8007. Knutson, T.; Kossin, J.P.; Mears, C.; Perlwitz, J.; Wehner, M.F (2017). Wuebbles, D.J; Fahey, D.W; Hibbard, K.A; Dokken, D.J; Stewart, B.C; Maycock, T.K (eds.). Ch. 3: Detection and Attribution of Climate Change (PDF). doi:10.7930/J01834ND. Archived from the original (PDF) on 4 November 2017. Global Methane Initiative (2020). Global Methane Emissions and Mitigation Opportunities (PDF) (Report). Global Methane Initiative. IPCC TAR WG1 (2001), Houghton, J.T.; Ding, Y.; Griggs, D.J.; Noguer, M.; van der Linden, P.J.; Dai, X.; Maskell, K.; Johnson, C.A. (eds.), Climate Change 2001: The Scientific Basis, Contribution of Working Group I to the Third Assessment Report of the Intergovernmental Panel on Climate Change, Cambridge University Press, ISBN 0-521-80767-0, archived from the original on 15 December 2019, retrieved 18 December 2019 (pb: 0-521-01495-6). Fahey, D. W.; Doherty, S. J.; Hibbard, K. A.; Romanou, A.; Taylor, P. C. (2017). \"Chapter 2: Physical Drivers of Climate Change\" (PDF). In USGCRP2017. Archived from the original (PDF) on 11 July 2023. Retrieved 13 March 2024. Kvande, H. (2014). \"The Aluminum Smelting Process\". Journal of Occupational and Environmental Medicine. 56 (5 Suppl): S2 – S4. doi:10.1097/JOM.0000000000000154. PMC 4131936. PMID 24806722. Lindsey, Rebecca (23 June 2022). \"Climate Change: Atmospheric Carbon Dioxide\". Climate.gov. Archived from the original on 24 June 2013. Retrieved 7 May 2023. Melillo, J. M.; Frey, S. D.; DeAngelis, K. M.; Werner, W. J.; et al. (2017). \"Long-term pattern",
    "label": 0
  },
  {
    "text": "Lindsey, Rebecca (23 June 2022). \"Climate Change: Atmospheric Carbon Dioxide\". Climate.gov. Archived from the original on 24 June 2013. Retrieved 7 May 2023. Melillo, J. M.; Frey, S. D.; DeAngelis, K. M.; Werner, W. J.; et al. (2017). \"Long-term pattern and magnitude of soil carbon feedback to the climate system in a warming world\". Science. 358 (6359): 101–105. Bibcode:2017Sci...358..101M. doi:10.1126/science.aan2874. hdl:1912/9383. PMID 28983050. \"Arctic amplification\". NASA. 2013. Archived from the original on 31 July 2018. National Academies (2008). Understanding and responding to climate change: Highlights of National Academies Reports, 2008 edition (PDF) (Report). National Academy of Sciences. Archived from the original (PDF) on 11 October 2017. Retrieved 9 November 2010. Olivier, J. G. J.; Peters, J. A. H. W. (2019). Trends in global CO2 and total greenhouse gas emissions (PDF). The Hague: PBL Netherlands Environmental Assessment Agency. Ramanathan, V.; Carmichael, G. (2008). \"Global and Regional Climate Changes due to Black Carbon\". Nature Geoscience. 1 (4): 221–227. Bibcode:2008NatGe...1..221R. doi:10.1038/ngeo156. Randel, William J.; Shine, Keith P.; Austin, John; Barnett, John; et al. (2009). \"An update of observed stratospheric temperature trends\". Journal of Geophysical Research. 114 (D2): D02107. Bibcode:2009JGRD..114.2107R. doi:10.1029/2008JD010421. HAL hal-00355600. Ritchie, Hannah (18 September 2020). \"Sector by sector: where do global greenhouse gas emissions come from?\". Our World in Data. Retrieved 28 October 2020. Documentary Sea Blind (Dutch Television) (in Dutch). RIVM: Netherlands National Institute for Public Health and the Environment. 11 October 2016. Archived from the original on 17 August 2018. Retrieved 26 February 2019. Seymour, Frances; Gibbs, David (8 December 2019). \"Forests in the IPCC Special Report on Land Use: 7 Things to Know\". World Resources Institute. Storelvmo, T.; Phillips, P. C. B.; Lohmann, U.; Leirvik, T.; Wild, M. (2016). \"Disentangling greenhouse warming and aerosol cooling to reveal Earth's climate sensitivity\" (PDF). Nature Geoscience. 9 (4): 286–289. Bibcode:2016NatGe...9..286S. doi:10.1038/ngeo2670.",
    "label": 0
  },
  {
    "text": "Use: 7 Things to Know\". World Resources Institute. Storelvmo, T.; Phillips, P. C. B.; Lohmann, U.; Leirvik, T.; Wild, M. (2016). \"Disentangling greenhouse warming and aerosol cooling to reveal Earth's climate sensitivity\" (PDF). Nature Geoscience. 9 (4): 286–289. Bibcode:2016NatGe...9..286S. doi:10.1038/ngeo2670. ISSN 1752-0908. Turetsky, Merritt R.; Abbott, Benjamin W.; Jones, Miriam C.; Anthony, Katey Walter; et al. (2019). \"Permafrost collapse is accelerating carbon release\". Nature. 569 (7754): 32–34. Bibcode:2019Natur.569...32T. doi:10.1038/d41586-019-01313-4. PMID 31040419. \"One-Fourth of Global Forest Loss Permanent: Deforestation Is Not Slowing Down\". The Sustainability Consortium. 13 September 2018. Archived from the original on 21 May 2019. Retrieved 1 December 2019. UN FAO (2016). Global Forest Resources Assessment 2015. How are the world's forests changing? (PDF) (Report). Food and Agriculture Organization of the United Nations. ISBN 978-92-5-109283-5. Retrieved 1 December 2019. USGCRP (2009). Karl, T. R.; Melillo, J.; Peterson, T.; Hassol, S. J. (eds.). Global Climate Change Impacts in the United States. Cambridge University Press. ISBN 978-0-521-14407-0. Archived from the original on 6 April 2010. Retrieved 19 January 2024. USGCRP (2017). Wuebbles, D. J.; Fahey, D. W.; Hibbard, K. A.; Dokken, D. J.; et al. (eds.). Climate Science Special Report: Fourth National Climate Assessment, Volume I. Washington, D.C.: U.S. Global Change Research Program. doi:10.7930/J0J964J6 (inactive 20 August 2025). Archived from the original on 3 November 2017. Retrieved 8 April 2018.{{cite book}}: CS1 maint: DOI inactive as of August 2025 (link) Wild, M.; Gilgen, Hans; Roesch, Andreas; Ohmura, Atsumu; et al. (2005). \"From Dimming to Brightening: Decadal Changes in Solar Radiation at Earth's Surface\". Science. 308 (5723): 847–850. Bibcode:2005Sci...308..847W. doi:10.1126/science.1103215. PMID 15879214. S2CID 13124021. WMO Statement on the State of the Global Climate in 2020. WMO-No. 1264. Geneva: World Meteorological Organization. 2021. ISBN 978-92-63-11264-4. Archived from the original on 19 April 2021. Retrieved 1 April 2024. Wolff, Eric W.; Shepherd, John G.;",
    "label": 0
  },
  {
    "text": "S2CID 13124021. WMO Statement on the State of the Global Climate in 2020. WMO-No. 1264. Geneva: World Meteorological Organization. 2021. ISBN 978-92-63-11264-4. Archived from the original on 19 April 2021. Retrieved 1 April 2024. Wolff, Eric W.; Shepherd, John G.; Shuckburgh, Emily; Watson, Andrew J. (2015). \"Feedbacks on climate in the Earth system: introduction\". Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 373 (2054) 20140428. Bibcode:2015RSPTA.37340428W. doi:10.1098/rsta.2014.0428. PMC 4608041. PMID 26438277. IPCC reports Attribution This article incorporates public domain material from EPA (2009), Endangerment and Cause or Contribute Findings for Greenhouse Gases under Section 202(a) of the Clean Air Act. EPA's Response to Public Comments, US Environmental Protection Agency (EPA), archived from the original on 14 August 2012, retrieved 23 June 2011. This article incorporates public domain material from Karl, T.R.; Melillo. J.; Peterson, T.; Hassol, S.J., eds. (2009). Global Climate Change Impacts in the United States (PDF). Cambridge University Press. ISBN 978-0-521-14407-0. Archived (PDF) from the original on 15 November 2019. Retrieved 23 December 2017.. Public-domain status of this report can be found on p.4 of source External links Intergovernmental Panel on Climate Change UK Met Office: Climate Guide NOAA Climate website – National Oceanic and Atmospheric Administration in the United States",
    "label": 0
  },
  {
    "text": "Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The modern-day rise in global temperatures is driven by human activities, especially fossil fuel (coal, oil and natural gas) burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Earth's atmosphere now has roughly 50% more carbon dioxide, the main gas driving global warming, than it did at the end of the pre-industrial era, reaching levels not seen for millions of years. Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common. Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline. Higher temperatures are also causing more intense storms, droughts, and other weather extremes. Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct. Even if efforts to minimize future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise. Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss. Human migration and conflict can also be a result. The World Health Organization calls climate change one of the biggest threats to global health in the 21st century. Societies and ecosystems will experience more severe risks without action to limit warming. Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some",
    "label": 0
  },
  {
    "text": "to global health in the 21st century. Societies and ecosystems will experience more severe risks without action to limit warming. Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some limits to adaptation have already been reached. Poorer communities are responsible for a small share of global emissions, yet have the least ability to adapt and are most vulnerable to climate change. Many climate change impacts have been observed in the first decades of the 21st century, with 2024 the warmest on record at +1.60 °C (2.88 °F) since regular tracking began in 1850. Additional warming will increase these impacts and can trigger tipping points, such as melting all of the Greenland ice sheet. Under the 2015 Paris Agreement, nations collectively agreed to keep warming \"well under 2 °C\". However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century. There is widespread support for climate action worldwide, and most countries aim to stop emitting carbon dioxide. Fossil fuels can be phased out by stopping subsidising them, conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind, solar, hydro, and nuclear power. Cleanly generated electricity can replace fossil fuels for powering transportation, heating buildings, and running industrial processes. Carbon can also be removed from the atmosphere, for instance by increasing forest cover and farming with methods that store carbon in soil. Terminology Before the 1980s, it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution. Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time. In the 1980s,",
    "label": 0
  },
  {
    "text": "the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution. Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time. In the 1980s, the terms global warming and climate change became more common, often being used interchangeably. Scientifically, global warming refers only to increased global average surface temperature, while climate change describes both global warming and its effects on Earth's climate system, such as precipitation changes. Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history. Global warming—used as early as 1975—became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate. Since the 2000s, usage of climate change has increased. Various scientists, politicians and media may use the terms climate crisis or climate emergency to talk about climate change, and may use the term global heating instead of global warming. Global temperature rise Temperatures prior to present-day global warming Over the last few million years the climate cycled through ice ages. One of the hotter periods was the Last Interglacial, around 125,000 years ago, where temperatures were between 0.5 °C and 1.5 °C warmer than before the start of global warming. This period saw sea levels 5 to 10 metres higher than today. The most recent glacial maximum 20,000 years ago was some 5–7 °C colder. This period has sea levels that were over 125 metres (410 ft) lower than today. Temperatures stabilized in the current interglacial period beginning 11,700 years ago. This period also saw the start of agriculture. Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age, did not occur at the same",
    "label": 0
  },
  {
    "text": "stabilized in the current interglacial period beginning 11,700 years ago. This period also saw the start of agriculture. Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions. Climate information for that period comes from climate proxies, such as trees and ice cores. Warming since the Industrial Revolution Around 1850 thermometer records began to provide global coverage. Between the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain, but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause global dimming. After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature. Ongoing changes in climate have had no precedent for several thousand years. Multiple datasets all show worldwide increases in surface temperature, at a rate of around 0.2 °C per decade. The 2014–2023 decade warmed to an average 1.19 °C [1.06–1.30 °C] compared to the pre-industrial baseline (1850–1900). Not every single year was warmer than the last: internal climate variability processes can make any year 0.2 °C warmer or colder than the average. From 1998 to 2013, negative phases of two such processes, Pacific Decadal Oscillation (PDO) and Atlantic Multidecadal Oscillation (AMO) caused a short slower period of warming called the \"global warming hiatus\". After the \"hiatus\", the opposite occurred, with 2024 well above the recent average at more than +1.5 °C. This is why the temperature change is defined in terms of a 20-year average, which reduces the noise of",
    "label": 0
  },
  {
    "text": "\"global warming hiatus\". After the \"hiatus\", the opposite occurred, with 2024 well above the recent average at more than +1.5 °C. This is why the temperature change is defined in terms of a 20-year average, which reduces the noise of hot and cold years and decadal climate patterns, and detects the long-term signal. A wide range of other observations reinforce the evidence of warming. The upper atmosphere is cooling, because greenhouse gases are trapping heat near the Earth's surface, and so less heat is radiating into space. Warming reduces average snow cover and forces the retreat of glaciers. At the same time, warming also causes greater evaporation from the oceans, leading to more atmospheric humidity, more and heavier precipitation. Plants are flowering earlier in spring, and thousands of animal species have been permanently moving to cooler areas. Differences by region Different regions of the world warm at different rates. The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature. This is because oceans lose more heat by evaporation and oceans can store a lot of heat. The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean. The rest has heated the atmosphere, melted ice, and warmed the continents. The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice. As these surfaces flip from reflecting a lot of light to",
    "label": 0
  },
  {
    "text": "have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat. Local black carbon deposits on snow and ice also contribute to Arctic warming. Arctic surface temperatures are increasing between three and four times faster than in the rest of the world. Melting of ice sheets near the poles weakens both the Atlantic and the Antarctic limb of thermohaline circulation, which further changes the distribution of heat and precipitation around the globe. Future global temperatures The World Meteorological Organization estimates there is almost a 50% chance of the five-year average global temperature exceeding +1.5 °C between 2024 and 2028. The IPCC expects the 20-year average to exceed +1.5 °C in the early 2030s. The IPCC Sixth Assessment Report (2021) included projections that by 2100 global warming is very likely to reach 1.0–1.8 °C under a scenario with very low emissions of greenhouse gases, 2.1–3.5 °C under an intermediate emissions scenario, or 3.3–5.7 °C under a very high emissions scenario. The warming will continue past 2100 in the intermediate and high emission scenarios, with future projections of global surface temperatures by year 2300 being similar to millions of years ago. The remaining carbon budget for staying beneath certain temperature increases is determined by modelling the carbon cycle and climate sensitivity to greenhouse gases. According to UNEP, global warming can be kept below 2.0 °C with a 50% chance if emissions after 2023 do not exceed 900 gigatonnes of CO2. This carbon budget corresponds to around 16 years of current emissions. Causes of recent global temperature rise The climate system experiences",
    "label": 0
  },
  {
    "text": "kept below 2.0 °C with a 50% chance if emissions after 2023 do not exceed 900 gigatonnes of CO2. This carbon budget corresponds to around 16 years of current emissions. Causes of recent global temperature rise The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example, El Niño events cause short-term spikes in surface temperature while La Niña events cause short term cooling. Their relative frequency can affect global temperature trends on a decadal timescale. Other changes are caused by an imbalance of energy from external forcings. Examples of these include changes in the concentrations of greenhouse gases, solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun. To determine the human contribution to climate change, unique \"fingerprints\" for all potential causes are developed and compared with both observed patterns and known internal climate variability. For example, solar forcing—whose fingerprint involves warming the entire atmosphere—is ruled out because only the lower atmosphere has warmed. Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo, are less impactful. Greenhouse gases Greenhouse gases are transparent to sunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time. While water vapour (≈50%) and clouds (≈25%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity. On the other hand, concentrations of gases such as CO2 (≈20%), tropospheric ozone, CFCs and nitrous oxide are added or removed independently from",
    "label": 0
  },
  {
    "text": "as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity. On the other hand, concentrations of gases such as CO2 (≈20%), tropospheric ozone, CFCs and nitrous oxide are added or removed independently from temperature, and are therefore considered to be external forcings that change global temperatures. Before the Industrial Revolution, naturally occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C warmer than it would have been in their absence. Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas), has increased the amount of greenhouse gases in the atmosphere. In 2022, the concentrations of CO2 and methane had increased by about 50% and 164%, respectively, since 1750. These CO2 levels are higher than they have been at any time during the last 14 million years. Concentrations of methane are far higher than they were over the last 800,000 years. Global human-caused greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases. CO2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity. Additional CO2 emissions come from deforestation and industrial processes, which include the CO2 released by the chemical reactions for making cement, steel, aluminium, and fertilizer. Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, and coal mining, as well as oil and gas extraction. Nitrous oxide emissions largely come from the microbial decomposition of fertilizer. While methane only lasts in the atmosphere for an average of 12 years, CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle. While plants on land and in",
    "label": 0
  },
  {
    "text": "from the microbial decomposition of fertilizer. While methane only lasts in the atmosphere for an average of 12 years, CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle. While plants on land and in the ocean absorb most excess emissions of CO2 every year, that CO2 is returned to the atmosphere when biological matter is digested, burns, or decays. Land-surface carbon sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions. The ocean has absorbed 20 to 30% of emitted CO2 over the last two decades. CO2 is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete. Land surface changes Around 30% of Earth's land area is largely unusable for humans (glaciers, deserts, etc.), 26% is forests, 10% is shrubland and 34% is agricultural land. Deforestation is the main land use change contributor to global warming, as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink. Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%. Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink. Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface",
    "label": 0
  },
  {
    "text": "as a carbon sink. Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns. In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler. At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains. Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect. Other factors Aerosols and clouds Air pollution, in the form of aerosols, affects the climate on a large scale. Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming, and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel. Smaller contributions come from black carbon (from combustion of fossil fuels and biomass), and from dust. Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much. Aerosols also have indirect effects on the Earth's energy budget. Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. They also reduce the growth of raindrops, which",
    "label": 0
  },
  {
    "text": "Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. They also reduce the growth of raindrops, which makes clouds more reflective to incoming sunlight. Indirect effects of aerosols are the largest uncertainty in radiative forcing. While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise. Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050. The effect of decreasing sulfur content of fuel oil for ships since 2020 is estimated to cause an additional 0.05 °C increase in global mean temperature by 2050. Solar and volcanic activity As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system. Solar irradiance has been measured directly by satellites, and indirect measurements are available from the early 1600s onwards. Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere). The upper atmosphere (the stratosphere) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling. This is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere. Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapour into the atmosphere, which adds to greenhouse gases and increases temperatures. These impacts on temperature only last for several years, because both water vapour and volcanic material have low",
    "label": 0
  },
  {
    "text": "block sunlight and reduce temperatures, or they can send water vapour into the atmosphere, which adds to greenhouse gases and increases temperatures. These impacts on temperature only last for several years, because both water vapour and volcanic material have low persistence in the atmosphere. volcanic CO2 emissions are more persistent, but they are equivalent to less than 1% of current human-caused CO2 emissions. Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution. Climate change feedbacks The climate system's response to an initial forcing is shaped by feedbacks, which either amplify or dampen the change. Self-reinforcing or positive feedbacks increase the response, while balancing or negative feedbacks reduce it. The main reinforcing feedbacks are the water-vapour feedback, the ice–albedo feedback, and the net cloud feedback. The primary balancing mechanism is radiative cooling, as Earth's surface gives off more heat to space in response to rising temperature. In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO2 on plant growth. Feedbacks are expected to trend in a positive direction as greenhouse gas emissions continue, raising climate sensitivity. These feedback processes alter the pace of global warming. For instance, warmer air can hold more moisture in the form of water vapour, which is itself a potent greenhouse gas. Warmer air can also make clouds higher and thinner, and therefore more insulating, increasing climate warming. The reduction of snow cover and sea ice in the Arctic is another major feedback, this reduces the reflectivity of the Earth's surface in the region and accelerates Arctic warming. This additional warming also contributes to permafrost thawing, which releases methane and CO2 into",
    "label": 0
  },
  {
    "text": "cover and sea ice in the Arctic is another major feedback, this reduces the reflectivity of the Earth's surface in the region and accelerates Arctic warming. This additional warming also contributes to permafrost thawing, which releases methane and CO2 into the atmosphere. Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans. This fraction is not static and if future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%. This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer. The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution. Uncertainty over feedbacks, particularly cloud cover, is the major reason why different climate models project different magnitudes of warming for a given amount of emissions. Modelling A climate model is a representation of the physical, chemical and biological processes that affect the climate system. Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing. Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks. Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere. The physical realism of models is tested by examining their ability to simulate current or past climates. Past models have underestimated the rate of Arctic shrinkage and underestimated the rate of precipitation increase. Sea level",
    "label": 0
  },
  {
    "text": "land surface and the atmosphere. The physical realism of models is tested by examining their ability to simulate current or past climates. Past models have underestimated the rate of Arctic shrinkage and underestimated the rate of precipitation increase. Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations. The 2017 United States-published National Climate Assessment notes that \"climate models may still be underestimating or missing relevant feedback processes\". Additionally, climate models may be unable to adequately predict short-term regional climatic shifts. A subset of climate models add societal factors to a physical climate model. These models simulate how population, economic growth, and energy use affect—and interact with—the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change. Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm. Impacts Environmental effects The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations. Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency. Extremely wet or dry events within the monsoon period have increased in India and East Asia. Monsoonal precipitation over the Northern Hemisphere has increased since 1980. The rainfall rate and intensity of hurricanes and typhoons is likely increasing, and the geographic range likely expanding poleward in response to climate warming. The frequency of tropical cyclones has not increased as a result of climate change. Global sea level is rising",
    "label": 0
  },
  {
    "text": "and intensity of hurricanes and typhoons is likely increasing, and the geographic range likely expanding poleward in response to climate warming. The frequency of tropical cyclones has not increased as a result of climate change. Global sea level is rising as a consequence of thermal expansion and the melting of glaciers and ice sheets. Sea level rise has increased over time, reaching 4.8 cm per decade between 2014 and 2023. Over the 21st century, the IPCC projects 32–62 cm of sea level rise under a low emission scenario, 44–76 cm under an intermediate one and 65–101 cm under a very high emission scenario. Marine ice sheet instability processes in Antarctica may add substantially to these values, including the possibility of a 2-meter sea level rise by 2100 under high emissions. Climate change has led to decades of shrinking and thinning of the Arctic sea ice. While ice-free summers are expected to be rare at 1.5 °C degrees of warming, they are set to occur once every three to ten years at a warming level of 2 °C. Higher atmospheric CO2 concentrations cause more CO2 to dissolve in the oceans, which is making them more acidic. Because oxygen is less soluble in warmer water, its concentrations in the ocean are decreasing, and dead zones are expanding. Tipping points and long-term impacts Greater degrees of global warming increase the risk of passing through 'tipping points'—thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state. For instance, the Greenland ice sheet is already melting, but if global warming reaches levels between 1.7 °C and 2.3 °C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5 °C or less, it will still lose a lot more ice than",
    "label": 0
  },
  {
    "text": "but if global warming reaches levels between 1.7 °C and 2.3 °C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5 °C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place. While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of major ocean currents like the Atlantic meridional overturning circulation (AMOC), and irreversible damage to key ecosystems like the Amazon rainforest and coral reefs can unfold in a matter of decades. The collapse of the AMOC would be a severe climate catastrophe, resulting in a cooling of the Northern Hemisphere. The long-term effects of climate change on oceans include further ice melt, ocean warming, sea level rise, ocean acidification and ocean deoxygenation. The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime. The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years. Oceanic CO2 uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years. Deep oceans (below 2,000 metres (6,600 ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date. Further, the West Antarctic ice sheet appears committed to practically irreversible melting, which would increase the sea levels by at least 3.3 m (10 ft 10 in) over approximately 2000 years. Nature and wildlife Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes. For instance, the range of hundreds of North American birds has shifted northward at an average rate of 1.5 km/year over the past 55",
    "label": 0
  },
  {
    "text": "and wildlife Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes. For instance, the range of hundreds of North American birds has shifted northward at an average rate of 1.5 km/year over the past 55 years. Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear. A related phenomenon driven by climate change is woody plant encroachment, affecting up to 500 million hectares globally. Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics. The size and speed of global warming is making abrupt changes in ecosystems more likely. Overall, it is expected that climate change will result in the extinction of many species. The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land. Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp, and seabirds. Ocean acidification makes it harder for marine calcifying organisms such as mussels, barnacles and corals to produce shells and skeletons; and heatwaves have bleached coral reefs. Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life. Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts. Plants have come under increased stress from damage by insects. Humans The effects of climate change are impacting humans everywhere in the world. Impacts can be observed on all continents and ocean regions,",
    "label": 0
  },
  {
    "text": "to climate change and other human impacts. Plants have come under increased stress from damage by insects. Humans The effects of climate change are impacting humans everywhere in the world. Impacts can be observed on all continents and ocean regions, with low-latitude, less developed areas facing the greatest risk. Continued warming has potentially \"severe, pervasive and irreversible impacts\" for people and ecosystems. The risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries. Health and food The World Health Organization calls climate change one of the biggest threats to global health in the 21st century. Scientists have warned about the irreversible harms it poses. Extreme weather events affect public health, and food and water security. Temperature extremes lead to increased illness and death. Climate change increases the intensity and frequency of extreme weather events. It can affect transmission of infectious diseases, such as dengue fever and malaria. According to the World Economic Forum, 14.5 million more deaths are expected due to climate change by 2050. 30% of the global population currently live in areas where extreme heat and humidity are already associated with excess deaths. By 2100, 50% to 75% of the global population would live in such areas. While total crop yields have been increasing in the past 50 years due to agricultural improvements, climate change has already decreased the rate of yield growth. Fisheries have been negatively affected in multiple regions. While agricultural productivity has been positively affected in some high latitude areas, mid- and low-latitude areas have been negatively affected. According to the World Economic Forum, an increase in drought in certain regions could cause 3.2 million deaths from malnutrition by 2050 and stunting in children. With 2 °C warming, global livestock headcounts could decline by 7–10% by 2050, as less",
    "label": 0
  },
  {
    "text": "to the World Economic Forum, an increase in drought in certain regions could cause 3.2 million deaths from malnutrition by 2050 and stunting in children. With 2 °C warming, global livestock headcounts could decline by 7–10% by 2050, as less animal feed will be available. If the emissions continue to increase for the rest of century, then over 9 million climate-related deaths would occur annually by 2100. Livelihoods and inequality Economic damages due to climate change may be severe and there is a chance of disastrous consequences. Severe impacts are expected in South-East Asia and sub-Saharan Africa, where most of the local inhabitants are dependent upon natural and agricultural resources. Heat stress can prevent outdoor labourers from working. If warming reaches 4 °C then labour capacity in those regions could be reduced by 30 to 50%. The World Bank estimates that between 2016 and 2030, climate change could drive over 120 million people into extreme poverty without adaptation. Inequalities based on wealth and social status have worsened due to climate change. Major difficulties in mitigating, adapting to, and recovering from climate shocks are faced by marginalized people who have less control over resources. Indigenous people, who are subsistent on their land and ecosystems, will face endangerment to their wellness and lifestyles due to climate change. An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities. While women are not inherently more at risk from climate change and shocks, limits on women's resources and discriminatory gender norms constrain their adaptive capacity and resilience. For example, women's work burdens, including hours worked in agriculture, tend to decline less than men's during climate shocks such as heat stress. Climate migration Low-lying islands and coastal communities are threatened",
    "label": 0
  },
  {
    "text": "gender norms constrain their adaptive capacity and resilience. For example, women's work burdens, including hours worked in agriculture, tend to decline less than men's during climate shocks such as heat stress. Climate migration Low-lying islands and coastal communities are threatened by sea level rise, which makes urban flooding more common. Sometimes, land is permanently lost to the sea. This could lead to statelessness for people in island nations, such as the Maldives and Tuvalu. In some regions, the rise in temperature and humidity may be too severe for humans to adapt to. With worst-case climate change, models project that areas almost one-third of humanity live in might become Sahara-like uninhabitable and extremely hot climates. These factors can drive climate or environmental migration, within and between countries. More people are expected to be displaced because of sea level rise, extreme weather and conflict from increased competition over natural resources. Climate change may also increase vulnerability, leading to \"trapped populations\" who are not able to move due to a lack of resources. Reducing and recapturing emissions Climate change can be mitigated by reducing the rate at which greenhouse gases are emitted into the atmosphere, and by increasing the rate at which carbon dioxide is removed from the atmosphere. To limit global warming to less than 2 °C global greenhouse gas emissions need to be net-zero by 2070. This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry. The United Nations Environment Programme estimates that countries need to triple their pledges under the Paris Agreement within the next decade to limit global warming to 2 °C. With pledges made under the Paris Agreement as of 2024, there would be a 66% chance that global warming is kept under 2.8 °C by the end of the century",
    "label": 0
  },
  {
    "text": "the next decade to limit global warming to 2 °C. With pledges made under the Paris Agreement as of 2024, there would be a 66% chance that global warming is kept under 2.8 °C by the end of the century (range: 1.9–3.7 °C, depending on exact implementation and technological progress). When only considering current policies, this raises to 3.1 °C. Globally, limiting warming to 2 °C may result in higher economic benefits than economic costs. Although there is no single pathway to limit global warming to 2 °C, most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions. To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in agriculture and forestry, such as preventing deforestation and restoring natural ecosystems by reforestation. Other approaches to mitigating climate change have a higher level of risk. Scenarios that limit global warming to 1.5 °C typically project the large-scale use of carbon dioxide removal methods over the 21st century. There are concerns, though, about over-reliance on these technologies, and environmental impacts. Solar radiation modification (SRM) is a proposal for reducing global warming by reflecting some sunlight away from Earth and back into space. Because it does not reduce greenhouse gas concentrations, it would not address ocean acidification and is not considered mitigation. SRM should be considered only as a supplement to mitigation, not a replacement for it, due to risks such as rapid warming if it were abruptly stopped and not restarted. The most-studied approach is stratospheric aerosol injection. SRM could reduce global warming and some of its impacts, though imperfectly. It poses environmental risks, such as changes to rainfall patterns, as well as political challenges, such as who",
    "label": 0
  },
  {
    "text": "and not restarted. The most-studied approach is stratospheric aerosol injection. SRM could reduce global warming and some of its impacts, though imperfectly. It poses environmental risks, such as changes to rainfall patterns, as well as political challenges, such as who would decide whether to use it. Clean energy Renewable energy is key to limiting climate change. For decades, fossil fuels have accounted for roughly 80% of the world's energy use. The remaining share has been split between nuclear power and renewables (including hydropower, bioenergy, wind and solar power and geothermal energy). Fossil fuel use is expected to peak in absolute terms prior to 2030 and then to decline, with coal use experiencing the sharpest reductions. Renewables represented 86% of all new electricity generation installed in 2023. Other forms of clean energy, such as nuclear and hydropower, currently have a larger share of the energy supply. However, their future growth forecasts appear limited in comparison. While solar panels and onshore wind are now among the cheapest forms of adding new power generation capacity in many locations, green energy policies are needed to achieve a rapid transition from fossil fuels to renewables. To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. Investment in coal would be eliminated and coal use nearly phased out by 2050. Electricity generated from renewable sources would also need to become the main energy source for heating and transport. Transport can switch away from internal combustion engine vehicles and towards electric vehicles, public transit, and active transport (cycling and walking). For shipping and flying, low-carbon fuels would reduce emissions. Heating could be increasingly decarbonized with technologies like heat pumps. There are obstacles to the continued rapid growth of clean energy, including",
    "label": 0
  },
  {
    "text": "vehicles, public transit, and active transport (cycling and walking). For shipping and flying, low-carbon fuels would reduce emissions. Heating could be increasingly decarbonized with technologies like heat pumps. There are obstacles to the continued rapid growth of clean energy, including renewables. Wind and solar produce energy intermittently and with seasonal variability. Traditionally, hydro dams with reservoirs and fossil fuel power plants have been used when variable energy production is low. Going forward, battery storage can be expanded, energy demand and supply can be matched, and long-distance transmission can smooth variability of renewable outputs. Bioenergy is often not carbon-neutral and may have negative consequences for food security. The growth of nuclear power is constrained by controversy around radioactive waste, nuclear weapon proliferation, and accidents. Hydropower growth is limited by the fact that the best sites have been developed, and new projects are confronting increased social and environmental concerns. Low-carbon energy improves human health by minimizing climate change as well as reducing air pollution deaths, which were estimated at 7 million annually in 2016. Meeting the Paris Agreement goals that limit warming to a 2 °C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5 °C could save millions and simultaneously increase energy security and reduce poverty. Improving air quality also has economic benefits which may be larger than mitigation costs. Energy conservation Reducing energy demand is another major aspect of reducing emissions. If less energy is needed, there is more flexibility for clean energy development. It also makes it easier to manage the electricity grid, and minimizes carbon-intensive infrastructure development. Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy. Several COVID-19 related changes in energy use patterns, energy",
    "label": 0
  },
  {
    "text": "manage the electricity grid, and minimizes carbon-intensive infrastructure development. Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy. Several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain. Strategies to reduce energy demand vary by sector. In the transport sector, passengers and freight can switch to more efficient travel modes, such as buses and trains, or use electric vehicles. Industrial strategies to reduce energy demand include improving heating systems and motors, designing less energy-intensive products, and increasing product lifetimes. In the building sector the focus is on better design of new buildings, and higher levels of energy efficiency in retrofitting. The use of technologies like heat pumps can also increase building energy efficiency. Agriculture and industry Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand. A set of actions could reduce agriculture and forestry-based emissions by two-thirds from 2010 levels. These include reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production. On the demand side, a key component of reducing emissions is shifting people towards plant-based diets. Eliminating the production of livestock for meat and dairy would eliminate about 3/4ths of all emissions from agriculture and other land use. Livestock also occupy 37% of ice-free land area on Earth and consume feed from the 12% of land area used for crops, driving deforestation and land degradation. Steel and cement production are responsible for about 13% of industrial CO2 emissions. In these industries, carbon-intensive materials such as coke and lime",
    "label": 0
  },
  {
    "text": "and consume feed from the 12% of land area used for crops, driving deforestation and land degradation. Steel and cement production are responsible for about 13% of industrial CO2 emissions. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production, so that reducing CO2 emissions requires research into alternative chemistries. Where energy production or CO2-intensive heavy industries continue to produce waste CO2, technology can sometimes be used to capture and store most of the gas instead of releasing it to the atmosphere. This technology, carbon capture and storage (CCS), could have a critical but limited role in reducing emissions. It is relatively expensive and has been deployed only to an extent that removes around 0.1% of annual greenhouse gas emissions. Carbon dioxide removal Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO2 beyond naturally occurring levels. Reforestation and afforestation (planting forests where there were none before) are among the most mature sequestration techniques, although the latter raises food security concerns. Farmers can promote sequestration of carbon in soils through practices such as use of winter cover crops, reducing the intensity and frequency of tillage, and using compost and manure as soil amendments. Forest and landscape restoration yields many benefits for the climate, including greenhouse gas emissions sequestration and reduction. Restoration/recreation of coastal wetlands, prairie plots and seagrass meadows increases the uptake of carbon into organic matter. When carbon is sequestered in soils and in organic matter such as trees, there is a risk of the carbon being re-released into the atmosphere later through changes in land use, fire, or other changes in ecosystems. The use of bioenergy in conjunction with carbon capture and storage (BECCS) can result in net negative emissions as CO2 is drawn from the atmosphere. It",
    "label": 0
  },
  {
    "text": "the atmosphere later through changes in land use, fire, or other changes in ecosystems. The use of bioenergy in conjunction with carbon capture and storage (BECCS) can result in net negative emissions as CO2 is drawn from the atmosphere. It remains highly uncertain whether carbon dioxide removal techniques will be able to play a large role in limiting warming to 1.5 °C. Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals. Adaptation Adaptation is \"the process of adjustment to current or expected changes in climate and its effects\". Without additional mitigation, adaptation cannot avert the risk of \"severe, widespread and irreversible\" impacts. More severe climate change requires more transformative adaptation, which can be prohibitively expensive. The capacity and potential for humans to adapt is unevenly distributed across different regions and populations, and developing countries generally have less. The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance. Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, and building flood controls. If that fails, managed retreat may be needed. There are economic barriers for tackling dangerous heat impact. Avoiding strenuous work or having air conditioning is not possible for everybody. In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control, and genetic improvements for increased tolerance to a changing climate. Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes. Education, migration and early warning systems can reduce climate vulnerability. Planting mangroves or encouraging other coastal vegetation",
    "label": 0
  },
  {
    "text": "improvements for increased tolerance to a changing climate. Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes. Education, migration and early warning systems can reduce climate vulnerability. Planting mangroves or encouraging other coastal vegetation can buffer storms. Ecosystems adapt to climate change, a process that can be supported by human intervention. By increasing connectivity between ecosystems, species can migrate to more favourable climate conditions. Species can also be introduced to areas acquiring a favourable climate. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest acts as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts. There are synergies but also trade-offs between adaptation and mitigation. An example for synergy is increased food productivity, which has large benefits for both adaptation and mitigation. An example of a trade-off is that increased use of air conditioning allows people to better cope with heat, but increases energy demand. Another trade-off example is that more compact urban development may reduce emissions from transport and construction, but may also increase the urban heat island effect, exposing people to heat-related health risks. Policies and politics Countries that are most vulnerable to climate change have typically been responsible for a small share of global emissions. This raises questions about justice and fairness. Limiting global warming makes it much easier to achieve the UN's Sustainable Development Goals, such as eradicating poverty and reducing inequalities. The connection is recognized in",
    "label": 0
  },
  {
    "text": "for a small share of global emissions. This raises questions about justice and fairness. Limiting global warming makes it much easier to achieve the UN's Sustainable Development Goals, such as eradicating poverty and reducing inequalities. The connection is recognized in Sustainable Development Goal 13 which is to \"take urgent action to combat climate change and its impacts\". The goals on food, clean water and ecosystem protection have synergies with climate mitigation. The geopolitics of climate change is complex. It has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from switching to a low-carbon economy themselves. Sometimes mitigation also has localized benefits though. For instance, the benefits of a coal phase-out to public health and local environments exceed the costs in almost all regions. Furthermore, net importers of fossil fuels win economically from switching to clean energy, causing net exporters to face stranded assets: fossil fuels they cannot sell. Policy options A wide range of policies, regulations, and laws are being used to reduce emissions. As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions. Carbon can be priced with carbon taxes and emissions trading systems. Direct global fossil fuel subsidies reached $319 billion in 2017, and $5.2 trillion when indirect costs such as air pollution are priced in. Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths. Money saved on fossil subsidies could be used to support the transition to clean energy instead. More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry. Several countries require utilities to increase the share of renewables in power production. A global carbon market coalition",
    "label": 0
  },
  {
    "text": "instead. More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry. Several countries require utilities to increase the share of renewables in power production. A global carbon market coalition proposed at COP30 (2025) was estimated to increase emissions reduction seven-fold over current policies, while delivering $200 billion per year for clean-energy and social programs. Climate justice Policy designed through the lens of climate justice tries to address human rights issues and social inequality. According to proponents of climate justice, the costs of climate adaptation should be paid by those most responsible for climate change, while the beneficiaries of payments should be those suffering impacts. One way this can be addressed in practice is to have wealthy nations pay poorer countries to adapt. Oxfam found that in 2023 the wealthiest 10% of people were responsible for 50% of global emissions, while the bottom 50% were responsible for just 8%. Production of emissions is another way to look at responsibility: under that approach, the top 21 fossil fuel companies would owe cumulative climate reparations of $5.4 trillion over the period 2025–2050. To achieve a just transition, people working in the fossil fuel sector would also need other jobs, and their communities would need investments. International climate agreements Nearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC). The goal of the UNFCCC is to prevent dangerous human interference with the climate system. As stated in the convention, this requires that greenhouse gas concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained. The UNFCCC does not itself restrict emissions but rather provides a framework",
    "label": 0
  },
  {
    "text": "concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained. The UNFCCC does not itself restrict emissions but rather provides a framework for protocols that do. Global emissions have risen since the UNFCCC was signed. Its yearly conferences are the stage of global negotiations. The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions. During the negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to \"[take] the lead\" in reducing their emissions, since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere. Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs. The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77. Associated parties aimed to limit the global temperature rise to below 2 °C. The accord set the goal of sending $100 billion per year to developing countries for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund. As of 2020, only 83.3 billion were delivered. Only in 2023 the target is expected to be achieved. In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 2.0 °C and contains an aspirational goal of keeping warming under 1.5 °C. The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years. The Paris",
    "label": 0
  },
  {
    "text": "Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years. The Paris Agreement restated that developing countries must be financially supported. As of March 2025, 194 states and the European Union have acceded to or ratified the agreement. The 1987 Montreal Protocol, an international agreement to phase out production of ozone-depleting gases, has had benefits for climate change mitigation. Several ozone-depleting gases like chlorofluorocarbons are powerful greenhouse gases, so banning their production and usage may have avoided a temperature rise of 0.5 °C–1.0 °C, as well as additional warming by preventing damage to vegetation from ultraviolet radiation. It is estimated that the agreement has been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so. The most recent amendment to the Montreal Protocol, the 2016 Kigali Amendment, committed to reducing the emissions of hydrofluorocarbons, which served as a replacement for banned ozone-depleting gases and are also potent greenhouse gases. Should countries comply with the amendment, a warming of 0.3 °C–0.5 °C is estimated to be avoided. National responses In 2019, the United Kingdom parliament became the first national government to declare a climate emergency. Other countries and jurisdictions followed suit. That same year, the European Parliament declared a \"climate and environmental emergency\". The European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050. In 2021, the European Commission released its \"Fit for 55\" legislation package, which contains guidelines for the car industry; all new cars on the European market must be zero-emission vehicles from 2035. Major countries in Asia have made similar pledges: South Korea and Japan have committed",
    "label": 0
  },
  {
    "text": "its \"Fit for 55\" legislation package, which contains guidelines for the car industry; all new cars on the European market must be zero-emission vehicles from 2035. Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon-neutral by 2050, and China by 2060. While India has strong incentives for renewables, it also plans a significant expansion of coal in the country. Vietnam is among very few coal-dependent, fast-developing countries that pledged to phase out unabated coal power by the 2040s or as soon as possible thereafter. As of 2021, based on information from 48 national climate plans, which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5 °C or 2 °C, respectively. Society and culture Denial and misinformation Public debate about climate change has been strongly affected by climate change denial and misinformation, which first emerged in the United States and has since spread to other countries, particularly Canada and Australia. It originated from fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists. Like the tobacco industry, the main strategy of these groups has been to manufacture doubt about climate-change related scientific data and results. People who hold unwarranted doubt about climate change are sometimes called climate change \"skeptics\", although \"contrarians\" or \"deniers\" are more appropriate terms. There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimize the negative impacts of climate change. Manufacturing uncertainty about the science later developed into a manufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community",
    "label": 0
  },
  {
    "text": "attribute it to natural influences, and some minimize the negative impacts of climate change. Manufacturing uncertainty about the science later developed into a manufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community to delay policy changes. Strategies to promote these ideas include criticism of scientific institutions, and questioning the motives of individual scientists. An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change. Public awareness and opinion Climate change came to international public attention in the late 1980s. Due to media coverage in the early 1990s, people often confused climate change with other environmental issues like ozone depletion. In popular culture, the climate fiction movie The Day After Tomorrow (2004) and the Al Gore documentary An Inconvenient Truth (2006) focused on climate change. Significant regional, gender, age and political differences exist in both public concern for, and understanding of, climate change. More highly educated people, and in some countries, women and younger people, were more likely to see climate change as a serious threat. College biology textbooks from the 2010s featured less content on climate change compared to those from the preceding decade, with decreasing emphasis on solutions. Partisan gaps also exist in many countries, and countries with high CO2 emissions tend to be less concerned. Views on causes of climate change vary widely between countries. Media coverage linked to protests has had impacts on public sentiment as well as on which aspects of climate change are focused upon. Higher levels of worry are associated with stronger public support for policies that address climate change. Concern has increased over time, and in 2021 a majority of citizens in 30 countries expressed a high level of worry about climate change, or view it as a global emergency. A",
    "label": 0
  },
  {
    "text": "public support for policies that address climate change. Concern has increased over time, and in 2021 a majority of citizens in 30 countries expressed a high level of worry about climate change, or view it as a global emergency. A 2024 survey across 125 countries found that 89% of the global population demanded intensified political action, but systematically underestimated other peoples' willingness to act. Climate movement Climate protests demand that political leaders take action to prevent climate change. They can take the form of public demonstrations, fossil fuel divestment, lawsuits and other activities. Prominent demonstrations include the School Strike for Climate. In this initiative, young people across the globe have been protesting since 2018 by skipping school on Fridays, inspired by Swedish activist and then-teenager Greta Thunberg. Mass civil disobedience actions by groups like Extinction Rebellion have protested by disrupting roads and public transport. Litigation is increasingly used as a tool to strengthen climate action from public institutions and companies. Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change. Lawsuits against fossil-fuel companies generally seek compensation for loss and damage. On 23 July 2025, the UN's International Court of Justice issued its advisory opinion, saying explicitly that states must act to stop climate change, and if they fail to accomplish that duty, other states can sue them. This obligation includes implementing their commitments in international agreements they are parties to, such as the 2015 Paris Climate Accord. History Early discoveries Scientists in the 19th century such as Alexander von Humboldt began to foresee the effects of climate change. In the 1820s, Joseph Fourier proposed the greenhouse effect to explain why Earth's temperature was higher than the Sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so",
    "label": 0
  },
  {
    "text": "von Humboldt began to foresee the effects of climate change. In the 1820s, Joseph Fourier proposed the greenhouse effect to explain why Earth's temperature was higher than the Sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so sunlight reaches the surface where it is converted to heat. However, the atmosphere is not transparent to heat radiating from the surface, and captures some of that heat, which in turn warms the planet. In 1856 Eunice Newton Foote demonstrated that the warming effect of the Sun is greater for air with water vapour than for dry air, and that the effect is even greater with carbon dioxide (CO2). In \"Circumstances Affecting the Heat of the Sun's Rays\" she concluded that \"[a]n atmosphere of that gas would give to our earth a high temperature\". Starting in 1859, John Tyndall established that nitrogen and oxygen—together totalling 99% of dry air—are transparent to radiated heat. However, water vapour and gases such as methane and carbon dioxide absorb radiated heat and re-radiate that heat into the atmosphere. Tyndall proposed that changes in the concentrations of these gases may have caused climatic changes in the past, including ice ages. Svante Arrhenius noted that water vapour in air continuously varied, but the CO2 concentration in air was influenced by long-term geological processes. Warming from increased CO2 levels would increase the amount of water vapour, amplifying warming in a positive feedback loop. In 1896, he published the first climate model of its kind, projecting that halving CO2 levels could have produced a drop in temperature initiating an ice age. Arrhenius calculated the temperature increase expected from doubling CO2 to be around 5–6 °C. Other scientists were initially sceptical and believed that the greenhouse effect was saturated so that adding more CO2 would make no difference, and",
    "label": 0
  },
  {
    "text": "an ice age. Arrhenius calculated the temperature increase expected from doubling CO2 to be around 5–6 °C. Other scientists were initially sceptical and believed that the greenhouse effect was saturated so that adding more CO2 would make no difference, and that the climate would be self-regulating. Beginning in 1938, Guy Stewart Callendar published evidence that climate was warming and CO2 levels were rising, but his calculations met the same objections. Development of a scientific consensus In the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum. This model predicted that increasing CO2 levels would cause warming. Around the same time, Hans Suess found evidence that CO2 levels had been rising, and Roger Revelle showed that the oceans would not absorb the increase. The two scientists subsequently helped Charles Keeling to begin a record of continued increase—the \"Keeling Curve\"—which was part of continued scientific investigation through the 1960s into possible human causation of global warming. Studies such as the National Research Council's 1979 Charney Report supported the accuracy of climate models that forecast significant warming. Human causation of observed global warming and dangers of unmitigated warming were publicly presented in James Hansen's 1988 testimony before a US Senate committee. The Intergovernmental Panel on Climate Change (IPCC), set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research. As part of the IPCC reports, scientists assess the scientific discussion that takes place in peer-reviewed journal articles. There is a nearly unanimous scientific consensus that the climate is warming and that this is caused by human activities. No scientific body of national or international standing disagrees with this view. As of 2019, agreement in recent literature reached over 99%. The 2021 IPCC Assessment Report stated that it is \"unequivocal\" that climate",
    "label": 0
  },
  {
    "text": "this is caused by human activities. No scientific body of national or international standing disagrees with this view. As of 2019, agreement in recent literature reached over 99%. The 2021 IPCC Assessment Report stated that it is \"unequivocal\" that climate change is caused by humans. Consensus has further developed that action should be taken to protect people against the impacts of climate change. National science academies have called on world leaders to cut global emissions. Recent developments Extreme event attribution (EEA), also known as attribution science, was developed in the early decades of the 21st century. EEA uses climate models to identify and quantify the role that human-caused climate change plays in the frequency, intensity, duration, and impacts of specific individual extreme weather events. Results of attribution studies allow scientists and journalists to make statements such as, \"this weather event was made at least n times more likely by human-caused climate change\" or \"this heatwave was made m degrees hotter than it would have been in a world without global warming\" or \"this event was effectively impossible without climate change\". Greater computing power in the 2000s and conceptual breakthroughs in the early to mid 2010s enabled attribution science to detect the effects of climate change on some events with high confidence. Scientists use attribution methods and climate simulations that have already been peer reviewed, allowing \"rapid attribution studies\" to be published within a \"news cycle\" time frame after weather events. References Sources This article incorporates text from a free content work. Licensed under CC BY-SA 3.0. Text taken from The status of women in agrifood systems – Overview​, FAO, FAO. IPCC reports Other peer-reviewed sources Books, reports and legal documents Non-technical sources External links Intergovernmental Panel on Climate Change: IPCC (IPCC) UN: Climate Change (UN) Met Office: Climate Guide (Met",
    "label": 0
  },
  {
    "text": "Climate crisis is a term that is used to describe global warming and climate change and their effects. This term and the term climate emergency have been used to emphasize the threat of global warming to Earth's natural environment and to humans, and to urge aggressive climate change mitigation and transformational adaptation. The term climate crisis is used by those who \"believe it evokes the gravity of the threats the planet faces from continued greenhouse gas emissions and can help spur the kind of political willpower that has long been missing from climate advocacy\". They believe, much as global warming provoked more emotional engagement and support for action than climate change, calling climate change a crisis could have an even stronger effect. A study has shown the term climate crisis invokes a strong emotional response by conveying a sense of urgency. However, some caution this response may be counter-productive and may cause a backlash due to perceptions of alarmist exaggeration. In the scientific journal BioScience, a January 2020 article that was endorsed by over 11,000 scientists states: \"the climate crisis has arrived\" and that an \"immense increase of scale in endeavors to conserve our biosphere is needed to avoid untold suffering due to the climate crisis\". Scientific basis Until the mid-2010s, the scientific community had been using neutral, constrained language when discussing climate change. Advocacy groups, politicians and media have traditionally been using more powerful language than that used by climate scientists. From around 2014, a shift in scientists' language connoted an increased sense of urgency. Use of the terms urgency, climate crisis and climate emergency in scientific publications and in mass media has grown. Scientists have called for more-extensive action and transformational climate-change adaptation that focuses on large-scale change in systems. In 2020, a group of over 11,000 scientists",
    "label": 0
  },
  {
    "text": "urgency, climate crisis and climate emergency in scientific publications and in mass media has grown. Scientists have called for more-extensive action and transformational climate-change adaptation that focuses on large-scale change in systems. In 2020, a group of over 11,000 scientists said in a paper in BioScience describing global warming as a climate emergency or climate crisis was appropriate. The scientists stated an \"immense increase of scale in endeavor\" is needed to conserve the biosphere. They warned about \"profoundly troubling signs\", which may have many indirect effects such as large-scale human migration and food insecurity; these signs include increases in dairy and meat production, fossil fuel consumption, greenhouse gas emissions and deforestation, activities that are all concurrent with upward trends in climate-change effects such as rising global temperatures, global ice melt and extreme weather. In 2019, scientists published an article in Nature saying evidence from climate tipping points alone suggests \"we are in a state of planetary emergency\". They defined emergency as a product of risk and urgency, factors they said are \"acute\". Previous research had shown individual tipping points could be exceeded with a 1–2 °C (1.8–3.6 °F) of global temperature increase; warming has already exceeded 1 °C (1.8 °F). A global cascade of tipping points is possible with greater warming. Definitions In the context of climate change, the word crisis is used to denote \"a crucial or decisive point or situation that could lead to a tipping point\". It is a situation with an \"unprecedented circumstance\". A similar definition states in this context, crisis means \"a turning point or a condition of instability or danger\" and implies \"action needs to be taken now or else the consequences will be disastrous\". Another definition defines climate crisis as \"the various negative effects that unmitigated climate change is causing or threatening to",
    "label": 0
  },
  {
    "text": "a condition of instability or danger\" and implies \"action needs to be taken now or else the consequences will be disastrous\". Another definition defines climate crisis as \"the various negative effects that unmitigated climate change is causing or threatening to cause on our planet, especially where these effects have a direct impact on humanity\". Use of the term 20th century Former U.S. Vice President Al Gore has used crisis terminology since the 1980s; the Climate Crisis Coalition, which was formed in 2004, formalized the term climate crisis. A 1990 report by the American University International Law Review includes legal texts that use the word crisis. \"The Cairo Compact: Toward a Concerted World-Wide Response to the Climate Crisis\" (1989) states: \"All nations ... will have to cooperate on an unprecedented scale. They will have to make difficult commitments without delay to address this crisis.\" 21st century In the late 2010s, the phrase climate crisis emerged \"as a crucial piece of the climate hawk lexicon\", and was adopted by the Green New Deal, The Guardian, Greta Thunberg, and U.S. Democratic political candidates such as Kamala Harris. At the same time, it came into more-popular use following a series of warnings from climate scientists and newly-energized activists. In the U.S. in late 2018, the United States House of Representatives established the House Select Committee on the Climate Crisis, the name of which was regarded as \"a reminder of how much energy politics have changed in the last decade\". The original House climate committee had been called the \"Select Committee on Energy Independence and Global Warming\" in 2007. It was abolished in 2011 when Republicans regained control of the House. The advocacy group Public Citizen reported that in 2018, less than 10% of articles in top-50 U.S. newspapers used the terms crisis or emergency",
    "label": 0
  },
  {
    "text": "Global Warming\" in 2007. It was abolished in 2011 when Republicans regained control of the House. The advocacy group Public Citizen reported that in 2018, less than 10% of articles in top-50 U.S. newspapers used the terms crisis or emergency in the context of climate change. In the same year, 3.5% of national television news segments in the U.S. referred to climate change as a crisis or an emergency (50 of 1,400). In 2019, Public Citizen launched a campaign called \"Call it a Climate Crisis\"; it urged major media organizations to adopt the term climate crisis. In the first four months of 2019, the number of uses of the term in U.S. media tripled to 150. Likewise, the Sierra Club, the Sunrise Movement, Greenpeace, and other environmental and progressive organizations joined in a June 6, 2019 Public Citizen letter to news organizations urging the news organizations to call climate change and human inaction \"what it is–a crisis–and to cover it like one\". In 2019, the language describing climate appeared to change: the UN Secretary General's address at the 2019 UN Climate Action Summit used more emphatic language; Al Gore's campaign The Climate Reality Project, Greenpeace and the Sunrise Movement petitioned news organizations to alter their language; and in May 2019, The Guardian changed its style guide to favor the terms \"climate emergency, crisis or breakdown\" and \"global heating\". Editor-in-Chief Katharine Viner said: \"We want to ensure that we are being scientifically precise, while also communicating clearly with readers on this very important issue. The phrase 'climate change', for example, sounds rather passive and gentle when what scientists are talking about is a catastrophe for humanity.\" The Guardian became a lead partner in Covering Climate Now, an initiative of news organizations Columbia Journalism Review and The Nation that was founded in",
    "label": 0
  },
  {
    "text": "rather passive and gentle when what scientists are talking about is a catastrophe for humanity.\" The Guardian became a lead partner in Covering Climate Now, an initiative of news organizations Columbia Journalism Review and The Nation that was founded in 2019 to address the need for stronger climate coverage. In May 2019, The Climate Reality Project promoted an open petition of news organizations to use climate crisis instead of climate change and global warming. The NGO said: \"it's time to abandon both terms in culture\". In June 2019, Spanish news agency EFE announced its preferred phrase was \"crisis climática\". In November 2019, Hindustan Times also adopted the term because climate change \"does not correctly reflect the enormity of the existential threat\". The Polish newspaper Gazeta Wyborcza also uses the term climate crisis rather than climate change; one of its editors described climate change as one of the most-important topics the paper has ever covered. Also in June 2019, the Canadian Broadcasting Corporation (CBC) changed its language guide to say: \"Climate crisis and climate emergency are OK in some cases as synonyms for 'climate change'. But they're not always the best choice ... For example, 'climate crisis' could carry a whiff of advocacy in certain political coverage\". Journalism professor Sean Holman does not agree with this and said in an interview: It's about being accurate in terms of the scope of the problem that we are facing. And in the media we, generally speaking, don't have any hesitation about naming a crisis when it is a crisis. Look at the opioid epidemic [in the U.S.], for example. We call it an epidemic because it is one. So why are we hesitant about saying the climate crisis is a crisis? In June 2019, climate activists demonstrated outside the offices of The New",
    "label": 0
  },
  {
    "text": "epidemic [in the U.S.], for example. We call it an epidemic because it is one. So why are we hesitant about saying the climate crisis is a crisis? In June 2019, climate activists demonstrated outside the offices of The New York Times; they urged the newspaper's editors to adopt terms such as climate emergency or climate crisis. This kind of public pressure led New York City Council to make New York the largest city in the world to formally adopt a climate emergency declaration. In November 2019, the website Oxford Dictionaries named climate emergency Word of the year for 2019, with climate crisis being on the shortlist. The term was chosen because it matches the \"ethos, mood, or preoccupations of the passing year\". In 2021, the Finnish newspaper Helsingin Sanomat created a free variable font called Climate Crisis that has eight weights that correlate with Arctic sea ice decline, visualizing historical changes in ice melt. The newspaper's art director said the font both evokes the aesthetics of environmentalism and is a data visualization graphic. In updates to the World Scientists' Warning to Humanity of 2021 and 2022, scientists used the terms climate crisis and climate emergency; the title of the publications is \"World Scientists' Warning of a Climate Emergency\". They said: \"we need short, frequent, and easily accessible updates on the climate emergency\". Within weeks of his second inauguration in 2025, U.S. President Donald Trump's administration flagged hundreds of words to limit or avoid on government websites, memos, and unofficial agency guidance—the list including climate crisis. Effectiveness In September 2019, Bloomberg journalist Emma Vickers said crisis terminology may be \"showing results\", citing a 2019 poll by The Washington Post and the Kaiser Family Foundation saying 38% of U.S. adults termed climate change \"a crisis\" while an equal number called it",
    "label": 0
  },
  {
    "text": "Bloomberg journalist Emma Vickers said crisis terminology may be \"showing results\", citing a 2019 poll by The Washington Post and the Kaiser Family Foundation saying 38% of U.S. adults termed climate change \"a crisis\" while an equal number called it \"a major problem but not a crisis\". Five years earlier, 23% of U.S. adults considered climate change to be a crisis. As of 2019, use of crisis terminology in non-binding climate-emergency declarations is regarded as ineffective in making governments \"shift into action\". Concerns about crisis terminology Emergency framing may have several disadvantages. Such framing may implicitly prioritize climate change over other important social issues, encouraging competition among activists rather than cooperation. It could also de-emphasize dissent within the climate-change movement. Emergency framing may suggest a need for solutions by government, which provides less-reliable long-term commitment than does popular mobilization, and which may be perceived as being \"imposed on a reluctant population\". Without immediate dramatic effects of climate change, emergency framing may be counterproductive by causing disbelief, disempowerment in the face of a problem that seems overwhelming, and withdrawal. There could also be a \"crisis fatigue\" in which urgency to respond to threats loses its appeal over time. Crisis terminology could lose audiences if meaningful policies to address the emergency are not enacted. According to researchers Susan C. Moser and Lisa Dilling of University of Colorado, appeals to fear usually do not create sustained, constructive engagement; they noted psychologists consider human responses to danger—fight, flight or freeze—can be maladaptive if they do not reduce the danger. According to Sander van der Linden, director of the Cambridge Social Decision-Making Lab, fear is a \"paralyzing emotion\". He favors climate crisis over other terms because it conveys a sense of both urgency and optimism, and not a sense of doom. Van der Linden said:",
    "label": 0
  },
  {
    "text": "Linden, director of the Cambridge Social Decision-Making Lab, fear is a \"paralyzing emotion\". He favors climate crisis over other terms because it conveys a sense of both urgency and optimism, and not a sense of doom. Van der Linden said: \"people know that crises can be avoided and that they can be resolved\". Climate scientist Katharine Hayhoe said in early 2019 crisis framing is only \"effective for those already concerned about climate change, but complacent regarding solutions\". She added it \"is not yet effective\" for those who perceive climate activists \"to be alarmist Chicken Littles\", and that \"it would further reinforce their pre-conceived—and incorrect—notions\". According to Nick Reimer, journalists in Germany say the word crisis may be misunderstood to mean climate change is \"inherently episodic\"—crises are \"either solved or they pass\"—or as a temporary state before a return to normalcy that is not possible. Arnold Schwarzenegger, organizer of the Austrian World Summit for climate action, said people are not motivated by the term climate change; according to Schwarzenegger, focusing on the word pollution might evoke be a more-direct and negative connotation. A 2023 U.S. survey found no evidence that climate crisis or climate emergency—terms less familiar to those surveyed—elicit more perceived urgency than climate change or global warming. Psychological and neuroscientific studies In 2019, an advertising consulting agency conducted a neuroscientific study involving 120 U.S. people who were equally divided into supporters of the Republican Party, the Democratic Party and independents. The study involved electroencephalography (EEG) and galvanic skin response (GSR) measurements. Responses to the terms climate crisis, environmental destruction, environmental collapse, weather destabilization, global warming and climate change were measured. The study found Democrats had a 60% greater emotional response to climate crisis than to climate change. In Republicans, the emotional response to climate crisis was three times stronger",
    "label": 0
  },
  {
    "text": "environmental collapse, weather destabilization, global warming and climate change were measured. The study found Democrats had a 60% greater emotional response to climate crisis than to climate change. In Republicans, the emotional response to climate crisis was three times stronger than that for climate change. According to CBS News, climate crisis \"performed well in terms of responses across the political spectrum and elicited the greatest emotional response among independents\". The study concluded climate crisis elicited stronger emotional responses than neutral and \"worn out\" terms like global warming and climate change. Climate crisis was found to encourage a sense of urgency, though not a strong-enough response to cause cognitive dissonance that would cause people to generate counterarguments. Related terminology Research has shown the naming of a phenomenon and the way it is framed \"has a tremendous effect on how audiences come to perceive that phenomenon\" and \"can have a profound impact on the audience's reaction\". Climate change, and its real and hypothetical effects, are usually described in scientific-and-practitioner literature in terms of climate risks. The many related terms other than climate crisis include: global weirding (author and environmentalist L. Hunter Lovins, as a variation of global warming, early 2000s) climate catastrophe (used with reference to a 2019 David Attenborough documentary, the 2019–20 Australian bushfire season, and the 2022 Pakistan floods) threats that impact the earth (World Wildlife Fund, 2012—) climate breakdown (climate scientist Peter Kalmus, 2018) climate chaos (\"The New York Times\" article title, 2019; U.S. Democratic candidates, 2019; and an Ad Age marketing team, 2019) climate ruin (U.S. Democratic candidates, 2019) global heating (Richard A. Betts, Met Office U.K., 2018) global overheating (Public Citizen, 2019) climate emergency (11,000 scientists' warning letter in BioScience, and in The Guardian, both 2019), ecological breakdown, ecological crisis and ecological emergency (all set forth by",
    "label": 0
  },
  {
    "text": "2019) global heating (Richard A. Betts, Met Office U.K., 2018) global overheating (Public Citizen, 2019) climate emergency (11,000 scientists' warning letter in BioScience, and in The Guardian, both 2019), ecological breakdown, ecological crisis and ecological emergency (all set forth by climate activist Greta Thunberg, 2019) global meltdown, Scorched Earth, The Great Collapse, and Earthshattering (an Ad Age marketing team, 2019) climate disaster (The Guardian, 2019) environmental Armageddon (Fiji Prime Minister Frank Bainimarama) climate calamity (Los Angeles Times, 2022) climate havoc (The New York Times, 2022) climate pollution, carbon pollution (Grist, 2022) global boiling (U.N. Secretary-General António Guterres speech, July 2023) climate breaking point (Stuart P.M. Mackintosh, The Hill, August 2023) (Has humanity) broken the climate (The Guardian, August 2023) (climate) abyss (spokesman for the United Nations secretary general, May 2024) climate hell (U.N. Secretary-General António Guterres, June 2024) climate warming (Nature Ecology and Evolution, June 2025) climate overshoot (Proceedings of the National Academy of Sciences Nexus, November 2025) greenhouse warming (Nature Climate Change, November 2025) Terms other than climate crisis have been investigated for their effects upon audiences, including global warming, climate change, climatic disruption, environmental destruction, weather destabilization and environmental collapse. In 2022, The New York Times journalist Amanda Hess said \"end of the world\" characterizations of the future, such as climate apocalypse, are often used to refer to the current climate crisis, and that the characterization is spreading from \"the ironized hellscape of the internet\" to books and film. See also Climate psychology – Field of psychology Economic analysis of climate change Environmental communication – Type of communication Extinction Rebellion – Environmental pressure group Human extinction risk estimates – End of the human species Media coverage of climate change Psychology of climate change denial Public opinion on climate change Footnotes References Further reading \"Act now and avert a",
    "label": 0
  },
  {
    "text": "The 1.5-degree target (also known as the 1.5-degree limit) is the climate goal of limiting the man-made global temperature increase caused by the greenhouse effect to 1.5 °C on a 20-year average, calculated from the beginning of industrialization to the year 2100. The average value for the years 1850 to 1900 is used as the pre-industrial value. Background At the 21st UN Climate Change Conference in 2015 (COP 21), almost all countries in the world signed a treaty in the Paris Agreement, according to which they intend to make efforts to achieve the 1.5-degree target. According to a special report by the Intergovernmental Panel on Climate Change (IPCC), meeting the 1.5-degree target would be significantly more favorable than if only the two-degree target could be achieved. However, according to the IPCC, the efforts made before 2023 to reduce greenhouse gas emissions are far from sufficient. Without an immediate increase in measures, the world could warm by around 3.2 °C over the next 70 years, with catastrophic consequences for people and the environment. The World Meteorological Organization (WMO) predicted that the 1.5-degree temperature threshold would be exceeded in at least one year between 2023 and 2027. Accordingly, 2024 was the first single calendar year above 1.5 °C relative to pre-industrial levels. A study published in 2025 shows that such a first year above 1.5 °C indicates that most probably Earth has already entered the 20-year period that will reach the Paris Agreement limit — that is, a 20-year period with average warming of 1.5 °C. According to the United Nations Environment Programme (UNEP), the international community is heading towards dangerous global warming of up to 2.9 °C if it fully implements its current climate protection commitments, which are not dependent on preconditions. Achievability In a 2017 study, the chances of achieving",
    "label": 0
  },
  {
    "text": "Programme (UNEP), the international community is heading towards dangerous global warming of up to 2.9 °C if it fully implements its current climate protection commitments, which are not dependent on preconditions. Achievability In a 2017 study, the chances of achieving the 1.5-degree target were rated as low. At that time, it was assumed that even without further greenhouse gas emissions, the global average temperature would still rise to at least 1.1 °C compared to pre-industrial times, and with a probability of 13% even to 1.5 °C or more. A second study from the same year considers it unlikely that global warming will even be limited to 2 °C by 2100, let alone 1.5 °C. According to models at the time, which relied on predictions about gross domestic product per capita and population development, among other things, the probability of achieving this target was estimated at just one percent. However, the IPCC's 1.5-degree global warming special report published in October 2018 concludes that the 1.5-degree target is still achievable. To achieve this, human CO2 emissions would have to start falling significantly long before 2030 and reach net zero emissions from around 2050. In order to reduce greenhouse gas emissions in this relatively short period of time, a shift away from fossil fuels towards renewable energy sources and a predominantly plant-based diet is needed. Simultaneously, carbon dioxide removal of up to 100 to 1000 billion tons CO2 are required until the end of the century, equaling 2.5 to 25 times of the yearly CO2 emissions of c. 40 gigatons. One option to achieve this through natural means would be through carbon dioxide removal measures (CDR) in context of agriculture, forestry and other land use (AFOLU) like afforestation and moorland restoration, however in most modeled emission pathways of the IPCC this is considered",
    "label": 0
  },
  {
    "text": "achieve this through natural means would be through carbon dioxide removal measures (CDR) in context of agriculture, forestry and other land use (AFOLU) like afforestation and moorland restoration, however in most modeled emission pathways of the IPCC this is considered insufficient. Additionally, carbon capture and storage would have to be used to cool down the Earth after exceeding the 1.5-degree target. The world climate council itself mentions that it isn't confirmed that such measurements would work in large scale application. One prognosis published in 2023 certified that the 1.5-degree target would collapse between 2033 and 2035 even in positive scenarios. One report from the same year considers the compliance of the 1.5-degree goal and the decarbonization to 2050 as \"not plausible\". One case study on London published in 2023 suggests that the biggest contribution to reaching the 1.5-degree target in major cities consists of a drastic reduction in private transport. The scientists recommend a mix of measures in form of neighborly carsharing, the restructuring of the street layout after a superblock model, comprehensive local supply based on a compact city model, the stop of big road construction projects as well as a dynamic toll for roads with considerably higher traffic jam or areas of high hazardous effect on health. The worldwide planned production volume of coal, oil and gas continuously exceeds the permissible dimensions needed for mitigation of the climate change. On basis of the undisputed correlation between CO2 emissions and the economic growth, growth-critical measurements in order to reduce the rate of economical growth are considered central to the adherence to the 1.5-degree and 2-degree targets. According to research by Jason Hickel for example, the growth targets of the International Monetary Fund (IMF) would be opposing to these goals. Recent works indicate that the achievability of the 1.5-degree target",
    "label": 0
  },
  {
    "text": "adherence to the 1.5-degree and 2-degree targets. According to research by Jason Hickel for example, the growth targets of the International Monetary Fund (IMF) would be opposing to these goals. Recent works indicate that the achievability of the 1.5-degree target through the retroactive withdrawal of CO2 emissions from the Earth's atmosphere (overshoot scenario) had been overestimated within climate research because of irreversible effects. According to a survey by The Guardian only 5 percent of the surveyed climate scientists at the beginning of 2024 expected the achievability of the 1.5-degree target. Advantages compared to the 2-degree target A special report issued by the IPCC to as global warming of 1.5 °C makes among other things the following central statements to the consequences of a global warming of 1.5 °C in comparison to one of 2 °C: A reduced increase to average temperatures, heat waves, droughts, heavy precipitation and precipitation deficits. The rise of the sea level would be reduced by 0.1 more meters by adhering to the 1.5-degree target instead of the 2-degree target. Based on the population counts of 2010 and without any adjustment measures 10 million fewer people would be affected by the rise of the sea level than by 2 °C of warming. The oceans will rise regardless, even when adhering to the 1.5-degree target, although with less speed than with a warming of 2 °C. There would be less extinction of species, less damage on ecosystems on land, in fresh water and coasts so that more of the services of the ecosystems are retained longer. A lower increase in ocean warming and acidification, along with a smaller decline in ocean oxygen levels, and a reduced loss of biodiversity and fishery yields. A summer without sea ice in the Arctic would statistically occur only once per century instead",
    "label": 0
  },
  {
    "text": "increase in ocean warming and acidification, along with a smaller decline in ocean oxygen levels, and a reduced loss of biodiversity and fishery yields. A summer without sea ice in the Arctic would statistically occur only once per century instead of once per decade. There would be lower risks to human health and safety, livelihoods, food and water supplies, and economic growth. Fewer adjustments to the new climate would be necessary. The limits of the adaptation capacity of some human and natural systems are reached with 1.5 degrees of global warming, but the losses due to exceeding the limits of climate change adaptation would be smaller than with 2 degrees of global warming. The risk of triggering tipping points in the Earth's climate system and uncontrollable chain reactions is significantly lower at 1.5 °C of warming. Tipping points in the cryosphere, according to a group of researchers in a 2019 commentary, could already be dangerously close. With warming of 1.5 °C to 2 °C, the Greenland ice sheet or Arctic sea ice could melt. The tipping point for the West Antarctic ice sheet may already have been surpassed, but warming of 1.5 °C would slow the melting process by a factor of ten compared to 2 °C of global warming, making it easier to adapt to a sharply rising sea level. However, the carbon budget of 500 billion tons for a 50 percent chance of reaching the 1.5-degree target may already have been used up. Compared to the current global warming projections by the Intergovernmental Panel on Climate Change (IPCC), achieving the 1.5-degree target could force 80 percent fewer people to migrate due to climate change, as fewer areas of the Earth would become uninhabitable. According to CAN Europe, aligning Europe with the 1.5-degree target could save one trillion euros",
    "label": 0
  },
  {
    "text": "achieving the 1.5-degree target could force 80 percent fewer people to migrate due to climate change, as fewer areas of the Earth would become uninhabitable. According to CAN Europe, aligning Europe with the 1.5-degree target could save one trillion euros by 2030. Assessment in the climate protection movement Due to the low confidence of the IPCC and climate researchers regarding the achievability of the target, various groups, such as Scientist Rebellion, argue that the 1.5-degree target should be declared politically unfeasible. Direct democratic initiatives, such as Climate Restart, the initiator of the 2023 Berlin climate neutrality referendum, emphasize that decarbonization in industrialized nations must still be implemented in line with the 1.5-degree target to mitigate risks. Political scientists Wim Carton and Andreas Malm (both from Lund University) criticize representatives of climate science for having based their simulations on the achievability of the 1.5-degree target for too long, while relying on implausible assumptions that distracted from real-world scenarios. Use as a slogan The words \"1.5 degrees\" or \"1.5°C\" are often used as slogans by activists, such as by Fridays for Future since July 2022, with the inscription \"We all for 1.5°C\" on the asphalt of Hamburg's Mönckebergstraße, or during the occupation of Lützerath starting in 2022 (\"1.5°C means: Lützerath stays!\"). References External links",
    "label": 0
  },
  {
    "text": "This article documents events, research findings, scientific and technological advances, and human actions to measure, predict, mitigate, and adapt to the effects of global warming and climate change—during the year 2019. Summaries In November, BioScience published a Warning article stating \"we declare, with more than 11,000 scientist signatories from around the world, clearly and unequivocally that planet Earth is facing a climate emergency\" and that an \"immense increase of scale in endeavors to conserve our biosphere is needed to avoid untold suffering due to the climate crisis\". Measurements and statistics NOAA's National Centers for Environmental Information (NCEI) and the WMO reported that 2019 was the second hottest year in its 140-year climate record—0.04°C (0.07°F) cooler than 2016—with the U.K. Met Office ranking it among the three hottest. NOAA also reported that ocean heat content—the amount of heat stored in the upper levels of the ocean—was the highest ever recorded. NOAA also reported that both the Antarctic and Arctic oceans recorded their second smallest average annual sea-ice coverage during the 1979–2019 period of record. The WMO Global Atmosphere Watch in-situ observational network showed that carbon dioxide (410.5±0.2 ppm), methane (1877±2 ppb) and nitrous oxide (332.0±0.1 ppb) reached new highs in 2019, respectively constituting 148%, 260% and 123% of pre-industrial levels. The fire season in Sakha (Siberia) was unprecedented in the 20-year MODIS record in terms of an earlier start and northern extent, with some fires burning only about 11 km from the Chukchi Sea. From March through June, the burned area was greater than 2.9 times the 20-year mean. The Rhodium Group estimated that China contributed over 27% of total 2019 global greenhouse gas emissions (14 of 52 gigatons), surpassing the emissions of all OECD countries combined, though trailing them in per capita emissions; China was followed by the U.S. (11%),",
    "label": 0
  },
  {
    "text": "Group estimated that China contributed over 27% of total 2019 global greenhouse gas emissions (14 of 52 gigatons), surpassing the emissions of all OECD countries combined, though trailing them in per capita emissions; China was followed by the U.S. (11%), India (6.6%), Europe-27 (6.4%). 1 February 2022: a study published in PLOS Climate reported that, in 2019, 57% of the global ocean surface recorded extreme heat, compared to 2% during the Second Industrial Revolution, and that, between the 1980s and 2010s, the global mean normalized heat index increased by 68.23%. Researchers stated that \"many parts of the subtropical and midlatitude regions have reached a near-permanent extreme warming state\". Events and phenomena Actions and goals Political, economic, cultural actions In 2019, Amazon and Global Optimism co-founded The Climate Pledge whose signatory companies pledge net-zero carbon emissions by 2040, stimulating investment in low-carbon products and services. In March, 16-year-old Swedish climate activist Greta Thunberg was nominated for the Nobel Peace Prize, also receiving a nomination the following year. In September, Thunberg spoke at the 2019 UN Climate Action Summit, criticizing world leaders for inaction on climate change. In December, Thunberg was named TIME Person of the Year. In Norway, electric cars comprised 54% of all new vehicle sales for 2019, making it the first country to have sold more electric cars than petrol, hybrid, and diesel engines in a year. The government planned to ban the sale of petrol and diesel cars by 2025. Mitigation goal statements Adaptation goal statements Public opinion and scientific consensus The consensus among research scientists on anthropogenic global warming grew to 100%, based on a review of 11,602 peer-reviewed articles on \"climate change\" and \"global warming\" published in the first 7 months of 2019. A 2019 survey indicated a clear majority of people around the world think",
    "label": 0
  },
  {
    "text": "global warming grew to 100%, based on a review of 11,602 peer-reviewed articles on \"climate change\" and \"global warming\" published in the first 7 months of 2019. A 2019 survey indicated a clear majority of people around the world think climate change is happening and that it is all or partly down to human actions. However, 17% of Americans polled agreed that \"the idea of manmade global warming is a hoax that was invented to deceive people\", only Saudi Arabia and Indonesia having a higher proportion of people doubtful of manmade climate change. Projections In January, the World Economic Forum listed top 10 risks by likelihood (extreme weather events as #1, failure of climate change mitigation and adaptation as #2, man-made environmental damage and disasters as #6) and by impact (failure of climate change mitigation and adaptation as #2, extreme weather events as #3, man-made environmental damage and disasters as #9). Significant publications \"Emissions Gap Report 2019\" (PDF). UNenvironment.org. U.N. Environment Programme. 2019. Archived (PDF) from the original on 5 November 2020. Herring, Stephanie C.; Christidis, Nikolaos; Hoell, Andrew; Hoerling, Martin P.; Stott, Peter A., eds. (January 2021). \"Explaining Extreme Events or 2019 From a Climate Perspective\" (PDF). AMetSoc.net. American Meteorological Society. Archived (PDF) from the original on 27 January 2021. Myers, Joe; Whiting, Kate (16 January 2019). \"These are the biggest risks facing our world in 2019\". WEForum.org. World Economic Forum. Archived from the original on 14 January 2021. Ripple, William J.; Wolf, Christopher; Newsome, Thomas M.; Baarnard, Phoebe; et al. (5 November 2019). \"World Scientists' Warning of a Climate Emergency\". BioScience. 70 (1): 8–12. doi:10.1093/biosci/biz088. hdl:1808/30278. Watts, Nick; Amann, Markus; Arnell, Nigel; Ayeb-Karlsson, Sonja; et al. (13 November 2019). \"The 2019 report of The Lancet Countdown on health and climate change: ensuring that the health of a child",
    "label": 0
  },
  {
    "text": "This article documents events, research findings, scientific and technological advances, and human actions to measure, predict, mitigate, and adapt to the effects of global warming and climate change—during the year 2020. Summaries The December 2020 Lancet Countdown review concluded that trends in 2020 showed \"a concerning paucity of progress\" in numerous sectors, including \"a continued failure to reduce the carbon intensity of the global energy system, an increase in the use of coal-fired power, and a rise in agricultural emissions and premature deaths from excess red meat consumption. These issues (were) in part counteracted by the growth of renewable energy and improvements in low-carbon transport.\" Despite increasing climate suitability for infectious disease transmission and falling crop yield potential, \"the global response has remained muted\" but \"2020 will probably be an inflection point for several of the indicators (to be) presented during the coming decade\". The survey further noted that \"the nature and extent of the economic impact and response to the COVID-19 pandemic will have a defining role in determining whether the world meets the commitments of the Paris Agreement\". Measurements and statistics NASA GISS reported 2020's globally averaged temperature was 1.84 °F (1.02 °C) warmer than the baseline 1951-1980 mean, slightly greater than 2016 but within the margin of error of the analysis, making 2020 effectively tie 2016 as the warmest year on record despite La Niña's cooling effect. Independently, Copernicus (EU) concurred with NASA's conclusion, while NOAA and the U.K. Met Office concluded 2020 was the second warmest. High northern latitudes: Siberia recorded its second warmest January–June temperatures on record—more than 5°C (9°F) above average—including up to 10°C (18°F) above average in June. Verkhoyansk, located north of the arctic circle, recorded a temperature of 38°C (100°F) on 20 June. An analysis showed that, without human-induced climate change, these",
    "label": 0
  },
  {
    "text": "on record—more than 5°C (9°F) above average—including up to 10°C (18°F) above average in June. Verkhoyansk, located north of the arctic circle, recorded a temperature of 38°C (100°F) on 20 June. An analysis showed that, without human-induced climate change, these January–June temperatures would happen less than once in every 80,000 years. The fire season in Sakha (Siberia) was unprecedented in the 20-year MODIS record in terms of an earlier start and northern extent, with some fires burning only about 11 km from the Chukchi Sea. From March through June, the burned area was greater than 2.9 times the 20-year mean. End-of-winter Arctic sea ice extent was the 11th lowest, and the end-of-summer extent the second-lowest, in the 1979-2020 satellite record. April 2020 Northern Hemisphere sea ice volume was approximately 1,000 km3 below the 2010-2019 average, and October 2020 sea ice volume showed the lowest value in the preceding 10 years because of a second largest summer loss of 15.215 km3. 6 February: in a 1 July 2021 press release, the World Meteorological Organization officially recognized a new record high temperature for the Antarctic continent of 18.3 °C (64.9 °F) reached on 6 February 2020, exceeding the previous high of 17.5 °C (24 March 2015) for the Antarctic region. First half of 2020: Ember reported that wind and solar's share of global electricity rose to 9.8%, up from 4.6% at the 2015 signing of the Paris Climate Agreement; also in 2020: Germany (42%), UK (33%), EU (21%), Turkey (13%), US (12%), China, India, Japan, Brazil (each 10%), and Russia (0.2%). The September 2020 monthly average CO2 concentration at Mauna Loa station was 411.29 ppm (up from 408.54 ppm in September 2019), and at Cape Grim in Tasmania, 410.8 ppm (up from 408.58 ppm in 2019). Antarctic ice in 2020 was close",
    "label": 0
  },
  {
    "text": "September 2020 monthly average CO2 concentration at Mauna Loa station was 411.29 ppm (up from 408.54 ppm in September 2019), and at Cape Grim in Tasmania, 410.8 ppm (up from 408.58 ppm in 2019). Antarctic ice in 2020 was close to or slightly above the 42-year mean. On 16 August, Death Valley reached 54.4 °C (129.9 °F), the highest known temperature in the world in at least the last 80 years. On 4 January, Penrith, New South Wales reached 48.9 °C (120 °F), the highest observed temperature in an Australian metropolitan area. In the Middle East, temperature records were set in Jerusalem (42.7 °C, 108.8 °F), Eilat (48.9 °C, 120 °F), Kuwait Airport (52.1 °C, 125.8 °F) and Baghdad (51.8 °C, 125.2 °F). Ocean heat content (OHC) in the upper 2000 m (6600 ft) reached a record high in 2020 (data collected since 1958), the average annual linear rate increase since 1986 being almost eight times larger than the linear rate from 1958 to 1985. NOAA's National Centers for Environmental Information (NCEI) reported (see chart) that in 2020, the U.S. experienced 22 weather and climate-related events costing at least a billion dollars, exceeding the 1980–2019 inflation-adjusted average of 6.6 such events. The U.N.'s 2020 Emissions Gap Report stated that the highest-earning 1% of the global population account for more than twice the combined greenhouse gas emissions of the lowest-earning 50%. To meet the 1.5 °C goal of the Paris Agreement, the 1 percent would need to reduce their current emissions by at least a factor of 30, while the per capita emissions of the poorest 50 per cent could increase by around three times their current levels. In April 2021, the United Nations Office for Disaster Risk Reduction estimated that in 2020, climate-related disasters were largely responsible for 389 recorded",
    "label": 0
  },
  {
    "text": "capita emissions of the poorest 50 per cent could increase by around three times their current levels. In April 2021, the United Nations Office for Disaster Risk Reduction estimated that in 2020, climate-related disasters were largely responsible for 389 recorded events resulting in 15,080 deaths, 98.4 million people affected, and economic losses of at least US$171.3 billion. Reported September 2021: a record 227 environmental activists were killed throughout the world in 2020, with especially high numbers throughout Latin America and the Amazon, most of those killed being small-scale farmers or indigenous people, most defending forests from extractive industries including logging, agribusiness and mining. 12 April 2022: a study of 2020 storms of at least tropical storm-strength published in Nature Communications concluded that human-induced climate change increased extreme 3-hourly storm rainfall rates by 10%, and extreme 3-day accumulated rainfall amounts by 5%. For hurricane-strength storms, the figures increased to 11% and 8%. Natural events and phenomena The COVID-19 pandemic: As a direct consequence of the pandemic, an 8% reduction in greenhouse gas emissions was projected for 2020, which would be the largest 1-year decline on record. However, this reduction was a result of reduced economic activity, not the decarbonization of the economy required to respond to climate change. The Global Carbon Project estimated that global daily CO2 emissions may have been reduced by 17% during the most intense periods of COVID-19 shutdowns, but the WMO's Greenhouse Gas Bulletin indicated that this short-term impact cannot cause 2020's annual figure to exceed the 1 ppm natural inter-annual variability. Australia bushfires: By March 2020, \"Black Summer\" fires burnt almost 19 million hectares (46.95 million acres), destroyed over 3,000 houses, and killed 33 people and more than a billion animals, and is estimated to have had a A$20 billion impact, exceeding the record A$4.4 billion",
    "label": 0
  },
  {
    "text": "\"Black Summer\" fires burnt almost 19 million hectares (46.95 million acres), destroyed over 3,000 houses, and killed 33 people and more than a billion animals, and is estimated to have had a A$20 billion impact, exceeding the record A$4.4 billion of 2009's Black Saturday fires. A model-based analysis of heat and drought conditions underlying these Australian bushfires found that anthropogenic climate change had caused the probability of extreme heat to increase by at least a factor of two, and the risk of a Fire Weather Index being \"severe\" or worse to increase by at least 30%. Smoke in the stratosphere from the Australian bushfires induced planetary-scale blocking of solar radiation larger than any previously documented wildfires, with the same order of radiative forcing as produced by moderate volcanic eruptions. U.S. wildfires: In only seven weeks early in the 2020 Western United States wildfire season, a record 2.7 million hectares (6.7 million acres) burned, leaving tens of thousands homeless. The 2020 Atlantic hurricane season included 30 named storms (a record), 14 hurricanes (second highest on record), and record water levels in several locations. 11 December: Research published by the American Geophysical Union estimated that at least 23 of 30 magnitude-5+ earthquakes in Alaska since 1770 were worsened by glacial isostatic adjustment, which is the upward movement of the Earth's crust due to loss of mass of glaciers as they melt. In December, Iceberg A-68 approached South Georgia island, retaining an area of 4,200 km2 (1,600 mi2) after having calved away much of its mass since it broke off from Antarctica in 2017 as a 5,800 km2 (2,200 mi2) iceberg estimated at one trillion tonnes. Actions and goal statements Science and technology In October, MOSAiC, the largest-ever ($177 million) Arctic science expedition, completed a nearly 13-month mission, the research vessel Polarstern having",
    "label": 0
  },
  {
    "text": "2017 as a 5,800 km2 (2,200 mi2) iceberg estimated at one trillion tonnes. Actions and goal statements Science and technology In October, MOSAiC, the largest-ever ($177 million) Arctic science expedition, completed a nearly 13-month mission, the research vessel Polarstern having been purposely locked in an Arctic ice floe and collecting more than 150 terabytes of data and 1,000 ice samples. The Korea Superconducting Tokamak Advanced Research (KSTAR) superconducting fusion device maintained a core condition in eventually generating fusion power, maintaining plasma at an ion temperature exceeding 100 million °C. (180 million °F) for a record 20 seconds. Political, economic, cultural actions In February, 17-year-old Swedish climate activist Greta Thunberg was nominated for the Nobel Peace Prize for a second time, having also received a nomination the preceding year. By April, the COVID-19 pandemic had forced the postponement of the COP26 climate change conference from November 2020 to the following year. In April, Austria and Sweden closed their last coal-fired power plants. In May, the Energy Information Administration announced that U.S. annual energy consumption from renewable sources exceeded coal consumption for the first time since before 1885. In the first half of 2020, the EU generated 40% of its electricity from renewables, and 34% from fossil fuels. On 14 October, a strategic alliance in Peru announced a climate commitment to make Machu Picchu the first of the New Seven Wonders of the World and the first international tourism destination to obtain carbon neutral certification. After October's Hurricane Delta struck Puerto Morelos, Mexico, an insurance company issued a payout (17 million pesos, or US$850,000) on a policy covering a coral reef. The policy was taken out to cover damages from hurricanes whose severity was expected to increase due to climate change. 4 November 2020 marked the completion of the process by which",
    "label": 0
  },
  {
    "text": "or US$850,000) on a policy covering a coral reef. The policy was taken out to cover damages from hurricanes whose severity was expected to increase due to climate change. 4 November 2020 marked the completion of the process by which U.S. President Trump withdrew the country from the Paris climate agreement, the U.S. becoming the only country in the world to do so. President-elect Joe Biden vowed to recommit to the Paris accord on the first day of his presidency. On 27 November, Tasmanian minister for energy Guy Barnett announced that the Australian island state had achieved 100% self-sufficiency in renewable energy. On 3 December, the Danish government voted to immediately end new oil and gas exploration in the Danish North Sea as part of a plan to phase out fossil fuel extraction by 2050, guaranteeing an end to Denmark's fossil fuel production. Denmark's vote followed similar actions by France (2017) and New Zealand (2018). 12 December 2020 was the fifth anniversary of the 2015 Paris Agreement; countries review and update their national commitments every 5 years. In December, the Net Zero Asset Managers initiative—involving 30 fund management companies managing $9 trillion—pledged investment portfolios to be carbon-neutral by 2050. 2020 was the first year in which renewables overtook fossil fuels as the European Union's main source of electricity. Corporate declarations to partially or totally divest from fossil fuel companies were made by New York state pension fund ($226 billion), Sweden's pension fund AP2 ($43 billion), all six major U.S. banks (re Arctic drilling), Cambridge University's endowment ($4.5 billion), twelve large cities (totaling $295 billion), the U.K. National Employment Savings Trust (pension fund), Minnesota's pension fund ($4 billion), University of California ($126 billion), Cornell University endowment ($9.6 billion), Oxford University endowment ($3.7 billion), Denmark's ATP pension fund ($133 billion), and BlackRock.",
    "label": 0
  },
  {
    "text": "cities (totaling $295 billion), the U.K. National Employment Savings Trust (pension fund), Minnesota's pension fund ($4 billion), University of California ($126 billion), Cornell University endowment ($9.6 billion), Oxford University endowment ($3.7 billion), Denmark's ATP pension fund ($133 billion), and BlackRock. Mitigation goal statements In January, the U.K.'s National Health Service announced its commitment to become the world's first net zero health system. In September, the government of China said it would achieve net-zero carbon emissions by 2060. On 28 October, South Korea president Moon Jae-in declared that the country—with only 6% of its electricity generated by renewables-would go carbon neutral by 2050, following similar declarations in Japan (earlier in the week) and the European Union (2019). Before the November 2020 U.S. presidential election, Joe Biden called climate change an existential threat to health, the economy and national security, proposing to make the U.S. carbon neutral by 2050; incumbent Donald Trump continued to question climate science, actually promoting fossil fuels. A 2 December United Nations report stated that at least 113 nations had pledged carbon neutrality by 2050, and that by early 2021, countries representing more than 65% of global CO2 emissions and more than 70% of the world economy, will have made ambitious commitments to carbon neutrality; and that 186 parties to the Paris Agreement had already developed NDCs. On 4 December, British Prime Minister Boris Johnson announced a target to reduce the UK's greenhouse gas emissions by 68% (previously 53%) by 2030 compared to 1990 levels, the government announcement saying the target \"commits the UK to cutting emissions at the fastest rate of any major economy so far\". On 11 December, EU leaders agreed to reduce greenhouse gases by 55% by 2030 (measured against 1990 CO2 emission levels); the European Parliament had called for a 60% cut, and Greenpeace",
    "label": 0
  },
  {
    "text": "the fastest rate of any major economy so far\". On 11 December, EU leaders agreed to reduce greenhouse gases by 55% by 2030 (measured against 1990 CO2 emission levels); the European Parliament had called for a 60% cut, and Greenpeace and Global 2000, 65%. On 12 December, UN secretary general António Guterres told world leaders at the Climate Ambition Summit that they should all declare a state of climate emergency until the world reaches net-zero CO2 emissions (only about 38 had already made such declarations). Guterres noted that G20 countries were spending 50% more in their COVID-19 stimulus packages on fossil fuels and CO2-intensive sectors than they were on low-CO2 energy. At the 12 December Climate Ambition Summit, China's leader Xi Jinping said that by 2030, China would reduce its carbon intensity by over 65 percent. Adaptation goal statements At least 51 countries had developed plans for national health system adaptation planning. However, only 9% of countries reported having the funds to fully implement their plans. May 2021: A Carbon Disclosure Project survey found that in 2020, about 43% of 800 surveyed cities (combined population: 400 million) did not have a climate adaptation plan. Projections 15 January publication: The World Economic Forum listed top 10 risks over the next 10 years by likelihood (extreme weather as #1, climate action failure as #2, human-made environmental disasters as #5) and by impact (climate action failure as #1, extreme weather as #4, human-made environmental disasters as #9). 26 May publication: A study published in the PNAS projected that in a business-as-usual scenario, the geographical position of humans' temperature niche will shift more over the next 50 years than it has in the past 6000 years, and that one third of the global population will experience a mean annual temperature currently found in only",
    "label": 0
  },
  {
    "text": "geographical position of humans' temperature niche will shift more over the next 50 years than it has in the past 6000 years, and that one third of the global population will experience a mean annual temperature currently found in only 0.8% of the Earth's land surface. 22 July publication: A review conducted under the World Climate Research Programme (WCRP) projected climate sensitivity—the range of global warming to be expected from a doubling of CO2 concentration—to be 2.6—3.9 °C (4.7—7.0 °F), narrower than longstanding (1979+) estimates of about 1.5—4.5 °C (2.7—8.1 °F). 2 December publication: It was projected that to reach the Paris Climate Accord's 1.5 °C target, the 56 gigatonnes of CO2 equivalent (GtCO2e) emitted in 2020 would need to drop to 25 GtCO2e by 2030, requiring a 7.6% reduction every year—an increase in national government ambition of a factor of five. 9 December publication: The U.N.'s Emissions Gap Report 2020 stated that, by 2030, annual emissions would need to be 15(range: 12–19) GtCO2e lower than current unconditional NDCs imply for a 2 °C goal, and 32(range: 29–36) GtCO2e lower for the 1.5 °C goal. Current NDCs were said to \"remain seriously inadequate to achieve the climate goals of the Paris Agreement and would lead to a temperature increase of at least 3°C by the end of the century\". Significant publications UNEP, UNEP DTU Partnership, World Adaptation Science Programme (WASP) (January 2021). \"Adaptation Gap Report 2020\". UNEP.org. United Nations Environment Programme. Archived from the original on 11 January 2021.{{cite web}}: CS1 maint: multiple names: authors list (link) (has link to PDF download) UNEP, UNEP DTU Partnership (9 December 2020). \"Emissions Gap Report 2020\". UNEP.org. United Nations Environment Programme. Archived from the original on 9 December 2020. Yearly review of the difference between where greenhouse emissions are predicted to be in",
    "label": 0
  },
  {
    "text": "PDF download) UNEP, UNEP DTU Partnership (9 December 2020). \"Emissions Gap Report 2020\". UNEP.org. United Nations Environment Programme. Archived from the original on 9 December 2020. Yearly review of the difference between where greenhouse emissions are predicted to be in 2030 and where they should be to avoid the worst impacts of climate change. Watts, Nick; Amann, Markus; Arnell, Nigel; Ayeb-Karlsson, Sonja; et al. (2 December 2020). \"The 2020 report of The Lancet Countdown on health and climate change: responding to converging crises\". The Lancet. 397 (10269): 129–170. doi:10.1016/S0140-6736(20)32290-X. hdl:10278/3733808. PMID 33278353. S2CID 227250862. \"Arctic Report Card: Update for 2020 / The sustained transformation to a warmer, less frozen and biologically changed Arctic remains clear\" (PDF). arctic.NOAA.gov. National Oceanic and Atmospheric Administration (NOAA). December 2020. Archived (PDF) from the original on 8 December 2020. The \"Report Card\" includes: Perovich, D.; Meier, W.; Tschudi, M.; Hendricks, S.; et al. (December 2020). \"Sea ice\". NOAA.gov. National Oceanic and Atmospheric Administration (NOAA). doi:10.25923/n170-9h57. Archived from the original on 8 December 2020. York, A.; Bhatt, U.S.; Gargulinski, E.; Grabinski, Z.; et al. (December 2020). \"Wildland Fire in High Northern Latitudes\". NOAA.gov. National Oceanic and Atmospheric Administration (NOAA). doi:10.25923/2gef-3964. Archived from the original on 10 December 2020. Edmond, Charlotte (15 January 2020). \"These are the top risks facing the world in 2020\". WEForum.org. World Economic Forum. Archived from the original on 29 November 2020. Larson, Eric; Grieg, Chris; Jenkins, Jesse; Mayfield, Erin; et al. (15 December 2020). \"Net-Zero America: Potential Pathways, Infrastructure and Impacts\" (PDF). Princeton.edu. Princeton University. Archived (PDF) from the original on 2 March 2021. \"The Sixth Status of Corals of the World: 2020 Report\". GCRMN.net. Global Coral Reef Monitoring Network. October 2021. Archived from the original on 5 October 2021. \"State of the Climate in Africa 2020 (WMO-No. 1275)\". WMO.int. World Meteorological",
    "label": 0
  },
  {
    "text": "March 2021. \"The Sixth Status of Corals of the World: 2020 Report\". GCRMN.net. Global Coral Reef Monitoring Network. October 2021. Archived from the original on 5 October 2021. \"State of the Climate in Africa 2020 (WMO-No. 1275)\". WMO.int. World Meteorological Organization. 19 October 2021. Archived from the original on 19 October 2021. See also 2020 in the environment and environmental sciences Climatology § History History of climate change policy and politics History of climate change science Politics of climate change § History Notes References External links Organizations The Intergovernmental Panel on Climate Change (IPCC) World Meteorological Organization (WMO) Surveys, summaries, and report lists Hausfather, Zeke (23 October 2020). \"State of the climate: 2020 on course to be warmest year on record\". CarbonBrief.org. Carbon Brief. Archived from the original on 21 December 2020. \"Articles in 2020\". Nature. \"Review Articles in 2020\". Nature. \"The Year in Climate\". The New York Times. December 2020. Archived from the original on 17 December 2020. \"Climate Reports\". UN.org. United Nations. December 2020. Archived from the original on 22 December 2020. Svoboda, Michael (19 August 2020). \"13 major climate change reports released so far in 2020\". YaleClimateConnections.org. Yale Climate Connections. Archived from the original on 22 November 2020.",
    "label": 0
  },
  {
    "text": "This is an environmental history of the 2020s. Environmental history refers to events and trends related to the natural environment and human interactions with it. Examples of human-induced events include biodiversity loss, climate change and holocene extinction. Global issues Anthropogenic effects Anthropocene As of July 2020, neither the International Commission on Stratigraphy (ICS) nor the International Union of Geological Sciences (IUGS) has officially approved the term as a recognized subdivision of geologic time, but in May 2019, the AWG voted in favor of submitting a formal proposal to the ICS by 2021, locating potential stratigraphic markers to the mid-twentieth century of the common era. Biodiversity loss According to the 2020 United Nations' Global Biodiversity Outlook report, of the 20 biodiversity goals laid out by the Aichi Biodiversity Targets in 2010, only 6 were \"partially achieved\" by the deadline of 2020. The report highlighted that if the status quo is not changed, biodiversity will continue to decline due to \"currently unsustainable patterns of production and consumption, population growth and technological developments\". The report also singled out Australia, Brazil and Cameroon and the Galapagos Islands (Ecuador) for having had one of its animals lost to extinction in the past 10 years. Following this, the leaders of 64 nations and the European Union pledged to halt environmental degradation and restore the natural world. Leaders from some of the world's biggest polluters, namely China, India, Russia, Brazil, and the United States, were not among them. Climate change The effects of climate change manifested in 2020 with a record 30 named Atlantic tropical storms and hurricanes; the highest heat in 80-years recorded at 54.4 Celsius; massive wildfires in Australia, the Western United States, and the Arctic; and the second-lowest annual Arctic sea ice coverage. A hundred people died and 18,000 were hospitalized in Japan while",
    "label": 0
  },
  {
    "text": "hurricanes; the highest heat in 80-years recorded at 54.4 Celsius; massive wildfires in Australia, the Western United States, and the Arctic; and the second-lowest annual Arctic sea ice coverage. A hundred people died and 18,000 were hospitalized in Japan while France reported 1,462 heat-related deaths in 2019, an El Niño year. 2,800,000 people came down with dengue, leading to 1,250 deaths. The Milne Ice Shelf, on Ellesmere Island in the northern Canadian territory of Nunavut, collapsed in two days at the end of July 2020. This was the last fully intact Arctic ice shelf. Environmental groups declared that 2020 was at or near the hottest year on record. NASA said 2020 was tied with 2016, but NOAA said it was the second or third. NOAA said 2020 averaged 58.77 °F (14.88 °C), a few hundredths of a degree behind 2016. Other groups (World Meteorological Organization, Copernicus Group, UK Meteorological Office) had slightly different measurements. The differences in rankings mainly occurred due to how scientists accounted for data gaps in the Arctic; the difference between first or second place is considered insignificant. COVID-19 pandemic Holocene extinction According to the World Wide Fund for Nature's 2020 Living Planet Report, wildlife populations have declined by 68% since 1970 as a result of overconsumption, population growth and intensive farming, which is further evidence that humans have unleashed a sixth mass extinction event. Natural events Earthquakes and tsunamis during the decade include the 2020 Caribbean earthquake and the 2020 Zagreb earthquake. Wildfires included the 2019–20 Australian bushfire season, 2020 Western United States wildfire season, 2020 Córdoba wildfires, and as well as 2021 Turkey wildfires. Major tropical storms and hurricanes have also made an appearance during the decade, such as Hurricane Ida and Hurricane Ian. The more-than-average amounts of rainfall, higher ground covered, and the intensifying",
    "label": 0
  },
  {
    "text": "Córdoba wildfires, and as well as 2021 Turkey wildfires. Major tropical storms and hurricanes have also made an appearance during the decade, such as Hurricane Ida and Hurricane Ian. The more-than-average amounts of rainfall, higher ground covered, and the intensifying high-speed winds that accompanied both hurricanes were indirectly alleged to be products of rising sea levels and higher atmospheric temperatures. In 2020, a huge swarm of desert locusts threatened to engulf massive portions of the Middle East, Africa, and Asia. In tandem with the COVID-19 pandemic, this posed major hazards to billions of people who might be affected. Although experts had thought the insects would die out during the dry season in December 2019, unseasonal rains caused the incursion to reach unanticipated and hazardous levels. History by region Africa The 2019–2022 locust infestation caused widespread devastation of food production in the Horn of Africa. Americas North America An extreme heat wave in Western North America began affecting much of the Pacific Northwest and Western Canada in late June 2021. The heat has affected northern California, Idaho, western Nevada, Oregon, and Washington in the United States, as well as British Columbia, and, in its later phase, Alberta, Manitoba, the Northwest Territories, Saskatchewan, and Yukon, all in Canada. It resulted in some of the highest temperatures ever recorded in the region, including the highest temperature ever measured in Canada at 49.6 °C (121.3 °F). Central America Hurricane Eta and Hurricane Iota (both Category 4) hit the region in November within weeks of each other, creating much devastation to the same areas. At least 250 people were killed, with billions of dollars of damage to property. Asia Turkey The 2020 Aegean Sea earthquake killed 117 people in İzmir (in addition to two in Greece) after 41 had died in the Elazığ earthquake in",
    "label": 0
  },
  {
    "text": "least 250 people were killed, with billions of dollars of damage to property. Asia Turkey The 2020 Aegean Sea earthquake killed 117 people in İzmir (in addition to two in Greece) after 41 had died in the Elazığ earthquake in the same year, while the 2020 Iran–Turkey earthquakes killed 10. Forty-one people were also killed by the 2020 Van avalanches. Over two hundred wildfires burnt 1,600 square kilometres of Turkey's forest in its Mediterranean Region in July and August 2021, the worst ever wildfire season in the country's history. Europe In July 2021, several European countries were affected by catastrophic floods, causing deaths and widespread damage. The floods affected several river basins, first in the United Kingdom and later across northern and central Europe including Belgium, Germany, Luxembourg, the Netherlands, Switzerland and Italy. At least 185 people died in the floods, including 157 in Germany, 27 in Belgium and one in Italy. Netherlands Milieudefensie v Royal Dutch Shell was a case heard by the district court of The Hague in the Netherlands in 2021 related to efforts by multinational corporations to curtail carbon dioxide emissions. The case was considered a landmark ruling in environmental law related to climate change: while previous lawsuits against governments have prevailed for improving emissions, this was considered the first major suit to hold a corporation to the tenets of the Paris Agreement. While the decision only has jurisdiction in the Netherlands, it is expected to set a precedent for other environmental lawsuits against other large companies with high emissions that have not taken sufficient steps to reduce their emissions. The impact of the court's decision was considered by legal experts to be strengthened due to its reliance on human rights standards and international measures on climate change. Russia The Norilsk oil spill was an industrial",
    "label": 0
  },
  {
    "text": "to reduce their emissions. The impact of the court's decision was considered by legal experts to be strengthened due to its reliance on human rights standards and international measures on climate change. Russia The Norilsk oil spill was an industrial disaster near Norilsk, Krasnoyarsk Krai, that began on 29 May 2020 when a fuel storage tank at Norilsk-Taimyr Energy's Thermal Power Plant No. 3 (owned by Nornickel) failed, flooding local rivers with up to 21,000 cubic metres (17,500 tonnes) of diesel oil. Russian President Vladimir Putin declared a state of emergency in early June. The accident has been described as the second-largest oil spill in modern Russian history. As a result of the spill, up to 21,000 cubic metres (17,500 tonnes) of diesel oil spilled into the Daldykan River. Greenpeace Russia compared the potential environmental effects of the Norilsk spill to that of the 1989 Exxon Valdez oil spill. In the aftermath of the Norilsk spill, Russia's Prosecutor General's office ordered safety checks at all dangerous installations built on the permafrost in Russia's Arctic. From June 2021, the taiga forests in Siberia and the Far East region of Russia were hit by unprecedented wildfires, following record-breaking heat and drought. For the first time in recorded history, wildfire smoke reached the North Pole. Causes of the fires include monitoring difficulties, the shifting patterns of the jet stream and climate change in Russia. Large amounts of carbon may be released from formerly frozen ground under the fires, especially peatlands which continued burning from the previous year. Oceania Australia The 2019–20 Australian bushfire season was particularly destructive, killing at least 28 and destroying no fewer than 3,000 homes. The fires were widespread, but New South Wales (NSW) was the hardest hit. In December 2019 the smoke around Sydney was so bad that air",
    "label": 0
  },
  {
    "text": "season was particularly destructive, killing at least 28 and destroying no fewer than 3,000 homes. The fires were widespread, but New South Wales (NSW) was the hardest hit. In December 2019 the smoke around Sydney was so bad that air quality was 11 times the \"hazardous\" level and temperatures were over 40 °C (113°-120 °F). Natural causes such as lightning strikes started most of the fires, which were exasperated by dry conditions and drought, although police in NSW arrested at least 24 people for deliberately starting fires. In total, 7.3 million hectares (17.9 million acres) have burned across Australia's six states—an area larger than Belgium and Denmark combined. Experts estimate 500 million animals died, not including bats, frogs, or insects; one-third of Australia's koalas were killed, according to Minister for the Environment Sussan Ley. See also Impact of the COVID-19 pandemic on the environment 2020 in climate change 2021 in climate change 2022 in climate change 2023 in climate change 2024 in climate change 2020 in the environment and environmental sciences 2021 in the environment and environmental sciences 2022 in the environment and environmental sciences 2023 in the environment 2024 in the environment Climate change Environmentalism Green recovery Pollution References",
    "label": 0
  },
  {
    "text": "This article documents events, research findings, scientific and technological advances, and human actions to measure, predict, mitigate, and adapt to the effects of global warming and climate change—during the year 2021. Summaries 26 February: The United Nations Synthesis Report on Nationally Determined Contributions under the Paris Agreement stated that \"estimated reductions referred to in paragraphs (on greenhouse gas emissions) fall far short of what is required, demonstrating the need for Parties to further strengthen their mitigation commitments under the Paris Agreement\". 21 June: the World Meteorological Organization wrote that \"2021 is a make-or-break year for climate action, with the window to prevent the worst impacts of climate change—which include ever more frequent more intense droughts, floods and storms—closing rapidly.\" 28 July: a follow-on to the 2019 World Scientists' Warning of a Climate Emergency noted \"an unprecedented surge in climate-related disasters since 2019\" and stated there is \"mounting evidence that we are nearing or have already crossed tipping points associated with critical parts of the Earth system\". 6 September: editors from over 200 health journals published a joint editorial stating \"The science is unequivocal; a global increase of 1.5 °C above the pre-industrial average and the continued loss of biodiversity risk catastrophic harm to health that will be impossible to reverse.... The greatest threat to global public health is the continued failure of world leaders to keep the global temperature rise below 1.5 °C and to restore nature.\" 30 September: UN Secretary-General António Guterres stated that \"time is running out. Irreversible climate tipping points lie alarmingly close.\" He called for more ambition as current NDCs will lead to a rise of 2.7 °C, saying that \"all leaders must recognize that we are in the middle of a climate emergency\". Measurements and statistics 25 January: a review article published in The Cryosphere",
    "label": 0
  },
  {
    "text": "ambition as current NDCs will lead to a rise of 2.7 °C, saying that \"all leaders must recognize that we are in the middle of a climate emergency\". Measurements and statistics 25 January: a review article published in The Cryosphere reported that Earth lost 28 trillion tonnes of ice between 1994 and 2017, 68% being from atmospheric melting and 32% by oceanic melting. The rate of ice loss rose 57% since the 1990s–from 0.8 to 1.2 trillion tonnes per year–raising global sea level 34.6 ±3.1 mm in that time period. 9 February: a study published in Environmental Research concluded that airborne fine particulate matter (PM2.5) caused by burning fossil fuels causes 8.7 million premature deaths annually, including China (2.4 million), India (2.5 million) and parts of eastern US, Europe and Southeast Asia. 16 February: study results published in the PNAS (study's time period: 1990–2018) reported widespread advances and lengthening of pollen seasons (up to 20 days) and increases in pollen concentrations (up to 21%) across North America, with human forcing of the climate system contributing about 50% of the trend in pollen seasons and about 8% of the trend in pollen concentrations. Atlantic meridional overturning circulation (AMOC): 25 February: a Nature Geoscience article reported an \"unprecedented\" (since AD 400) decline in the twentieth century of the Atlantic meridional overturning circulation (AMOC), which is now in its weakest state in more than 1,000 years. The AMOC redistributes heat on the planet and has a major impact on climate. In particular, weakness in the AMOC, which includes the Florida Current and the Gulf Stream, counteracts its moderating effect on the climate in Europe. 5 August: a study published in Nature Climate Change presented \"spatially consistent empirical evidence that, in the course of the last century, the AMOC may have evolved from relatively",
    "label": 0
  },
  {
    "text": "Gulf Stream, counteracts its moderating effect on the climate in Europe. 5 August: a study published in Nature Climate Change presented \"spatially consistent empirical evidence that, in the course of the last century, the AMOC may have evolved from relatively stable conditions to a point close to a critical transition\". February: measurements from Mauna Loa Observatory showed that, for the first time, atmospheric CO2 levels reached 417 parts per million (ppm), a concentration 50% higher than the 278ppm pre-industrial level. March: Global Energy Monitor published a study on coal mine methane emissions which found that hundreds of newly proposed coal mines could emit 13.5 million tonnes of methane annually. 8 March: a study published in Nature Climate Change—studying the combined effects of average global sea level rise and natural and human-induced subsidence—estimated that subsiding coastal locations may locally experience up to four times more relative sea level rise than could be attributed to global sea level rise alone. 17 March: a study by the International Federation of Red Cross and Red Crescent Societies estimated that, globally between September 2020 and February 2021, 12.5 million people were displaced by adverse impacts of climate change, the annual average exceeding 20 million. 17 March: a study published in Nature estimated that trawling's disturbance of carbon stored in sea beds can re-mineralize sedimentary carbon into CO2 amounts equivalent to 15–20% of the atmospheric CO2 absorbed by the ocean each year, and comparable to that of terrestrial farming. 18 March: a study accepted for publication in Environmental Research Letters estimated that the severity of heatwave and drought impacts on crop production in Europe roughly tripled over the preceding 50 years, from –2.2 (1964–1990) to -7.3% (1991–2015). 1 April: a study published in Nature Climate Change estimated that anthropogenic climate change has reduced global agricultural total",
    "label": 0
  },
  {
    "text": "and drought impacts on crop production in Europe roughly tripled over the preceding 50 years, from –2.2 (1964–1990) to -7.3% (1991–2015). 1 April: a study published in Nature Climate Change estimated that anthropogenic climate change has reduced global agricultural total factor productivity by about 21% since 1961, and 26–34% in warmer regions such as Africa, Latin America and the Caribbean. 7 April: NOAA reported carbon dioxide levels were higher than at anytime in the past 3.6 million years, in the Mid-Pliocene Warm Period when sea level was about 24 mm (78 ft) higher than today and the average temperature was about 4 °C (7 °F) higher than in pre-industrial times. NOAA's redefinition of \"average\" and \"normal\": April: NOAA's Climate Prediction Center (CPC) states it will use 1991–2020 as the new 30-year period of record, with \"average\" numbers of named Atlantic storms rising from 12 to 14, hurricanes from 6 to 7, and major hurricanes remaining at 3; Eastern Pacific and Central Pacific numbers remain unchanged over 1981–2010. May: NOAA's NCEI supersedes weather and climate data from 1981–2010 with data from 1991–2020 to change its designation of \"Climate Normal\", resulting in fewer days being characterized as having \"above normal\" temperature. 28 April: a study published in Nature attributed 21±3% of the observed sea-level rise from 2000–2019 to melting glaciers (267±16 gigatonnes per year), and identified a mass loss acceleration of 48±16 gigatonnes per year per decade. 11 May: a study published in Nature Communications estimated that land use change affected 32% of the global land area from 1960 to 2019, about four times greater than previously estimated. 20 May: The Arctic Monitoring and Assessment Programme reported that, from 1971 to 2019, the annually averaged Arctic near-surface air temperature increased by 3.1 °C, three times faster than the global average. 21 May:",
    "label": 0
  },
  {
    "text": "four times greater than previously estimated. 20 May: The Arctic Monitoring and Assessment Programme reported that, from 1971 to 2019, the annually averaged Arctic near-surface air temperature increased by 3.1 °C, three times faster than the global average. 21 May: a study published in Geophysical Research Letters reported that, despite greater raw warming in high latitudes, the tropics have greater normalized warming and actually experienced more record-breaking heat events from 1960 to 2019. 24 May: a study published in Nature Geoscience reported mercury in Greenland ice sheet meltwater being two orders of magnitude higher than from Arctic rivers, and, accounting for about 10% of the estimated global riverine flux, estimated it to be globally significant. 31 May: a study published in Nature Climate Change concluded that 37% of warm-season heat-related deaths from 1991 to 2018 can be attributed to anthropogenic climate change and that increased mortality is evident on every continent. 4 June: a study published in Science Advances concluded that previous estimates of CO2 emissions caused by human cultivation of peatlands from 1750 to 2018 should be increased by 18% to account for emissions from cultivated northern peatlands in calculating the carbon budget. 15 June: a study accepted for publication in Geophysical Research Letters reported that satellite and in situ observations independently show an approximate doubling of Earth's Energy Imbalance (EEI) from mid-2005 to mid-2019. 28 July: a study published in Nature Communications revealed a significant positive global energy imbalance based on satellite observations from 2001 to 2020, and concluded that there is less than 1% probability that this imbalance can be explained by natural internal variability of the climate system. 31 August: the WMO published an Atlas of Mortality and Economic Losses from Weather, Climate and Water Extremes (1970–2019), indicating that the number of disasters has increased by",
    "label": 0
  },
  {
    "text": "imbalance can be explained by natural internal variability of the climate system. 31 August: the WMO published an Atlas of Mortality and Economic Losses from Weather, Climate and Water Extremes (1970–2019), indicating that the number of disasters has increased by a factor of five, driven by climate change, more extreme weather and improved reporting; but because of improved early warnings and disaster management the number of deaths decreased almost three-fold. 1 September: a study published in Nature found that since 2001, fires in the Amazon rainforest had potentially impacted ranges of 77.3–85.2% of threatened species in the region, reducing the biodiversity that contributes to the ecological and climatic stability of the Amazon Basin. 12 October: a study published in the Proceedings of the NAS estimated a nearly 200% increase in urban heat extremes among 13,115 urban areas from 1983 to 2016. October: The Global Coral Reef Monitoring Network's Status of Coral Reefs of the World reported that \"between 2009 and 2018, there was a progressive loss amounting to 14% of the coral from the world's coral reefs, which is more than all the coral currently living on Australia's coral reefs\". 30 March 2022: Ember's Global Electricity Review reported that in 2021, wind and solar power reached a record 10% of global electricity, with clean power being 38% of supply, more than coal's 36%. However, demand growth rebounded, leading to a record rise in coal power and emissions. 7 April 2022: NOAA reported an annual increase in global atmospheric methane of 17 parts per billion (ppb) in 2021—averaging 1,895.7 ppb in that year—the largest annual increase recorded since systematic measurements began in 1983. The increase during 2020 was 15.3 ppb, itself a record increase. Natural events and phenomena 7 February: a rock-ice avalanche in the Chamoli district in the Indian Himalayan",
    "label": 0
  },
  {
    "text": "that year—the largest annual increase recorded since systematic measurements began in 1983. The increase during 2020 was 15.3 ppb, itself a record increase. Natural events and phenomena 7 February: a rock-ice avalanche in the Chamoli district in the Indian Himalayan Mountains killed dozens and left hundreds missing. The death count grew to 204, with 27 million cubic meters of rock and ice collapsing. March: a Science Brief review of >90 peer-reviewed scientific articles reported consensus that ocean warming from human-induced climate change is likely fueling more powerful tropical cyclones with increased precipitation rates (through enhanced atmospheric moisture), the increased power and rising sea levels amplifying flooding. Models project that some regions will experience increases in rapid intensification, a poleward migration of the latitude of maximum intensity or a slowing of the forward motion of the storms. Most climate model studies project the annual number of tropical cyclones to decrease or remain approximately the same. 5 March: an article published in Science concluded that the Atlantic Multidecadal Oscillation is not an internal multidecadal (40- to 60-year) oscillation distinct from climate noise, but is instead a manifestation of competing time-varying effects of anthropogenic greenhouse gases and sulfate aerosols. 11 March: a review article published in Frontiers in Forests and Global Change concluded that warming from non-CO2 agents (especially CH4 and N2O) in the Amazon basin largely offsets—and most likely exceeds—the climate change mitigating effect of the region's CO2 uptake. 22 March: a study published in Geophysical Research Letters concluded that accelerated decline in terrestrial water storage (TWS) caused by glacial ice melting was the main driver of a rapid eastward drift of the geographic North Pole after the 1990s. 26 March: the full bloom date of cherry blossoms in Kyoto, Japan—when the majority of buds are open to the skies—occurred earlier than",
    "label": 0
  },
  {
    "text": "was the main driver of a rapid eastward drift of the geographic North Pole after the 1990s. 26 March: the full bloom date of cherry blossoms in Kyoto, Japan—when the majority of buds are open to the skies—occurred earlier than any time since records began in the year 812 CE; historically, the bloom date occurs about 17 April. 9 April: a study published in Nature Communications citing multiple complementary lines of evidence, reported methane-oxidising bacteria (MOB) dwelling in the bark of Melaleuca quinquenervia (a paper bark tree common in Australia) reduced methane emissions by 36±5%. 13 April: a study of fruitflies published in Nature Communications found that the temperature at which male fertility is lost is much lower than critical thermal limits (CTLs) for survival, suggesting that species, especially tropical species, are more vulnerable to extinction than previously presumed, and that evolution and plasticity are unlikely to rescue populations from extinction. 16 April: Science published results of a study of boreal forests, concluding that forest fires shifted tree dominance from slow-growing black spruce to fast-growing deciduous broadleaf trees, resulting in a net increase in carbon storage and suggesting potential mitigation of the feedback effect of boreal forest fires to global warming. 17 April: winds of Typhoon Surigae rapidly intensified by 170 km/h (105 mph) in 36 hours to reach 306 km/h (190 mph), becoming the strongest tropical cyclone of 2021 worldwide. 14 June: a study published in the Proceedings of the National Academy of Sciences concluded that Rocky Mountain subalpine forests are burning more than at any point in the past 2,000 years, with contemporary rates of burning being 22% higher than the maximum rate reconstructed over the past two millennia. 18 June: a study published in Nature Communications—accounting for sea level rise, storm surge, and wave runup at exposed",
    "label": 0
  },
  {
    "text": "past 2,000 years, with contemporary rates of burning being 22% higher than the maximum rate reconstructed over the past two millennia. 18 June: a study published in Nature Communications—accounting for sea level rise, storm surge, and wave runup at exposed open coasts—estimated that globally aggregated annual overtopping hours had increased by almost 50% over the preceding two decades. Late June: the 2021 Western North America heat wave set a new all-time Canadian temperature record of 49.6 °C (121.28 °F), World Weather Attribution concluding that heat waves of such intensity would be at least 150 times rarer without human-induced climate change. 14 July: a study published in Nature found that the intensification of the dry season and an increase in deforestation seem to promote higher carbon emissions in the eastern Amazon, in line with studies that indicate an increase in tree mortality as a result of climatic changes across Amazonia. 10 August: studying the 2020 heat wave in Siberia, a study published in the PNAS suggested that gas hydrates trapped in carbonate rock formations became unstable, possibly \"add(ing) unknown quantities of methane to the atmosphere in the near future\"—in addition to that long known to be produced from microbial decay of organic matter. 14 August: the >3000 m peak of the Greenland ice sheet experienced rain for the first known time in recorded history, in one of nine instances in the past 2,000 years in which the temperature exceeded the freezing point. 2 March 2023: a study published in Science said that boreal fires, typically accounting for 10% of global fire CO2 emissions, contributed 23% in 2021, by far the highest fraction since 2000. 2021 was an abnormal year because North American and Eurasian boreal forests synchronously experienced their greatest water deficit. Actions, and goal statements Science and technology 8 February:",
    "label": 0
  },
  {
    "text": "CO2 emissions, contributed 23% in 2021, by far the highest fraction since 2000. 2021 was an abnormal year because North American and Eurasian boreal forests synchronously experienced their greatest water deficit. Actions, and goal statements Science and technology 8 February: XPrize announced a competition to bestow its largest-ever prize, $100 million donated by Elon Musk to be awarded in 2025, for technology to remove carbon from air or water. Winning entries must show an ability to scale up to removing billions of metric tons of carbon. February: Porsche announced trials to start in 2022 to develop synthetic fuel that it claims will have the same \"well to wheel impact\"—CO2 produced throughout manufacture and sale—as electric vehicles. February and earlier: Aptera Motors indicated it would produce in 2021 a three-wheel, highly aerodynamic electric vehicle powered by 34 square feet of solar cells, also having rechargeable batteries. Late February: a Cambridge University study estimated that bitcoin mining energy consumption—at that time on the order of 100 terawatt-hours annually—possessed a carbon footprint equivalent to Argentina's, a figure likely increased by interest in bitcoin in early 2021 from major Wall Street institutions. March: The Guardian reported on the design of \"Vortex Bladeless\", a curved-top cylindrical turbine whose main body oscillates resonantly with the wind to generate electricity, the design occupying a much smaller footprint than blade-driven wind turbines. 18 March: a feasibility study published in Nature Sustainability described how suspending solar panels above water canals not only reduces evaporation and mitigates land use, but increases the efficiency of the panels due to the water's cooling effect. Reported 30 March: taking advantage of generally stronger winds further from shore, the world's first floating windfarm, a 30 megawatt facility 15 mi (24 km) off Aberdeenshire, Scotland, broke records for energy output. 31 March: a study published",
    "label": 0
  },
  {
    "text": "cooling effect. Reported 30 March: taking advantage of generally stronger winds further from shore, the world's first floating windfarm, a 30 megawatt facility 15 mi (24 km) off Aberdeenshire, Scotland, broke records for energy output. 31 March: a study published in the PNAS concluded that if food waste is diverted from landfills to avoid methane emissions, food-waste-derived n-paraffin volatile fatty acid-based sustainable aviation fuels could enable up to a 165% reduction in greenhouse gas emissions relative to fossil-derived aviation fuels. 9 April: the World Economic Forum described how companies can use microorganisms to convert CO2 into a protein powder for use in animal feed. 14 May: a study published in Science Advances described a distributed temperature sensing (DTS) system achieving a vertical resolution of ~0.65 m (~25 in.) along a fiber-optic cable, a two-order-of-magnitude improvement over discretely-spaced sensor arrangements. In the Greenland ice sheet, the optical fiber system discovered strong spatial heterogeneity in deformation between and within different ice sections. 8 June: a study published in Environmental Research Letters concluded that artificial ocean alkalinisation (AOA), if carried out with sufficient magnitude and duration, can use current technology to reverse the impact of global ocean acidification on the Great Barrier Reef until atmospheric CO2 concentrations return to today's values—possibly centuries in the future. August reports: in the first customer delivery of its type in history, Swedish company Hybrit said it was delivering \"green steel\" to truck-maker Volvo AB for prototype vehicles, the steel made using renewable electricity and hydrogen rather than coking coal. 8 September: the largest direct air capture plant, collecting about 4,000 tons of atmospheric CO2} a year to store it underground, began operation in Iceland, selling the most expensive carbon offset in the world for as much as almost $1,400 per ton. 5 October: the Nobel Prize in",
    "label": 0
  },
  {
    "text": "collecting about 4,000 tons of atmospheric CO2} a year to store it underground, began operation in Iceland, selling the most expensive carbon offset in the world for as much as almost $1,400 per ton. 5 October: the Nobel Prize in Physics was awarded \"for the physical modeling of Earth's climate, quantifying variability and reliably predicting global warming\" to atmospheric physicist Syukuro Manabe (modeled a 40 km (25 mi) high vertical column) and Klaus Hasselmann (developed a model incorporating stochastics (chaotic systems) and identifying human \"fingerprints\" in climatic effects). Political, economic, legal, and cultural actions From 1 March 2019: the United Nations declared 2021 to be the beginning of the UN Decade on Ecosystem Restoration, having an \"aim of supporting and scaling up efforts to prevent, halt and reverse the degradation of ecosystems worldwide and raise awareness of the importance of successful ecosystem restoration\". 5 January 2021: a Senate run-off election in the U.S. state of Georgia placed the Democratic party in narrow control of both houses of Congress, as both U.S. Senate Democratic candidates from the state of Georgia, Raphael Warnock and Jon Ossoff win those elections, improving Democratic President Biden's prospects for implementing climate-related policies. Both Raphael Warnock and Jon Ossoff are supporters and advocates the Green New Deal, proposed for the United States. 15 January: France's Total—among Europe's top energy companies that had accelerated plans to cut emissions and build large renewable energy businesses—became the first major global energy company to quit the American Petroleum Institute lobby group, whose largest members resisted investor pressure to diversify to renewables. 20 January: on the afternoon of his inauguration, U.S. President Joe Biden signed a letter re-committing the nation to the 2015 Paris climate accord, reversing Donald Trump's withdrawal that took formal effect on 4 November 2020 (the U.S. had been",
    "label": 0
  },
  {
    "text": "20 January: on the afternoon of his inauguration, U.S. President Joe Biden signed a letter re-committing the nation to the 2015 Paris climate accord, reversing Donald Trump's withdrawal that took formal effect on 4 November 2020 (the U.S. had been the only country in the world not signatory to the accord.) The White House website was promptly changed to recite that Biden \"will take swift action to tackle the climate emergency\", reversing Trump's removing mention of greenhouse gas emissions on his first day in office in 2017. 28 January: General Motors said that by 2035 it will end sale of all gasoline and diesel powered passenger cars and light SUVs (excluding medium and heavy duty trucks), and will sell about 30 types of electric vehicles, and planned to halt and review new oil and gas leases on federal lands and waters. January: newly elected U.S. President Joe Biden promised to make the federal government's fleet of 645,000 vehicles 100% all-electric by 2030. Late January: NRG Energy announced that it would be indefinitely shutting down the U.S.'s only remaining facility for carbon capture and storage (CCS), generally presented by the fossil fuel industry as a \"clean coal\" technology. 1 February: ExxonMobil announced it would invest $3 billion through 2025 (about 3% to 4% of its planned annual capital expenditures) on lower-emission energy technologies, primarily carbon capture and storage projects—distinguished from BP and Royal Dutch Shell who are pursuing renewables. Reported in February: Mexico's populist president Andrés Manuel López Obrador indicated intentions to pursue fossil fuel projects and curtail clean energy, pursuing energy sovereignty with state-run bodies and relegating private clean energy companies to a secondary role. 16 February: billionaire philanthropist Bill Gates published the book, How to Avoid a Climate Disaster. 17 February: Ford said that by 2026 its European division,",
    "label": 0
  },
  {
    "text": "energy sovereignty with state-run bodies and relegating private clean energy companies to a secondary role. 16 February: billionaire philanthropist Bill Gates published the book, How to Avoid a Climate Disaster. 17 February: Ford said that by 2026 its European division, with 5% of that region's passenger car market, will offer only electric and plug-in hybrid models, and by 2030 all its passenger cars will run solely on batteries. 2 March: Volvo said that it will convert its entire lineup to battery power by 2030 and will sell them exclusively online—no longer selling cars with internal combustion engines, including hybrids. 25 March: the Supreme Court of Canada ruled constitutional, the Greenhouse Gas Pollution Pricing Act (2018), which required provinces and territories to implement carbon gas pricing systems or adopt one imposed by the federal government. April: JPMorgan Chase set a goal to finance $2.5 trillion over the following 10 years to combat climate change and advance sustainable development, and Citigroup said it would back $1 trillion of similar efforts by 2030. These announcement followed a similar one by Bank of America. 22–23 April: beginning on Earth Day, U. S. President Joe Biden hosted a virtual Leaders Summit on Climate attended by 40 world leaders, aiming to return the U.S. to being a leader in the global effort to reduce greenhouse gas emissions, which CNN called a \"stark departure\" from the Trump administration. 29 April: Germany's Federal Constitutional Court unanimously ruled that the German government must set clear goals for reducing greenhouse gas emissions beyond 2030, stating that existing law placed too much of a burden on future generations to reduce greenhouse gas emissions. 12 May: The U.S. administration granted final approval to the nation's first large-scale offshore wind farm about 15 miles off the coast of Martha's Vineyard, Massachusetts, expected to",
    "label": 0
  },
  {
    "text": "much of a burden on future generations to reduce greenhouse gas emissions. 12 May: The U.S. administration granted final approval to the nation's first large-scale offshore wind farm about 15 miles off the coast of Martha's Vineyard, Massachusetts, expected to generate 800 megawatts (enough to power about 400,000 homes), with an ultimate goal to deploy enough offshore wind turbines by 2030 to power 10 million homes. A June 7 article in The New York Times reported that Europe had 5,400 offshore wind turbines, compared to seven (7) in the United States. 28 May: court and shareholder actions succeeded against Shell Oil (Dutch court ordering Shell to cut emissions by 45% within 10 years), Exxon-Mobil (two climate activist hedge fund candidates receiving board positions), and Chevron (shareholders imposing emissions targets). 11–13 June: leaders at the 47th G7 summit reaffirmed their goal to limit global heating to 1.5 °C and promised to cut collective emissions in half by 2030, but did not clearly lay out a plan to raise $100 billion a year for poorer countries to adopt clean energy, and did not agree on a timeline to end use of coal for electric power. 24 June: the European Parliament approved a landmark law to make the EU's greenhouse gas emissions targets legally binding, setting targets to reduce net EU emissions by 55% by 2030 from 1990 levels and eliminate net emissions by 2050. 15 July: the government of Greenland decided to cease issuing new licenses for oil and gas exploration \"based upon economic calculations, but considerations of the impact on climate and the environment also play a central role in the decision\". 18 August: a study published in Nature, considering the effect of ultraviolet radiation on the growth of plants serving as a carbon sink, estimated that the Montreal Protocol's late",
    "label": 0
  },
  {
    "text": "and the environment also play a central role in the decision\". 18 August: a study published in Nature, considering the effect of ultraviolet radiation on the growth of plants serving as a carbon sink, estimated that the Montreal Protocol's late 1980s prohibition of ozone-depleting chemicals may have prevented an additional 115—235 parts per million of atmospheric CO2, which might have led to a 0.50–1.0 °C increase in global average temperature by 2100. Mid-September: China began enforcing the Kigali Amendment (2016) to the Montreal Protocol, pledging to immediately stop emitting HFC-23, a greenhouse gas 14,600 times more powerful than carbon dioxide. 21 September: China announced it will stop funding overseas coal projects, estimated to affect 54 gigawatts, the cancellation averting about three months worth of global greenhouse gas emissions. 1–12 November: 2021 United Nations Climate Change Conference (COP26), postponed for a year because of the COVID-19 pandemic, takes place in Glasgow, Scotland, resulting in the Glasgow Climate Pact. 10 November: in a case involving mining in a protected region of the Ecuadorian rainforest, the Constitutional Court of Ecuador issued a landmark decision interpreting the country's constitutional provisions to grant rights and confer protections to ecosystems. 8 April 2022: the World Economic Forum reported that for the first time, wind and solar generated more than 10% of electricity globally in 2021, with fifty countries having crossed the 10% threshold. However, power from coal rose 9% to a new record high. Mitigation goal statements 27 January: newly elected U.S. President Joe Biden signed executive orders designed to put the country on a path to 100 percent carbon-free electricity by 2035 and net-zero greenhouse gas emissions by 2050. February: IBM pledged to have net-zero emissions by 2030 (cutting emissions by 65% by 2025 compared to 2010 levels), following similar pledges by Microsoft (to be",
    "label": 0
  },
  {
    "text": "to 100 percent carbon-free electricity by 2035 and net-zero greenhouse gas emissions by 2050. February: IBM pledged to have net-zero emissions by 2030 (cutting emissions by 65% by 2025 compared to 2010 levels), following similar pledges by Microsoft (to be \"carbon negative\" by 2030) and Amazon (net-zero by 2040). 21 April: co-legislators of the European Climate Law reached a provisional agreement on a key element of the European Green Deal, which the European Commission said \"enshrines the EU's commitment to reaching climate neutrality by 2050 and the intermediate target of reducing net greenhouse gas emissions by at least 55% by 2030, compared to 1990 levels\". 22 April: At the 2021 Leaders' Climate Summit on Earth Day, U.S. President Joe Biden announced a new target for the US, aiming to reduce greenhouse gas emissions by 50-52% by 2030 relative to 2005 levels. Adaptation goal statements May: A Carbon Disclosure Project survey found that in 2020, about 43% of 800 surveyed cities (combined population: 400 million) did not have a climate adaptation plan. Public opinion and scientific consensus In January, the United Nations Development Programme released results of the Peoples Climate Vote (1.2 million respondents in over 50 countries), which found that 64% said that climate change was an emergency. In June, the Yale Program on Climate Change Communication and Facebook Data for Good jointly published International Public Opinion on Climate Change, describing beliefs, attitudes, policy preferences, and behaviors of Facebook users in 31 countries and territories worldwide, including knowledge and beliefs, perceived risks, support for government action, economic concerns, and activism. 19 October: based on a review of 3,000 peer-reviewed publications randomly chosen from a dataset of 88,125 published since 2012, a study published in Environmental Research Letters concluded with high statistical confidence that the scientific consensus on human-caused contemporary climate",
    "label": 0
  },
  {
    "text": "19 October: based on a review of 3,000 peer-reviewed publications randomly chosen from a dataset of 88,125 published since 2012, a study published in Environmental Research Letters concluded with high statistical confidence that the scientific consensus on human-caused contemporary climate change exceeds 99% in the peer-reviewed scientific literature. Projections 24 January, the World Economic Forum listed top 10 risks by likelihood (extreme weather as #1, climate action failure as #2, human environmental damage as #3) and by severity (climate action failure as #2, human environmental damage as #6, extreme weather as #8). 9 February: a Communications Earth & Environment article concluded that emissions reductions must increase by 80% beyond nationally determined contributions (NDCs) (from 1% to 1.8% per year) to meet the 2 °C target of the 2015 Paris Convention. 19 February: a study published in Geophysical Research Letters studied 1952–2011 data on the timing of seasons and projected that, by 2100, summer in the northern mid-latitudes will last nearly half a year and winter will last less than 2 months. 8 March: a study published in Nature Geoscience concluded that \"limiting global warming to 1.5 °C will prevent most of the tropics from reaching a TW of 35 °C (95 °F), the limit of human adaptation\". 16 March: the International Renewable Energy Agency's Outlook indicated that energy transition investment would have to increase by 30% over planned investment to a total of US$131 trillion between 2021 and 2050—$4.4 trillion/year—to meet 2050 CO2 reduction targets. 8 April: a study published in Geophysical Research Letters projected that limiting 21st-century warming to 2 °C will halve the Antarctic ice shelf area susceptible to collapsing and disintegrating, compared to the 34% of all Antarctic ice shelf loss projected for 4 °C warming. 9 April: a study published in Science Advances used higher resolution",
    "label": 0
  },
  {
    "text": "2 °C will halve the Antarctic ice shelf area susceptible to collapsing and disintegrating, compared to the 34% of all Antarctic ice shelf loss projected for 4 °C warming. 9 April: a study published in Science Advances used higher resolution climate models that included modeling of ocean eddies, to project that global mean sea level rise at the end of this century would be about 25% lower than previous models. 20 April: a study accepted for publication in Environmental Research Letters concluded that immediately pursuing all presently available methane emission reduction measures could avoid 0.25 °C additional global mean warming by mid-century, and set a path to avoid more than 0.5 °C warming by 2100. 22 April: Swiss re-insurer Swiss Re forecast that, compared to growth levels without climate change, the world will have 11—14% less economic output (as much as $23 trillion less, annually) by 2050. 30 April: a study published in Science Advances projected that the positive feedback effect of crustal rebound as the West Antarctic Ice Sheet melts, could cause an 18% amplification of the 21st century's global mean sea level (GMSL) rise, and 1 meter additional GMSL rise over the next millennium. 5 May: a study published in Nature projected that limiting global warming to 1.5 °C would reduce the land ice contribution to sea level rise by 2100 from 25 cm to 13 cm (from 10 to 6 in.), with glaciers responsible for half the sea level rise contribution. 5 May: a study published in Nature used an observationally calibrated ice sheet–shelf model to project that with 2 °C global warming, Antarctic ice loss will continue at its current pace; but that current policies would allow 3 °C warming and give an abrupt jump around 2060 to an order of magnitude increase in the rate",
    "label": 0
  },
  {
    "text": "that with 2 °C global warming, Antarctic ice loss will continue at its current pace; but that current policies would allow 3 °C warming and give an abrupt jump around 2060 to an order of magnitude increase in the rate of sea-level rise (to 0.5 cm/yr) by 2100. 5 May: a study accepted for publication in Environmental Research Letters reported that greenhouse gas emissions have heated the troposphere and cooled the stratosphere so that stratospheric thickness has shrunk over decades, and projected an additional thinning of 1.3 km by 2080 if Earth follows an RCP 6.0 scenario. 5 May: The United Nations Environment Programme's Global Methane Assessment forecast that human-caused methane emissions can be reduced by up to 45 percent this decade and would avoid nearly 0.3 °C of global warming by 2045, and can be consistent with keeping the 1.5˚C goal for the century. May: Bloomberg NEF projected that by 2027, battery-powered electric vehicle prices would reach price parity with internal combustion engine vehicles in all light vehicle segments in Europe. 20 May: a study published in Nature Communications applied palaeoecological evidence (14,000–3600 years ago) to conclude that alpine areas actually developed less plant biodiversity with the upward advance of forest treelines, the researchers' simulation projecting a substantive decrease in plant biodiversity in response to global warming-related treeline rise. 20 May: the Arctic Monitoring and Assessment Programme reported climate models projecting that the probability of an ice-free Arctic summer is 10 times greater under a 2 °C global warming scenario compared with a 1.5 °C scenario. 26 May: an article published in the Proceedings of the National Academy of Sciences projected that under RCP 8.5 (\"business as usual\" scenario), the temperature experienced by an average human will change more in coming decades than over the past six millennia; the",
    "label": 0
  },
  {
    "text": "article published in the Proceedings of the National Academy of Sciences projected that under RCP 8.5 (\"business as usual\" scenario), the temperature experienced by an average human will change more in coming decades than over the past six millennia; the mean human-experienced temperature rise by 2070 will amount to an estimated 7.5 °C—about 2.3 times the mean global temperature rise; and 3.5 billion people will be exposed to mean annual temperature ≥29.0 °C−presently found in 0.8% of the global land surface (mainly the Sahara) but projected to cover 19% of global land in 2070. 29 July: a study published in Nature Communications estimated that adding 4,434 metric tons of CO2—the lifetime emissions of 3.5 average Americans—will cause one excess death globally between 2020 and 2100. The study included only heat-related mortality impacts, and not indirect impacts such as flooding, storms, and crop failures. Significant publications WGI AR6 (9 August 2021). \"Climate Change 2021 / The Physical Science Basis / Working Group I contribution to the WGI Sixth Assessment Report of the Intergovernmental Panel on Climate Change\" (PDF). IPCC.ch. Intergovernmental Panel on Climate Change. Archived (PDF) from the original on 9 August 2021. (Full report: >250MBytes; all 3,949 pages) Link to Summary for Policymakers (41 pages) Fleming, Sean (19 January 2021). \"These are the world's greatest threats in 2021\". WEForum.org. World Economic Forum. Archived from the original on 20 January 2021. U.N. secretariat (26 February 2021). \"Nationally determined contributions under the Paris Agreement — Synthesis Report (Advance Version)\" (PDF). unfccc.int. United Nations. Archived (PDF) from the original on 26 February 2021. Office of the Director of National Intelligence (March 2021). \"Global Trends 2040 - A More Contested World\" (PDF). DNI.gov. National Intelligence Council. Archived (PDF) from the original on 11 April 2021. Zarnetske, Phoebe L.; Gurevitch, Jessica; Franklin, Janet; Groffman, Peter",
    "label": 0
  },
  {
    "text": "Office of the Director of National Intelligence (March 2021). \"Global Trends 2040 - A More Contested World\" (PDF). DNI.gov. National Intelligence Council. Archived (PDF) from the original on 11 April 2021. Zarnetske, Phoebe L.; Gurevitch, Jessica; Franklin, Janet; Groffman, Peter M.; Harrison, Cheryl S.; Hellmann, Jessica J.; Hoffman, Forrest M.; Kothari, Shan; Robock, Alan; Tilmes, Simone; Visioni, Daniele (13 April 2021). \"Potential ecological impacts of climate intervention by reflecting sunlight to cool Earth\". Proceedings of the National Academy of Sciences. 118 (15) e1921854118. Bibcode:2021PNAS..11821854Z. doi:10.1073/pnas.1921854118. ISSN 0027-8424. PMC 8053992. PMID 33876741. \"Global Methane Assessment / Summary for Decision Makers / Executive Summary\" (PDF). UNEP.org. United Nations Environment Programme. 5 May 2021. Archived (PDF) from the original on 10 August 2021. \"The Role of Critical Minerals in Clean Energy Transitions / World Energy Outlook Special Report\" (PDF). IEA.org. International Energy Agency. May 2021. Archived (PDF) from the original on 7 May 2021. \"Net Zero by 2050 / A Roadmap for the Global Energy Sector\" (PDF). IEA.org. International Energy Agency. 18 May 2021. Archived (PDF) from the original on 18 May 2021. (extract and archive thereof) AMAP Secretariat (20 May 2021). \"Arctic Climate Change Update 2021: Key Trends and Impacts / Summary for Policy Makers\". AMAP.no. Tromsø, Norway: Arctic Monitoring and Assessment Programme. Archived from the original on 20 May 2021. Sachs, Jeffrey D.; Kroll, Christian; Lafortune, Guillaume; Fulleer, Grayson; Woelm, Finn (June 2021). Sustainable Development Report 2021 / The Decade of Action for the Sustainable Development Goals (PDF). Cambridge, U.K.: Cambridge University Press. doi:10.1017/9781009106559. ISBN 978-1-009-10655-9. S2CID 236309770. Archived (PDF) from the original on 23 June 2021. Ripple, William J.; Wolf, Christopher; Newsome, Thomas M.; Gregg, Jillian W.; et al. (28 July 2021). \"World Scientists' Warning of a Climate Emergency 2021\". BioScience. 71 (9) biab079. doi:10.1093/biosci/biab079. hdl:10871/126814. ISSN 0006-3568. Blunden, J.;",
    "label": 0
  },
  {
    "text": "the original on 23 June 2021. Ripple, William J.; Wolf, Christopher; Newsome, Thomas M.; Gregg, Jillian W.; et al. (28 July 2021). \"World Scientists' Warning of a Climate Emergency 2021\". BioScience. 71 (9) biab079. doi:10.1093/biosci/biab079. hdl:10871/126814. ISSN 0006-3568. Blunden, J.; Boyer, T. (August 2021). Blunden, J.; Boyer, T. (eds.). \"State of the Climate in 2020\" (PDF). Bulletin of the American Meteorological Society. 102 (8): S1 – S475. Bibcode:2021BAMS..102Q...1B. doi:10.1175/2021BAMSStateoftheClimate.1. S2CID 238760782. (Executive Summary) WMO Atlas of Mortality and Economic Losses from Weather, Climate and Water Extremes (1970–2019). Geneva: World Meteorological Organization (WMO). August 2021. ISBN 978-92-63-11267-5. Archived from the original on 1 September 2021. (WMO-No. 1267; 90 pp). \"The Sixth Status of Corals of the World: 2020 Report\". GCRMN.net. Global Coral Reef Monitoring Network. October 2021. Archived from the original on 5 October 2021. \"State of the Climate in Africa 2020 (WMO-No. 1275)\". WMO.int. World Meteorological Organization. 19 October 2021. Archived from the original on 19 October 2021. \"National Intelligence Estimate / Climate Change and International Responses Increasing Challenges to US National Security Through 2040 (NIC-NIE-2021-10030-A)\" (PDF). DNI.gov. National Intelligence Council. 21 October 2021. Archived (PDF) from the original on 21 October 2021. UNEP, UNEP DTU Partnership (26 October 2021). \"Emissions Gap Report 2021\" (PDF). UNEP.org. United Nations Environment Programme (UNEP). Archived (PDF) from the original on 27 October 2021. \"State of Climate in 2021: Extreme events and major impacts (Press Release Number: 31102021)\". WMO.int. World Meteorological Organization. 31 October 2021. Washington Post Staff (13 November 2021). \"The Glasgow climate pact, annotated\". The Washington Post. Archived from the original on 14 November 2021. \"State of the Global Climate 2021\". WMO.int. World Meteorological Organization (WMO-No. 1290). 18 May 2022. Archived from the original on 18 May 2022. See also 2021 in the environment and environmental sciences Climatology § History History of",
    "label": 0
  },
  {
    "text": "This article documents events, research findings, scientific and technological advances, and human actions to measure, predict, mitigate, and adapt to the effects of global warming and climate change—during the year 2022. Summaries ~22 January: the International Monetary Fund stated that \"Much larger coordinated global policies—including carbon price floors—will be needed to meet the new goals laid out at the (Nov 2021) Glasgow climate conference and stave off catastrophic global climate change. ... Such national-level measures will need to be reinforced with adequately resourced multilateral climate finance initiatives to ensure that all countries can invest in needed mitigation and adaptation measures.\" 13 September: The United in Science 2022 report is published by the WMO, summarizing latest climate science-related updates and assessing recent climate change mitigation progress as \"going in the wrong direction\". 26 October: At the 30th anniversary of the World Scientists' Warning to Humanity, scientists in a BioScience study concluded that \"We are now at 'code red' on planet Earth\", presenting new or updated information about \"recent climate-related disasters, assess[ed] planetary vital signs, and [...] policy recommendations\". The Global Carbon Project reports that carbon emissions in 2022 remain at record levels, with no sign of the decrease that is needed to limit global warming to 1.5 °C. At the current rate, the carbon that can still be emitted while still meeting the 1.5 °C global goal will likely (at a 50% chance) be emitted within only around nine years. Measurements and statistics 13 January: Australia matched its hottest reliably recorded temperature near the West Australian town of Onslow, registering 50.7 °C (123.3 °F). February: the 2022 Winter Olympics in Beijing was the first to rely 100% on artificial snow, exceeding Pyeongchang (2018, 90%) and Sochi (2014, 80%). If global warming continue the trajectory of the preceding two decades, by 2100",
    "label": 0
  },
  {
    "text": "°C (123.3 °F). February: the 2022 Winter Olympics in Beijing was the first to rely 100% on artificial snow, exceeding Pyeongchang (2018, 90%) and Sochi (2014, 80%). If global warming continue the trajectory of the preceding two decades, by 2100 the winter games were predicted to be unviable at 20 of 21 former host venues. 1 February: a study published in PLOS Climate reported that, in 2019, 57% of the global ocean surface recorded extreme heat, compared to 2% during the Second Industrial Revolution, and that, between the 1980s and 2010s, the global mean normalized heat index increased by 68.23%. Researchers stated that \"many parts of the subtropical and midlatitude regions have reached a near-permanent extreme warming state\". 14 February: a study published in Nature Climate Change concluded that the southwestern North American megadrought that began in 2000 was the driest 22-year period in southwestern North America since at least 800 CE, and forecast that this megadrought would very likely persist through 2022, matching the duration of a late-1500s megadrought. 7 March: researchers report in Nature Climate Change that more than three-quarters of the Amazon rainforest has been losing resilience due to deforestation and climate change since the early 2000s as measured by recovery-time from short-term perturbations (\"critical slowing down\" (CSD)), reinforcing the theory that it is approaching a critical transition. On March 11, INPE reports satellite data that show record-high levels of Amazon deforestation in Brazil for a February (199 km2). 15 March: a Global Energy Monitor report based on mine-level data and modeling determined that coal mining emits 52.3 million tonnes of methane per year, rivaling oil (39 million tonnes) and gas (45 million tonnes), and comparable to the climate impact of the CO2 emissions of all coal plants in China. 24 March: a study published in Frontiers",
    "label": 0
  },
  {
    "text": "52.3 million tonnes of methane per year, rivaling oil (39 million tonnes) and gas (45 million tonnes), and comparable to the climate impact of the CO2 emissions of all coal plants in China. 24 March: a study published in Frontiers in Forests and Global Change review the biophysical mechanisms by which forests influence climate, showing that beyond 50°N large scale deforestation leads to a net global cooling, that tropical deforestation leads to substantial warming from non-CO2-impacts, and that standing tropical forests help cool the average global temperature by more than 1 °C. 30 March: Ember's Global Electricity Review reported that in 2021, wind and solar power reached a record 10% of global electricity, with clean power being 38% of supply, more than coal's 36%. However, demand growth rebounded, leading to a record rise in coal power and emissions. 7 April: NOAA reported an annual increase in global atmospheric methane of 17 parts per billion (ppb) in 2021—averaging 1,895.7 ppb in that year—the largest annual increase recorded since systematic measurements began in 1983. The increase during 2020 was 15.3 ppb, itself a record increase. 12 April: a study of 2020 storms of at least tropical storm-strength published in Nature Communications concluded that human-induced climate change increased extreme 3-hourly storm rainfall rates by 10%, and extreme 3-day accumulated rainfall amounts by 5%. For hurricane-strength storms, the figures increased to 11% and 8%. 26 April: The Global Carbon Budget 2021 (published in Earth System Science Data) concludes that fossil CO2 emissions rebounded by around +4.8% relative to 2020 emissions – returning to 2019 levels, identifies three major issues for improving reliable accuracy of monitoring, shows that China and India surpassed 2019 levels (by 5.7% and 3.2%) while the EU and the US stayed beneath 2019 levels (by 5.3% and 4.5%), quantifies various changes",
    "label": 0
  },
  {
    "text": "levels, identifies three major issues for improving reliable accuracy of monitoring, shows that China and India surpassed 2019 levels (by 5.7% and 3.2%) while the EU and the US stayed beneath 2019 levels (by 5.3% and 4.5%), quantifies various changes and trends, for the first time provides models' estimates that are linked to the official country GHG inventories reporting, and shows that the remaining carbon budget at 1. Jan 2022 for a 50% likelihood to limit global warming to 1.5 °C is 120 GtC (420 GtCO2) – or 11 years of 2021 emissions levels. 26 April: Scientists propose and preliminarily evaluate in Nature Reviews Earth & Environment a likely transgressed planetary boundary for green water in the water cycle, measured by root-zone soil moisture deviation from Holocene variability. A study published one day earlier in Earth's Future integrates \"green water\" along with \"blue water\" into an index to measure and project water scarcity in agriculture for climate change scenarios. 27 April: the second edition of the United Nations Convention to Combat Desertification's Global Land Outlook concluded that \"humans have already transformed more than 70% of the Earth's land area from its natural state, causing unparalleled environmental degradation and contributing significantly to global warming\". May: the Great Barrier Reef Marine Park Authority reported that a March 2022 aerial survey of the park indicated that 91% of the coral reefs showed \"some bleaching\", with bleaching patterns \"largely consistent with the spatial distribution of heat stress accumulation\". 12 May: researchers identify the 425 biggest fossil fuel extraction projects globally, of which 40% as of 2020 are new projects that haven't yet started extraction. They conclude in the Energy Policy study that \"defusing\" these \"carbon bombs\" would be necessary for climate change mitigation of global climate goals. On 17 May, a separate study in",
    "label": 0
  },
  {
    "text": "of 2020 are new projects that haven't yet started extraction. They conclude in the Energy Policy study that \"defusing\" these \"carbon bombs\" would be necessary for climate change mitigation of global climate goals. On 17 May, a separate study in Environmental Research Letters finds that \"staying within a 1.5 °C carbon budget (50% probability) implies leaving almost 40% of 'developed reserves' of fossil fuels unextracted\". On 26 May, a study in Nature Climate Change calculates climate policies-induced future lost financial profits from global stranded fossil-fuel assets. 26 May: a study in Nature Climate Change reveals that storms in the Southern Hemisphere have already reached intensity levels previously predicted to occur only in the year 2080. 3 June: the NOAA reports that the global concentration of carbon dioxide in Earth's atmosphere is now 50% greater than in pre-industrial times, and is likely at a level last seen 4.1 to 4.5 million years ago, at 421 parts per million (ppm). 20 June: a study in Nature Food suggests global food miles CO2 emissions are 3.5–7.5 times higher than previously estimated, with transport accounting for about 19% of total food-system emissions, albeit shifting towards plant-based diets remains substantially more important. 25 June: a study published in Geophysical Research Letters indicates that the Arctic is warming four times faster than global warming now, substantially faster than current CMIP6 models could project. 13 July: A study in Nature affirms (see 7 March) that critical slowing down indicators suggest that tropical, arid and temperate forests are substantially losing resilience. On 4 July, Brazil's INPE reports that the country's regions of the Amazon rainforest have been deforested by a record amount in the first half of 2022. 18 July: a study in Global Change Biology shows that climate change-related exceptional marine heatwaves in the Mediterranean Sea during",
    "label": 0
  },
  {
    "text": "the country's regions of the Amazon rainforest have been deforested by a record amount in the first half of 2022. 18 July: a study in Global Change Biology shows that climate change-related exceptional marine heatwaves in the Mediterranean Sea during 2015–2019 resulted in widespread mass sealife die-offs in five consecutive years. 8 August: a study published in Nature Climate Change found that 58% of infectious diseases confronted by humanity have been at times aggravated by climatic hazards, and that empirical cases revealed 1,006 unique pathways in which climatic hazards led to pathogenic diseases. 22 August: a study published in The Cryosphere estimated that 51.5 ±8.0% of Swiss glacier volume was lost between 1931 and 2016, finding that low-elevation, high-debris-cover, and gently sloping glacier termini are conducive to particularly high mass losses. 1 September: a study published in Nature estimated the social cost of carbon (SCC) to be $185 per tonne of CO2—3.6 times higher than the U.S. government's then current value of $51 per tonne. 3 September: for the first time on record, temperatures at the summit of the Greenland ice sheet exceeded the melting point in September. 29 September: a study published in Science reported that the Arctic Ocean experienced acidification rates three to four times higher than in other ocean basins, attributing the acidification to reduced sea ice coverage on a decadal time scale. Reduced sea ice coverage exposes seawater to the atmosphere and promotes rapid uptake of atmospheric carbon dioxide, leading to sharp declines in pH. 29 September: A study published in Science adds to the accumulating research showing that oil and gas industry methane emissions are much larger than thought. 5 October: a study published by World Weather Attribution concluded that, for the Northern Hemisphere extratropics in 2022, human-induced climate change made drought 20 times worse",
    "label": 0
  },
  {
    "text": "research showing that oil and gas industry methane emissions are much larger than thought. 5 October: a study published by World Weather Attribution concluded that, for the Northern Hemisphere extratropics in 2022, human-induced climate change made drought 20 times worse for root zone soil moisture, and 5 times worse for surface soil moisture. 25 October: The Lancet published a report stating that transitioning to clean energy and improved energy efficiency can prevent 1.2 million annual deaths resulting from exposure to fossil fuel-derived PM2·5 particulates, and that extreme heat due to climate change accounted for an estimated 98 million more people reporting moderate to severe food insecurity in 2020 than the 1981–2010 average. 28 October: a study published in Science Advances estimated that from 1992 to 2013, cumulative global losses due to extreme heat were more than US$16 trillion (likely range: $5–29.3 trillion), also finding that human-caused increases in heat waves depressed economic output most in the poor tropical regions least culpable for warming. 9 November: The largest global inventory and interactive map of greenhouse gas emission sources is released by Climate TRACE. 11 November: a study published in Earth System Science Data estimated that global carbon dioxide emissions from fossil fuels and cement increased by 1.0% in 2022, hitting a new record high of 36.6 billion tonnes of carbon dioxide (GtCO2). December: Christian Aid's Counting the cost 2022: a year of climate breakdown reported climate-related losses for Pakistan flooding ($30 billion), U.S./Cuba Hurricane Ian ($100 billion), Europe/UK heatwaves ($20 billion), with each of the top ten costing at least $3 billion. * 26 January 2023: Bloomberg NEF's \"Energy Transition Investment Trends\" report estimated that, for the first time, energy transition investment matched global fossil fuel investment—$1.1 trillion in 2022, including China with $546 billion, the US with $141 billion, and",
    "label": 0
  },
  {
    "text": "billion. * 26 January 2023: Bloomberg NEF's \"Energy Transition Investment Trends\" report estimated that, for the first time, energy transition investment matched global fossil fuel investment—$1.1 trillion in 2022, including China with $546 billion, the US with $141 billion, and the EU if treated as a bloc, $180 billion. 29 August 2023: an International Renewable Energy Agency (IRENA) publication stated that ~86% (187 GW) of renewable capacity added in 2022 had lower costs than electricity generated from fossil fuels. 28 October 2025: The Lancet estimated that in 2022, 2.52 million deaths globally were attributable to ambient air pollution from burning fossil fuels. Natural events and phenomena 10 March: results of a 22-month study reported in Nature Portfolio's Scientific Reports indicated that several species of coral can survive and cope with future ocean conditions (temperature and acidity) consistent with then-current (late 2021) commitments under the 2015 Paris Climate Agreement, \"provid(ing) hope for future reef ecosystem function globally\". Reported in March: a coral bleaching event caused severe bleaching in 60 percent of the corals in Australia's Great Barrier Reef, in the reef's first such event occurring in a La Niña (cooling) year. 28 April: a study published in Nature stated that climate and land use change will produce novel opportunities for transmission of viruses between previously geographically isolated species of wildlife, so that species will aggregate in new combinations to drive new cross-species transmission of their viruses an estimated 4,000 times. The study concluded that holding warming under 2 °C within the century would not reduce future viral sharing. 27 June: with a small catalog of unknown bacteria, researchers suggest, in a Nature Biotechnology study, work on microbes soon to be released from melting glaciers across the world to identify and understand potential threats in advance and understand extremophiles. 28 June: A",
    "label": 0
  },
  {
    "text": "a small catalog of unknown bacteria, researchers suggest, in a Nature Biotechnology study, work on microbes soon to be released from melting glaciers across the world to identify and understand potential threats in advance and understand extremophiles. 28 June: A review in Environmental Research: Climate elucidates the current state of climate change extreme event attribution science, concluding probabilities and costs-severity of links as well as identifying potential ways for its improvement. 4 July: scientists report in Nature Communications that heatwaves in western Europe are increasing \"three-to-four times faster compared to the rest of the northern midlatitudes over the past 42 years\" and that certain atmospheric dynamical changes can (partly) explain their increase. 25 August: a study published in Scientific Reports concluded that the 2019–2020 Australian wildfires caused an abrupt rise in global mean lower stratosphere temperatures and extended the duration of the Antarctic ozone hole, validating concerns that wildfires intensified by global warming would undo progress achieved through the Montreal Protocol in preserving the ozone layer. Reported 1 September: Swiss Re Institute's economic insights report stated that insured losses from floods doubled to $80 billion globally during 2011–2020 compared to the previous decade, while insurance penetration remained at about 18%. September: stating that climate change is already \"an important threat\", with \"climate change and severe weather\" endangering 34% of species, BirdLife International's State of the World's Birds 2022 reported that 49% of bird species worldwide have declining populations (only 6% are increasing). Actions, and goal statements Science and technology 17 January: researchers published in WIREs Climate Change an argument against solar geoengineering, saying it \"is not governable in a globally inclusive and just manner within the current international political system\", and advocating for an International Non-Use Agreement. 21 January: a transport ship set sail from Australia to Japan with liquid",
    "label": 0
  },
  {
    "text": "solar geoengineering, saying it \"is not governable in a globally inclusive and just manner within the current international political system\", and advocating for an International Non-Use Agreement. 21 January: a transport ship set sail from Australia to Japan with liquid hydrogen in its insulated hold, in what project participants claim is the first time the non-CO2-emitting fuel has been transported by sea to an international market. However, the project producing the hydrogen used brown coal (lignite), a high-emitting energy source. March: the first wind farm in the Mediterranean Sea is being constructed near Taranto, Italy, and is designed to power 21,000 homes. April: Researchers publishing in the International Journal of Information Management argue that although advancements in science and technology are key in providing a solution for global warming, they also have numerous harmful impacts, including E-waste, CO2 emissions, and resource consumption. 23 May: a study in Proceedings of the National Academy of Sciences shows why decarbonization must be accompanied by strategies to reduce the levels of short-lived climate pollutants with near-term effects for climate goals. June: Progress in climate change mitigation (CCM) living review-like works:The living document-like aggregation, assessment, integration and review website Project Drawdown adds 11 new CCM solutions to its organized set of mitigation techniques. The website's modeling framework is used in a study document available in the journal Resources, Conservation and Recycling to show that metal recycling has significant potential for CCM. A revised or updated version of a major worldwide 100% renewable energy proposed plan and model is published in the journal Energy & Environmental Science. July: a 5 MW floating solar park was installed in the Alqueva Dam reservoir, Portugal, enabling solar power and hydroelectric energy to be combined. Separately, a German engineering firm committed to integrating an offshore floating solar farm with an",
    "label": 0
  },
  {
    "text": "Science. July: a 5 MW floating solar park was installed in the Alqueva Dam reservoir, Portugal, enabling solar power and hydroelectric energy to be combined. Separately, a German engineering firm committed to integrating an offshore floating solar farm with an offshore wind farm to use ocean space more efficiently. The projects involve \"hybridization\"—in which different renewable energy technologies are combined in one site. 1 July: Scientists show in One Earth why climate benefits from nature restoration are \"dwarfed by the scale of ongoing fossil fuel emissions\". 5 December: Lawrence Livermore National Laboratory (LLNL) achieved fusion ignition—a reaction producing more energy from nuclear fusion than laser energy used to drive it—for the first time ever, at its National Ignition Facility. The LLNL director projected that it would take \"a few decades of research on the underlying technologies\" to enable a clean-energy power plant to be built. Political, economic, legal, and cultural actions 24 January: BBC Science Focus reported that \"well over 100\" countries had constitutions recognizing a human right to a healthy environment, leading to legal actions and petitions to governments. March: The World Bank issued the world's first wildlife conservation bond, raising $150 million and paying investors returns based on the rate of growth of black rhinoceros populations in South Africa's Addo Elephant National Park and Great Fish River Nature Reserve. 31 March: The first Middle East and North Africa Climate Week (MENACW 2022) concluded in Dubai, United Arab Emirates, after hosting about 4000 participants, 200 sessions, and 500 speakers from 147 countries. 8 April: the World Economic Forum reported that for the first time, wind and solar generated more than 10% of electricity globally in 2021, with fifty countries having crossed the 10% threshold. However, power from coal rose 9% to a new record high. 6 May: the Commission",
    "label": 0
  },
  {
    "text": "that for the first time, wind and solar generated more than 10% of electricity globally in 2021, with fifty countries having crossed the 10% threshold. However, power from coal rose 9% to a new record high. 6 May: the Commission on Human Rights of the Philippines issued a non-binding \"National Inquiry on Climate Change\" stating that countries have a special duty to protect human rights in the context of climate change, and business enterprises have a responsibility, distinct from legal liability, to respect human rights. 27 May: energy and environment ministers from all Group of Seven countries agreed to end taxpayer funding for oil, gas and coal projects overseas. 12 August: The National Centers for Environmental Information publish a report called Assessing the Global Climate in July 2022, where they state an all-time record cold temperature occurred in Australia during the month. On October 7, 2022, Zack Labe, a climate scientist for the NOAA Geophysical Fluid Dynamics Laboratory released a statement and a climate report from Berkeley Earth denying the all-time record cold temperature occurred saying, \"There are still no areas of record cold so far in 2022.\" Labe's statement also denied the record cold temperatures in Brazil, reported by the National Institute of Meteorology in May 2022, a month before the official start of winter, was also not record cold temperatures. 16 August: U.S. President Joe Biden signed into law the Inflation Reduction Act, which contains the largest climate investment by the U.S. federal government in history, including over $430 billion to reduce carbon emissions. The bill, passing by a 51–50 vote in the Senate, explicitly defined carbon dioxide as an air pollutant under the Clean Air Act to make the Act's EPA enforcement provisions harder to challenge in court. 25 August: The California Air Resources Board approved the",
    "label": 0
  },
  {
    "text": "a 51–50 vote in the Senate, explicitly defined carbon dioxide as an air pollutant under the Clean Air Act to make the Act's EPA enforcement provisions harder to challenge in court. 25 August: The California Air Resources Board approved the Advanced Clean Cars II regulation that requires all new cars and light trucks sold in the state of California to be zero-emission vehicles by 2035. 29 August: five climate scientists, joined by a political scientist who studies social movements, wrote in Nature Climate Change to urge colleagues to commit acts of civil disobedience to counter the \"grim trajectory on which the Earth is headed\". 14 September: The WHO joins health associations and scientists in calling for a global fossil fuel non-proliferation treaty to protect lives of current and future generations. Late September: the United Nations Human Rights Committee declared that the Australian government violated the human rights of Indigenous Torres Strait Islanders by failing to adequately protect them from the impacts of climate change, the ruling being the first time a judicial body focused on human rights has told a government to pay for harm caused by climate change. 29 September: Global Witness reported that, in the past decade, more than 1,700 land and environmental defenders were killed, about one every two days. Brazil, Colombia, Philippines, and Mexico were the deadliest countries. 27 October: the International Energy Agency's World Energy Outlook 2022 stated that Russia's invasion of Ukraine \"can be a historic turning point towards a cleaner and more secure energy system thanks to the unprecedented response from governments around the world\". 4 November: the Senate of the French Republic passed a bill requiring solar panels to be installed over outdoor parking lots having more than 80 places, with a 2026 deadline for larger lots and 2028 for others. 6–20",
    "label": 0
  },
  {
    "text": "the world\". 4 November: the Senate of the French Republic passed a bill requiring solar panels to be installed over outdoor parking lots having more than 80 places, with a 2026 deadline for larger lots and 2028 for others. 6–20 November: The 2022 United Nations Climate Change Conference (COP27, in Sharm El Sheikh, Egypt) arrived at a \"loss and damage\" fund for countries most affected by climate change, a development that the BBC said was hailed as a \"historic moment\". However, the conference failed to commit to \"phasing out\" fossil fuels (referring instead to \"low emission and renewable energy\"), and, according to the BBC, \"faltered\" on the 1.5 °C goal of the Paris Agreement. A large presence of representatives from fossil fuel companies influenced the conference. 19 December: 190 countries (excluding the U.S. and Vatican) approved a United Nations Convention on Biological Diversity agreement to protect 30 percent of the planet's land and oceans by 2030, compared to 2022's protection of ~17 percent of land and ~8 percent of oceans. The 2022 pact includes provisions to make targets measurable and to monitor countries' progress. December: sixteen communities in Puerto Rico (U.S.) filed a first of its kind lawsuit against oil and coal companies under the 1970 Racketeer Influenced and Corrupt Organizations (RICO) Act—originally intended to combat criminal enterprises like the mafia—alleging the companies conspired to deceive the public about the climate crisis. 2022: climate change protesters turned increasingly to disruptive tactics, risking arrest and widespread disapproval (e.g., glueing themselves to airport runways or museum artworks, throwing a can of soup at the glass protecting Vincent van Gogh's \"Sunflowers\" painting). 24 August: the International Monetary Fund reported that global fossil fuel subsidies in 2022 were $7 trillion ($13 million per minute), amounting to 7.1% of GDP. Mitigation goal statements 29 June:",
    "label": 0
  },
  {
    "text": "at the glass protecting Vincent van Gogh's \"Sunflowers\" painting). 24 August: the International Monetary Fund reported that global fossil fuel subsidies in 2022 were $7 trillion ($13 million per minute), amounting to 7.1% of GDP. Mitigation goal statements 29 June: Environment ministers for European Union countries reached an agreement to eliminate carbon emissions from new cars by 2035, defining the states' stance for talks with the EU Parliament and European Commission on the Fit for 55 package. 11 October: stating that the energy sector accounts for almost three quarters of greenhouse gas emissions, the World Meteorological Organization quoted the International Energy Agency as stating that energy supply from low-emissions sources must double by 2030 to achieve net zero by 2050. Adaptation goal statements February: The U.S. Army's Climate Strategy includes providing 100% carbon-pollution-free electricity for Army installations' needs by 2030, achieving 50% reduction from 2005 levels in GHG emissions from all Army buildings by 2032, attaining net-zero GHG emissions from Army installations by 2045, fielding an all-electric light-duty non-tactical vehicle fleet by 2027, fielding purpose-built hybrid-drive tactical vehicles by 2035 and fully electric tactical vehicles by 2050, achieving carbon-pollution free contingency basing by 2050, and attaining net-zero GHG emissions from all Army procurements by 2050. 1 November: a key finding of the United Nations Environment Programme's Adaptation Gap Report 2022 is that \"evidence suggests that for developing countries, estimated adaptation costs–and likely adaptation financing needs–could be five to ten times greater than current international adaptation finance flows\". Public opinion and scientific consensus 12 January: a survey conducted by the Yale Program on Climate Change Communication and the George Mason University Center for Climate Change Communication indicated that Americans are \"alarmed\" (33%), \"concerned\" (25%), \"cautious\" (17%), \"disengaged\" (5%), \"doubtful\" (10%), and \"dismissive\" (9%) about climate change. 25 July: in IEEE Access",
    "label": 0
  },
  {
    "text": "Program on Climate Change Communication and the George Mason University Center for Climate Change Communication indicated that Americans are \"alarmed\" (33%), \"concerned\" (25%), \"cautious\" (17%), \"disengaged\" (5%), \"doubtful\" (10%), and \"dismissive\" (9%) about climate change. 25 July: in IEEE Access researchers review the scientific literature on 100% renewable energy, addressing various issues, outlining open research questions, and concluding there to be growing consensus, research and empirical evidence concerning its feasibility worldwide. 29 September: A study published in Nature Sustainability estimates the disproportionality of drivers of climate change by wealth and concludes that to total emissions, investments of the global top 1% are far more important than their consumption and that the pollution gap is larger within countries than between countries. Projections January: the World Economic Forum's Global Risks Perception Survey 2021–2022 listed climate inaction failure, extreme weather, and biodiversity loss as the most severe risks on a global scale over the next 10 years. January: Deloitte published a report forecasting that failing to take sufficient action on climate change could result in economic losses to the US economy of $14.5 trillion(in present-value terms) over the next 50 years, and that decarbonization could catalyze transformational growth in the US economy that could result in $3 trillion added to the economy over that time period. 1 February: a study published in PLOS Climate projected a decline in global thermal refugia for coral reefs from 84% (2022) to 0.2% (at 1.5 °C of global warming), and 0% (at 2.0 °C of global warming), stating that management efforts on thermal refugia may only be effective in the short term. 15 February: NOAA's Global and Regional Sea Level Rise Scenarios said that relative sea level along the contiguous U.S. coastline is expected to rise on average as much over the next 30 years—25 to 30",
    "label": 0
  },
  {
    "text": "effective in the short term. 15 February: NOAA's Global and Regional Sea Level Rise Scenarios said that relative sea level along the contiguous U.S. coastline is expected to rise on average as much over the next 30 years—25 to 30 centimetres (9.8 to 11.8 in)—as it has over the preceding 100 years. 23 February: the United Nations Environment Programme projected that climate change and land-use change will make wildfires more frequent and intense, with a global increase of extreme fires of up to 14% by 2030, 30% by 2050, and 50% by 2100. March: a study published in Urban Climate projected that the air temperature in Singapore would increase to 2.2-3.8 °C in the 2080s using global modelling results that was accommodated to city scale and taking into account the future urbanization projects. 30 March: an American Lung Association report stated that a national shift to 100 percent sales of zero-emission passenger vehicles (by 2035) and medium- and heavy-duty trucks (by 2040), coupled with renewable electricity, would generate over $1.2 trillion in public health benefits and avoid up to 110,000 premature deaths. 28 April: a study published in Science cited ocean warming and oxygen depletion, and concluded that \"under business-as-usual global temperature increases, marine systems are likely to experience mass extinctions on par with past great extinctions based on ecophysiological limits alone\", with polar species at highest risk. 9 May: a World Meteorological Organization update stated that there is a 50:50 chance of the annual average global temperature temporarily reaching 1.5 °C above pre-industrial level for at least one of the ensuing five years; in 2015 that probability was estimated as \"close to zero\". 16 May: a study published in GeoHealth concluded that eliminating energy-related fossil fuel emissions in the United States would prevent 46,900–59,400 premature deaths each year and",
    "label": 0
  },
  {
    "text": "the ensuing five years; in 2015 that probability was estimated as \"close to zero\". 16 May: a study published in GeoHealth concluded that eliminating energy-related fossil fuel emissions in the United States would prevent 46,900–59,400 premature deaths each year and provide $537–$678 billion in benefits from avoided PM2.5-related illness and death. 20 May: a study published in One Earth concluded that rising temperatures will continue to shorten sleep, primarily through delayed onset, increasing the probability of insufficient sleep and impacting human functioning, productivity, and health. Those living in warmer climates were found to lose more sleep per degree of temperature rise, and elderly, women, with residents of lower-income countries being most impacted. 12 August: a study published in Science Advances stated that climate-caused changes in atmospheric rivers affecting California has already doubled the likelihood of megafloods—which can involve 100 inches (250 cm) of rain and/or melted snow in the mountains per month, or 25 to 34 feet (7.6 to 10.4 m) of snow in the Sierra Nevada—and runoff in a future extreme storm scenario is predicted to be 200 to 400% greater than historical values in the Sierra Nevada. 25 August: a study published in Communications Earth & Environment projected that, even if global warming is constrained to within 2.0 °C, by 2100 the \"extremely dangerous\" heat index threshold 124 °F (51 °C) is likely to be exceeded on more than 15 days each year in sub-Saharan Africa, parts of the Arabian peninsula, and much of the Indian subcontinent. Exposure to \"dangerous\" (exceeding 103 °F (39 °C)) heat index levels are projected to likely increase by 50–100% across much of the tropics and increase by a factor of 3–10 in many regions throughout the midlatitudes. 29 August: a study published in Nature Climate Change projected, based on 2000–2019 climatology, that",
    "label": 0
  },
  {
    "text": "are projected to likely increase by 50–100% across much of the tropics and increase by a factor of 3–10 in many regions throughout the midlatitudes. 29 August: a study published in Nature Climate Change projected, based on 2000–2019 climatology, that 3.3% of the Greenland ice sheet will melt, resulting in 274 millimetres (10.8 in) of global sea level rise—with \"most\" of the rise within the 21st century—regardless of how well greenhouse gas release is limited. 9 September: A study published in Science describes how multiple tipping elements in the climate system could be triggered if global warming exceeds 1.5 °C. 26 October: the United Nations' synthesis report of nationally determined contributions estimated that the best estimate of peak temperature in the twenty-first century is 2.1–2.9 °C. Assuming full implementation of NDCs, including all conditional elements, the best estimate for peak global mean temperature is 2.1–2.4 °C. 27 October: the United Nations Environment Programme's Emissions Gap Report 2022 projected that \"policies currently in place with no additional action are projected to result in global warming of 2.8 °C over the twenty-first century. Implementation of unconditional and conditional NDC scenarios reduce this to 2.6 °C and 2.4 °C respectively\". 7 November: Scientists warned in Ecological Monographs about summarized effects of climate change on insects, among other novel stressors, which may \"drastically reduce our ability to build a sustainable future based on healthy, functional ecosystems\", providing several recommended mitigation options. 2 December: the International Energy Agency projected that, in large part because of the 2022 global energy crisis, renewable energy will surpass prior predictions, and will become the largest source of global electricity generation by early 2025, surpassing coal. Significant publications IPCC Sixth Assessment Report, Working Group II: Working Group II (27 February 2022). \"Climate Change 2022 / Impacts, Adaptation and Vulnerability /",
    "label": 0
  },
  {
    "text": "predictions, and will become the largest source of global electricity generation by early 2025, surpassing coal. Significant publications IPCC Sixth Assessment Report, Working Group II: Working Group II (27 February 2022). \"Climate Change 2022 / Impacts, Adaptation and Vulnerability / Summary for Policymakers\" (PDF). IPCC.ch. Intergovernmental Panel on Climate change. Archived (PDF) from the original on 28 February 2022. (36 pages; 10 MB) Working Group II (27 February 2022). \"Technical Summary\" (PDF). IPCC.ch. Intergovernmental Panel on Climate Change. Archived (PDF) from the original on 28 February 2022. Accepted, subject to final edits (96 pages; 20 MB) Working Group II (27 February 2022). \"Climate Change 2022 / Impacts, Adaptation and Vulnerability (full report)\" (PDF). IPCC.ch. Intergovernmental Panel on Climate Change. Archived (PDF) from the original on 28 February 2022. (3675 pages; 280 MB) IPCC Sixth Assessment Report, Working Group III: Working Group III (4 April 2022). \"Climate Change 2022 / Mitigation of Climate Change / Summary for Policymakers\" (PDF). IPCC.ch. Intergovernmental Panel on Climate Change. Archived (PDF) from the original on 4 April 2022. (64 pages; 5 MB) Working Group III (4 April 2022). \"WG III contribution to the Sixth Assessment Report / Technical Summary\" (PDF). IPCC.ch. Intergovernmental Panel on Climate Change. Archived (PDF) from the original on 4 April 2022. (145 pages; 10 MB) Working Group III (4 April 2022). \"Climate Change 2022 / Mitigation of Climate Change / Full Report\" (PDF). IPCC.ch. Intergovernmental Panel on Climate Change. Archived (PDF) from the original on 4 April 2022. (2913 pages; 88 MB) \"State of the Global Climate 2021\". WMO.int. World Meteorological Organization (WMO-No. 1290). 18 May 2022. Archived from the original on 18 May 2022. \"Global Electricity Review 2022\" (PDF). Ember. March 2022. Archived (PDF) from the original on 30 March 2022. \"Nationally determined contributions under the Paris Agreement / Synthesis",
    "label": 0
  },
  {
    "text": "Organization (WMO-No. 1290). 18 May 2022. Archived from the original on 18 May 2022. \"Global Electricity Review 2022\" (PDF). Ember. March 2022. Archived (PDF) from the original on 30 March 2022. \"Nationally determined contributions under the Paris Agreement / Synthesis report by the secretariat\" (PDF). UNFCCC.int. United Nations Framework Convention on Climate Change. 26 October 2022. p. 30. Archived (PDF) from the original on 26 October 2022. \"Emissions Gap Report 2022 / The Closing Window\". United Nations Environment Programme. 27 October 2022. Archived from the original on 27 October 2022. (includes download link) \"World Energy Outlook 2022\" (PDF). IEA.org. International Energy Agency. 27 October 2022. Archived (PDF) from the original on 27 October 2022. \"Adaptation Gap Report 2022 / Too Little, Too Slow / Climate Adaptation Failure Puts World at Risk\". UNEP.org. United Nations Environment Programme. 1 November 2022. p. 18. Archived from the original on 1 November 2022. \"WMO Provisional State of the Global Climate 2022\". WMO.int. World Meteorological Organization. 6 November 2022. Archived from the original on 6 November 2022. Friedlingstein, Pierre; O'Sullivan, Michael; Jones, Matthew W.; Andrew, Robbie M.; et al. (11 November 2022). \"Global Carbon Budget 2022 / Data Description Paper\". Earth System Science Data. 14 (11): 4811–4900. Bibcode:2022ESSD...14.4811F. doi:10.5194/essd-14-4811-2022. hdl:20.500.11850/594889. \"Arctic Report Card 2022 / The warming Arctic reveals shifting seasons, widespread disturbances, and the value of diverse observations\" (PDF). NOAA.gov. Arctic Program, National Oceanic and Atmospheric Administration. 13 December 2022. Archived (PDF) from the original on 13 December 2022. 21 April 2023: \"State of the Global Climate 2022\". WMO.int. World Meteorological Organization (WMO). 21 April 2023. Archived from the original on 21 April 2023. See also 2022 in the environment 2022 in environmental sciences 2022 in science Climatology § History History of climate change policy and politics History of climate change science Politics of",
    "label": 0
  },
  {
    "text": "This article documents events, research findings, scientific and technological advances, and human actions to measure, predict, mitigate, and adapt to the effects of global warming and climate change—during the year 2023. Summaries 6 February: U.N. Secretary-General António Guterres said \"I have a special message for fossil fuel producers and their enablers scrambling to expand production and raking in monster profits: If you cannot set a credible course for net-zero, with 2025 and 2030 targets covering all your operations, you should not be in business.\" 20 March – The final synthesis of the IPCC Sixth Assessment Report is published. It summarises the state of knowledge relating to climate change with assessed levels of confidence. Conclusions in the summary for contemporary policy-makers include that the extent to which both current and future generations will be impacted depends on choices now and in the near-term, with \"high confidence\" that policies implemented by the end of 2020 are \"projected to result in higher global GHG emissions in 2030 than emissions implied by NDCs\" and would fail to meet global climate goals. 6 September: U.N. Secretary-General António Guterres said \"Our planet has just endured a season of simmering — the hottest summer on record. Climate breakdown has begun.\" 24 October: BioScience's \"2023 state of the climate report\" stated that \"we must shift our perspective on the climate emergency from being just an isolated environmental issue to a systemic, existential threat\". 27 December: Inside Climate News summarized the year: \"The push and pull of progress and catastrophe made 2023 one of the most discordant—and consequential—years for the world's climate. ... In 2023, clean energy progress and the horrors of a radically warming climate fought almost to a draw.\" Measurements and statistics 3 January: the National Snow and Ice Data Center reported that Antarctic sea ice extent",
    "label": 0
  },
  {
    "text": "the world's climate. ... In 2023, clean energy progress and the horrors of a radically warming climate fought almost to a draw.\" Measurements and statistics 3 January: the National Snow and Ice Data Center reported that Antarctic sea ice extent stood at the lowest in the 45-year satellite record—more than 500,000 square kilometers (193,000 square miles) below the previous record (2018), with four of the five lowest years for the last half of December having occurred since 2016. 26 January: Bloomberg NEF's \"Energy Transition Investment Trends\" report estimated that, for the first time, energy transition investment matched global fossil fuel investment—$1.1 trillion in 2022, including China with $546 billion, the US with $141 billion, and the EU if treated as a bloc, $180 billion. 3 April: An unexplained rise of emissions of five chlorofluorocarbons (CFCs), successfully banned by the Montreal Protocol of 1989, is reported in Nature Geoscience. Their climate impact in 2020 is roughly equivalent to that of the CO2e from Denmark in 2018. Reported 10 May: Drax Electric Insights reported that in the first three months of 2023, Britain's wind turbines generated more electricity (32.4%) than gas-fired power stations (31.7%) for the first time. 18 May: a study published in Science reported that more than 50% of freshwater lakes and reservoirs lost volume from 1992 to 2020. 31 May: an international study in Nature, using modelling and literature assessment, codifies, integrates into and quantifies \"safe and just Earth system boundaries\" (ESBs) with the context of Earth system stability and minimization of human harm. They expand upon earlier boundary frameworks by incorporating concepts such as intra- and intergenerational justice, propose that their framework may better enable a quantitative foundation for safeguarding the global commons, and report many of the ESBs are already exceeded. 15 June: the Copernicus Climate Change",
    "label": 0
  },
  {
    "text": "frameworks by incorporating concepts such as intra- and intergenerational justice, propose that their framework may better enable a quantitative foundation for safeguarding the global commons, and report many of the ESBs are already exceeded. 15 June: the Copernicus Climate Change Service said that for 11 days, global surface air temperatures had risen to 1.5 °C (34.7 °F) above pre-industrial levels for the first time—the limit aspired to in the 2015 Paris Agreement—the rise occurring near the beginning of an El Niño warming phase. 24 July: the National Data Buoy Center recorded an unprecedented temperature of 101.1 °F (38.4 °C) at a depth of 5 feet (1.5 m) in Florida Bay, Florida, US, raising concerns about catastrophic coral bleaching. 8 August: in coastal Iran, the heat index reached 70 °C (158 °F). 29 August: an International Renewable Energy Agency (IRENA) publication stated that ~86% (187 GW) of renewable capacity added in 2022 had lower costs than electricity generated from fossil fuels. 31 August: an article in Geophysical Research Letters reported a March 2022 \"unprecedented heatwave\" in the Antarctic reaching 39 °C (70 °F) above average—the largest temperature anomaly ever recorded globally—attributed 2 °C of the increase to global warming, and projected possible heatwaves of an additional 5–6 °C (9.0–10.8 °F) warmer by 2100. 29 September: a study published in Nature Communications estimated the global costs of extreme weather attributable to climate change in the last twenty years to be US$143 billion per year, 63% of which is due to human loss of life. 19 October: a study published in Scientific Reports said that the number of North Atlantic tropical cyclones that intensify from a Category 1 into a major hurricane within 36 hours, has more than doubled from 1971-1990 to 2001-2020. 20 November: the Copernicus European Centre for Medium-Range Weather Forecasts",
    "label": 0
  },
  {
    "text": "said that the number of North Atlantic tropical cyclones that intensify from a Category 1 into a major hurricane within 36 hours, has more than doubled from 1971-1990 to 2001-2020. 20 November: the Copernicus European Centre for Medium-Range Weather Forecasts reported that 17 November was the first day that the global average surface temperature exceeded pre-industrial levels by more than 2°C. 29 November: a study published in The BMJ concluded that about 5.13 million excess deaths per year globally are attributable to ambient air pollution from fossil fuel use. 5 December: the annual Global Carbon Budget study finds fossil CO2 emissions are still rising when if they stayed the same, the 50% likelihood to limit global warming to 1.5 °C would be exceeded around 2031. 8 May 2024 (reported): Ember reported that for the first time, renewable energy generated a 30% of global electricity in 2023. 28 August 2024: a study published in the journal Nature concluded that the June-September 2023 Canadian wildfires caused carbon emissions that exceeded annual fossil fuel emissions of all nations except India, China and the US. 10 September 2025: a study published in Nature attributed 3,400–7,400 acute deaths in North America and 37,800–90,900 chronic deaths in North America and Europe to PM2.5 (fine particulate matter) exposure from the 2023 Canadian wildfires. Natural events and phenomena 7 February: a study published in Nature Communications concluded that 15 million people globally are exposed to impacts from potential glacial lake outburst floods (GLOFs), more than half being from India, Pakistan, Peru, and China. Climate change has intensified glacial ice melt and expanded glacial lakes. 13 February: a study published in the Proceedings of the National Academy of Sciences reported that increasing abundance of a thermotolerant symbiotic alga hosted by corals has facilitated maintenance of high coral cover after",
    "label": 0
  },
  {
    "text": "ice melt and expanded glacial lakes. 13 February: a study published in the Proceedings of the National Academy of Sciences reported that increasing abundance of a thermotolerant symbiotic alga hosted by corals has facilitated maintenance of high coral cover after three mass coral bleaching events, suggesting that future reefs might maintain high cover for several decades, albeit with low diversity and provided that other stressors are minimized. 2 March: a study published in Science said that boreal fires, typically accounting for 10% of global fire CO2 emissions, contributed 23% in 2021, by far the highest fraction since 2000. 2021 was an abnormal year because North American and Eurasian boreal forests synchronously experienced their greatest water deficit. 13 March: a study published in Nature Water found that total intensity of extreme events (droughts and pluvials (rainfall events)) is strongly correlated with global mean temperature, and concluded that continued warming of the planet will cause more frequent, more severe, longer and/or larger of such extreme events, and that \"distortion of the water cycle... will be among the most conspicuous consequences of climate change\". 15 February: Two joint studies by the British Antarctic Survey and the US Antarctic programme finds that glaciers on the icy continent may be more sensitive to changes in sea temperature than previously thought. Researchers used sensors and an underwater robot beneath the Thwaites glacier to study melting. One day earlier, a new record low Antarctic sea ice extent is reported by the National Snow and Ice Data Center in the US, beating the previous record set a year earlier. 29 March: a study published in Nature concluded that under a high-emissions scenario, abyssal warming is set to accelerate over the next 30 years, and that meltwater input around Antarctica drives a contraction of Antarctic Bottom Water (AABW), opening",
    "label": 0
  },
  {
    "text": "earlier. 29 March: a study published in Nature concluded that under a high-emissions scenario, abyssal warming is set to accelerate over the next 30 years, and that meltwater input around Antarctica drives a contraction of Antarctic Bottom Water (AABW), opening a pathway that allows warm circumpolar deep water greater access to the continental shelf and results in warming and aging of the abyssal ocean. The study described the \"critical importance of Antarctic meltwater in setting the abyssal ocean overturning, with implications for global ocean biogeochemistry and climate that could last for centuries\". On 25 May, observational evidence for problematic fast slowdown of the Antarctic bottom water current is presented in Nature Climate Change. 7 April: citing reduced air density caused by global warming, a study published in the Bulletin of the American Meteorological Society estimated global warming has enabled more than 500 excess home runs in Major League Baseball since 2010, and projected hundreds more in this century, explaining that \"even the elite billion-dollar sports industry is vulnerable to unexpected impacts\" of global warming. 5 June: a study published in Current Biology estimated that fungi can fix (remove from the atmosphere) the equivalent of ~36% of global fossil fuel Greenhouse gas emissions. 8 June: NOAA published an \"ENSO update\" declaring that \"El Niño is here\", estimating the odds of it becoming a strong event (56%), at least a moderate event (84%), and \"fizzling out\" (4-7%). 11 June: Fluchthorn, a mountain between Switzerland and Austria, experienced a landslide of 3.5 million ft3 (99,000 m3) and a loss of 60 feet (18 m) in height, that has been attributed to melting of permafrost. 20 June (reported): the Panama Canal is experiencing its lowest rainfall since inception, lowering water levels and requiring restrictions for some vessels to limit their cargo by about 25%",
    "label": 0
  },
  {
    "text": "m) in height, that has been attributed to melting of permafrost. 20 June (reported): the Panama Canal is experiencing its lowest rainfall since inception, lowering water levels and requiring restrictions for some vessels to limit their cargo by about 25% to maintain a safe draft and avoid running aground. 4 July: the WMO formally declared \"onset of El Niño conditions\", projecting it to be \"at least of moderate strength\". 11 July: a study of Chicago structures published in Communications Engineering found that in urban settings, subsurface heat islands caused by global warming cause significant deformations and displacements that may be \"incompatible with the operational requirements of civil structures\". 28 July: Yale Environment 360 reported that, adding to ongoing climate change's dominant warming influence, additional factors contributing to current temperature increases include: (1) 2022 eruption of an underwater volcano near Tonga, vaporizing large amounts of sea water and contributing an estimated 0.03 °C (0.054 °F) of warming, (2) solar radiance increasing towards its ~2025 11-year peak when it may contribute 0.05 °C (0.090 °F) of warming, and (3) the Pacific Ocean entering its El Niño phase, projected to contribute 0.14 °C (0.25 °F) of warming. 23 August (reported): the Panama Canal experienced an unprecedented dry season causing a decline in water levels and prompting canal administrators to limit daily vessel passages from 36 to 32, and forcing some ships to carry up to 40% less cargo to avoid hitting the bottom. 24 August: a study published in Nature Communications concluded that tropical cyclone rapid intensification (RI) events in offshore areas within 400 kilometres (250 mi) of coastlines, tripled in frequency from 1980 to 2020. 8 September (date of report): for the first time in recorded history, all seven tropical ocean basins saw cyclones/hurricanes reach Category 5 strength in the same year.",
    "label": 0
  },
  {
    "text": "400 kilometres (250 mi) of coastlines, tripled in frequency from 1980 to 2020. 8 September (date of report): for the first time in recorded history, all seven tropical ocean basins saw cyclones/hurricanes reach Category 5 strength in the same year. 13 September: a study published in Communications Earth & Environment concluded that, while for many years Antarctic sea ice had increased, from recent record lows in Antarctic sea ice coverage \"it appears that we may now be seeing the inevitable decline, long projected by climate models\", and that a \"regime shift\" may be taking place \"in which previously important relationships no longer dominate sea ice variability\". 13 September: a study published in Science Advances indicated that six of the nine \"planetary boundaries\"—delimiting the \"safe operating space\"—had been exceeded. Carbon dioxide concentration and radiative forcing were among the boundaries that had been exceeded. 27 September: studying tropical cyclones from 1981–2017, a study published in Nature found that cyclones formed almost two weeks sooner, on average, which authors said was \"closely related to the seasonal advance of rapid intensification events\". The time advance shifts cyclones from autumn into summer, increasing overlap with the peak rainfall season. 6 December: a study published in Nature Geoscience said that marine methane hydrate—an ice-like substance found in sediment beneath water depths greater than ~450–700 m—can vent into the ocean to such a degree that it should considered for estimating climate change-induced release of methane, a greenhouse gas. Actions and goal statements Science and technology 8 February: Scientists in the U.S. propose mining the lunar soil and launching it towards the Sun to form a shield (space sunshade) against global warming. 3 March: After a study (31 Jan) indicated that in building heating in the EU, the feasibility of staying within planetary boundaries is possible only through",
    "label": 0
  },
  {
    "text": "launching it towards the Sun to form a shield (space sunshade) against global warming. 3 March: After a study (31 Jan) indicated that in building heating in the EU, the feasibility of staying within planetary boundaries is possible only through electrification, with green hydrogen heating being 2–3 times more expensive than heat pump costs, a study indicates that replacing gas boilers with heat pumps is the fastest way to cut German gas consumption. 17 April – A study in Earth System Science Data expands upon the international Earth heat inventory from 2020, which provides a measure of the Earth energy imbalance (EEI) and allows for quantifying how much and where heat has accumulated in the Earth system with comprehensive data. It suggests that the EEI is the \"most fundamental global climate indicator\" to gauge climate change mitigation efforts. 8 May: a study published in the Proceedings of the National Academy of Sciences concluded that studies extending the reach of \"vertical fingerprinting\" to the mid to upper stratosphere provide \"incontrovertible evidence of anthropogenic impact on Earth's climate\". 3 August (reported): floating, vertical-axis wind turbines with a \"twirling\" operation generate power regardless of wind direction. 17 October: a study published in Nature Communications concluded that, subject to various uncertainties, \"a global irreversible solar tipping point may have passed where solar energy gradually comes to dominate global electricity markets, without any further climate policies\". Political, economic, legal, and cultural actions 1 January: Extinction Rebellion made a statement that for 2023 it had made \"a controversial resolution to temporarily shift away from public disruption as a primary tactic\", after 2022's traffic blockages and throwing soup on the case of Vincent van Gogh's \"Sunflowers\" painting. 5 January: A paywalled meta-analysis in Nature Climate Change reports \"required technology-level investment shifts for climate-relevant infrastructure until 2035\" within",
    "label": 0
  },
  {
    "text": "as a primary tactic\", after 2022's traffic blockages and throwing soup on the case of Vincent van Gogh's \"Sunflowers\" painting. 5 January: A paywalled meta-analysis in Nature Climate Change reports \"required technology-level investment shifts for climate-relevant infrastructure until 2035\" within the EU, which it finds to be \"most drastic for power plants, electricity grids and rail infrastructure\", ~87€ billion above the planned budgets and in need of sustainable finance policies. 11 January: the French National Assembly adopted the Acceleration of Renewable Energies bill, which includes a requirement to install solar panels on all car parks (parking lots) of over 1,500 square metres (16,100 square feet). 12 January: A study in Environmental Research Letters suggests that applying the principle of extended producer responsibility to fossil fuels could deconflict energy security and climate policy at an affordable cost, in particular authors suggest the responsibility could be used to establish the financing of CO2 storage and nature-based solutions. 25 January: A paper in Harvard Environmental Law Review suggests that according to already-existing law fossil fuel companies may be chargeable with homicide due to climate change effects and e.g. partly their deception of the public and proactive prevention of regulations or adequate regulations. The paper is focused on corporate actors and does not address e.g. politicians' and policymakers' responsibilities, economic pressures or incentives, and responsibilities for solutions to these underlying economic issues. 14 February: the European Parliament effectively banned sale of new petrol and diesel cars in the European Union from 2035, and set a 55% cut over 2021 CO2 emission levels for new cars sold from 2030. 19 February: A study in Ethics, Policy & Environment reports that rationing has been neglected as a policy option for mitigating climate change, and, partly based on historical data and economic analysis, concludes that such personal",
    "label": 0
  },
  {
    "text": "cars sold from 2030. 19 February: A study in Ethics, Policy & Environment reports that rationing has been neglected as a policy option for mitigating climate change, and, partly based on historical data and economic analysis, concludes that such personal carbon allowances (PCAs) could help states reduce emissions rapidly and fairly. March: the UN 2023 Water Conference was held in New York. 21 April: a review study published in One Earth stated that opinion polls show that most people perceive climate change as occurring now and close by. The study concluded that seeing climate change as more distant does not necessarily result in less climate action, and reducing psychological distancing does not reliably increase climate action. 21 April: the Director General of the United Nations' International Organization for Migration said that there are more people displaced because of climate change than because of conflicts, explaining that climate change and conflict interact as triggers of displacement. 24 April: A policy study in Nature Communications identifies reduction of car travel activity as the most important transportation policy option in reducing GHG emissions to levels comparable to carbon budget levels, with a \"decrease car distance driven and car ownership by over 80% as compared to current levels\" by 2027 being effective in \"edging close to the designated carbon budget\" in their case-study of London and electrification being highly insufficient. 19 May: a study published in One Earth estimated that the top 21 fossil fuel companies will owe cumulative climate reparations of $5.4 trillion over the period 2025–2050. 12 June: the trial phase of Held v. Montana, the first constitutional climate trial in US, began in the U.S. state of Montana. Sixteen young residents filed the suit based on the state's 1972 constitution requiring that the \"state and each person shall maintain and improve",
    "label": 0
  },
  {
    "text": "Held v. Montana, the first constitutional climate trial in US, began in the U.S. state of Montana. Sixteen young residents filed the suit based on the state's 1972 constitution requiring that the \"state and each person shall maintain and improve a clean and healthful environment in Montana for present and future generations\". On August 14, 2023, the trial court judge ruled in the youth plaintiffs' favor, though the state indicated it would appeal the decision. 23 June: a global summit on finance and climate ended in Paris without creating a tax on greenhouse gas emissions from maritime transport, or fulfilling promises to transfer money to poor countries through the International Monetary Fund. 11 July: A study suggests in One Earth that carbon taxation approaches or instruments would be more effective and fairer when distinguishing between luxury- and basic goods and services. A separate study (17 July) in Nature Energy finds that for energy demand reduction (EDR), \"capping energy use of the top quintile of consumers\" would be effective, more equitable, and increase public acceptance of transformative climate action in Europe. July: at a meeting in Chennai, India, G20 climate and environment ministers did not come to agreement on four of 68 points considered, including achieving peak emissions by 2025, converting to clean energy, or taxing carbon. 14 August: the trial court judge in Held v. Montana ruled in favor of the youth plaintiffs, declaring certain Montana laws violated Montana's state constitution. The judge's 103-page ruling concluded that the \"MEPA Limitation violates Youth Plaintiffs' right to a clean and healthful environment and is unconstitutional on its face\". 17 August: Scientists publish in PLOS Climate what could be the first study both investigating climate-polluting investments and proposing taxation thereof as transformative revenue for climate finance, i.a. indicating \"40% of total U.S. emissions",
    "label": 0
  },
  {
    "text": "and is unconstitutional on its face\". 17 August: Scientists publish in PLOS Climate what could be the first study both investigating climate-polluting investments and proposing taxation thereof as transformative revenue for climate finance, i.a. indicating \"40% of total U.S. emissions were associated with income flows to the highest earning 10% of households\" in 2019 with a growing emissions inequality. 18 August: A study in One Earth investigating public policies and spending as well as lobbying activities regarding a transition to a sustainable food system finds that governments \"largely ignore the climate-mitigation potential of animal product analogs\" and that food production has 'lock-in' problems. 24 August: the International Monetary Fund reported that global fossil fuel subsidies in 2022 were $7 trillion ($13 million per minute), amounting to 7.1% of GDP. 6 September: the first Africa Climate Summit concluded with the \"Nairobi Declaration\", in which African leaders requested global taxes on carbon pollution, phasing out coal use, and ending fossil fuel subsidies. 6 September: A study in PLOS Climate using a global food system model suggests that net-negative greenhouse gas emissions could be possible in a sustainable food system achievable with full global deployment of diverse interventions, with the most promising options including hydrogen-powered fertilizer production, livestock feeds, organic and inorganic soil amendments, agroforestry, sustainable seafood harvesting practices, and adoption of flexitarian diets. 25 October: over 200 health journals called on the World Health Organization to declare the \"indivisible climate and nature crisis\" a global health emergency, saying that treating the climate crisis and the nature crisis as separate challenges is \"a dangerous mistake\". 30 November − 12 December: the 2023 United Nations Climate Change Conference (COP28) convened in Dubai, United Arab Emirates. Controversial because Emirati oil executive Sultan Al Jaber presided over the conference and because of the presence of record",
    "label": 0
  },
  {
    "text": "dangerous mistake\". 30 November − 12 December: the 2023 United Nations Climate Change Conference (COP28) convened in Dubai, United Arab Emirates. Controversial because Emirati oil executive Sultan Al Jaber presided over the conference and because of the presence of record numbers of fossil fuel lobbyists, COP28 was the first COP to explicitly call on participants to \"transition away from fossil fuels\" so as to achieve net zero emissions by 2050, though not adopting calls for a \"complete phase-out\" of fossil fuels. Mitigation goal statements 15 April: a communique from a meeting of G7 ministers pledged to collectively increase offshore wind capacity by 150 gigawatts by 2030 and solar capacity to more than 1 terawatt, and agreed to accelerate the phase-out of unabated (without recapture) fossil fuels to achieve net zero by 2050. They stopped short of endorsing a 2030 deadline for phasing out coal, and left the door open for continued investment in gas to help address potential energy shortfalls. 19 May: a policies study review in One Earth, based on a systematic examination of existing methane policies across sectors, concludes that both only \"about 13% of methane emissions are covered by methane mitigation policies and that the effectiveness of these policies \"is far from clear\". Consensus 8 June: a study published in PLOS Climate studied defensive and secure forms of national identity—respectively called \"national narcissism\" and \"secure national identification\"—for their correlation to support for policies to mitigate climate change and to transition to renewable energy. The researchers concluded that secure national identification tends to support policies promoting renewable energy; however, national narcissism was found to be inversely correlated with support for such policies—except to the extent that such policies, as well as greenwashing, enhance the national image. Right-wing political orientation, which may indicate susceptibility to climate conspiracy beliefs, was",
    "label": 0
  },
  {
    "text": "however, national narcissism was found to be inversely correlated with support for such policies—except to the extent that such policies, as well as greenwashing, enhance the national image. Right-wing political orientation, which may indicate susceptibility to climate conspiracy beliefs, was also concluded to be negatively correlated with support for genuine climate mitigation policies. 7 August: A global survey study of climate policy researchers, published in Nature Sustainability, finds these experts substantially doubt the prevailing green growth narrative, \"underscor[ing] the importance of considering alternative post-growth perspectives\" that include approaches of agrowth and degrowth. Projections 2 January: a study published in Earth's Future (American Geophysical Union) concluded that the greatest increase in the amount of coastal area below mean sea level will occur in the early stages of sea level rise (SLR), contrary to earlier assessments, shortening time for adaptation efforts. Latest projections indicate that SLR is certain to exceed 2 metres (6.6 ft) in coming centuries, and a rise by 4 metres (13 ft) is considered possible. 5 January: a study published in Science stated that, based on then-current pledges, global mean temperature is projected to increase by +2.7 °C, which would cause loss of about half of Earth's glaciers by 2100, causing a sea level rise of 115±40 millimeters (not counting ice sheet melt). 30 January: Climate scientists predict, using artificial intelligence, in Proceedings of the National Academy of Sciences that global warming will exceed 1.5 °C in the next decade (scenario SSP2-4.5), and a nearly 70% chance of 2 °C between 2044 and 2065 (~2054)—a substantial probability of exceeding the 2 °C threshold—even if emissions rapidly decline (scenario SSP1-2.6). 30 January: A study in Nature Sustainability outlines challenges of aviation decarbonization by 2050 whose identified factors mainly are future demand, continuous efficiency improvements, new short-haul engines, higher SAF (biofuel)",
    "label": 0
  },
  {
    "text": "the 2 °C threshold—even if emissions rapidly decline (scenario SSP1-2.6). 30 January: A study in Nature Sustainability outlines challenges of aviation decarbonization by 2050 whose identified factors mainly are future demand, continuous efficiency improvements, new short-haul engines, higher SAF (biofuel) production, CO2 removal to compensate for non-CO2 forcing, and related policy-options. With constant air transport demand and aircraft efficiency, decarbonizing aviation would require nearly five times the 2019 worldwide biofuel production, competing with other hard-to-decarbonize sectors and land-use (or food security). 6 February: A study in Nature Climate Change integrates policy as an aspect into an integrated assessment model, showing that Powering Past Coal Alliance-based (from COP23) coal phase-out is highly unlikely (<5%) with current policies where both coal-use would substantially only shift from power to other industries (steel, cement, and chemicals) and China will now potentially \"dangerously delay\" the phase-out. February: the International Energy Agency's Electricity Market Report 2023 projected that low-emissions sources will constitute almost all the growth in global electricity demand through 2025, with renewables' portion of global power generation rising from 29% in 2022 to 35% in 2025. 6 March: The highest-granularity study on food GHGs, published in Nature Climate Change, reports that global food consumption alone would lead to failed climate goals with constant patterns, with ~75% of the projected warming due to ruminant meat, dairy and rice, albeit consumption currently shifts towards higher emissions overall as economic development is expected to facilitate acquisitions of undifferentiated goods like beef. 13 March: a study published in Nature Sustainability forecast that floating photovoltaic (FPV) systems on reservoirs could provide 9,434±29 terawatt-hours/year—over a third of global electricity. 27 March: a study in Geophysical Research Letters attempts to provide estimations of the tipping point(s) of the Greenland ice sheet. 5 April: in its Boom and Bust Coal publication, Global",
    "label": 0
  },
  {
    "text": "could provide 9,434±29 terawatt-hours/year—over a third of global electricity. 27 March: a study in Geophysical Research Letters attempts to provide estimations of the tipping point(s) of the Greenland ice sheet. 5 April: in its Boom and Bust Coal publication, Global Energy Monitor stated that phasing out operating coal power by 2040 would require an average of 117 GW of retirements per year—4.5 times the capacity retired in 2022. An average of 60 GW/yr for OECD countries, and 91 GW/year for non-OECD countries, must come offline. 17 May: the WMO Global Annual to Decadal Climate Update projected that the chance of global near-surface temperature exceeding 1.5 °C above preindustrial levels for at least one year between 2023 and 2027 is 66%, though it is unlikely (32%) that the five-year mean will exceed 1.5 °C. 20 May: a study published in One Earth found that increased temperature delays sleep onset and increases the probability of insufficient sleep, estimating that global warming may erode 50–58 hours of sleep per person-year while producing geographic inequalities that scale with future emissions. 22 May: a study published in Nature Sustainability projected that current policies leading to ~2.7 °C global warming could leave 22–39% of humans outside their \"human climate niche\"—defined as \"the historically highly conserved distribution of relative human population density with respect to mean annual temperature.\" The study projected that reducing warming from 2.7 to 1.5 °C would result in a ~5-fold decrease in population exposed to unprecedented heat. 6 June: a study published in Nature Communications projected that under all SSP emissions scenarios considered, the Arctic would be ice-free in September as soon as the 2030s, sooner than the IPCC's earlier projection of mid-century. 7 June: the American Lung Association projected that, by 2050, as the U.S. moves to 100 percent zero emission new",
    "label": 0
  },
  {
    "text": "the Arctic would be ice-free in September as soon as the 2030s, sooner than the IPCC's earlier projection of mid-century. 7 June: the American Lung Association projected that, by 2050, as the U.S. moves to 100 percent zero emission new passenger vehicles sales and clean electricity generation, the resulting cleaner air could bring $978 billion in public health benefits, 89,300 fewer premature deaths, 2.2 million fewer asthma attacks, and 10.7 million fewer lost workdays. 14 June: the International Energy Agency's Oil 2023: Analysis and forecast to 2028 said that demand for oil from combustible fossil fuels is on course to peak in 2028 (the final year of the forecast), and that growth is set to reverse after 2023 for gasoline and after 2026 for transport fuels overall. 25 July: a study published in Nature Communications projected a tipping point collapse of the Atlantic meridional overturning circulation (AMOC) by mid-century, causing long term cooling of Europe. Earlier IPCC projections were that such a collapse is not likely within the century. 19 August: a study published in Energies projected that global warming reaching 2 °C this century will cause premature deaths in roughly 1 billion humans. The study cited the order-of-magnitude estimate in the \"1000-ton rule\" that states that a future person is killed every time 1,000 tons of fossil carbon are burned. 28 August: a study published in Nature Climate Change projected that without snowmaking, 53% of ski resorts in 28 European countries will be at \"very high risk for snow supply\" under global warming of 2 °C (98% under 4 °C warming). 5 September: A study in The Lancet Planetary Health shows decoupling rates in high-income countries are inadequate for Paris Agreement commitments and suggests post-growth approaches such as demand reduction strategies and reorienting the economy. 2 October: a study",
    "label": 0
  },
  {
    "text": "°C warming). 5 September: A study in The Lancet Planetary Health shows decoupling rates in high-income countries are inadequate for Paris Agreement commitments and suggests post-growth approaches such as demand reduction strategies and reorienting the economy. 2 October: a study published in Nature Communications studied the effect of the expected reduction in the amount of dark-colored, light-absorbing atmospheric particles (LAPs) that snow would absorb, making the snow reflect more sunlight, thus reducing radiative forcing that would otherwise warm the Earth. The study concluded that there would be a reduction in radiative forcing from 0.65 W/m2 (1995–2014) to 0.49 W/m2 (in 2081–2100). 23 October: a Washington Post analysis concluded that by 2040, longer warm and moist transmission seasons and expanding habitats for mosquitos (both caused by climate change), coupled with expected demographic growth, could put more than 5 billion people at risk of contracting malaria. 23 October: a study published in Nature Climate Change projected that ocean warming at about triple the historical rate is likely unavoidable in the 21st century, with no significant difference between mid-range emissions scenarios versus achieving the most ambitious targets of the Paris Agreement—suggesting that greenhouse gas mitigation has limited ability to prevent collapse of the West Antarctic Ice Sheet. 24 October: the International Energy Agency's World Energy Outlook 2023 stated that \"the momentum behind clean energy transitions is now sufficient for global demand for coal, oil and natural gas to all reach a high point before 2030 in the STEPS\" (Stated Policies Scenario). 2 November: a study published in Oxford Open Climate Change (co-author: James E. Hansen) projected that the recent decline of aerosol emissions should increase the global warming rate of 0.18 °C per decade (1970–2010) to at least 0.27 °C per decade, so that \"under the present geopolitical approach to GHG emissions\", warming",
    "label": 0
  },
  {
    "text": "E. Hansen) projected that the recent decline of aerosol emissions should increase the global warming rate of 0.18 °C per decade (1970–2010) to at least 0.27 °C per decade, so that \"under the present geopolitical approach to GHG emissions\", warming will exceed 1.5 °C in the 2020s and 2 °C before 2050. 5 December: a report from the United Nations Environment Programme projected that on current growth trends, electricity for cooling equipment (air conditioning) would more than double by 2050, and that under a business-as-usual scenario, emissions from cooling would account for more than 10% of global emissions in 2050. Significant publications February: \"Electricity Market Report 2023\" (PDF). iea.org. International Energy Agency. February 2023. Archived (PDF) from the original on 9 February 2023. 22 March: \"AR6 Synthesis Report / Headline Statements\". IPCC.ch. Intergovernmental Panel on Climate Change. March 2023. Archived from the original on 22 March 2023. 22 March: \"Longer Report of the Synthesis Report of the IPCC Six Assessment Report (AR6)\" (PDF). IPCC.ch. Intergovernmental Panel on Climate Change. March 2023. Archived (PDF) from the original on 22 March 2023. 23 March: Gilmore, Anna B; et al. (April 2023). \"Defining and conceptualising the commercial determinants of health\". The Lancet. 401 (10383): 1194–1213. doi:10.1016/s0140-6736(23)00013-2. hdl:1893/35148. ISSN 0140-6736. PMID 36966782. S2CID 257698115. 21 April: \"State of the Global Climate 2022\". WMO.int. World Meteorological Organization (WMO). 21 April 2023. Archived from the original on 21 April 2023. 17 May: \"WMO Global Annual to Decadal Climate Update\". World Meteorological Organization. 17 May 2023. Archived from the original on 17 May 2023. 22 August: Committee on the Rights of the Child (22 August 2023). \"Convention on the Rights of the Child / General comment No. 26 (2023) on children's rights and the environment, with a special focus on climate change\". United Nations Commission on Human",
    "label": 0
  },
  {
    "text": "on the Rights of the Child (22 August 2023). \"Convention on the Rights of the Child / General comment No. 26 (2023) on children's rights and the environment, with a special focus on climate change\". United Nations Commission on Human Rights. Archived from the original on 29 August 2023. 4 October: \"Apostolic Exhortation / Laudate Deum of the Holy Father Francis to all people of good will on the climate crisis\". vatical.va. The Vatican. 4 October 2023. Archived from the original on 26 October 2023. 24 October: World Energy Outlook 2023 (PDF). International Energy Agency. 24 October 2023. Archived (PDF) from the original on 26 October 2023. 24 October: Ripple, William J.; Wolf, Christopher; Gregg, Jillian W.; Rockström, Johan; et al. (24 October 2023). \"The 2023 state of the climate report: Entering uncharted territory\". BioScience. biad080 (12): 841–850. doi:10.1093/biosci/biad080. 14 November: Fifth National Climate Assessment, U.S. Global Change Research Program. (overview, 47 pp) (Report in Brief, 144pp) 30 November: \"Provisional State of the Global Climate 2023\" (PDF). WMO.int. World Meteorological Organization (WMO). 30 November 2023. Archived (PDF) from the original on 30 November 2023. 19 March 2024: \"State of the Global Climate 2023\". WMO.int. World Meteorological Organization. 19 March 2024. Archived from the original on 19 March 2024. WMO-No. 1347. Notes See also 2023 in the environment 2023 in science Climatology § History Fifth National Climate Assessment (NCA5) 2023 History of climate change policy and politics History of climate change science Politics of climate change § History Timeline of sustainable energy research 2020 to the present References External links Organizations The Intergovernmental Panel on Climate Change (IPCC) World Meteorological Organization (WMO) Climate indicators at the U.S. Environmental Protection Agency Surveys, summaries and report lists Crownhart, Casey (21 December 2023). \"2023 is breaking all sorts of climate records\". MIT Technology Review.",
    "label": 0
  },
  {
    "text": "This article documents events, research findings, scientific and technological advances, and human actions to measure, predict, mitigate, and adapt to the effects of global warming and climate change—during the year 2024. Summaries 19 March: \"The climate crisis is the defining challenge that humanity faces and is closely intertwined with the inequality crisis, as witnessed by growing food insecurity and population displacement, and biodiversity loss.\" —Prof. Celeste Saulo, Secretary-General of the World Meteorological Organization, in State of the Climate 2023. October: \"For the first time in human history, the hydrological cycle is out of balance, undermining an equitable and sustainable future for all.\" 12 November: At the COP29 conference, U.N. Secretary General António Guterres described 2024 as a \"master class in climate destruction. 7 April 2025: a study published in npj Climate and Atmospheric Science found that the climate extremes of 2023–2024 were exceptional even compared to recent warming trends, with record-breaking global surface air and sea surface temperatures. The research attributes these extremes to a combination of a positive decadal trend in Earth's Energy Imbalance, three consecutive years of La Niña, and a rapid transition to El Niño, resulting in over 75% more heating between 2022 and 2023 than during similar past events. The authors warn that if the current trend in Earth's energy accumulation continues, natural climate fluctuations like ENSO will increasingly produce amplified, record-breaking impacts in the future. Measurements and statistics 5 February: a study published in the Proceedings of the National Academy of Sciences proposed adding a \"Category 6\" to the Saffir–Simpson hurricane wind scale to adequately convey storms' risk to the public, the researchers noting a number of storms have already achieved that intensity. 5 February: a study published in Nature Climate Change, based on 300 years of ocean mixed-layer temperature records preserved in sclerosponge skeletons,",
    "label": 0
  },
  {
    "text": "convey storms' risk to the public, the researchers noting a number of storms have already achieved that intensity. 5 February: a study published in Nature Climate Change, based on 300 years of ocean mixed-layer temperature records preserved in sclerosponge skeletons, concluded that modern global warming began in the 1860s (over 80 years earlier than indicated by sea surface temperature records) and was already 1.7 °C above pre-industrial levels by 2020—a figure 0.5 °C higher than IPCC estimates. February (reported): a Copernicus Climate Change Service analysis indicated that from February 2023 through January 2024, the running average global average air temperature exceeded 1.5 °C for the first time. This single-year breach does not violate the 1.5 °C long-term average agreed on in the 2015 Paris Agreement. 13 February: a study published in Current Issues in Tourism concluded that U.S. average ski seasons (incl. snowmaking) decreased from 1960–1979 to 2000–2019 by between 5.5 and 7.1 days per season, with direct economic losses estimated at $252 million annually. 18 March (reported): the University of Maine's Climate Reanalyzer analyzed NOAA data and concluded that the average global ocean surface temperature reached a record daily high in mid-March 2023, and remained at unprecedented high levels every day since. 21 March: a study published in Communications Earth & Environment concluded that higher temperatures increase inflation persistently over twelve months in both higher- and lower-income countries, with inflation pressures largest at low latitudes and having strong seasonality at high latitudes. 8 May (reported): Ember reported that for the first time, renewable energy generated a 30% of global electricity in 2023. 28 May: a study published by Climate Central, the Red Cross Red Crescent Climate Centre and World Weather Attribution concluded that over the preceding twelve months, human-caused climate change caused a worldwide average of 26 additional days",
    "label": 0
  },
  {
    "text": "electricity in 2023. 28 May: a study published by Climate Central, the Red Cross Red Crescent Climate Centre and World Weather Attribution concluded that over the preceding twelve months, human-caused climate change caused a worldwide average of 26 additional days of extreme heat. 11 June: a study published in Earth System Science Data estimated that total annual anthropogenic nitrous oxide (N2O) emissions increased 40% from 1980 to 2020, exceeding projected levels under all scenarios in the CMIP6 model. 1 July (reported): Hurricane Beryl, the earliest Category 5 storm on record in the Atlantic, broke records for rapid intensification (65 mph in 24 hours), overall strength, and location for June. 9 July (reported): for the first time, in each month in a 12-month period (through June 2024), Earth's average temperature exceeded 1.5 °C above the pre-industrial baseline. 21 July: the highest daily global average temperature is recorded at 17.09 °C (62.76 °F), surpassing the previous record of 17.08 °C (62.74 °F) on 6 July 2023. 28 August: a study published in the journal Nature concluded that the June–September 2023 Canadian wildfires caused carbon emissions that exceeded annual fossil fuel emissions of all nations except India, China and the US. 20 November: a study published in Environmental Research: Climate, applied to 2024 Atlantic hurricanes to date, estimated that climate change's increase of water temperatures intensified peak wind speeds in all eleven such hurricanes by 9–28 mph (14–45 km/h), including Helene (16 mph (26 km/h)) and Milton (24 mph (39 km/h)). 30 November: a report from the Potsdam Institute for Climate Impact Research, Stepping back from the precipice: Transforming land management to stay within planetary boundaries, estimated an annual land degradation rate of 1,000,000 square kilometres (390,000 mi2), in addition to previous accumulated land degradation of 15,000,000 square kilometres (5,800,000 mi2). July 2025:",
    "label": 0
  },
  {
    "text": "Stepping back from the precipice: Transforming land management to stay within planetary boundaries, estimated an annual land degradation rate of 1,000,000 square kilometres (390,000 mi2), in addition to previous accumulated land degradation of 15,000,000 square kilometres (5,800,000 mi2). July 2025: the International Renewable Energy Agency's Renewable Power Generation Costs 2024 said that on a levelized cost of electricity (LCOE) basis, 91% of new renewable utility-scale capacity delivered power at a lower cost than the cheapest new fossil fuel-based alternative. New onshore wind projects had an LCOE of $0.034/kWh, solar photovoltaic $0.043/kWh, and hydropower $0.057/kWh. 16 October 2025: the World Meteorological Organization's Greenhouse Gas Bulletin reported that from 2023 to 2024, atmospheric carbon dioxide increased by 3.5 ppm, constituting the largest single-year increase since measurements began in 1957. Natural events and phenomena January: a study published in Annual Review of Marine Science reported that sea level rise's (SLR's) elevation of coastal water tables and shifting of their salinity landward—whose damage is \"largely concealed and imperceptible\"—makes potentially 1,546 coastal communities vulnerable to impacts decades before SLR-induced surface inundation. 12 February: a study published by the nonprofit First Street Foundation reported that improvements in air quality brought about by environmental regulation are being partially reversed by a \"climate penalty\" caused by climate change, especially with increases in PM2.5 particulates caused by increased wildfires. 28 February: a study published in Weather and Climate Dynamics statistically linked recent Arctic ice loss with warmer and drier weather in Europe, enabling \"an enhanced predictability of European summer weather at least a winter in advance\". 25 March: a study published in Oecologia concluded that global warming, and increased intensity and frequency of precipitation and wildfires, have reduced pollen diversity, negatively affecting pollen richness in the Great Basin and Sierra Nevada. 26 March: a study published in Nature concluded",
    "label": 0
  },
  {
    "text": "study published in Oecologia concluded that global warming, and increased intensity and frequency of precipitation and wildfires, have reduced pollen diversity, negatively affecting pollen richness in the Great Basin and Sierra Nevada. 26 March: a study published in Nature concluded that under some circumstances, change in albedo (Earth's surface's reflection of sunlight back into space) resulting from planting more trees can cause a significant \"albedo offset\" that reduces the benefits of the trees' removal of carbon from the atmosphere. 27 March: a study published in Nature concluded that accelerated melting of ice in Greenland and Antarctica has decreased Earth's rotational velocity, affecting Coordinated Universal Time (UTC) adjustments and causing problems for computer networks that rely on UTC. 8 April: recognizing that climate warming causes many meteorites to be lost from the surface by melting into the Antarctic ice sheet, a study in Nature Climate Change concluded that about 5,000 meteorites become inaccessible each year. About 24% are projected to be lost by 2050, potentially rising to ~76% by 2100 under a high-emissions scenario. (Over 60% of meteorite finds on Earth originate from Antarctica.) 11 April: a study published in Science noted that the effect of soil inorganic carbon (SIC) on future atmospheric carbon concentrations has been inadequately studied, and projected that soil acidification associated with nitrogen additions to terrestrial ecosystems will cause release into the atmosphere up to 23 billion tonnes of carbon over the next 30 years. 11 May (reported): Venezuela became the first country in modern times to lose all of its glaciers, with the Humboldt Glacier having shrunk to the point that climate scientists reclassified it as an ice field. 20 May: a study published in the Proceedings of the National Academy of Sciences concluded that rushing of seawater beneath grounded ice over considerable distances makes Thwaites",
    "label": 0
  },
  {
    "text": "shrunk to the point that climate scientists reclassified it as an ice field. 20 May: a study published in the Proceedings of the National Academy of Sciences concluded that rushing of seawater beneath grounded ice over considerable distances makes Thwaites Glacier, Antarctica, more vulnerable to melting than previously anticipated, which in turn increases projections of ice mass loss. 6 June: a study published in Geophysical Research Letters concluded that, from 1980 through 2022, internal climate variability has enhanced Arctic warming but suppressed global warming, specifically involving warming in the Barents Sea and Kara Sea but cooling in the tropical Eastern Pacific Ocean and Southern Ocean. 13 June: the National Oceanic and Atmospheric Administration published a notice that El Niño conditions had given way to ENSO-neutral conditions in the preceding month, ending a year-long period during which ocean and air temperatures reached into record-setting territory. 24 June: a study published in Nature Ecology & Evolution reported that the frequency of extreme wildfires increased by a factor of 2.2 from 2003 to 2023, with the most recent 7 years including the 6 most extreme. 15 July: noting that global warming-induced melting of glaciers and polar ice sheets has moved mass from polar regions toward the equator to significantly change Earth's shape and increase the length of days (LOD), a study published in Proceedings of the National Academy of Sciences concluded that mass variations at the Earth's surface lengthened days between 0.3–1.0 ms/year in the 20th century, and accelerated to about 1.33 ms/year in the 21st century. Under a high emissions scenario, LOD could increase to 2.62±0.79 ms/year by 2100. LOD variations make precise timekeeping and space navigation more difficult. 25 July: from measurements of CO2 and methane emissions from exposed sediments of Great Salt Lake, a study published in One Earth concluded",
    "label": 0
  },
  {
    "text": "could increase to 2.62±0.79 ms/year by 2100. LOD variations make precise timekeeping and space navigation more difficult. 25 July: from measurements of CO2 and methane emissions from exposed sediments of Great Salt Lake, a study published in One Earth concluded that such emissions are high enough that they should be accounted for in regional carbon budgets, and warrant efforts to halt and reverse the loss of saline lakes around the world. 9 December: a report from the UNCCD concluded that more than three-quarters of the Earth's land \"has become permanently dryer in recent decades\", that \"drier climates now affecting vast regions across the globe will not return to how they were\", and that a quarter of the global population lives in expanding drylands. December (reported): NOAA's 2024 Arctic Report Card stated that the Arctic tundra region had shifted from being a CO2 sink to being a CO2 source. The region continued to be a methane source. 5 December: a study published in Science attributed 2023's unexpectedly large rise in global average temperature to a \"record low planetary albedo\" resulting from a continuation of a multi-year trend of diminishing low cloud cover in the northern mid-latitudes and tropics. 20 December: a study published in Science of the Total Environment enumerated motivations for studying the effect of climate change on zoonotic (animal-to-human) disease transmission. 18 September 2025: the World Meteorological Organization's State of Global Water Resources Report 2024 concluded that only one third of river basins had normal hydrological conditions in 2024, and that all glacier regions lost ice for third straight year. Actions and goal statements Science and technology 2 January: the first commercial-scale offshore wind farm in the U.S. began operation 15 miles (24 km) off the coast of Martha's Vineyard, Massachusetts, initially providing 5 MW from one wind turbine,",
    "label": 0
  },
  {
    "text": "year. Actions and goal statements Science and technology 2 January: the first commercial-scale offshore wind farm in the U.S. began operation 15 miles (24 km) off the coast of Martha's Vineyard, Massachusetts, initially providing 5 MW from one wind turbine, but planning an eventual 62 turbines capable of powering 400,000 homes and businesses. 11 January: a study in Nature Cities presents results of a Riyadh-based trial of eight urban heat mitigation scenarios, finding large cooling effects with combinations that include reflective rooftop materials, irrigated greenery, and retrofitting. 18 January: the first successful test of a solar farm in space—collecting solar power from a photovoltaic cell and beaming energy down to Earth—constituted an early feasibility demonstration. February: an underwater generator operating on the principle of a kite travels a figure-8 pattern, moving faster than the current that drives it. A 1.2 MW utility-scale generator began providing power to the grid of the Faroe Islands. 9 February: researchers use simulations to develop an early-warning signal for a potential collapse of the AMOC published in Science Advances and suggest it indicates the AMOC is \"on route to tipping\". 5 March: in a non-unanimous vote, the IUGS's Subcommission on Quaternary Stratigraphy voted against declaring the Anthropocene a new geological epoch. The vote leaves open more informally classifying human impacts as a geological event that unfolds gradually over a long period. March: the largest inventory of methane emissions from U.S. oil and gas production, published in Nature, finds them to be largely concentrated and around three times the national government inventory estimate. Methane emissions from U.S. landfills are quantified in Science, with super-emitting point-sources accounting for almost 90% thereof. March (reported inventions): a wind-powered electrodynamic screen (EDS) generates strong electric fields that repel dust and contaminants from the surface of solar panels, thereby increasing the",
    "label": 0
  },
  {
    "text": "U.S. landfills are quantified in Science, with super-emitting point-sources accounting for almost 90% thereof. March (reported inventions): a wind-powered electrodynamic screen (EDS) generates strong electric fields that repel dust and contaminants from the surface of solar panels, thereby increasing the panels' efficiency while avoiding manual cleaning. Researchers demonstrate simultaneous radiative cooling and solar power generation from the same area. April (reported): a new glass-ceramic material placed atop solar panels transforms ultraviolet light into visible light, effectively increasing the amount of usable light from the sun (the material passes visible light, as normal). April (reported): \"rock flour\"—rock that has been finely ground by glaciers and having large surface area per unit volume—has been found to enhance \"chemical weathering\" that removes carbon from air when spread across ground surfaces. A similar sequestration process, using concrete particles 1 millimeter in diameter, has also been tested. 2 April: the first outdoor test in the U.S. of marine cloud brightening technology—designed to brighten clouds and reflect sunlight back into space—tested whether a machine could consistently spray the right size salt aerosols through the open air outside of a lab. Local authorities halted the project the following month, citing concerns for public health and safety. 3 April: a study published in Communications Earth & Environment reasoned that reductions of planet-cooling aerosol emissions due to air quality legislation will worsen Earth's energy imbalance in addition to that caused by greenhouse gas emissions, concluding that accelerated global warming in this decade is to be expected. 30 May: a study published in Communications Earth & Environment concluded that a 2020 International Maritime Organization fuel regulation to reduce sulfur emissions from international shipping reduced aerosol pollution along shipping lanes, but caused an increase in radiative forcing (global warming effect) that the researchers called an \"inadvertent geoengineering termination shock with global",
    "label": 0
  },
  {
    "text": "2020 International Maritime Organization fuel regulation to reduce sulfur emissions from international shipping reduced aerosol pollution along shipping lanes, but caused an increase in radiative forcing (global warming effect) that the researchers called an \"inadvertent geoengineering termination shock with global impact\". 5 June: a study published in Nature introduced a \"charge-sorbent\" material having reactive hydroxide ions embedded in the pores of an activated carbon material, the ions removing CO2 from the air through bicarbonate formation. After being saturated with CO2, the charge-sorbent material's properties can be renewed at low temperatures 90 to 100 °C (194 to 212 °F). June (reported): a consortium of maritime experts proposed a fuel use reduction system in which ships coordinate non-conflicting arrival times at ports, to avoid the conventional \"sail fast, then wait\" practice by cruising at generally slower, fuel-saving speeds. July (reported): warming climate is found to create glacial meltwater that washes away temporally ordered layers of trapped aerosols that researchers use as an historical record of environmental events. The Ice Memory Foundation plans to store additional ice cores in Antarctica in advance of this impending loss of data. October: A paper published in arXiv proposing twelve policy recommendations surrounding Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency to combat the effects of Artificial Intelligence and data centers on climate change with reference to the shortcomings of the EU AI Act. November (reported): U.S. government agencies are operating an airborne early warning system for detecting small concentrations of aerosols to detect where other countries might be carrying out geoengineering attempts. Solar radiation modification is thought to have unpredictable effects on climate. Political, economic, legal, and cultural actions 8 February: climate scientist Michael E. Mann won a $1 million judgment for punitive damages in",
    "label": 0
  },
  {
    "text": "countries might be carrying out geoengineering attempts. Solar radiation modification is thought to have unpredictable effects on climate. Political, economic, legal, and cultural actions 8 February: climate scientist Michael E. Mann won a $1 million judgment for punitive damages in a defamation lawsuit filed in 2012 against bloggers who attacked his hockey stick graph of global temperature rise, one of the bloggers having called Mann's work \"fraudulent\". 14 February: a study in Energy Research & Social Science reviewed educational content of 18,400 universities worldwide, finding higher education is not transitioning from fossil fuels to renewable energy curricula, failing to meet the growing demand for a clean energy workforce. March (reported): website Realtor.com added property-specific tools describing individual properties' vulnerability to heat, wind, and air quality, publishing current risks and projected risks 30 years into the future. 22 March: the Nauta provincial court (Peru) ruled that the Marañón River has \"intrinsic\" value and possesses the rights to exist, flow, and be free from pollution. The ruling was the first time Peru has legally recognized \"rights of nature\". 29 March: the Inter-American Court of Human Rights, based in Costa Rica, ruled that the government of Peru is liable for physical and mental harm to people caused by a metallurgical facility's pollution, and ordered the government to provide free medical care and monetary compensation to victims. 9 April: in its first ruling on climate litigation, the European Court of Human Rights ruled that Switzerland's failure to adequately tackle the climate crisis breached 2000 women plaintiffs' human rights to effective protection from \"the serious adverse effects of climate change on lives, health, well-being and quality of life\". 30 April: G7 ministers agreed to end unabated coal power plants by 2035, giving leeway for countries whose power plants are fitted with carbon capture technology. 1",
    "label": 0
  },
  {
    "text": "adverse effects of climate change on lives, health, well-being and quality of life\". 30 April: G7 ministers agreed to end unabated coal power plants by 2035, giving leeway for countries whose power plants are fitted with carbon capture technology. 1 May: a three-judge panel of the 9th U.S. Circuit Court of Appeals ruled that Juliana v. United States should be dismissed. The lawsuit was filed in 2015 by 21 young people claiming the U.S. government's energy policies violate their rights to be protected from climate change, more specifically, violating their rights to due process and equal protection under the U.S. Constitution. 21 May: in an advisory opinion that could provide precedent for other cases, the International Tribunal for the Law of the Sea ruled that greenhouse gas emissions absorbed by the oceans constitute marine pollution, and that countries have a legal obligation to monitor and reduce such emissions. The tribunal laid out specific requirements for environmental impact assessments. 29 May: a study published in Cell Reports Sustainability estimated 2022 climate and health benefits of using wind and solar rather than fossil fuels to be $143/MWh (wind) and $100/MWh (solar). The study estimated $249 billion of climate and air quality benefits in the U.S. from 2019-2022. 30 May: Vermont became the first U.S. state to enact a law, the Climate Superfund Act, requiring the state to charge fossil fuel companies for climate impacts of their fossil fuel emissions. 2 June: Mexico elected as president, a climate scientist with a doctorate in energy engineering who had helped to write Intergovernmental Panel on Climate Change (IPCC) reports. 5 June: UN Secretary-General António Guterres called on all countries to ban advertising from fossil fuel companies, calling them \"the Godfathers of climate chaos\". 20 June: the governor of the U.S. state of Hawaii announced a",
    "label": 0
  },
  {
    "text": "Climate Change (IPCC) reports. 5 June: UN Secretary-General António Guterres called on all countries to ban advertising from fossil fuel companies, calling them \"the Godfathers of climate chaos\". 20 June: the governor of the U.S. state of Hawaii announced a court-approved settlement in the constitutional climate case, Navahine v. Hawaiʻi Department of Transportation. In the settlement, the state acknowledges the constitutional right of Hawaii's youth to a \"life-sustaining climate\", and commits the state to implement \"transformative changes of Hawaii's transportation system to achieve the state's goal of net-negative emissions by 2045\". 29 August: the Constitutional Court of Korea ruled that the absence of legally binding targets for greenhouse gas reductions for 2031-2049 violated the constitutional rights of future generations, saying that this lack of long-term targets shifted an excessive burden to the future. 30 September: Britain's coal-powered Ratcliffe-on-Soar Power Station closed, ending the U.K.'s 142-year use of coal to generate electricity. 15 October (reported): the University of California, San Diego implemented a graduation requirement to take courses that cover at least 30% climate-related content and address two of four areas: scientific foundations, human impacts, mitigation strategies and project-based learning. 11–22 November: negotiators at the COP29 conference in Baku, Azerbaijan eventually came to an agreement in which wealthy nations pledged to provide $300 billion per year in support by 2035, up from a previous target of $100 billion but less than the $1.3 trillion per year that independent experts said is needed to keep global warming under 1.5 °C. 11–22 November: after years of deadlock, governments attending the COP29 conference agreed to rules on creating, trading and registering emission reductions and removals as carbon credits that higher-emission countries can buy, thus providing funding for low-emission technologies. 2 December: The International Court of Justice in The Hague, Netherlands began hearings to form",
    "label": 0
  },
  {
    "text": "to rules on creating, trading and registering emission reductions and removals as carbon credits that higher-emission countries can buy, thus providing funding for low-emission technologies. 2 December: The International Court of Justice in The Hague, Netherlands began hearings to form advisory opinions on how countries should control greenhouse gas emissions, and the consequences if they do not. The ICJ's advisory opinions are non-binding but are considered legally and politically significant. 18 December: the Montana Supreme Court affirmed the August 14, 2023 trial court decision in Held v. Montana, which held that the state's limiting consideration of environmental factors when deciding oil and gas permits, violated the youth plaintiffs' right to a safe environment recited in Montana's constitution. Mitigation goal statements January (reported in TIME): The IEA has outlined that by 2030, we must triple our reliance on renewable sources of energy, double energy efficiency, significantly cut methane emissions, and increase electrification with existing technologies. Adaptation goal statements 4 February (reported): to reduce sea level rise caused by melting of Antarctica's Thwaites and Pine Island glaciers, scientists proposed a \"Seabed Curtain\" 100 kilometres (62 mi) long, moored to and rising from the bed of the Amundsen Sea, designed to reduce the amount of warm ocean water that would melt the base of those glaciers. Consensus 9 February: a global survey of almost 130,000 individuals whose results were published in Nature Climate Change found that 69% of respondents were willing to contribute 1% of their income to support action against climate change, 86% endorsed pro-climate social norms, and 89% demanded greater political action. However, the world was said to be in a state of pluralistic ignorance, in which people underestimate the willingness of others to act. 18 July: an analysis found that 100 U.S. Representatives and 23 U.S. Senators—23% of the 535",
    "label": 0
  },
  {
    "text": "action. However, the world was said to be in a state of pluralistic ignorance, in which people underestimate the willingness of others to act. 18 July: an analysis found that 100 U.S. Representatives and 23 U.S. Senators—23% of the 535 members of the U.S. Congress—were climate change deniers. All were Republicans. 7 August: a study published in PLOS One found that even a single repetition of a claim was sufficient to increase the perceived truth of both climate science-aligned claims and climate change skeptic/denial claims—\"highlighting the insidious effect of repetition\". This effect was found even among climate science endorsers. 12 August (published): 2023 U.S. survey found no evidence that climate crisis or climate emergency—terms less familiar to those surveyed—elicit more perceived urgency than climate change or global warming. 15 October (reported): a global survey of 3,000 risk experts and 20,000 members of the public by insurance company Axa found that Europeans ranked climate change as a pressing emerging risk the highest out of any group, with 67% of experts and 49% of the public putting it in their top five risks. The corresponding US numbers were 43% and 38% respectively, although climate was the single most concerning future risk. Projections January: the World Economic Forum projected that, by 2050, directly and indirectly, climate change will cause 14.5 million deaths and $12.5 trillion in economic losses. 13 February: a study published in Current Issues in Tourism projected that for the 2050s, U.S. ski seasons will shorten between 14–33 days (low emissions scenario) and 27 to 62 days (high emissions scenario), with direct economic losses of $657 million to 1.352 billion annually. 5 March: a study published in Nature Reviews Earth & Environment projected that the first single occurrence (September; not year-round) of an ice-free Arctic \"could occur in 2020–2030s under all",
    "label": 0
  },
  {
    "text": "direct economic losses of $657 million to 1.352 billion annually. 5 March: a study published in Nature Reviews Earth & Environment projected that the first single occurrence (September; not year-round) of an ice-free Arctic \"could occur in 2020–2030s under all emission trajectories and are likely to occur by 2050\". Daily ice-free conditions are expected approximately 4 years earlier on average. 6 March: a study in Nature finds U.S. land area of ~1,200 km2 (460 mi2) is threatened by coastal subsidence by 2050 due to sea level rise. 13 March: a study published in PLOS One projected that 13% of all current ski areas are projected to completely lose natural annual snow cover by 2100. 17 April: a study published in Nature forecast that by 2050, climate change will cause average incomes to fall by almost 20% and will cause $38 trillion of destruction each year. In December 2025, the Nature article was retracted due to errors in Uzbekistan economic data, and these two figures, among others, were corrected downward to 17% and $32 trillion, respectively. 8 May (reported): in a poll by The Guardian of contactable lead authors or review editors of IPCC reports since 2018, 76.3% of respondents projected at least 2.5 °C of global warming; only 5.79% forecast warming of 1.5 °C or less. March: a study published in The Lancet Planetary Health projected both \"substantial losses\" of habitat for venomous snakes by 2070, and migration of venomous species across international borders presenting new dangers to public health. 14 May: a study published in Nature Communications forecast that by 2050, 177–246 million older adults will be exposed to dangerous acute heat, the most severe effects forecast in Asia and Africa which also have the lowest adaptive capacity. 24 September: a study published in Nature Communications concluded that rapid",
    "label": 0
  },
  {
    "text": "by 2050, 177–246 million older adults will be exposed to dangerous acute heat, the most severe effects forecast in Asia and Africa which also have the lowest adaptive capacity. 24 September: a study published in Nature Communications concluded that rapid permafrost thaw will lead to soil drying, surface warming, and reduction of relative humidity over the Arctic-Subarctic region, which will cause rapid intensification of wildfires in western Siberia and Canada. 4 October: the World Health Organization (WHO) projected that without \"urgent action\", climate change will cause 5 million people to die from undernutrition, malaria, diarrhea and heat stress from 2030 to 2050. 14 October: a study in the Proceedings of the National Academy of Sciences projected that with continued grid decarbonization, electrification of vehicles in the U.S. would reduce harmful air quality–related health impacts by $84 to $188 billion from 2022 through 2050. 11 December: applying a fusion of probabilistic projections, a study published in the American Geophysical Union's Earth's Future journal projected \"very likely\" (5th–95th %ile) sea level rise by the year 2100 to be 0.3–1.0 m (12–39 in) under a low-emissions scenario and 0.5–1.9 m (20–75 in) under a high-emissions scenario. Significant publications \"State of the World's Migratory Species\" (PDF). UN Environment Programme World Conservation Monitoring Centre (UNEP-WCMC). February 2024. Archived (PDF) from the original on 12 February 2024. \"State of the Global Climate 2023\". WMO.int. World Meteorological Organization. 19 March 2024. Archived from the original on 19 March 2024. WMO-No. 1347. \"Europe is not prepared for rapidly growing climate risks\". European Environment Agency. 10 March 2024. Retrieved 15 May 2024. Ripple, William J.; Wolf, Christopher; Gregg, Jillian W.; Rockstrom, Johan; Mann, Michael E.; Oreskes, Naomi; et al. (8 October 2024). \"The 2024 state of the climate report: Perilous times on planet Earth\". BioScience. biae087 (12): 812–824. doi:10.1093/biosci/biae087.",
    "label": 0
  },
  {
    "text": "15 May 2024. Ripple, William J.; Wolf, Christopher; Gregg, Jillian W.; Rockstrom, Johan; Mann, Michael E.; Oreskes, Naomi; et al. (8 October 2024). \"The 2024 state of the climate report: Perilous times on planet Earth\". BioScience. biae087 (12): 812–824. doi:10.1093/biosci/biae087. \"Surging Seas in a Warming World: The latest science on present-day impacts and future projections of sea-level rise\" (PDF). United Nations. 26 August 2024. Archived (PDF) from the original on 20 October 2024. Olhoff, Anne; Bataille, Chris; Christensen, John; Den Elzen, Michel; Fransen, Taryn; Grant, Neil; Blok, Kornelis; Kejun, Jiang; Soubeyran, Eleonore; Lamb, William; Levin, Kelly; Portugal-Pereira, Joana; Pathak, Minal; Kuramochi, Takeshi; Strinati, Costanza; Roe, Stephanie; Rogelj, Joeri (24 October 2024). Emissions Gap Report 2024 (PDF). United Nations Environment Programme. doi:10.59117/20.500.11822/46404. ISBN 978-92-807-4185-8. \"2024 Living Planet Report - A System in Peril\" (PDF). World Wildlife Fund. 27 October 2024. Archived (PDF) from the original on 30 October 2024. \"Adaptation Gap Report 2024\". United Nations Environment Programme. 7 November 2024. Archived from the original on 10 November 2024. \"State of the Cryosphere 2024 / Lost Ice, Global Damage\" (PDF). International Cryosphere Climate Initiative. 14 November 2024. \"The Future of Geothermal Energy\" (PDF). International Energy Agency (IEA). December 2024. Archived (PDF) from the original on 14 December 2024. Van Dijk, A.I.J.M., H.E. Beck, E. Boergens, R.A.M. de Jeu, W.A. Dorigo, C. Edirisinghe, E. Forootan, E. Guo, A. Güntner, J. Hou, N. Mehrnegar, S. Mo, W. Preimesberger, J. Rahman, P. Rozas Larraondo. \"Global Water Monitor 2024, Summary Report\". www.globalwater.online. Global Water Monitor Consortium. 2 January 2025. Archived from the original on 6 January 2025. Blunden, J.; Reagan, J.; Dunn, R. J. H. (August 2025). \"State of the Climate in 2024\" (PDF). Journal of the American Meteorological Society. 106 (8): S1 – S513. Bibcode:2025BAMS..106Q...1B. doi:10.1175/2025BAMSStateoftheClimate.1. See also Meteorology in the 21st century Climatology §",
    "label": 0
  },
  {
    "text": "2025. Blunden, J.; Reagan, J.; Dunn, R. J. H. (August 2025). \"State of the Climate in 2024\" (PDF). Journal of the American Meteorological Society. 106 (8): S1 – S513. Bibcode:2025BAMS..106Q...1B. doi:10.1175/2025BAMSStateoftheClimate.1. See also Meteorology in the 21st century Climatology § History History of climate change policy and politics History of climate change science Politics of climate change § History Timeline of sustainable energy research 2020–present References External links Organizations The Intergovernmental Panel on Climate Change (IPCC) World Meteorological Organization (WMO) Climate indicators at the U.S. Environmental Protection Agency Surveys, summaries and report lists Horton, Helena; Swan, Lucy; Paz, Ana Lucia Gonzalez; Symons, Harvey (20 November 2024). \"The climate crisis in charts: how 2024 has set unwanted new records\". The Guardian. Archived from the original on 19 December 2024. Freedman, Andrew (16 December 2024). \"2023, 2024 climate change records defy scientific explanation\". Axios. Archived from the original on 27 December 2024. Pare, Sascha (26 December 2024). \"The most important and shocking climate stories of 2024\". Live Science. Archived from the original on 29 December 2024. Horn, Paul (29 December 2024). \"These Graphics Help Explain What Climate Change Looked Like in 2024\". Inside Climate News. Archived from the original on 29 December 2024.",
    "label": 0
  },
  {
    "text": "This article documents notable events, research findings, scientific and technological advances, and human actions to measure, predict, mitigate, and adapt to the effects of global warming and climate change—during the year 2025. Summaries May: André Corrêa do Lago, director of the 2025 United Nations Climate Change Conference (COP30, in Brazil), said, \"It is not possible to have [scientific] denialism at this stage, after everything that has happened in recent years. So there is a migration from scientific denial to a denial that economic measures against climate change can be good for the economy and for people.\" 20 May: a study published in BioScience said that climate change has joined overexploitation and habitat alteration, to become a third major threat to Earth's animals. June: the Energy Institute's Statistical Review of World Energy suggested that 2025 may be the beginning of a paradigm shift in which the energy transition is driven by pursuit of energy security through energy independence (\"risk hedging\" in a volatile and uncertain environment, and seeking \"resilient, decentralized, and clean energy systems\"), rather pursuing climate change mitigation per se. Measurements and statistics 10 January: a summary from the Copernicus Climate Change Service stated that 2024 was the warmest year since records began in 1850, with an average global surface temperature reaching 1.6 °C above pre-industrial levels, surpassing for the first time the 1.5 °C warming target set by the Paris Agreement. The summary also stated that 2024 was the second consecutive year with the hottest global temperature, surpassing 2023 by +0.12 °C. 21 January: a study published in Nature Climate Change concluded that at least 30% of the Arctic has become a net source of carbon dioxide. 28 January: a study published in Environmental Research Letters reported that global mean sea surface temperature increases had more than quadrupled, from",
    "label": 0
  },
  {
    "text": "Climate Change concluded that at least 30% of the Arctic has become a net source of carbon dioxide. 28 January: a study published in Environmental Research Letters reported that global mean sea surface temperature increases had more than quadrupled, from 0.06 K per decade during 1985–89 to 0.27 K per decade for 2019–23, and projected that the increase inferred over the past 40 years would likely be exceeded within the next 20 years. 3 February: a study co-authored by James Hansen published in Environment concluded that the IPCC had underestimated the effect of aerosols' planet-cooling radiative forcing, after enactment of international regulation of maritime aerosol emissions in the 2020s designed to improve air quality. This underestimation of aerosols' effect was said to cause underestimation of climate sensitivity, Hansen et al. writing that the reduction in aerosols explains the unexpectedly large global warming experienced in 2023-2024. 10 February: a study published in Nature Climate Change shows that the first single calendar year above 1.5 °C relative to pre-industrial levels in 2024 indicates that most probably Earth has already entered the 20-year period that will reach the Paris Agreement limit — that is, a 20-year period with average warming of 1.5 °C. 24 February: a study published by the non-profit Initiative on GHG Accounting of War estimated that the three years of the Russian invasion of Ukraine had caused 230 MtCO2e (metric tons of carbon dioxide equivalent) emissions—citing warfare, buildings reconstruction, landscape fires, damage to energy infrastructure, refugee and civil aviation displacement. 7 March: a study by World Weather Attribution concluded that in a February 2025 heat wave, climate change made extreme heat at least 2 °C hotter and at least ten times more likely. 21 March: on the first World Glaciers Day (established by the UN General Assembly), the World Meteorological",
    "label": 0
  },
  {
    "text": "in a February 2025 heat wave, climate change made extreme heat at least 2 °C hotter and at least ten times more likely. 21 March: on the first World Glaciers Day (established by the UN General Assembly), the World Meteorological Organization reported that five of the past six years witnessed the most rapid glacier retreat on record, and 2022-2024 witnessed the largest three-year loss of glacier mass on record. 14 April: a study published in the Proceedings of the National Academy of Sciences concluded that since 1940, global warming has tripled the number of days per year that the oceans experience extreme surface heat conditions, and is responsible for a 1 °C increase in maximum intensity of marine heatwave events. 10 May: a study published in AGU Advances said that observations from space show that Earth's energy imbalance in 2023 reached values that were twice that of the best estimate from the IPCC, and cautioned that the ability to observe this imbalance is deteriorating because satellites are being decommissioned. 30 May: a study jointly published by Climate Central, the Red Cross Red Crescent Climate Centre, and World Weather Attribution concluded that in the preceding year, four billion people experienced at least thirty additional days of extreme heat because of climate change; and that in 195 countries and territories, climate change at least doubled the number of extreme heat days as compared to a world without climate change. 5 June (reported): for the first time, NOAA's Mauna Loa Observatory measured an atmospheric carbon dioxide concentration of over 430 parts per million. 16 June: a study published in Nature Communications found that higher temperatures (27.3 °C vs. 6.4 °C) were associated with a 45% higher probability of having obstructive sleep apnea on a given night. 18 June: a study published in Nature",
    "label": 0
  },
  {
    "text": "16 June: a study published in Nature Communications found that higher temperatures (27.3 °C vs. 6.4 °C) were associated with a 45% higher probability of having obstructive sleep apnea on a given night. 18 June: a study published in Nature concluded that global warming reduces crop production by 4.4% of recommended caloric consumption for each 1 °C of global warming, though adaptation measures can reduce losses. July: the International Renewable Energy Agency's Renewable Power Generation Costs 2024 said that on a levelized cost of electricity (LCOE) basis, 91% of new renewable utility-scale capacity delivered power at a lower cost than the cheapest new fossil fuel-based alternative. New onshore wind projects had an LCOE of $0.034/kWh, solar photovoltaic $0.043/kWh, and hydropower $0.057/kWh. 7 July: a study published in Nature Geoscience reported that changes in long-duration heatwaves increase nonlinearly with temperature, with increments of warming increasing the durations more than proportionately. 11 August: applying extreme event attribution, a study published in Nature Ecology & Evolution concluded that human-caused intensification of heat extremes caused a 25–38% reduction in global abundance of tropical birds from 1950 to 2020. 25 August: an article published in Nature Climate Change reported the results of a 15-year study that concluded that long-term exposure to heat waves accelerated the aging process, with manual workers, rural residents and participants from communities with fewer air conditioners being more susceptible to health impacts. 27 August: a study published in Nature Climate Change concluded that tropical deforestation-induced local warming is associated with 23,610–33,560 heat-related deaths per year. 3 September: noting that the planet's carbon sequestration capacity is not unlimited, a study published in Nature concluded that fully using Earth's geologic storage capacity would help limit global warming by only 0.7 °C (1.3 °F). 10 September: applying extreme event attribution to 213 historical heatwaves",
    "label": 0
  },
  {
    "text": "carbon sequestration capacity is not unlimited, a study published in Nature concluded that fully using Earth's geologic storage capacity would help limit global warming by only 0.7 °C (1.3 °F). 10 September: applying extreme event attribution to 213 historical heatwaves from 2000–2023, a study published in Nature concluded that human-caused climate change made the median of heatwaves in 2000–2009 about 20 times more likely than a pre-industrial baseline, and heatwaves in 2010–2019 about 200 times more likely. Specifically, the study concluded that fossil fuel and cement producers enabled 16–53 heatwaves that would have been virtually impossible in a preindustrial climate. 10 September: a study published in Nature attributed 3,400–7,400 acute deaths in North America and 37,800–90,900 chronic deaths in North America and Europe to PM2.5 (fine particulate matter) exposure from the 2023 Canadian wildfires. 7 October: Ember's Global Electricity Mid-Year Insights 2025 reported that in the first half of 2025, renewables overtook coal generation for the first time on record (34.3% vs. 33.1% of global electricity). 13 October: Global Tipping Points Report 2025 said that \"warm-water coral reefs are crossing their thermal tipping point and experiencing unprecedented dieback\". 13 October: a global study of ~320,000 sleep-tracker users, published in the journal Sleep, found that high ambient temperatures (99th vs 50th percentile: 27.3 °C vs 12.2 °C) were associated with a mean sleep loss of approximately 15–17 minutes and a ~40 % increase in the probability of a short-sleep night. 16 October: the World Meteorological Organization's Greenhouse Gas Bulletin reported that from 2023 to 2024, atmospheric carbon dioxide increased by 3.5 ppm, constituting the largest single-year increase since measurements began in 1957. 28 October: The Lancet estimated that in 2022, 2.52 million deaths globally were attributable to ambient air pollution from burning fossil fuels. 12 November: a report from Amnesty International",
    "label": 0
  },
  {
    "text": "ppm, constituting the largest single-year increase since measurements began in 1957. 28 October: The Lancet estimated that in 2022, 2.52 million deaths globally were attributable to ambient air pollution from burning fossil fuels. 12 November: a report from Amnesty International said that at least 2 billion people, including 520 million children, live within 5 kilometres (3.1 mi) of fossil fuel infrastructure. 12 November: a report from the American Association for the Advancement of Science (AAAS) said that the remaining carbon budget to limit global warming to 1.5 °C is \"virtually exhausted\"—equivalent to four years at 2025 emissions levels. May: an EIT Urban Mobility study of non-exhaust emissions published by the European Institute of Innovation and Technology said that because of regenerative braking, electric vehicles produce ~83% less brake dust than equivalent conventional vehicles, though their increased weight leads to 20% higher tire and road wear emissions. 9 December: a report from the Global Coral Reef Monitoring Network concluded that, from 1980 to 2024, Caribbean hard coral cover declined by 48%; and that, between 1985 and 2024, mean sea surface temperature over coral reef areas increased by +1.07 °C. Natural events and phenomena 1 January: a study published in Science Advances concluded that faster flow of the Antarctic Circumpolar Current (ACC) at higher latitudes causes upwelling of isotopically light deep waters around Antarctica, likely increasing atmospheric carbon dioxide levels and thereby potentially constituting a critical positive feedback for future warming. 6 January: a study published in Nature Climate Change stated that a fungal pathogen (Entomophaga maimaiga) that had successfully controlled the defoliation of the spongy moth in North American forests was becoming less effective due to climate change producing hotter, drier conditions. The study predicts this will lead to significantly decreased forest biodiversity and productivity by spongy moths, evidenced by recent",
    "label": 0
  },
  {
    "text": "defoliation of the spongy moth in North American forests was becoming less effective due to climate change producing hotter, drier conditions. The study predicts this will lead to significantly decreased forest biodiversity and productivity by spongy moths, evidenced by recent increases in defoliation. 7 January: NOAA's Arctic Report Card 2024 reported that, including the impact of increased wildfires, the Arctic tundra region had shifted from storing carbon in the soil to becoming a carbon dioxide source, and that the Arctic remained a consistent source of methane—both adding to planet-warming greenhouse gases in the air. 8 January: a study published in Nature concluded that one-quarter of 23,496 decapod crustaceans, fishes and odonates studied, some of which provide climate change mitigation, are threatened with extinction. One-fifth of threatened freshwater species are affected by climate change and severe weather events. 9 January: a study published in Nature Reviews Earth & Environment estimated that since the mid-twentieth century, global-averaged 3-month and 12-month \"hydroclimate whiplash\" events have increased by 31–66% and 8–31%, respectively. Such increases amplify hazards associated with rapid swings between wet and dry states, including flash floods, wildfires, landslides and disease outbreaks. (Hydroclimate volatility refers to \"sudden, large and/or frequent transitions between very dry and very wet conditions\".) 15 January: a study published in Weather reported that the terrestrial biosphere's rate of natural carbon dioxide sequestration has fallen since its 2008 peak at a rate of 0.25% per year, a decline that will accelerate climate change. 16 January: an interdisciplinary and transdisciplinary study published in Communications Earth & Environment identified five key hazards of permafrost thaw: infrastructure failure, disruption of mobility and supplies, decreased water quality, challenges for food security, and exposure to diseases and contaminants. 21 January: a study published in the Proceedings of the National Academy of Sciences reported an \"abrupt,",
    "label": 0
  },
  {
    "text": "of permafrost thaw: infrastructure failure, disruption of mobility and supplies, decreased water quality, challenges for food security, and exposure to diseases and contaminants. 21 January: a study published in the Proceedings of the National Academy of Sciences reported an \"abrupt, coherent, climate-driven transformation\" from \"blue\" (more transparent) to \"brown\" (less transparent) states of lakes in Greenland after a season of both record heat and rainfall drove a state change in these systems. This change was said to alter \"numerous physical, chemical, and biological lake features\", and the state changes were said to be unprecedented. February: though polar bears and grizzly bears traditionally occupy distinct habitats (marine and land), arctic warming has forced polar bears inland into grizzly bear habitats, where the two species mate to produce hybrid \"grolar bears\" that have characteristics poorly adapted to either marine or land habitats. 4 February: a study published in Environmental Research Letters concluded that, in addition to the 2023 marine heat wave, an extensive river plume caused a ~10–20 m (33–66 ft) thick strongly stratified barrier layer that contributed to Hurricane Idalia's rapid intensification (Category 1 to Category 4 in less than 24 hours). 21 February: a study published in npj Natural Hazards unified studies of climatic trends and of wildland–urban interface (WUI) expansion, and concluded that climatic factors increased the frequency of high-risk fire weather by a factor of 2.5, and that the combination of climatic factors with WUI expansion led to a 4.1-fold increase in the frequency of conditions conducive to extreme-impact wildfires from 1990 to 2022 across California. 26 February: a study published in Nature concluded that the Atlantic meridional overturning circulation (AMOC) is resilient to extreme greenhouse gas and North Atlantic freshwater forcings across 34 climate models, suggesting that an AMOC collapse is unlikely in the 21st century. 26",
    "label": 0
  },
  {
    "text": "a study published in Nature concluded that the Atlantic meridional overturning circulation (AMOC) is resilient to extreme greenhouse gas and North Atlantic freshwater forcings across 34 climate models, suggesting that an AMOC collapse is unlikely in the 21st century. 26 February: a study published in Science Advances concluded that short-, mid-, and long-term ambient outdoor heat can significantly accelerate epigenetic aging in older adults. 5 March: a study published in Geophysical Research Letters concluded that redistribution of mass caused by melting of ice sheets and glaciers will shift the Earth's rotational poles from 12–27 metres (39–89 ft) by 2100, depending on the degree of future global warming. Changing Earth's rotational axis affects spacecraft navigation and orientation of deep space telescopes. 10 March: a study published in Nature Sustainability projected that climate change's cooling of the atmosphere occupied by space debris in low Earth orbit will reduce the atmosphere's drag on the debris, extending the debris' lifetime and potentially causing a 50–66% reduction in satellite carrying capacity at altitudes 200–1,000 km (120–620 mi). 19 March: an article published in Nature said that two major natural events occurred at the end of the last ice age: around 10,300 and 8,300 years ago, sea levels surged rapidly due to meltwater releases from the North American and Antarctic ice sheets, with peak rates of sea-level rise reaching nearly 9 mm/year — comparable to projections for the year 2100 under high-emissions scenarios. 25 March: an article published in Nature said that volcanic eruptions over the past 603 years have significantly impacted the strength and tilt of the Atlantic jet stream, which in turn influenced extreme weather events in Europe. These jet stream shifts contributed to increased droughts when moving north and heightened flood risks when shifting south. 10 April: NOAA published a statement that after",
    "label": 0
  },
  {
    "text": "of the Atlantic jet stream, which in turn influenced extreme weather events in Europe. These jet stream shifts contributed to increased droughts when moving north and heightened flood risks when shifting south. 10 April: NOAA published a statement that after a few months of La Niña conditions, the tropical Pacific was ENSO-neutral and was likely to remain so through autumn, stating the most recent La Niña phase was very brief. 21 April: NOAA confirmed the Earth was experiencing its fourth global coral bleaching event, which it called the biggest to date: from 1 January 2023 to 20 April 2025, bleaching-level heat stress had impacted 83.7% of the world's coral reef area, and mass coral bleaching had been documented in at least 83 countries and territories. 30 April: a study published in Nature Communications described how melting of sea ice caused by global warming allows increased areas of open water—which passes a narrower spectrum of light than the smooth continuum of frequencies passed by sea ice—to \"cause major changes in both the pigment and species composition of primary producers in polar ecosystems\". These changes affect photosynthesis in ocean phytoplankton. 31 March: a study published in the American Geophysical Union's JGR Oceans said that most climate model simulations project a decline or disappearance of the Beaufort Gyre before 2100, so that the gyre region would no longer accumulate freshwater, thus impacting oceanic properties in the Arctic and North Atlantic.. 6 May: a study published in Nature Communications concluded that glacial melt is accelerated even when ocean temperatures are constant, a phenomenon caused when cold fresh meltwater rushing from beneath glaciers creates upward turbulence, drawing in warm ocean water from below. 8 May: NOAA's April 2025 U.S. Climate Report documented a slow-moving storm system that brought over 150 tornadoes to the South and",
    "label": 0
  },
  {
    "text": "when cold fresh meltwater rushing from beneath glaciers creates upward turbulence, drawing in warm ocean water from below. 8 May: NOAA's April 2025 U.S. Climate Report documented a slow-moving storm system that brought over 150 tornadoes to the South and Midwest, resulting in at least 24 fatalities and widespread flooding. This event underscores the increasing frequency of severe convective storms in a warming climate. 20 May: a study published in Communications Earth & Environment concluded that merely maintaining a global average temperature of +1.5 °C is inadequate to avoid extensive loss and damage to coastal populations in coming centuries, and hypothesized that \"+1 °C above pre-industrial, possibly even lower\" is required to avoid such loss and damage. 31 May: a study published in Communications Earth & Environment suggested that as glacial melt exposes soil, glacial meltwater's net sequestration of carbon dioxide gives way to water from exposed soil that enhances methane and carbon dioxide production, switching from a negative to positive warming feedback. 4 June: a study published in Nature concluded that atmospheric evaporative demand (AED)—\"atmospheric thirst\" causing moisture to be removed from the Earth's surface—has increased drought severity by an average of 40% globally. 9 June: a study published in Global Change Biology suggested that by 2020, global ocean acidity conditions had already crossed into the uncertainty range of the Planetary Boundary Framework. 16 June: a study published in the Proceedings of the National Academy of Sciences reported a tripling in the past half-century of the number of planetary wave events caused by quasi-resonant amplification (QRA). Such wave events underlie extreme weather events such as heatwaves, flooding, wildfires, and heat dome events. 7 July: findings in a study published in Proceedings of the National Academy of Sciences suggested that current Earth system models overestimate the ability of northern temperate",
    "label": 0
  },
  {
    "text": "underlie extreme weather events such as heatwaves, flooding, wildfires, and heat dome events. 7 July: findings in a study published in Proceedings of the National Academy of Sciences suggested that current Earth system models overestimate the ability of northern temperate forests to absorb carbon, because the models do not consider the impacts of shrinking snowpack and increased soil freeze/thaw cycles. 14 July: a study published in Communications Earth & Environment said that accelerated East Asian reduction in aerosol emissions unmasks greenhouse gas-driven warming, enough to be a main driver of the accelerated global warming since 2010. 5 August: a study published in Geophysical Research Letters concluded that a slowdown in the rate of Arctic sea ice decline since 2005 is largely due to natural climate variation. The study forecast that the slowdown could continue for another five to ten years, although a faster-than-average sea ice decline would be more likely thereafter. 2 September: a study published in the Proceedings of the National Academy of Sciences reported that the Gulf of Panama's seasonal upwelling failed in early 2025, breaking a >40-year record of reliability, which the researchers said \"underscores how climate disruption can threaten wind-driven tropical upwelling systems\". 10 September: a study published in Aerospace assessed how a warming atmosphere could reduce aircraft climb angles by 2035–2064 and concluded that the number of people affected by aircraft-generated noise pollution could increase by up to 4%, and that the effects are maximised for the most damaging and psychologically annoying (low) frequencies. 18 September: the World Meteorological Organization's State of Global Water Resources Report 2024 concluded that only one third of river basins had normal hydrological conditions in 2024, and that all glacier regions lost ice for third straight year. 24 September: a report from the Potsdam Institute for Climate Impact Research assessed",
    "label": 0
  },
  {
    "text": "Report 2024 concluded that only one third of river basins had normal hydrological conditions in 2024, and that all glacier regions lost ice for third straight year. 24 September: a report from the Potsdam Institute for Climate Impact Research assessed that the Ocean Acidification Boundary had been breached for the first time, with ocean acidity entering its danger zone—the seventh of nine critical Earth system boundaries to have been exceeded. 15 October: a study published in Nature concluded that Australian rainforests had transitioned from carbon sink (on balance, removing carbon dioxide from the atmosphere) to carbon source (on balance, adding carbon dioxide to the atmosphere). 15 October: a study published in Nature concluded that global sea-level rise rates since 1900 are faster than in any century over at least the past four millennia, with human activities causing over 94% of observed urban subsidence in southeastern China. 28 November: a study published in Nature Scientific Reports reported that between 2010 and 2017, Africa’s forests and woody savannas transitioned from being a carbon sink to a carbon source, a transition largely attributed to deforestation. 8 December: a study published in the Journal of Child Psychology and Psychiatry concluded that children exposed to average maximum temperatures above 32 °C (90 °F) were less likely to be developmentally on track compared to those exposed to cooler temperatures, with literacy and numeracy skills being most affected. Actions, and goal statements Science and technology 12 February: a study published in Bird Study found that solar farms can benefit bird abundance and biodiversity in arable-dominated landscapes, especially when managed with biodiversity in mind. 3 March: advising policy makers to assimilate uncertainty into decision making to increase decarbonization, a study published in the Proceedings of the National Academy of Sciences urged a \"portfolio approach\" of planting diverse species",
    "label": 0
  },
  {
    "text": "when managed with biodiversity in mind. 3 March: advising policy makers to assimilate uncertainty into decision making to increase decarbonization, a study published in the Proceedings of the National Academy of Sciences urged a \"portfolio approach\" of planting diverse species of trees, as that approach \"reduces exposure to downside cost extremes\". May (reported): a technique involves pumping water from beneath Arctic ice and spraying it on top of the ice. It quickly refreezes, thickening the ice in that location so that it lasts longer into the warm months and increases the amount of planet-warming sunlight that is reflected back into space. Spring: two Chinese companies announced battery technology to enable electric vehicles to drive hundreds of miles on a five-minute charge. BYD Company developed a battery with a peak charging capacity of 1,000 kilowatts, compared to U.S. chargers with peak rates of 400 kilowatts or less. 15 December: the Copernicus Programme's Earth System Science Data released the GloSAT reference analysis, a gridded data set of air temperature change across global land and ocean extending back to the 1780s, using marine air temperature observations rather than the sea surface temperature measurements. Political, economic, legal, and cultural actions 20 January: within hours of his inauguration, U.S. President Donald Trump signed an executive order to withdraw the country from the 2015 Paris Agreement, joining Iran, Libya and Yemen as the only countries not party to the agreement. 4 March: the administration of U.S. President Donald Trump withdrew the country's representatives from the board of the United Nations' Loss and Damage Fund, which was formed to help poor and vulnerable nations cope with climate change-fueled disasters. 13 March: the Costa Rica-based Inter-American Court of Human Rights ordered the Ecuadorian government to protect Indigenous people from oil operations, to leave oil in the ground underneath",
    "label": 0
  },
  {
    "text": "formed to help poor and vulnerable nations cope with climate change-fueled disasters. 13 March: the Costa Rica-based Inter-American Court of Human Rights ordered the Ecuadorian government to protect Indigenous people from oil operations, to leave oil in the ground underneath their lands, and to ensure future oil operations do not impact Indigenous peoples living in voluntary isolation. 14 March: 440 Olympic athletes from 90 countries and 50 sports published an open letter to candidates for President of the International Olympic Committee, urging them to keep \"the spirit of the Games alive by ensuring that sport remains accessible and safe for future generations\". 24 March: the Supreme Court of the United States declined to hear the appeal in Juliana v. United States, ending the case in which youth plaintiffs asserted in their 2015 filing that the government had violated their constitutional rights by encouraging use of fossil fuels. Certain other lawsuits with similar strategies were more successful. 30 March: an open letter signed by 1900 researchers called on the administration of US President Donald Trump to stop its \"wholesale assault on U.S. science\", including its blocking of research on climate change, and causing researchers to live in a \"climate of fear\" amid the administration's flagging of terms such as climate change as objectionable. 28 April: U.S. President Donald Trump dismissed the scientists and experts who compile the National Climate Assessments (NCAs) that are required by the U.S. Congress, the next assessment having been planned for 2028. This decision disrupted the NCA's long-standing process of providing Congress and the public with credible, science-based climate information. Within the week, the American Meteorological Society (AMS) and the American Geophysical Union (AGU) said that together they will produce over 29 peer-reviewed journals to cover climate change; that while not claiming they will replace the NCA,",
    "label": 0
  },
  {
    "text": "science-based climate information. Within the week, the American Meteorological Society (AMS) and the American Geophysical Union (AGU) said that together they will produce over 29 peer-reviewed journals to cover climate change; that while not claiming they will replace the NCA, they will carry on its work. 7 May: a study of emissions inequality from 1990 to 2020 published in Nature Climate Change concluded that two-thirds of global warming is attributable to the wealthiest 10%, and 20% of warming is attributable to the wealthiest 1%. Numerous other statistics were presented. 8 May (reported): the National Oceanic and Atmospheric Administration (NOAA) under U.S. President Donald Trump said that NOAA's National Centers for Environmental Information would no longer update its Billion-Dollar Weather and Climate Disasters database beyond 2024, and that its information—going as far back as 1980—would be archived. 30 June: the globalchange.gov website of the U.S. Global Change Research Program, established in 1990 to host legislatively mandated reports including the National Climate Assessments, was taken down by the Trump administration. July (widely announced): the south Pacific island of Tuvalu, with an average elevation 2 metres (6.6 ft) above sea level, embarks on a plan to evacuate its entire population due to sea level rise, which is projected to inundate most of the territory at high tide by 2050. Australia formed a migration system allownig 280 Tuvaluans per year to settle in Australia as permanent residents. 29 July: the chief administrator of the US Environmental Protection Agency announced that the Trump administration would rescind the 2009 endangerment finding, which concluded that planet-warming greenhouse gases pose a threat to public health. (The endangerment finding is the scientific determination that underpins the U.S. government's legal authority to combat climate change.) A 13 August Carbon Brief fact-check concluded that there were at least 100 false or",
    "label": 0
  },
  {
    "text": "gases pose a threat to public health. (The endangerment finding is the scientific determination that underpins the U.S. government's legal authority to combat climate change.) A 13 August Carbon Brief fact-check concluded that there were at least 100 false or misleading statements in the Trump administration's A Critical Review of Impacts of Greenhouse Gas Emissions on the U.S. Climate that had been published on 23 July to support rescinding of the endangerment finding. On 30 August, Climate Experts' Review of the DOE Climate Working Group Report was published as a compendium of comments from 85 climate experts to counter the government's Critical review. The government's five-member Climate Working Group that authored the Critical Review was dissolved by 5 September in response to a lawsuit filed by the Environmental Defense Fund and Union of Concerned Scientists. On 17 September, the National Academies of Sciences, Engineering, and Medicine (NASEM) published a 137-page paper stating that \"the evidence for current and future harm to human health and welfare created by human-caused GHGs is beyond scientific dispute.\" 22 September: Climate Action Tracker downgraded the US's rating from \"insufficient\" to \"critically insufficient\", citing US climate action under the Trump Administration. 28 October: Bill Gates wrote that \"we’ve made great progress\" fighting climate change, but that funds should not be cut for \"health and development\" for promoting human resilience. He wrote that people \"will be able to live and thrive in most places on Earth for the foreseeable future\", and that much of the climate community now \"focus(es) too much on near-term emissions goals\". November (reported): the University of California, San Diego is thought to be the first major public university in the U.S. to require undergraduate students to take a class on climate change to earn their degree, with other schools adding environmental sustainability requirements.",
    "label": 0
  },
  {
    "text": "the University of California, San Diego is thought to be the first major public university in the U.S. to require undergraduate students to take a class on climate change to earn their degree, with other schools adding environmental sustainability requirements. November: the 2025 United Nations Climate Change Conference (COP30) took place in Belém, Brazil, without the United States sending high-level representatives. Administrators granted access to about 1600 fossil fuel lobbyists, constituting one in 25 attendees and outnumbering representatives from any individual country except Brazil. Negotiations came close to collapse and petrostates blocked measures for transition away from fossil fuels, though there was agreement of a just transition mechanism (JTM). 1 December (reported): US real estate listing service Zillow removed a feature, added in September, that listed individual properties' risk of wildfire, flood, extreme heat, wind and poor air quality. Removal was prompted by what The Guardian described as \"complaints from real estate agents and some homeowners that the rankings appeared arbitrary, could not be challenged and harmed house sales\". 10 December (reported): the US Environmental Protection Agency (EPA) removed mention of fossil fuels from its web page explaining the causes of climate change, retaining mention of only natural phenomena as causes. Mitigation goal statements March: the OECD's Investing in Climate for Growth and Development concluded that accelerated climate action provides economic gains, specifically, that under an Enhanced NDCs scenario, global GDP in 2040 would be 0.2% higher than under the Current Policies Scenario. October: UN Secretary General António Guterres said that based on nationally determined contribution (NDCs)s received so far, there is a predicted 10% reduction of emissions. He said that this reduction does not meet the 60% reduction needed to stay within the 1.5 °C of the Paris Agreement, so that overshooting the 1.5 °C threshold \"is now inevitable\".",
    "label": 0
  },
  {
    "text": "there is a predicted 10% reduction of emissions. He said that this reduction does not meet the 60% reduction needed to stay within the 1.5 °C of the Paris Agreement, so that overshooting the 1.5 °C threshold \"is now inevitable\". 9 December: the European Union Parliament announced a provisional political agreement of a 90% reduction target for greenhouse gas emissions by 2040 compared with 1990 levels, to achieve net-zero emissions in the EU by 2050. Adaptation goal statements 9 March: US Secretary of Defense Peter Hegseth posted on social media website X that \"The (US Department of Defense) does not do climate change crap\", though a 2018 Pentagon study found that nearly half of all U.S. military sites were threatened by weather linked to climate change. Hegseth also called for a review of mission statements and military planning documents to ensure there are no \"references to climate change and related subjects\", though not prohibiting the Pentagon from \"assessing weather-related impacts on operations, mitigating weather-related risks (or) conducting environmental assessments\". The Defense Department had announced cancellation of 91 studies, including research on climate change impacts and global migration patterns, to save about 0.03% of the department's budget in 2024. Consensus 17 April: a study published in Nature Human Behaviour found that presenting people with binary climate data—for example, a lake freezing versus not freezing—significantly increases the perceived impact of climate change compared to when continuous data such as temperature change is presented. The researchers said the findings confirmed the boiling frog effect for climate change communication. 5 June: results of a 20-country survey published in Nature Human Behaviour show strong majority support—including 75% of Europeans and half of Americans—for the Global Climate Scheme (a variant of Cap and Share) a global carbon price and cash redistribution plan. The study also found",
    "label": 0
  },
  {
    "text": "20-country survey published in Nature Human Behaviour show strong majority support—including 75% of Europeans and half of Americans—for the Global Climate Scheme (a variant of Cap and Share) a global carbon price and cash redistribution plan. The study also found widespread support for increased foreign aid or a wealth tax to fund low-income countries. 22 August: results of a public survey of the global south published in Nature Climate Change found that scientists are perceived as the most trusted information source in all countries surveyed except Vietnam, and that trust in scientists correlates with increased climate knowledge. Projections 6 January: A study published in Scientific Reports comparing projected heat-related deaths from climate change with COVID-19 mortality rates across 38 global cities found that in half, annual heat-related deaths would likely exceed COVID-19 death rates within 10 years if global temperatures rise by 3.0 °C above pre-industrial levels. The study projected that cities in North America and Europe, particularly in Mediterranean and Central European regions, would have most dramatic increases in projected heat mortality. 3 February: climate risk financial modeling company First Street Foundation projected that by 2055, 70,026 U.S. neighborhoods (84% of all census tracts) may experience $1.47 trillion in net climate-related property value losses, citing insurance pressures and shifting consumer demand. 4 February: a review article published in Nature Reviews Earth & Environment, linking physical climate science with heat mortality risk, projected that human-caused global warming reaching 2 °C above preindustrial levels will cause tripling of global land area that is \"uncompensable\" (beyond which human core body temperature rises uncontrollably) for young adults. 26 February: a study published in Nature concluded that the Atlantic meridional overturning circulation (AMOC) is resilient to extreme greenhouse gas and North Atlantic freshwater forcings across 34 climate models, suggesting that an AMOC collapse is",
    "label": 0
  },
  {
    "text": "uncontrollably) for young adults. 26 February: a study published in Nature concluded that the Atlantic meridional overturning circulation (AMOC) is resilient to extreme greenhouse gas and North Atlantic freshwater forcings across 34 climate models, suggesting that an AMOC collapse is unlikely in the 21st century. 28 February: based on the Earth's orbital characteristics (precession, obliquity, and eccentricity), a study published in Science concluded that glacial-interglacial periodicity has been largely deterministic in a ~100,000 year cycle, and projected that in the absence of human-caused forcing, the next ice age would start in ~10,000 years. 5 March: a paper published in Nature cited the International Energy Agency as stating that data centers caused 1–1.3% of world electricity demand in 2022, but that, with electricity consumption expected to grow by more than 80% by 2050 owing to all sources, data centers were projected to \"account for a relatively small share of overall electricity demand growth\". 10 March: a study published in Nature Sustainability projected that climate change's cooling of the atmosphere that is occupied by space debris in low Earth orbit will reduce the atmosphere's drag on the debris, extending the debris' lifetime and potentially causing a 50–66% reduction in satellite carrying capacity at altitudes 200–1,000 km (120–620 mi). 12 March: the Boston Consulting Group's Landing the Economic Case for Climate Action with Decision Makers concluded that the net cost of failing to invest to reduce global warming from a current forecast 3.0 °C to 2.0 °C equates to 11% to 27% of cumulative economic output. 20 May: noting that current global temperatures have quadrupled the rate of ice sheet loss since the 1990s, a study published in Nature Communications concluded that maintaining current global temperatures will cause several metres of sea level rise over coming centuries, \"causing extensive loss and damage to",
    "label": 0
  },
  {
    "text": "have quadrupled the rate of ice sheet loss since the 1990s, a study published in Nature Communications concluded that maintaining current global temperatures will cause several metres of sea level rise over coming centuries, \"causing extensive loss and damage to coastal populations and challenging the implementation of adaptation measures\". 28 May: the World Meteorological Organization projected a 70% chance that the average global warming for 2025-2029 will be more than 1.5 °C, up from 47% in last year's report for 2024-2028. 29 May: a study published in Science estimated that if temperatures remain steady, glaciers globally will lose 39% of their 2020 mass, causing a sea level rise of 113 millimetres (4.4 in). More than twice as much glacier mass was projected to remain at the Paris Agreement's +1.5 °C goal than the 2.7 °C predicted from current policies (53% versus 24%). 16 June: a study published in Nature Communications projected that with ≥1.8 °C global warming, there would be a 1.2- to 3-fold increase in obstructive sleep apnea by 2100. 10 September: a report by Common Goal and Football for Future forecast that by 2050, 14 of 16 World Cup stadiums will face extreme heat conditions (unsafe without adaptation), and 11 stadiums will experience unplayable heat (where matches cannot be safely staged). Also, every grassroots pitch analyzed already breached at least three major climate risk thresholds, and two-thirds of grassroots pitches will face unsafe or unplayable heat conditions. 17 September: a study published in Nature projected that if global warming exceeds 2 °C (3.6 °F), at least 99% of western Atlantic coral reefs will be eroding by 2100. 18 September: referring to effects of PM2.5 (fine particulate matter), a study published in Nature projected 34,930-98,430 excess deaths per year in the US by 2050 under a high warming scenario,",
    "label": 0
  },
  {
    "text": "western Atlantic coral reefs will be eroding by 2100. 18 September: referring to effects of PM2.5 (fine particulate matter), a study published in Nature projected 34,930-98,430 excess deaths per year in the US by 2050 under a high warming scenario, and suggested that \"the health impacts of climate-driven wildfire smoke could be among the most important and costly consequences of a warming climate in the US\". 23 September: a study published in Nature Communications projected that many regions may face high risk of \"Day Zero Drought\" (DZD) conditions by the 2020s and 2030s, with the length of time between successive DZD events being shorter DZD durations. 24 October: a study published in Nature Climate Change projected that under current climate policies, emissions until 2050 lock in 0.3 metres (0.98 ft) additional global mean sea level rise by 2300 than historical emissions until 2020, increasing to 0.8 metres (2.6 ft) for emissions until 2090. 17 November: a study published in Environmental Research: Climate examined heatwave changes after greenhouse gas emissions reach net zero, simulating assuming various years in which net zero is achieved. The research concluded that most regional trends show no decline over the entire 1000 years of each simulation, and projected that some regions will experience millennial-scale heating trends when net zero occurs on or after 2050. 18 November: a study published in Nature Communications Earth and Environment concluded that global warming will weaken Andes mountains glaciers' buffering role against megadroughts, estimating that Andes glacier runoff could decline by up to 20% (annual) and 48% (summer) during end-of-century megadroughts compared to pre-2010 levels. 19 November: a study published in Nature Communications reconstructed European summer durations over the last ten millennia and projected that by 2100, European summers could last up to 42 days longer under a business-as-usual scenario. 10",
    "label": 0
  },
  {
    "text": "compared to pre-2010 levels. 19 November: a study published in Nature Communications reconstructed European summer durations over the last ten millennia and projected that by 2100, European summers could last up to 42 days longer under a business-as-usual scenario. 10 December: a study published in Nature concluded that, by 2100 under a high emissions scenario, a large part of the Amazon rainforest will shift to a hotter \"hypertropical\" climate in which \"temperature and moisture conditions during typical dry season months will more frequently exceed identified drought mortality thresholds, elevating the risk of forest dieback\". 17 December: the International Energy Agency forecast that global coal consumption has reached a plateau and may well decline by 2030. Significant publications Moon, Twila A.;Druckenmiller, Matthew L.;Thoman, Richard L., editors. Moon, Twila A.; Druckenmiller, Matthew L.; Thoman, Richard L. (2024). \"Arctic Report Card 2024\" (PDF). Noaa Technical Report Oar Arc ; 24-01 (Arctic Report Card). National Oceanic and Atmospheric Administration (NOAA) Office of Oceanic and Atmospheric Research. doi:10.25923/b7c7-6431. Archived (PDF) from the original on 11 March 2025. Elbeyi, Ece; Bruhn Jensen, Klaus; Aronczyk, Melissa; Asuka, Jusen; Ceylan, Gizem; Cook, John; Erdelyi, Gabor; Ford, Heather; Milani, Carlos; Mustafaraj, Eni; Ogenga, Fredrick; Yadin, Sharon; Howard, Philip N.; Valenzuela, Sebastián (June 2025). Brulle, Robert; Jacquet, Jennifer; Lewandowsky, Stephan; Roberts, Timmons (eds.). \"Information Integrity about Climate Science: A Systematic Review\" (PDF). Zurich, Switzerland: International Panel on the Information Environment (IPIE). doi:10.61452/BTZP3426. Archived (PDF) from the original on 19 June 2025. Guastello, Paula; Smith, Kelly Helm; Knutson, Cody; Svoboda, Mark (July 2025). \"Drought Hotspots Around the World 2023-2025\" (PDF). Prepared by the United States National Drought Mitigation Center for the United Nations Convention to Combat Desertification. Archived (PDF) from the original on 4 July 2025. \"Obligations of States in Respect of Climate Change / Advisory Opinion\" (PDF). International Court of",
    "label": 0
  },
  {
    "text": "by the United States National Drought Mitigation Center for the United Nations Convention to Combat Desertification. Archived (PDF) from the original on 4 July 2025. \"Obligations of States in Respect of Climate Change / Advisory Opinion\" (PDF). International Court of Justice. 23 July 2025. Blunden, J.; Reagan, J.; Dunn, R. J. H. (August 2025). \"State of the Climate in 2024\" (PDF). Journal of the American Meteorological Society. 106 (8): S1 – S513. Bibcode:2025BAMS..106Q...1B. doi:10.1175/2025BAMSStateoftheClimate.1. Kelley, Douglas I.; et al. (16 October 2025). \"State of Wildfires 2024–2025\". Earth System Science Data. 17 (10): 5377–5488. Bibcode:2025ESSD...17.5377K. doi:10.5194/essd-17-5377-2025. \"Report on the Migratory Species and Climate Change Expert Workshop, Edinburgh, United Kingdom\" (PDF). Edinburgh, UK: Convention on the Conservation of Migratory Species of Wild Animals (CMS). February 2025. State of the Global Climate 2024 (WMO-No. 1368). World Meteorological Organization (WMO). 2025. ISBN 978-92-63-11368-9. Ripple, William J.; Wolf, Christopher; Mann, Michael E.; Rockström, Johan; Gregg, Jillian W.; Xu, Chi; Wundrling, Nico; Perkins-Kirkpatrick, Sarah E.; Schaeffer, Roberto; Broadgate, Wendy J.; Newsome, Thomas M.; Schuckburth, Emily; Gleick, Peter H. (29 October 2025). \"The 2025 state of the climate report: a planet on the brink\". BioScience. 75 (12) biaf149. doi:10.1093/biosci/biaf149. Emissions Gap Report 2025 / Off target / Continued collective inaction puts global temperature goal at risk. United Nations Environment Programme (UNEP). November 2025. doi:10.59117/20.500.11822/48854. ISBN 978-92-807-4239-8. \"No Escape II - The Way Forward - Bringing Climate Solutions to the Frontlines of Displacement and Conflict\" (PDF). United Nations High Commissioner for Refugees (UNHCR). November 2025. Archived (PDF) from the original on 13 November 2025. \"World Energy Outlook 2025\" (PDF). International Energy Agency (IEA). November 2025. Archived (PDF) from the original on 19 November 2025. \"State of the Climate / Update for COP30\". World Meteorological Organization (WMO). November 2025. (permalink to download page) Romanello, Marina; et al. (28 October",
    "label": 0
  },
  {
    "text": "(PDF). International Energy Agency (IEA). November 2025. Archived (PDF) from the original on 19 November 2025. \"State of the Climate / Update for COP30\". World Meteorological Organization (WMO). November 2025. (permalink to download page) Romanello, Marina; et al. (28 October 2025). \"The 2025 report of the Lancet Countdown on health and climate change\" (PDF). The Lancet. 406 (10521): 2804–2857. doi:10.1016/S0140-6736(25)01919-1. PMID 41175887. Druckenmiller, Matthew L.; Thoman, Richart L.; Moon, Twila A. (December 2025). \"Arctic Report Card 2025\" (PDF). National Oceanic and Atmospheric Administration (NOAA). Archived (PDF) from the original on 20 December 2025. See also 2025 in science Climatology § History History of climate change policy and politics History of climate change science Politics of climate change § History Timeline of sustainable energy research 2020–present References External links Organizations The Intergovernmental Panel on Climate Change (IPCC) World Meteorological Organization (WMO) Climate indicators at the U.S. Environmental Protection Agency Copernicus Climate Change Service (C3S) Surveys, summaries and report lists Abram, Nerilie J.; Purich, Ariaan; England, Matthew H.; McCormack, Felicity S.; et al. (20 August 2025). \"Emerging evidence of abrupt changes in the Antarctic environment\". Nature. 644 (8077): 621–633. Bibcode:2025Natur.644..621A. doi:10.1038/s41586-025-09349-5. PMID 40836134. Withers, Alison; Daigle, Katie (9 November 2025). \"COP30: The latest in climate science, from faster warming to coral collapse\". Reuters. Archived from the original on 9 November 2025.",
    "label": 0
  },
  {
    "text": "An abrupt climate change occurs when the climate system is forced to transition at a rate that is determined by the climate system energy-balance. The transition rate is more rapid than the rate of change of the external forcing, though it may include sudden forcing events such as meteorite impacts. Abrupt climate change therefore is a variation beyond the variability of a climate. Past events include the end of the Carboniferous Rainforest Collapse, Younger Dryas, Dansgaard–Oeschger events, Heinrich events and possibly also the Paleocene–Eocene Thermal Maximum. The term is also used within the context of climate change to describe sudden climate change that is detectable over the time-scale of a human lifetime. Such a sudden climate change can be the result of feedback loops within the climate system or tipping points in the climate system. Scientists may use different timescales when speaking of abrupt events. For example, the duration of the onset of the Paleocene–Eocene Thermal Maximum may have been anywhere between a few decades and several thousand years. In comparison, climate models predict that under ongoing greenhouse gas emissions, the Earth's near surface temperature could depart from the usual range of variability in the last 150 years as early as 2047. Definitions Abrupt climate change can be defined in terms of physics or in terms of impacts: \"In terms of physics, it is a transition of the climate system into a different mode on a time scale that is faster than the responsible forcing. In terms of impacts, an abrupt change is one that takes place so rapidly and unexpectedly that human or natural systems have difficulty adapting to it. These definitions are complementary: the former gives some insight into how abrupt climate change comes about; the latter explains why there is so much research devoted to it.\" Timescales",
    "label": 0
  },
  {
    "text": "unexpectedly that human or natural systems have difficulty adapting to it. These definitions are complementary: the former gives some insight into how abrupt climate change comes about; the latter explains why there is so much research devoted to it.\" Timescales Timescales of events described as abrupt may vary dramatically. Changes recorded in the climate of Greenland at the end of the Younger Dryas, as measured by ice-cores, imply a sudden warming of +10 °C (+18 °F) within a timescale of a few years. Other abrupt changes are the +4 °C (+7.2 °F) on Greenland 11,270 years ago or the abrupt +6 °C (11 °F) warming 22,000 years ago on Antarctica. By contrast, the Paleocene–Eocene Thermal Maximum may have initiated anywhere between a few decades and several thousand years. Finally, Earth System's models project that under ongoing greenhouse gas emissions as early as 2047, the Earth's near surface temperature could depart from the range of variability in the last 150 years. Past events Several periods of abrupt climate change have been identified in the paleoclimatic record. Notable examples include: About 25 climate shifts, called Dansgaard–Oeschger cycles, which have been identified in the ice core record during the glacial period over the past 100,000 years. The Younger Dryas event, notably its sudden end. It is the most recent of the Dansgaard–Oeschger cycles and began 12,800 years ago and moved back into a warm-and-wet climate regime about 11,600 years ago.\"Abrupt Climate Change - What scientific evidence do we have that abrupt climate change has happened before?\". ocp.ldeo.columbia.edu. It has been suggested that \"the extreme rapidity of these changes in a variable that directly represents regional climate implies that the events at the end of the last glaciation may have been responses to some kind of threshold or trigger in the North Atlantic climate",
    "label": 0
  },
  {
    "text": "extreme rapidity of these changes in a variable that directly represents regional climate implies that the events at the end of the last glaciation may have been responses to some kind of threshold or trigger in the North Atlantic climate system.\" A model for this event based on disruption to the thermohaline circulation has been supported by other studies. The Paleocene–Eocene Thermal Maximum, timed at 55 million years ago, which may have been caused by the release of methane clathrates, although potential alternative mechanisms have been identified. This was associated with rapid ocean acidification The Permian–Triassic Extinction Event, in which up to 95% of all species became extinct, has been hypothesized to be related to a rapid change in global climate. Life on land took 30 million years to recover. The Carboniferous Rainforest Collapse occurred 300 million years ago, at which time tropical rainforests were devastated by climate change. The cooler, drier climate had a severe effect on the biodiversity of amphibians, the primary form of vertebrate life on land. There are also abrupt climate changes associated with the catastrophic draining of glacial lakes. One example of this is the 8.2-kiloyear event, which is associated with the draining of Glacial Lake Agassiz. Another example is the Antarctic Cold Reversal, c. 14,500 years before present (BP), which is believed to have been caused by a meltwater pulse probably from either the Antarctic ice sheet or the Laurentide Ice Sheet. These rapid meltwater release events have been hypothesized as a cause for Dansgaard–Oeschger cycles. A five-year study led by the Oxford School of Archaeology and additionally conducted by Royal Holloway, University of London, the Oxford University Museum of Natural History, and the National Oceanography Centre Southampton completed in 2013 called \"Response of Humans to Abrupt Environmental Transitions\" and referred to as \"RESET\"",
    "label": 0
  },
  {
    "text": "of Archaeology and additionally conducted by Royal Holloway, University of London, the Oxford University Museum of Natural History, and the National Oceanography Centre Southampton completed in 2013 called \"Response of Humans to Abrupt Environmental Transitions\" and referred to as \"RESET\" aimed to see if the hypothesis that humans have major development shifts during or immediately after abrupt climate changes with the aid of knowledge pulled from research on the palaeoenvironmental conditions, prehistoric archaeological history, oceanography, and volcanic geology of the last 130,000 years and across continents. It also aimed to predict possible human behavior in the event of climate change, and the timing of climate change. A 2017 study concluded that similar conditions to today's Antarctic ozone hole (atmospheric circulation and hydroclimate changes), ~17,700 years ago, when stratospheric ozone depletion contributed to abrupt accelerated Southern Hemisphere deglaciation. The event coincidentally happened with an estimated 192-year series of massive volcanic eruptions, attributed to Mount Takahe in West Antarctica. Possible precursors Most abrupt climate shifts are likely due to sudden circulation shifts, analogous to a flood cutting a new river channel. The best-known examples are the several dozen shutdowns of the North Atlantic Ocean's Meridional Overturning Circulation during the last ice age, affecting climate worldwide. The current warming of the Arctic, the duration of the summer season, is considered abrupt and massive. Antarctic ozone depletion caused significant atmospheric circulation changes. There have also been two occasions when the Atlantic's Meridional Overturning Circulation lost a crucial safety factor. The Greenland Sea flushing at 75 °N shut down in 1978, recovering over the next decade. Then the second-largest flushing site, the Labrador Sea, shut down in 1997 for ten years. While shutdowns overlapping in time have not been seen during the 50 years of observation, previous total shutdowns had severe worldwide climate consequences. It",
    "label": 0
  },
  {
    "text": "decade. Then the second-largest flushing site, the Labrador Sea, shut down in 1997 for ten years. While shutdowns overlapping in time have not been seen during the 50 years of observation, previous total shutdowns had severe worldwide climate consequences. It has been postulated that teleconnections – oceanic and atmospheric processes on different timescales – connect both hemispheres during abrupt climate change. Climate feedback effects One source of abrupt climate change effects is a feedback process, in which a warming event causes a change that adds to further warming. The same can apply to cooling. Examples of such feedback processes are: Ice–albedo feedback in which the advance or retreat of ice cover alters the albedo (\"whiteness\") of the earth and its ability to absorb the sun's energy. Soil carbon feedback is the release of carbon from soils in response to global warming. The dying and the burning of forests by global warming. The probability of abrupt change for some climate related feedbacks may be low. Factors that may increase the probability of abrupt climate change include higher magnitudes of global warming, warming that occurs more rapidly and warming that is sustained over longer time periods. Tipping points in the climate system Possible tipping elements in the climate system include regional effects of climate change, some of which had abrupt onset and may therefore be regarded as abrupt climate change. Scientists have stated, \"Our synthesis of present knowledge suggests that a variety of tipping elements could reach their critical point within this century under anthropogenic climate change\". Volcanism Isostatic rebound in response to glacier retreat (unloading) and increased local salinity have been attributed to increased volcanic activity at the onset of the abrupt Bølling–Allerød warming. They are associated with the interval of intense volcanic activity, hinting at an interaction between climate and",
    "label": 0
  },
  {
    "text": "to glacier retreat (unloading) and increased local salinity have been attributed to increased volcanic activity at the onset of the abrupt Bølling–Allerød warming. They are associated with the interval of intense volcanic activity, hinting at an interaction between climate and volcanism: enhanced short-term melting of glaciers, possibly via albedo changes from particle fallout on glacier surfaces. Impacts In the past, abrupt climate change has likely caused wide-ranging and severe impacts as follows: Mass extinctions, most notably the Permian–Triassic extinction event (often referred colloquially to as the Great Dying) and the Carboniferous Rainforest Collapse, have been suggested as a consequence of abrupt climate change. Loss of biodiversity: without interference from abrupt climate change and other extinction events, the biodiversity of Earth would continue to grow. Sea ice decline in the North Atlantic and Nordic Seas, specifically in the Baffin Bay and Labrador Sea. Changes in ocean circulation such as: Increasing frequency of El Niño events Potential disruption to the thermohaline circulation, such as that which may have occurred during the Younger Dryas event. Changes to the North Atlantic oscillation Changes in Atlantic meridional overturning circulation (AMOC) which could contribute to more severe weather events. See also Climate sensitivity Effects of climate change Nuclear winter Volcanic winter Impact winter References",
    "label": 0
  },
  {
    "text": "Assisted migration is \"the intentional establishment of populations or meta-populations beyond the boundary of a species' historic range for the purpose of tracking suitable habitats through a period of changing climate....\" It is therefore a nature conservation tactic by which plants or animals are intentionally moved to geographic locations better suited to their present or future habitat needs and climate tolerances — and to which they are unable to migrate or disperse on their own. In conservation biology, the term first appeared in publications in 2004. It signified a type of species translocation intended to reduce biodiversity losses owing to climate change. In the context of endangered species management, assisted colonization (2007) and managed relocation (2009) were soon offered as synonyms — the latter in a paper entailing 22 coauthors. In forestry science and management, assisted migration is discussed in its own journals and from perspectives different from those of conservation biologists. This is, in part, because paleoecologists had already concluded that there were significant lags in northward movement of even the dominant canopy trees in North America during the thousands of years since the final glacial retreat. In the 1990s, forestry researchers had begun applying climate change projections to their own tree species distribution modelling efforts, and some results on the probable distances of future range shifts prompted attention. As well, translocation terminology was not controversial among forestry researchers because migration was the standard term used in paleoecology for natural movements of tree species recorded in the geological record. Another key difference between forestry practices and conservation biology is that the former, necessarily, was guided by \"seed transfer guidelines\" whenever a timber or pulp harvest was followed up by reforestation plantings. The provincial government of British Columbia in Canada was the first to update their guidelines with, what they",
    "label": 0
  },
  {
    "text": "that the former, necessarily, was guided by \"seed transfer guidelines\" whenever a timber or pulp harvest was followed up by reforestation plantings. The provincial government of British Columbia in Canada was the first to update their guidelines with, what they call, \"climate-based seed transfer.\" Overall, debate concerning the ethics of assisted migration in forestry practice was both short-term and muted compared to that which prevails in conservation biology. For this reason, a separate Wikipedia page titled Assisted migration of forests in North America was launched in 2021 and made into a useful teaching tool for climate adaptation education and decision-making in the forestry profession. The remainder of this page therefore focuses on the topic of assisted migration in conservation biology and especially its applications for management of endangered species. Background Climate change is expected to drive many species out of parts of their current ranges while creating new suitable habitats elsewhere. In order to avoid population declines and extinction, many species will need to either adapt or colonize newly suitable areas. Using a niche modeling approach, scientists have predicted that a failure to migrate or adapt will result in about a quarter of the world's species dying out this century under moderate climate change. The natural dispersal rates for many species are far slower than those needed to keep pace with projected habitat shifts in many regions of the world. Prehistoric climatic changes have resulted in massive global extinctions, and the rate of warming projected for the near future is many times faster than changes in the past 10,000 years. Climate change is increasingly altering geographic ranges where species survive, which can lead to rapid habitat loss and population decline. The inability of some species to naturally keep pace with human-caused climate change has led scientists and land managers to",
    "label": 0
  },
  {
    "text": "Climate change is increasingly altering geographic ranges where species survive, which can lead to rapid habitat loss and population decline. The inability of some species to naturally keep pace with human-caused climate change has led scientists and land managers to consider assisted migration as a means for preventing extinctions. Trees, amphibians, and corals are projected to disperse too slowly to match climate change already underway. Geographic or human-caused barriers to natural dispersal amplify the problem and thus contributed to the listing as \"critically endangered\" two small-range endemic species for which assisted migration is now underway: Australia's western swamp tortoise and America's Florida torreya tree. As of 2023, however, there have been few other examples of assisted migration experiments: A review paper concludes, \"Assisted migration was most common for plants (particularly trees), followed by birds, and was rarely implemented for other taxa.\" Assisted migration v. species introduction Assisted migration is a specific type of species introduction. Species introduction is any act of establishing a species in a habitat it does not currently occupy. It often refers to a long-distance relocation, such as the accidental introduction of an invasive species from one continent to another, or the intentional relocation of a species in decline to a habitat where it can persist. By contrast, assisted migration acknowledges that the natural dispersal rate of many species may be too low to naturally respond to rapid human-caused climate change and instead focuses on where the species would be able to disperse fast enough via natural selection to keep pace with the changing environment. Assisted migration practitioners consider helping the species disperse into such sites, which are often immediately adjacent to the species' historical range. In their eyes, assisted migration represents a small artificial boost to an otherwise natural process. Controversy While assisted migration has the",
    "label": 0
  },
  {
    "text": "practitioners consider helping the species disperse into such sites, which are often immediately adjacent to the species' historical range. In their eyes, assisted migration represents a small artificial boost to an otherwise natural process. Controversy While assisted migration has the potential to allow species that have poor natural dispersal abilities to avoid extinction, it has also sparked debate over the possibility that the migrated species might spread diseases or even become too successful (that is, invasive) in the recipient ecosystems. Even so, several assisted migration projects or experiments have begun for several critically endangered species. Beginning around 2007, opposing pro and con positions became apparent in the field of conservation biology, while still relatively unknown to public promoters of conservation and managers of conservation lands. Supporters generally believe that the expected benefits of assisted migration, including saving and strengthening species, outweigh the potential harm of any project. Detractors generally believe that other conservation techniques which do not include the high risk of invasive species are not only better suited but are also more likely to succeed. This debate continued throughout the literature, generally due to a lack of real-world applications and follow-ups. Though these conservation efforts are becoming increasingly common, few long term looks at their success have been conducted. In 2022 a review paper by seven researchers in the United States assessed shifts in what they called \"conservation strategies for the climate crisis.\" Among the \"novel strategies\" surveyed was \"climate-adaptive assisted migration.\" The team found that academic publications became less focused on the pros and cons of the concept through the years. Instead, more attention was given to modeling or mapping where particular species could be moved. While plants had been the focus of most of the early publications, animals took the lead in recent years. Corals, invertebrates, mammals,",
    "label": 0
  },
  {
    "text": "through the years. Instead, more attention was given to modeling or mapping where particular species could be moved. While plants had been the focus of most of the early publications, animals took the lead in recent years. Corals, invertebrates, mammals, and birds were the leading types of animals assessed for assisted migration needs and prospects. Even so, \"most authors presented assisted species migration as appropriate only for species under exceptionally high threat from climate change.\" By 2023, a news article in the journal Nature reported, \"attitudes towards assisted migration are slowly shifting as conservationists realise just how fast the climate is changing.\" Invasive species risk Perhaps the principal concern scientists have expressed over assisted migration is the potential for relocated species to be invasive in their new habitats, driving out native species. The fear that assisted migration will facilitate invasions stems mostly from observations of the vast numbers of species that have become invasive outside their native ranges by (often inadvertent) introduction by humans. Although most agree that assisted migration efforts, unlike accidental introductions, should involve detailed planning and risk assessment, for some, any threat of introducing invasive species, no matter how small, disqualifies assisted migration as a viable management response to climate change. Those who wish to keep assisted migration on the table often note that the vast majority of historical species invasions have resulted from continent-to-continent or continent-to-island transportation of species and that very few invasions have resulted from the comparatively short-distance, within-continent movement of species proposed for assisted migration. For example, Mueller and Hellman reviewed 468 documented species invasions and found that only 14.7% occurred on the same continent where the species originated. Of the 14.7%, the vast majority were fish and crustaceans. Terrestrial species that became invasive on the same continent where they originated were often",
    "label": 0
  },
  {
    "text": "species invasions and found that only 14.7% occurred on the same continent where the species originated. Of the 14.7%, the vast majority were fish and crustaceans. Terrestrial species that became invasive on the same continent where they originated were often transported across large biogeographic barriers, such as mountain ranges. These long-distance, within-continent translocations are unlike expected uses of assisted migration, which generally involve helping species colonize habitats immediately adjacent to their current ranges. Uncertainty in the planning process To identify populations at risk and locate new potential habitats, conservationists often use niche models. These models predict the suitability of habitats in the future based on how closely their climates resemble the climate currently inhabited by the species. Though useful for describing broad trends, these models make a number of unrealistic assumptions that restrict the usefulness of their predictions. For instance, they do not consider the possibility that species may be able to develop tolerance of new climates through acclimatization or adaptation. Further, they do not account for the fact that a given species may perform better (e.g., become invasive) or worse (e.g., fail to establish) in a new habitat than in its current range if the community of competitor, predator, and mutualist species is different there. Additionally, because different climate variables (e.g., minimum January temperature, average annual precipitation) rarely shift in unison, it is possible that few areas will exactly match the historical climates of species threatened by climate change. Such multi-directional climate shifts will make it especially difficult to determine the species that are at greatest risk of habitat loss due to climate change and to predict future suitable habitat. The uncertainties in predictions of future suitable habitat limits confidence in assisted migration decisions and has led some to reject assisted migration entirely. Despite the uncertainty inherent in predictions",
    "label": 0
  },
  {
    "text": "loss due to climate change and to predict future suitable habitat. The uncertainties in predictions of future suitable habitat limits confidence in assisted migration decisions and has led some to reject assisted migration entirely. Despite the uncertainty inherent in predictions of future suitable habitat, some studies have demonstrated that predictions can be quite accurate. A study of Hesperia comma butterflies in Britain identified unoccupied habitat sites that were likely to support the species under a warmer climate based on their similarity to occupied sites. As the climate warmed, the butterfly colonized many of the sites; most of the sites it did not colonize were located far from existing populations, suggesting they were uncolonized because the butterfly could not reach them on its own. The data suggested that the suitable, uncolonized sites could be good targets for assisted migration. The results suggested that if investigators can demonstrate their model makes reliable predictions with real-world data, models might be trusted for informing assisted migration decisions. Risks and benefits The science is clear that climate change will drive many species extinct, and a traditional, land-preservation ethic will not prevent extinctions. Those wary of moving species instead suggest expanding networks of habitat corridors, allowing species to naturally migrate into newly suitable areas. Under the rates of climate change projected for the coming decades, however, even perfectly connected habitats will probably be insufficient. Species that cannot naturally keep pace with shifting climates will be at risk regardless of habitat connectivity. Evidence suggests that slowly evolving and slowly dispersing species (including species that are dispersal-limited due to habitat fragmentation) will decline or go extinct in the absence of assisted migration programs. In their rejection of assisted migration, Ricciardi and Simberloff cite the precautionary principle, stating that any unknown risk, no matter how small, of assisted migration",
    "label": 0
  },
  {
    "text": "to habitat fragmentation) will decline or go extinct in the absence of assisted migration programs. In their rejection of assisted migration, Ricciardi and Simberloff cite the precautionary principle, stating that any unknown risk, no matter how small, of assisted migration resulting in the creation of new invasive species is enough to require that it not be undertaken. Many scientists reject this position, however, noting that in many cases where extinctions due to climate change are likely, the risks of extinction from not facilitating migration are probably far worse than the risks of facilitating migration. They argue that the precautionary principle cuts both ways, and the risks of inaction must be compared against the risks of action. Others note that the ethics of assisting migration will depend on the values of the stakeholders involved in a specific decision rather than the position of scientists on assisted migration in general. At the very least, some note, scientists should conduct further research into assisted migration and improve our capacity to predict specific outcomes instead of outright rejecting it. Because confidence in expected outcomes is often greater in the short-term (e.g., 20 years) than the long-term future, it may be more reasonable to use short-term projections to guide actions. However, it is also important to consider whether the climate will remain suitable long enough for colonizing species to mature and reproduce, if that is the management goal. Due to climate change, accidental species introductions, and other global changes, there is nowhere on the planet free of human disturbance. Thus, the idea that land managers should refrain from creating human-altered communities through assisted migration may be moot given that all communities have been altered by humans to some degree whether managers undertake assisted migration or not. Given the reality of global change, it will be",
    "label": 0
  },
  {
    "text": "should refrain from creating human-altered communities through assisted migration may be moot given that all communities have been altered by humans to some degree whether managers undertake assisted migration or not. Given the reality of global change, it will be impossible to maintain past ecological communities indefinitely. Many therefore believe we should strive to maintain biodiversity and functioning ecosystems in the face of climate change, even if it means actively moving species beyond their native ranges. In the absence of assisted migration, climate change is already causing many highly mobile species, such as butterflies, to colonize areas they have not previously inhabited. Through assisted migration, managers could help rare or less-mobile species keep pace, possibly preventing future extinctions due to a their inability to colonize new areas fast enough. Though some argue that nature often responds to challenges more effectively in the absence of human intervention, others note that current climate change, itself, is a human intervention. Many species that would have been effective dispersers under slower, natural climate change may be left behind by more mobile species under current rates of human-caused climate change. Thus, through changing the climate, humans may already be artificially segregating species even without actively relocating them. Critics may also have major concerns about different genetic issues when considering assisted migration such as maladaptation to novel environmental conditions and hybridization with similar species. These often depend on the genetic structure and level of genetic variation in the source populations. The environmental conditions in which these populations are being introduced must also be taken into account. In order to enhance genetic variation, and thus adaptive potential, material could be sourced from multiple populations. This is known as composite provenancing. However, if the environmental gradient is well known, such as predictable changes in elevation or aridity, source",
    "label": 0
  },
  {
    "text": "In order to enhance genetic variation, and thus adaptive potential, material could be sourced from multiple populations. This is known as composite provenancing. However, if the environmental gradient is well known, such as predictable changes in elevation or aridity, source populations should be 'genetically matched' to recipient sites as best as possible to ensure that the translocated individuals ae not maladapted. This strategy of moving species beyond their current range has been suggested for those that are severely threatened or endangered. By moving them outside their native range, hopefully the immediate threats of predation, disease, and habitat loss can be avoided. However, these species are usually already suffering from some sort of genetic issue resulting from low effective population size such as inbreeding depression, loss in genetic diversity, or maladaptation. Therefore, caution must be taken with what few individuals remain and rapid population growth must be the primary goal. In the case of some species, this can be accomplished with a captive breeding program Alternatives Even under rapid climate change, dispersal into new areas may not be necessary for some species to persist. Instead of tracking climate shifts through space, some species may be able to survive in their present locations by developing tolerance to new conditions through acclimatization and adaptation. The potential for acclimatization or adaptation to allow persistence in the face of climate change varies by species and is generally poorly understood. One study determined that evolution of higher temperature tolerances in some species of amphibians and reptiles will likely occur fast enough to allow these species to survive a 3 °C temperature increase over 100 years, consistent with low- to mid-range projections of global warming. By contrast, many species, such as most temperate trees, have longer generation times and therefore may adapt more slowly; they may take",
    "label": 0
  },
  {
    "text": "survive a 3 °C temperature increase over 100 years, consistent with low- to mid-range projections of global warming. By contrast, many species, such as most temperate trees, have longer generation times and therefore may adapt more slowly; they may take thousands of years to evolve a similar increase in temperature tolerance. Adaptation this slow would be insufficient for keeping up with expected future global warming if migration of new habitats is not an option. In addition to acclimatization and adaption, assisted evolution is an alternative to assisted migration that has been growing in popularity recently due to the worldwide coral reef crisis. Assisted evolution is the practice of using human intervention to accelerate the rate of natural evolutionary processes. There are three main types of assisted evolution. Stress conditioning Stress conditioning consists of exposing organisms to sublethal stress, with the goal of inducing physiological changes that increase tolerance to future stress events. There has been documented evidence that some changes can be passed throughout generations in both plants and animals. Stress conditioning can be artificially induced in a laboratory environment to create desired responses based on their environments. Notable examples include a 1989 experiment which used stress conditioning via heat shock on rat kidneys to extend their safe cold storage time to 48 hours. More recently, stress conditioning is being studied as a potential solution for the preservation of coral reefs as they are continually exposed to ocean warming and acidification. Assisted gene flow Assisted gene flow (AGF) works to increase the presence of desired naturally occurring genes in offspring. AGF relies on pre-existing genes within the species' genome, rather than the artificial creation and insertion of genetic code within the genome of the species. Assisted gene flow can also introduce related species' genomes into the gene pool to allow",
    "label": 0
  },
  {
    "text": "AGF relies on pre-existing genes within the species' genome, rather than the artificial creation and insertion of genetic code within the genome of the species. Assisted gene flow can also introduce related species' genomes into the gene pool to allow for the introduction of previously impossible behaviors into the new species. AGF identifies genes that produce desired behaviors or tolerance to environmental conditions, and works to increase the chance that parental transmission of the gene in question occurs (also known as heritability). Determining which genes within the genome produce desired behaviors or environmental tolerance consist of experiments which measure the growth, survival, and behavior exhibition of offspring with varying genotypes. AGF is one possible strategy to preserve species that are threatened by climate change, and can be applied to both plants (e.g. forest restoration) or animal populations. Currently, different coral colonies of the Great Barrier Reef are being interbred to test whether offspring display increased resistance to warmer living conditions. Increased resistance to warmer living conditions allow for the preservation of the Great Barrier Reef even as water temperatures continue to rise. Hybridization Hybridization refers to the process where an egg and sperm from two different species can fertilize and produce young. Hybridization was studied in the 1800s by Johann Gregor Mendel, who posthumously has been credited with the discovery of genes and alleles and their impact on an offspring's genotype. Benefits of hybridization include the increase in genetic diversity and the potential for genetic combinations which are able to adapt to, and reproduce in, increasingly difficult environments. Hybridization of coral reefs during the annual coral spawning is being experimented with to create hybrid offspring that will hopefully have higher survival and growth rates in a variety of climate change related conditions. In contrast, for neoendemic species, hybridization could result",
    "label": 0
  },
  {
    "text": "coral reefs during the annual coral spawning is being experimented with to create hybrid offspring that will hopefully have higher survival and growth rates in a variety of climate change related conditions. In contrast, for neoendemic species, hybridization could result in a loss of biodiversity because closely related species that are offered a chance to interbreed could result in a single species rather than the original two or more. Cupressus abramsiana is such an example. The 2016 federal update of the recovery plan for this threatened cypress tree, endemic to a small geographic region along the California coast, warned of the dangers of hybridization. A section of the plan titled \"Genetic introgression\" (also known as introgressive hybridization) explains how the integrity of this species is threatened by nearby horticultural plantings of a sister species, Monterey cypress, whose historically native range is nearby: on the opposite side of Monterey Bay. Hybridization is known to occur between the two endemics — as well as with a widely planted sister species native to Arizona: Arizona cypress. Other consequences of hybridization include the accumulation of deleterious genes, outbreeding depression, and genetic swamping. In the case of outbreeding depression which reduces the fitness of the hybrid species, there is a risk of extinction. This occurs when the population growth rate is below the replacement rate, wasting the reproductive potential of two populations. Governmental policies Global conservation policy A review paper published in the journal Science in 1989, titled \"Translocation as a Species Conservation Tool: Status and Strategy,\" compiled the use of translocations for rare species (of birds and mammals) from 1973 to 1989 in the United States, Hawaii, Canada, Australia, and New Zealand. Habitat destruction, habitat fragmentation, and hunting were the primary causes of decline listed in that paper. Climate change was not mentioned as",
    "label": 0
  },
  {
    "text": "(of birds and mammals) from 1973 to 1989 in the United States, Hawaii, Canada, Australia, and New Zealand. Habitat destruction, habitat fragmentation, and hunting were the primary causes of decline listed in that paper. Climate change was not mentioned as a cause for concern. Three decades later, the International Union for the Conservation of Nature (IUCN) published that \"climate change currently affects at least 10,967 species on the IUCN Red List of Threatened Species.\" In another IUCN publication in 2021, climate change was mentioned 20 times in a 355-page report by the organization's Conservation Translocation Specialist Group; but \"assisted colonization\" as an adaptive response was mentioned just once. In 2022, the update of the United Nations Convention on Biological Diversity agreement recognized \"climate change\" as the third most significant threat to global biodiversity. \"Changes in land and sea use\" along with \"direct exploitation of organisms\" were regarded as greater threats. Because the agreement was at the level of \"goals\" and \"targets,\" no mention was made of conservation tools, such as translocation, nor its climate-adaptive form (assisted migration). USA Endangered Species Act Although the Endangered Species Act of 1973 did not in itself restrict assisted migration, a regulatory change in 1984 regarding \"experimental populations\" made prospective translocations more difficult to justify. June 2022, the U.S. Fish and Wildlife Service published a proposed rule in the Federal Register that would \"revise section 10(j) regulations under the Endangered Species Act to better facilitate recovery by allowing for the introduction of listed species to suitable habitats outside of their historical ranges. The proposed change will help improve the conservation and recovery of imperiled ESA-listed species in the coming decades, as growing impacts from climate change and invasive species cause habitats within their historical ranges to shift and become unsuitable.\" The comment period ended August",
    "label": 0
  },
  {
    "text": "will help improve the conservation and recovery of imperiled ESA-listed species in the coming decades, as growing impacts from climate change and invasive species cause habitats within their historical ranges to shift and become unsuitable.\" The comment period ended August 2022, with more than 500 comments posted online by supporters and opponents. The final decision is scheduled for publication June 2023. A 2010 paper in Conservation Letters had pointed out that, while no statutory changes appeared necessary to facilitate this newly proposed form of climate adaptation, \"current regulations are an impediment to assisted colonization for many endangered animal species, whereas regulations do not necessarily restrict assisted colonization of endangered plants.\" The U.S. Department of Interior in June 2023 announced its decision to modify the section 10(j) rule by deleting reference to \"historical range\" as a parameter for where \"experimental populations\" were authorized to be located. This effectively authorized assisted species migration for endangered or threatened plants and animals. The press release summarized the reason for the change as, \"At the time the original 10(j) regulations were established, the potential impact of climate change on species and their habitats was not fully realized, yet in the decades since have become even more dramatic. These revisions will help prevent extinctions and support the recovery of imperiled species by allowing the Service and our partners to implement proactive, conservation-based species introductions to reduce the impacts of climate change and other threats such as invasive species.\" Implementation A number of scholarly reports have documented natural poleward range shifts of mobile species — notably, butterflies and birds, during the past several decades of global warming. This is especially the case in the United Kingdom, where natural history observations are reputable and reach back several centuries. It has also been documented that plants are being sold",
    "label": 0
  },
  {
    "text": "and birds, during the past several decades of global warming. This is especially the case in the United Kingdom, where natural history observations are reputable and reach back several centuries. It has also been documented that plants are being sold in nurseries in Europe far north of their historically native ranges, and with apparent success in the colder habitats. Evidence of such \"inadvertent assisted migration\", owing to the horticultural trade, has also been documented for plants in the United States. In the North American context, assisted migration is most often discussed in the context of the relocalization of the continent's forests. In the late 2000s and early 2010s, the Canadian provinces of Alberta and British Columbia modified their tree reseeding guidelines to account for the northward movement of forest's optimal ranges. British Columbia even gave the green light for the relocation of a single species, the Western Larch, 1000 km northward. In the series below of actual and prospective assisted migration projects, all but one (Florida torreya tree) are being advocated and implemented by professional scientists, and usually with oversight by governmental endangered species programs. Taxonomic significance in successfully translocating plant and animal species range from (a) maintaining the genetics of an isolated population (American pika), to (b) preventing extinction of a subspecies (Quino checkerspot butterfly), to (c) preventing the extinction of a species (Florida torreya tree), and to (d) preventing extinction of a genus (Western swamp tortoise). First projects Florida torreya, US The Florida torreya (Torreya taxifolia) is a critically endangered tree of the yew family, Taxaceae, found in the Southeastern United States, at the state border region of northern Florida and southwestern Georgia. A self-organized group of conservationists called the Torreya Guardians was created in 2004 to undertake the assisted migration of this glacial relict tree by rewilding",
    "label": 0
  },
  {
    "text": "the Southeastern United States, at the state border region of northern Florida and southwestern Georgia. A self-organized group of conservationists called the Torreya Guardians was created in 2004 to undertake the assisted migration of this glacial relict tree by rewilding it in more northern parts of the United States. The controversy that developed was that the citizens used an exception (just for plants) in the United States Endangered Species Act of 1973 to begin their own assisted migration of a listed critically endangered species — even while the official recovery plan did not yet allow for it. By 2018 the citizens had accomplished documentation of species thrival in a dozen legacy horticultural plantings — including seed production and next-generation saplings at several sites in North Carolina. In 2018 their own plantings in Cleveland, Ohio, began producing seeds (turquoise star in image at right). Early scholarly debates on the pros and cons of assisted migration as a climate-adaptation tool for endangered species conservation often mentioned the Florida torreya project., as did international media Western Swamp Tortoise, Australia The western swamp tortoise (Pseudemydura umbrina) is a critically endangered reptile that is endemic to a small portion of southwestern Australia. It was deemed extinct until it was rediscovered in 1954 and reported in 1981 to be \"a relict species of a monotypic genus, of very restricted range and specialized habitat.\" This species is notable in conservation history for being the first example of an endangered vertebrate that was experimentally translocated to a distant location (300 kilometers poleward) expressly because of climate change. By the time assisted migration trials began, the sole remaining original refuge for this species was inhabited only by captive-bred tortoises that had been reintroduced. The first trial began in 2016, with the release of 24 captive-raised juveniles. In contrast to",
    "label": 0
  },
  {
    "text": "the time assisted migration trials began, the sole remaining original refuge for this species was inhabited only by captive-bred tortoises that had been reintroduced. The first trial began in 2016, with the release of 24 captive-raised juveniles. In contrast to the Florida torreya tree example, this first experiment in assisted migration of an endangered species in Australia was \"preceded by detailed planning and research.\" A generally positive result, despite impediments to statistically significant data, was reported in a journal article four years later. A second trial began in 2022 in the same region, this time in Scott National Park. Lead scientist is Nicola Mitchell, an associate professor of conservation physiology at the University of Western Australia. She openly spoke to the International New York Times about the ethical imperative: Should humans just let nature run its course, thereby dooming this species to extinction because of climate change? \"Or do we have an ethical responsibility\" to act in its behalf? Mark Schwartz, a conservation scientist at the University of California, Davis, was quoted in the article. Speaking about the scale of biodiversity threats posed by climate change, versus using assisted migration as an adaptive strategy, Schwartz said: \"To move enough species to resolve this threat basically seems untenable.\" Yet he also noted that climate-responsive translocations were more acceptable than another approach under consideration: initiating gene editing to make species more climate-proof. Lead scientist Mitchell acknowledged the risks, while offering \"we can potentially undo our mistakes by recapturing them.\" Additional translocations of baby turtles continued into 2023. Early advocacy without implementation Quino Checkerspot butterfly, US The Quino checkerspot (Euphydryas editha quino) is a butterfly native to southern California and northwestern Baja California. It is a subspecies of the common Edith's checkerspot (Euphydryas editha), which ranges as far north as southern British Columbia",
    "label": 0
  },
  {
    "text": "Checkerspot butterfly, US The Quino checkerspot (Euphydryas editha quino) is a butterfly native to southern California and northwestern Baja California. It is a subspecies of the common Edith's checkerspot (Euphydryas editha), which ranges as far north as southern British Columbia and Alberta. In 1997 it became the second subspecies of Edith's checkerspot to be listed under the federal Endangered Species Act. (The first was Bay checkerspot, which was listed as \"threatened\" in 1987.) Notably, it became the first endangered species for which climate change was reported as a current threat and thus a factor to be considered in its recovery plan. However, as reported in The Guardian April 2014 A butterfly species whose population collapsed because of climate change and habitat loss has defied predictions of extinction to rapidly move to cooler climes and change its food plant. The quino checkerspot (Euphydryas editha quino), found in Mexico and California, has shifted to higher altitudes and surprisingly chosen a completely different species of plant on which to lay its eggs, according to research presented at the Butterfly Conservation's seventh international symposium in Southampton.... \"Every butterfly biologist who knew anything about the quino in the mid-1990s thought it would be extinct by now, including me,\" said Prof Camille Parmesan of the Marine Sciences Institute at Plymouth University....\" In a paper titled, \"Endangered Quino checkerspot butterfly and climate change: Short-term success but long-term vulnerability?\", the authors acknowledged the butterfly's surprising ability to utilize a new larval plant food in a cooler nearby habitat and concluded: \"Quino appears resilient to warming climate. However, projections indicate that most or all of Quino's current range in the USA, including the new high elevation expansion, will become uninhabitable. Our most frequent projected future range (circa 2050) is c. 400 km northward from current populations, hence conservation of",
    "label": 0
  },
  {
    "text": "indicate that most or all of Quino's current range in the USA, including the new high elevation expansion, will become uninhabitable. Our most frequent projected future range (circa 2050) is c. 400 km northward from current populations, hence conservation of Quino may eventually require assisted colonization.\" American Pika (rodent) and Joshua Tree, US Within the first decade of the terms assisted migration and assisted colonization appearing in the journals of conservation biology science, two wide-ranging species in the western USA (image right) were scrutinized for possible application of the new climate adaptation tool. The American pika, Ochotona princeps, (a close relative of rabbits) and the Joshua tree, Yucca brevifolia, (the iconic tree of the Mojave Desert) were experiencing climate-driven range contractions in their southernmost populations. At the time, when climate-adaptation tools were originally discussed, scientific applications to well-known species garnered media attention. Because successfully capturing, transporting, and releasing an alpine mammal would require planning and \"considerable financial resources,\" serious advocacy for launching such a project for the pika did not occur. As for Joshua tree, in 2019 the U.S. Fish and Wildlife Service ruled against listing this desert plant as a \"threatened\" species, and California state government did the same in 2022. During this time, three U.S. Fish & Wildlife Service scientists aggregated existing research (including range shift climate modelling) into a report titled, \"Examining the Past, Present, and Future of an Iconic Mojave Desert Species, the Joshua Tree.\" It was published in December 2020. No mention was made of any form of translocation, including assisted migration, for securing the species against future climate change. Stitchbird (hihi), New Zealand The stitchbird, also known as the Hihi, is a bird endemic to New Zealand. Changes in climate have shown to have a profound effect on the hihi's ability to thrive and",
    "label": 0
  },
  {
    "text": "species against future climate change. Stitchbird (hihi), New Zealand The stitchbird, also known as the Hihi, is a bird endemic to New Zealand. Changes in climate have shown to have a profound effect on the hihi's ability to thrive and reproduce. As a result, human caused climate change is an existential threat to the species. The hihi's current native habitat is becoming unstable due to rising temperatures, and suitable temperatures are shifting further south. Assisted migration is being considered as a means of ensuring the hihi can remain in its current natural habitat. Critics, however, argue the risks that are presented to the new host environments are not worth the potential benefits assisted migration may present. Coupled plant and animal project underway In 2024, positive results of a successful combined assisted migration project were reported in the volcanic region near Mexico City. A native, high-altitude tree favored by North America's overwintering population of Monarch butterfly had been test planted on an extinct volcano that offered higher altitudes for trees to grow than the existing monarch sanctuary 75 kilometers away. Thirteen scientists coauthored a paper titled, \"Establishing monarch butterfly overwintering sites for future climates: Abies religiosa upper altitudinal limit expansion by assisted migration.\" They wrote, \"We conclude that the establishment of A. religiosa at 3,600 and 3,800 meters is feasible and that planted stands could eventually serve as overwintering sites for the Monarch butterfly under projected future climates.\" Establishing a slow-growing tree to serve as a mature forest sanctuary by century's end for a migratory insect is a second-level complication beyond the single-species projects that are still considered controversial. Additionally, the team first had to establish at the higher experimental elevations a native shrub, Baccharis conferta, to serve as nurse plants protecting the newly planted fir seedlings from frost and drought",
    "label": 0
  },
  {
    "text": "the single-species projects that are still considered controversial. Additionally, the team first had to establish at the higher experimental elevations a native shrub, Baccharis conferta, to serve as nurse plants protecting the newly planted fir seedlings from frost and drought during their early years. The lead author recounted important sightings of the butterfly near the tree translocation site: \"Monarch butterflies have over recent years established new and large colonies at colder places within the Nevado de Toluca, which suggests that they already are searching for new places to overwinter. Once our seedlings are fully grown, they will hopefully discover our planting site, too.\" This site was chosen because it offers mountain slopes more than a thousand meters higher than the existing butterfly preserve. Inadvertent assisted migration Mature horticultural plantings of trees northward of their native ranges are a form of assisted migration already underway. Because the original plantings likely did not include the goal of helping the trees migrate northward in a warming climate, this form of unintended climate adaptation enabled by humans can be called inadvertent assisted migration. Jesse Bellemare and colleagues may have coined the term in a paper published in 2017: \"It appears that a subset of native plants, particularly those with ornamental value, might already have had opportunities to shift their ranges northward via inadvertent human assistance.\" A subcanopy tree native to the southeastern United States, umbrella magnolia, that had fully naturalized into a forest adjacent to its original horticultural planting in Massachusetts was the subject of an earlier paper by Bellemare. This and other examples suggest not only that poleward assisted migration of plants can be successful, but that distinguishing native from non-native species in this time of rapid climate change will require novel standards. Reports of full naturalization of poleward horticultural plantings of other",
    "label": 0
  },
  {
    "text": "suggest not only that poleward assisted migration of plants can be successful, but that distinguishing native from non-native species in this time of rapid climate change will require novel standards. Reports of full naturalization of poleward horticultural plantings of other native trees have been used as support for intentional deployment of assisted migration at larger scales as a tool for climate adaptation. Coast redwood (native to California) and Torreya taxifolia (native to Florida) are two examples. In 2022 a Canadian Forestry Service publication pointed to the success of horticultural plantings in British Columbia and Washington state, along with a review of research detailing redwood's paleobiogeography and current range conditions, as grounds for proposing that Canada's Vancouver Island already offered \"narrow strips of optimal habitat\" for extending the range of coast redwood. As to Florida torreya, documentation of \"historic groves in northward states\" was presented by the group Torreya Guardians as a supportive factor in their 2019 \"Petition to Downlist from endangered to threatened Torreya taxifolia\". Two years later, a decision was issued and published, with no change in species status of imperilment. But Factor E of the decision, \"Documentation of Historical Groves,\" did acknowledge the citizen accomplishments in this regard. See also Climate change adaptation Forest management Effect of climate change on plant biodiversity Hemerochory Escaped plant References",
    "label": 0
  },
  {
    "text": "A Biodiversity Impact Credit (BIC) is a transferable biodiversity credit designed to reduce global species extinction risk. The underlying BIC metric, developed by academics working at Queen Mary University of London and Bar-Ilan University, is given by a simple formula that quantifies the positive and negative effects that interventions in nature have on the mean long-term survival probability of species. In particular, an organisation's global footprint in terms of BICs can be computed from PDF-based biodiversity footprints. The metric is broadly applicable across taxa (taxonomic groups) and ecosystems. Organisations whose overall biodiversity impact is positive in terms of the BIC metric contribute to achieving the objective of the Global Biodiversity Framework to \"significantly reduce extinction risk\". Use of BICs by businesses has been recommended by the Task Force on Nature-related Financial Disclosures and the first provider of BICs for sale is Botanic Gardens Conservation International (BGCI). The credits are generated by BGCI's international member organisations by rebuilding the populations of tree species at high risk of extinction under the IUCN Red List methodology. Theory Definition Users of BICs distinguish between the metric's scientific definition and how metric values are estimated through methodologies and approximations suitable for particular contexts. This mirrors the situation with carbon credits, which are designed to quantify avoidance or reductions of atmospheric carbon dioxide load but in practice are estimated using a broad variety of context-specific methodologies. For a given taxonomic or functional group of S {\\displaystyle S} species, let N i {\\displaystyle N_{i}} be a measure of the current global population size of the i {\\displaystyle i} th species. This can be measured, e.g., by the number of mature individuals or population biomass, in some cases even by the number of colonies, whichever approximates total reproductive value well. Denote by Δ N i {\\displaystyle \\Delta N_{i}}",
    "label": 0
  },
  {
    "text": "i} th species. This can be measured, e.g., by the number of mature individuals or population biomass, in some cases even by the number of colonies, whichever approximates total reproductive value well. Denote by Δ N i {\\displaystyle \\Delta N_{i}} the change in the global population of species i {\\displaystyle i} resulting from a specific intervention in nature. The corresponding Biodiversity Impact Credits are then given by BIC = ∑ i S Δ N i N i ∗ + N i , {\\displaystyle {\\text{BIC}}=\\sum _{i}^{S}{\\frac {\\Delta N_{i}}{N_{i}^{*}+N_{i}}},} where N i ∗ {\\displaystyle N_{i}^{*}} denotes the population size of species i {\\displaystyle i} at which environmental and demographic stochasticity are of the same magnitude. Calculation Depending on the kind of intervention, the system affected and the available data, a variety of methods is available to estimate BICs. Since typical values of N i ∗ {\\displaystyle N_{i}^{*}} lie in the range of 1 to 100 adult individuals, the contribution of N i ∗ {\\displaystyle N_{i}^{*}} in the definition above is often negligibly small compared to N i {\\displaystyle N_{i}} . The formula then simplifies to BIC = ∑ i S Δ N i N i . {\\displaystyle {\\text{BIC}}=\\sum _{i}^{S}{\\frac {\\Delta N_{i}}{N_{i}}}.} In projects that aim to rebuild the population of a single endangered species i {\\displaystyle i} , the term associated with that species will often dominate the sum in the formula above so that it simplifies further to BIC = Δ N i N i . {\\displaystyle {\\text{BIC}}={\\frac {\\Delta N_{i}}{N_{i}}}.} When a species restoration project has increased the population of a species by an amount that is much larger than the original population (and N i ∗ {\\displaystyle N_{i}^{*}} ) and no comparable increases in the population of that species have occurred elsewhere, then the species' current population N i {\\displaystyle",
    "label": 0
  },
  {
    "text": "species by an amount that is much larger than the original population (and N i ∗ {\\displaystyle N_{i}^{*}} ) and no comparable increases in the population of that species have occurred elsewhere, then the species' current population N i {\\displaystyle N_{i}} is nearly identical to the increase Δ N i {\\displaystyle \\Delta N_{i}} of the population achieved. In this case, the formula above simplifies to BIC = 1. {\\displaystyle {\\text{BIC}}=1.} For use over large areas, approximations expressing BICs in terms of Range Size Rarity, Potentially Disappearing Fraction (PDF) of species, or combinations thereof are available. In particular, an organisation's global footprint in terms of BICs can be computed from PDF-based biodiversity footprints. Interpretation As a simple interpretation, the BIC metric measures the equivalent number of endangered species whose populations have been restored or (for negative BIC) the number of species that should be restored to achieve net zero biodiversity impact. This follows from above approximation that BIC = 1 for the restoration of a single threatened species. However, the BIC metric goes beyond simply counting the number of threatened species that have been restored. It takes into account that decline or recovery of a species can be the result of many small impacts by different actors and attributes both positive and negative credits accordingly. Specifically, it is constructed such that, according to a simple model, BIC > 0 implies that the underlying intervention or combination of interventions leads to a reduction of mean long-term global species extinction risk for the taxonomic or functional group considered. According to the same model, a perfect market for BICs would lead to near-optimal allocation of resources to long-term species conservation. Compatibility with other standards The BIC metric aligns with other globally-recognised biodiversity measures such as the Range Size Rarity, the Species Threat Abatement and",
    "label": 0
  },
  {
    "text": "a perfect market for BICs would lead to near-optimal allocation of resources to long-term species conservation. Compatibility with other standards The BIC metric aligns with other globally-recognised biodiversity measures such as the Range Size Rarity, the Species Threat Abatement and Recovery Metric (START) by IUCN/TNFD, and the Ecosystem Damage Metric underlying the Biodiversity Footprint for Financial Institutions (BFFI). Biodiversity Impact Credits in practice Rationale The search for standardised systems to quantify biodiversity impacts has gained momentum in light of the accelerating rates of biodiversity loss worldwide. Traditional biodiversity conservation efforts can lack scalability and are hard to measure: Improving one area of land or river has a different impact on local biodiversity from improving another, so their impacts are difficult to compare. BICs were developed with the aim to simplify assessments of biodiversity change by focusing on reducing species' extinction risks. The 2022 United Nations Biodiversity Conference emphasised the importance of global collaboration to halt biodiversity loss, marking the adoption of the Kunming-Montreal Global Biodiversity Framework (GBF). BICs are designed to address Target 4 of this framework (\"to halt extinction of known threatened species ... and significantly reduce extinction risk\" and Target 15: \"[Take measures] to ensure that large transnational companies and financial institutions [...] transparently disclose their risks, dependencies and impacts on biodiversity ... in order to progressively reduce negative impacts.\" The Task Force on Nature-related Financial Disclosures via their LEAP methodology recommends use of BICs to quantify impacts on species extinction risk in version 1.1 of their disclosure recommendations. The BIC methodology was one of four recognised metrics for assessing extinction risk. Trees are at the base of the ecological pyramid. Countless species rely on native trees for survival, including fungi, lichen, insects, birds and other vertebrates. Repopulating native tree species improves local biodiversity, helps prevents soil erosion,",
    "label": 0
  },
  {
    "text": "for assessing extinction risk. Trees are at the base of the ecological pyramid. Countless species rely on native trees for survival, including fungi, lichen, insects, birds and other vertebrates. Repopulating native tree species improves local biodiversity, helps prevents soil erosion, conserves water and helps cools the planet as well as being a carbon store. BGCI developed the GlobalTreeSearch database which is the only comprehensive, geo-referenced list of all the world's c.60,000 tree species. Working with the International Union for Conservation of Nature (IUCN) they then produced the Global Tree Assessment which concluded that more than 17,500 tree species (c.30%) are threatened with extinction. Finally, BGCI's Global Tree Conservation Program is the only global programme dedicated to saving the world's threatened tree species. Even before BICs were are launched, over 400 rare and threatened tree species had already been conserved in over 50 countries. Implementation One of the critical components of the BIC system is that it is being driven by conservation organisations like BGCI and their international network of members, and backed by theoretical analyses by several Queen Mary University London academics. These organisations provide the practical know-how and decades of experience in species conservation, focusing particularly on native trees which play a pivotal role in local ecosystems. BGCI is now mediating issuance of transferable BIC certificates to organisations who sponsor tree conservation projects by BGCI member organisations. The BIC system has been designed for easy adoption and scalability. This is crucial for engaging financial institutions and other large corporations that require streamlined, global, comparable, and straightforward metrics to set their sustainability goals. BGCI unveiled their Global Biodiversity Standard at the 2021 United Nations Climate Change Conference – a global biodiversity accreditation framework. BICs are due to be formally launched in early 2024. Critique Biodiversity credits have been criticised by",
    "label": 0
  },
  {
    "text": "their sustainability goals. BGCI unveiled their Global Biodiversity Standard at the 2021 United Nations Climate Change Conference – a global biodiversity accreditation framework. BICs are due to be formally launched in early 2024. Critique Biodiversity credits have been criticised by some who say that putting a monetary value on nature can oversimplify its ecological complexity and intrinsic value, making quantification unreliable or inappropriate. Concerns also remain that credits might be used simply to offset damage to nature, rather than to support proactive conservation efforts Biodiversity credits have also been criticised as a way for companies to make false sustainability claims, a practice called greenwashing. Nonetheless, proponents highlight significant potential benefits: Private investment in nature-based solutions has surged to approximately US $102 billion by mid‑2024, with voluntary biodiversity markets estimated to grow to US $1–2 billion by 2030 (and potentially US $69 billion by 2050) Emerging hybrid mechanisms—such as debt-for-nature swaps and integrated carbon–biodiversity credit frameworks—can channel finance toward verifiable conservation outcomes The voluntary market is gaining ground: since 2022, around US $325,000–1.9 million in credits have been transacted globally The Biodiversity Credit Alliance defines such credits as quantifiable, additional gains, e.g., a 1 percent biodiversity increase per hectare over one year. Pilot schemes—such as New Zealand’s early-stage voluntary market expansion announced in June 2025—are testing the model However, deep challenges remain, including a US $700 billion/year global funding gap, technical complexity in measurement, and governance risks identified in 2025 policy briefs. Ensuring integrity, transparency and robust regulation will be critical to realising the promise of biodiversity credits. See also Biodiversity offsetting Biodiversity banking References",
    "label": 0
  },
  {
    "text": "Lauren B. Buckley is an evolutionary ecologist and professor of biology at the University of Washington. She researches the relationship between organismal physiological and life history features and response to global climate change. Early life and education Lauren Buckley grew up on Conanicut Island, Rhode Island. She developed an early interest in biology while exploring her home island with her parents, both marine biologists. Buckley earned her Bachelor of Arts in biology and mathematics (honors) at Williams College in 2000. She conducted research as a graduate student at Stanford University, where she earned her PhD in 2005. Following her time at Stanford, Buckley pursued postdoctoral fellowships at the National Center for Ecological Analysis and Synthesis (NCEAS) and the Santa Fe Institute. Buckley became an assistant professor at the University of North Carolina Chapel Hill, a position she held from 2009 to 2013. In 2013, she joined the faculty at the University of Washington Department of Biology, and has been a professor from 2019 onward. Her lab, Buckley Lab, is affiliated with the University of Washington, and conducts research on the adaptive and ecological responses of organisms to global climate change. Research Climate change modeling Buckley's work has focused on improving the modeling and forecasting of organismal responses to climate change by integrating field and experimental data. She has demonstrated that the most commonly used models in climate change research are insufficient predictors of species and population behavior, as they neglect essential features such as thermal physiology, behavior, size, and abiotic constraints, all of which factor into species fitness and performance. The mechanistic species distribution models she has developed incorporate essential aspects of species biology to improve predictions and assess the generalizability of certain phenotypes to other systems. Another central concern these modeling improvements seek to address is a how much",
    "label": 0
  },
  {
    "text": "The mechanistic species distribution models she has developed incorporate essential aspects of species biology to improve predictions and assess the generalizability of certain phenotypes to other systems. Another central concern these modeling improvements seek to address is a how much organisms will be able to move to adapt to changing climate patterns, and how this ability changes given an array of biotic and abiotic variables. Ectotherm studies Buckley has utilized a variety of systems to study the responses of ectotherms to climate and weather patterns. Beginning with her dissertation, Buckley’s early research focused heavily on insular lizard population dynamics. Through the study of island lizard populations, she has documented the impact of energy constraints as key limiting factors to the density, abundance, and community structure of ectotherm populations. She has also studied and described the importance of incorporating physiological and life history features into models when predicting the range constraints and shifts of species. Her more recent research has focused insect systems where historical data can be used to test predictions of responses to climate change. In particular, she focuses on the fitness implications of climate change for a species, given its biological and ecological features, which are often-overlooked components of climate change research. Her work has been instrumental in clarifying the performance and fitness consequences of thermal stress for ectotherm species, and brought attention to the particular vulnerability of mid-latitude species to climate change. Buckley has also analyzed the influence of a number of variables on a given species' ability to respond to climate change, especially thermal developmental plasticity. Further, she has drawn important connections between developmental plasticity and maladaptive phenological shifts that may reduce fitness and disrupt ecological interactions, as well as the role of plasticity in attenuating climate change impacts. Her primary insect study systems are Colias",
    "label": 0
  },
  {
    "text": "Further, she has drawn important connections between developmental plasticity and maladaptive phenological shifts that may reduce fitness and disrupt ecological interactions, as well as the role of plasticity in attenuating climate change impacts. Her primary insect study systems are Colias butterflies and grasshopper communities in Colorado. Teaching Buckley teaches undergraduate biology courses at the University of Washington, including BIOL 315: Biological Impacts of Climate Change and BIOL 421: Ecological and Evolutionary Physiology of Animals. Her classes align with her research, emphasizing the interactions between biology and climate change. Public outreach and TrEnCh Project Buckley received an NSF CAREER grant to pursue the development of the Translating Environmental Change (TrEnCh) project. The goal of the TrEnCh project is to develop tools for visualizing the impact and response of environmental and climate change on organisms. It offers open-source, high-resolution data for sophisticated modeling and forecasting with greater detail and accuracy than traditional models that use general parameters. The project integrates a variety of open-source historical and observational data to promote dissemination of the material, and to connect the public to accessible means of understanding climate effects. One of the tools available through the project is TrenchR, a package for organismal energy modeling using the R programming language. The TrEnCh project also uses infrared imaging to better collect data on the thermal conditions of ectotherms, and elucidate the possible resulting climate change impacts on thermally-sensitive processes. The TrEnCh-ed portion of the project is designed to allow students to interact with data and observe climate-related correlations in organismal responses. Honors and awards Select publications Buckley, L. B., Cannistra, A. F., & John, A. (2018). Leveraging organismal biology to forecast the effects of climate change. Integrative and comparative biology, 58(1), 38–51. Buckley, L. B., & Kingsolver, J. G. (2012). Functional and phylogenetic approaches to forecasting",
    "label": 0
  },
  {
    "text": "Climate Action Africa commonly referred to as CMA is a climate resilience organization located at GRA Ikeja, Lagos. It was founded on July 14, 2021, by Grace Oluchi Mbah and Chukwuemeka Fred Agbata. It provides a platform for climate consciousness with data resources to support policies on evidence-based climate issues in Africa. In April 2024, Climate Action Africa formerly known as CMA announced its change of acronym to CAA. This was announced in a climate workshop in Awka, Anambra State, Nigeria. Climate Action Africa participated in the Africa Climate Summit 2023 from September 4 to 6, 2023 at the Kenyatta International Convention Centre (KICC) in Nairobi. It is a known fact that climate change affects every facet of life, thus, the UN Climate Change News of September 2023 featured the Africa Climate Week 2023 (ACW) with a focus on strategizing solutions, especially across the continent of Africa. History Climate Action Africa officially started operations in July 2021 by Oluchi Grace and Chukwuemeka Fred Agbata at 53 Oladipo Bateye Street, G.R.A. Ikeja, Lagos as a center for accelerating innovative climate-tech startup ecosystem in Africa. Its methodology is said to be hinged on community engagement on open knowledge with free articles, reports, infographics, videos, and interactive tools. They are said to be known for offering available documents on awareness and deep knowledge of climate issues to policymakers, communities, stakeholders, individuals, and civil society organizations. The research discoveries are translated for climate-tech organizations and startups for effective use in sustainable climate solutions. Their emphases are known to be on clean energy, sustainable agriculture, and eco-friendly technologies. Climate Action Africa happens to be one of the climate-tech innovation hubs in Africa and has localized its solution to spotlight over 50 innovators that drive climate issues in African communities. CMA is known to provide free",
    "label": 0
  },
  {
    "text": "and eco-friendly technologies. Climate Action Africa happens to be one of the climate-tech innovation hubs in Africa and has localized its solution to spotlight over 50 innovators that drive climate issues in African communities. CMA is known to provide free access to over 2,000 climate knowledge resources in different formats and sources. The free accurate and authentic sources are said to have impacted over 10,000 individuals. Climate Action Africa, Labs (CMA Labs) The Climate Action Africa, Labs (CMA Labs) were created to support the climate-tech startup ecosystem in Africa, innovate on supporting communities' impacts, as well as physical and online mentoring, funding, monitoring, and follow-up. This has also generated collaborations with researchers, experts in climate issues, scientists, and the government as well as with local and international partners, reaching out to over 5,000 people whose focus on climate consciousness has been supported. Again, reports have it that \"The Partech State of Tech in Africa 2022 report showed that the climate-tech sector raised $863 million in equity funding.\" It seems to drive the global actors in the climate sectors into enforcing positive climate sustainability in Africa. It should be remembered that the African continent has witnessed several challenges to climate change. In Nigeria, there have been a series of flood actions from 2012. The United Nations report of 2023 on Nigeria's climate stated that \"in 2022 alone, flooding killed at least 662 people, injured 3,174, displaced about 2.5 million, and destroyed 200,000 houses individuals\". There have been climate effects on agriculture, the environment, businesses, and well-being of the people. Again, the reports of the Premium Times stated that 'the climate scourge affecting Africa and the world is evident in the fluctuating weather events (increased/low rainfall, flood, drought, warming oceans and increasing temperatures among others) which are disrupting food systems, and businesses",
    "label": 0
  },
  {
    "text": "reports of the Premium Times stated that 'the climate scourge affecting Africa and the world is evident in the fluctuating weather events (increased/low rainfall, flood, drought, warming oceans and increasing temperatures among others) which are disrupting food systems, and businesses and causing havoc to lives and livelihoods. Perhaps this is the reason, the United Nations called on the governments, businesses, finance, local authorities, and civil society to respond more on climate issues for a more sustainable economy on what they call \"The world is watching – and the planet can't wait.\" This statement is a confirmation that challenges and obstacles are weighing the climate change activities down. In other words, the call imbues all ideation, support and funds for more sustainable environment, especially in Africa. Climate Action Africa Forum 2024 (CAAF24) The changes in the weather and the environment have opened more learning and fora on discussions, that bother the management of the planet. As many organizations and stakeholders come together to brainstorm on the best practices for saving the environment, Climate Action Africa organized a forum titled, Climate Action Africa Forum 2024 (CAAF24). This took place in Lagos from June 17th to 19th 2024 at the Landmark Event Centre, Lagos, Nigeria. The Theme of the conference was, \"Green Economies, Brighter Future - Innovating and Investing in Africa's Climate-Smart Development.\" To buttress the reason behind CAAF24, the Executive Director & Co-founder of Climate Action Africa Forum, Grace Oluchi Mbah stated \"I stand before you today filled with a profound sense of purpose, but also a deep urgency. The very air we breathe, the land that sustains us, the future we dream for our children – all are under threat from a changing climate.\" There was a campaign on a tree-planting which was tagged 'Billion Trees for Africa.' It was organized",
    "label": 0
  },
  {
    "text": "we breathe, the land that sustains us, the future we dream for our children – all are under threat from a changing climate.\" There was a campaign on a tree-planting which was tagged 'Billion Trees for Africa.' It was organized to mitigate the effects of environmental factors in the society. Some of the agencies and organizations that participated at the CAAF24 included The Nigeria Sovereign Investment Authority, The Catalyst Fund and the Africa Enterprise Challenge Fund are also supporters. The African Development Bank described it an \"innovative platform connecting high-impact climate innovators in Africa with potential investors committed to accelerating sustainable solutions.\" Also in attendance were the government officials, business leaders, academics, civil society representatives, climate experts and the media. There were panel discussions, workshops in breakout sessions, Deal Room for building potential investors for the acceleration of climate-sustainable solutions in Africa. Activities of Climate Action Africa In September 2023, Chukwuemeka Fred Agbata announced the participation of the CMA Team at the Africa Climate Summit 2023. This took place at the Kenyatta International Convention Centre (KICC) in Nairobi, Kenya, from September 4 and 6, 2023. The summit developed the Nairobi Declaration which addressed the challenges of climate change in Africa. World leaders, governments, and policymakers participated in the summit. At the Summit, Grace Oluchi stated, \"As we embark on this journey, let us remember that our collective actions today will determine the world we leave for future generations.\" It becomes critical to work towards saving our planet from further damage by climate issues. CMA has collaborated with tech organizations and hubs which include Creative Space Startups, ISN Hubs, Techbuild, AfriLabs, Founder Institute, and Growth4Her. It has also been known to do joint programs with the Nigerian Library Association (Anambra State Chapter) and Our Lady Queen of Nigeria Catholic Church. The",
    "label": 0
  },
  {
    "text": "This is a list of abbreviations relating to climate change causation, adaptation and mitigation. A ADP - Ad Hoc Working Group on the Durban Platform for Enhanced Action AGN - African Group of Negotiators APA - Ad Hoc Working Group on the Paris Agreement APP - Asia-Pacific Partnership on Clean Development and Climate AR4 - Fourth Assessment Report of the IPCC (2007) AR5 - Fifth Assessment Report of the IPCC (2014) AR5 SYR - Synthesis Report of AR5 AR6 - Sixth Assessment Report of the IPCC (published on 9 August 2021) AWG-KP - Ad Hoc Working Group on further Commitments for Annex I Parties under the Kyoto Protocol AWG-LCA - Ad Hoc Working Group on Long-term Cooperative Action AYCC - Australian Youth Climate Coalition B BAP - Bali Action Plan C C&C - Contraction & Convergence, a global CO2 emissions management model promoted by the Global Commons Institute CAIT - The World Resources Institute's Climate Data Explorer archive CAPP - Climate Action Pacific Partnership CAPP II - Climate Action Pacific Partnership (CAPP) Conference II – 2018 CAPP III - Third meeting of the Climate Action Pacific Partnership Conference (29-30 April 2019) CCA - Climate Change Agreement (UK) CCAFS - Climate Change, Agriculture and Food Security Research Program CCAF - Climate Change Action Fund (Australia) CCC - Committee on Climate Change (UK) CCCEP - Centre for Climate Change Economics and Policy CCCR - Canada's Changing Climate Report CCCS - Centre for Climate Change Studies, University of Dar es Salaam CCF - The Scottish Government's Climate Challenge Fund CCIA - Climate Change in Australia CC:iNet - Climate Change Information Network CCL - Climate Change Levy (UK) CCl2F2 - Dichlorodifluoromethane (greenhouse gas) CCLS - Cambridge Climate Lecture Series CCRA - Climate Change Risk Assessment CCS - Carbon Capture and Storage CCSO - Climate",
    "label": 0
  },
  {
    "text": "in Australia CC:iNet - Climate Change Information Network CCL - Climate Change Levy (UK) CCl2F2 - Dichlorodifluoromethane (greenhouse gas) CCLS - Cambridge Climate Lecture Series CCRA - Climate Change Risk Assessment CCS - Carbon Capture and Storage CCSO - Climate Change Support Office, within the United States' Department of State. CCUS - Carbon capture, utilization, and sequestration CDM - Clean Development Mechanism CDP - Organisation formerly known as the Carbon Disclosure Project CDR - Carbon dioxide removal CER - Certified Emission Reduction CFC - Chlorofluorocarbon CFRF - Climate Financial Risk Forum (UK) CF4 - Carbon tetrafluoride or tetrafluoromethane (greenhouse gas) CGE - Consultative Group of Experts CHClF2 - Chlorodifluoromethane (greenhouse gas) CH4 - Methane CINC - Interdepartmental Committee of Climate Negotiators CLP - The Carbon Literacy Project (CLP) CMA - Meeting of the Parties to the Paris Agreement CMA1 - First meeting of the Parties to the Paris Agreement (7-18 November 2016) CMA1.2 - The second part of the first session of the Conference of the meeting of the Parties to the Paris Agreement (6-17 November 2017) CMA1.3 - The third part of the first session of the Conference of the meeting of the Parties to the Paris Agreement (2-14 December 2018) CMA2 - Second meeting of the Parties to the Paris Agreement (2–13 December 2019) CMA3 - Third meeting of the Parties to the Paris Agreement (postponed to 1–12 December 2021) CMIP - Coupled Model Intercomparison Project CMIP5 - Coupled Model Intercomparison Project, Phase 5 CMP - Conference of the Parties Serving as the Meeting of Parties to the Kyoto Protocol CMP9 - 9th meeting of the Parties to the Kyoto Protocol (11-23 November 2013) CMP10 - 10th meeting of the Parties to the Kyoto Protocol (1-12 December 2014) CMP11 - 11th meeting of the Parties to the Kyoto",
    "label": 0
  },
  {
    "text": "Kyoto Protocol CMP9 - 9th meeting of the Parties to the Kyoto Protocol (11-23 November 2013) CMP10 - 10th meeting of the Parties to the Kyoto Protocol (1-12 December 2014) CMP11 - 11th meeting of the Parties to the Kyoto Protocol (30 November-12 December 2015) CMP12 - 12th meeting of the Parties to the Kyoto Protocol (7-18 November 2016) CMP13 - 13th meeting of the Parties to the Kyoto Protocol (6-17 November 2017) CMP14 - 14th meeting of the Parties to the Kyoto Protocol (2-15 December 2018) CMP15 - 15th meeting of the Parties to the Kyoto Protocol (2–13 December 2019) CMP16 - 16th meeting of the Parties to the Kyoto Protocol (postponed to 1–12 December 2021) CNZ - Carbon Net Zero CO2 - Carbon dioxide CO2-e - Carbon dioxide equivalent, also CO2-eq CoM - Covenant of Mayors for Climate and Energy (Europe) COP - Conference of the Parties [to the UNFCCC] COP1 - First UNFCCC Conference of the Parties (28 March to 7 April 1995) COP2 - Second UNFCCC Conference of the Parties (8-18 July 1996) COP3 - Third UNFCCC Conference of the Parties (1-10 December 1997) COP4 - Fourth UNFCCC Conference of the Parties (2-14 November 1998) COP5 - Fifth UNFCCC Conference of the Parties (25 October to 5 November 1999) COP6 - Sixth UNFCCC Conference of the Parties (13–25 November 2000) COP6-bis - Resumed Session of COP6 (16-27 July 2001) COP7 - Seventh UNFCCC Conference of the Parties (29 October - 10 November 2001) COP8 - Eighth UNFCCC Conference of the Parties (23 October - 1 November 2002) COP9 - Ninth UNFCCC Conference of the Parties (1-12 December 2003) COP10 - Tenth UNFCCC Conference of the Parties (6-14 December 2004) COP11 - Eleventh UNFCCC Conference of the Parties (28 November - 9 December 2005) COP12 - Twelfth",
    "label": 0
  },
  {
    "text": "2002) COP9 - Ninth UNFCCC Conference of the Parties (1-12 December 2003) COP10 - Tenth UNFCCC Conference of the Parties (6-14 December 2004) COP11 - Eleventh UNFCCC Conference of the Parties (28 November - 9 December 2005) COP12 - Twelfth UNFCCC Conference of the Parties (6-17 November 2006) COP13 - 13th UNFCCC Conference of the Parties (3-15 December 2007) COP14 - 14th UNFCCC Conference of the Parties (1-12 December 2008) COP15 - 15th UNFCCC Conference of the Parties (7-18 December 2009) COP16 - 16th UNFCCC Conference of the Parties (29 November - 10 December 2010) COP17 - 17th UNFCCC Conference of the Parties (28 November - 11 December 2011) COP18 - 18th UNFCCC Conference of the Parties (26 November - 6 December 2012) COP19 - 19th UNFCCC Conference of the Parties (11-23 November 2013) COP20 - 20th UNFCCC Conference of the Parties (1-12 December 2014) COP21 - 21st UNFCCC Conference of the Parties (30 November-12 December 2015) COP22 - 22nd UNFCCC Conference of the Parties (7-18 November 2016) COP23 - 23rd UNFCCC Conference of the Parties (6-17 November 2017) COP24 - 24th UNFCCC Conference of the Parties (2-15 December 2018) COP25 - 25th UNFCCC Conference of the Parties (2-13 December 2019) COP26 - 26th UNFCCC Conference of the Parties (1-12 November 2021) COP27 - 27th UNFCCC Conference of the Parties (6-18 November 2022) COP28 - 28th UNFCCC Conference of the Parties (30 November - 13 December 2023), held in the United Arab Emirates COP29 - 29th UNFCCC Conference of the Parties (11-22 November 2024), held in Baku COP30 - 2025 UNFCCC Conference of the Parties (10-21 November 2025), to take place in Belém, Brazil. COP31 - anticipated 2026 UNFCCC Conference of the Parties COP32 - anticipated 2027 UNFCCC Conference of the Parties, to be held in an African host city.",
    "label": 0
  },
  {
    "text": "UNFCCC Conference of the Parties (10-21 November 2025), to take place in Belém, Brazil. COP31 - anticipated 2026 UNFCCC Conference of the Parties COP32 - anticipated 2027 UNFCCC Conference of the Parties, to be held in an African host city. COP33 - anticipated 2028 UNFCCC Conference of the Parties. Indian Prime Minister Narendra Modi has proposed hosting COP33 in India. CORSIA - Carbon Offsetting and Reduction Scheme for International Aviation CPA - Carbon Pricing Act (Singapore) CPRS - Carbon Pollution Reduction Scheme (Australia) CRC - CRC Energy Efficiency Scheme, formerly Carbon Reduction Commitment (UK) CREWS - Climate Risk and Early Warning Systems CRU - Climatic Research Unit at the University of East Anglia CRU TS - Climatic Research Unit Time Series datasets CSA - Climate-Smart Agriculture CVF - Climate Vulnerable Forum C40 - group of cities (now 96) focused on fighting the climate crisis: see C40 Cities Climate Leadership Group D DECC - Department of Energy and Climate Change (UK), now Department for Business, Energy and Industrial Strategy E ECI - Environmental Change Institute at the University of Oxford EPR - Extended Producer Responsibility ETC - Energy Transition Council ETS - Emissions Trading System ETSWAP - Emissions Trading Scheme Workflow Automation Project operated by the UK's Environment Agency F FAR - First Assessment Report of the IPCC (1990) F-gas - Fluorinated gas FICER - Fund for Innovative Climate and Energy Research FOLU - Forestry and other land use FFF - Fridays for Future G GCF - Green Climate Fund GCoM - Global Covenant of Mayors for Climate and Energy GHG - Greenhouse gas GST - Global Stocktake GtC - Gigatonnes of carbon GWP - Global warming potential H HadCM3 - Hadley Centre Coupled Model, version 3 HadGEM - Hadley Centre Global Environmental Model HadGEM1 - HCFC - Hydrochlorofluorocarbon HFC -",
    "label": 0
  },
  {
    "text": "- Greenhouse gas GST - Global Stocktake GtC - Gigatonnes of carbon GWP - Global warming potential H HadCM3 - Hadley Centre Coupled Model, version 3 HadGEM - Hadley Centre Global Environmental Model HadGEM1 - HCFC - Hydrochlorofluorocarbon HFC - Hydrofluorocarbon I ICLEI - International Council for Local Environmental Initiatives IKI - International Climate Initiative (German: Internationalen Klimaschutzinitiative), a German Federal Government initiative IPCC - Intergovernmental Panel on Climate Change IPCC-50 - the IPCC's 50th session (2019) IPCC-NGGIP - IPCC National Greenhouse Gas Inventories Programme ISO 1406x Series - ISO standards for climate change mitigation ISO 14090:2019 - ISO standard for adaptation to climate change — Principles, requirements and guidelines ISO/DIS 14091 - Adaptation to climate change — Guidelines on vulnerability, impacts and risk assessment ISO/TS 14092 - Adaptation to climate change — Requirements and guidance on adaptation planning for local governments and communities J JI - Joint Implementation K KLD - Ministry of Climate and Environment (Klima- og miljødepartementet), Norway L LCDI - Low Carbon Development Indonesia LDCF - Least Developed Countries Fund LECBP - Low Emission Capacity Building Programme LEDS - Low-Emission Development Strategies LSCE - Laboratoire des sciences du climat et de l'environnement, Gif-sur-Yvette, France LULUCF - Land use, land-use change, and forestry M MCC - Mercator Research Institute on Global Commons and Climate Change, Berlin MPGCA - Marrakech Partnership for Global Climate Action MoCC - Ministry of Climate Change (Pakistan) MoEFCC - Ministry of Environment, Forest and Climate Change (India) MOP1 - 1st Meeting of the Parties to the Kyoto Protocol (28 November - 9 December 2005) MOP2 - 2nd Meeting of the Parties to the Kyoto Protocol (6-17 November 2006) MOP3 - 3rd Meeting of the Parties to the Kyoto Protocol (3-15 December 2007) MOP4 - 4th Meeting of the Parties to the Kyoto Protocol",
    "label": 0
  },
  {
    "text": "2005) MOP2 - 2nd Meeting of the Parties to the Kyoto Protocol (6-17 November 2006) MOP3 - 3rd Meeting of the Parties to the Kyoto Protocol (3-15 December 2007) MOP4 - 4th Meeting of the Parties to the Kyoto Protocol (1-12 December 2008) MOP5 - 5th Meeting of the Parties to the Kyoto Protocol (7-18 December 2009) MRF - Materials Recovery Facility MWE/CCD - Climate Change Department of the Ministry of Water and Environment (Uganda) N NAMA - Nationally Appropriate Mitigation Actions NAPA - National Adaptation Programme of Action NAZCA - Non-state Actor Zone for Climate Action NC - National Communication (under the Paris Agreement) NCQG - New Collective Quantified Goal: see 2024 United Nations Climate Change Conference NDC - Nationally Determined Contributions NECIA - Northeast Climate Impacts Assessment (USA) N2O - Nitrous Oxide NRSP - National Reports Submission Portal O O3 - Ozone P PATPA - Partnership on Transparency in the Paris Agreement P-CAN - Place-based Climate Action Networks, a UK-based partnership between university researchers and the public, private and third sectors in tackling climate change, aiming to accelerate and sustain the transition to a low-carbon, climate-resilient society through the creation of local climate commissions. PCD - Petersberg Climate Dialogue PCD X - Petersberg Climate Dialogue 10 (13-14 May 2019) PCD XI - Petersberg Climate Dialogue (27-28 April 2020) PFC - Perfluorocarbon PIK - Potsdam Institute for Climate Impact Research (German: Potsdam-Institut für Klimafolgenforschung) S SAR - Second Assessment Report of the IPCC (1995) SB 52 - Fifty-second session of the Subsidiary Body for Scientific and Technological Advice (SBSTA 52) and the fifty-second session of the Subsidiary Body for Implementation (SBI 52) (postponed to 2021) SB 56 - the Bonn Climate Change Conference, 56th session of the subsidiary bodies, held on 6 to 16 June 2022 SBI - Subsidiary",
    "label": 0
  },
  {
    "text": "52) and the fifty-second session of the Subsidiary Body for Implementation (SBI 52) (postponed to 2021) SB 56 - the Bonn Climate Change Conference, 56th session of the subsidiary bodies, held on 6 to 16 June 2022 SBI - Subsidiary Body for Implementation SBI 46 - Forth-sixth session of the Subsidiary Body for Implementation (8-18 May 2017) SBI 47 - Forty-seventh session of the Subsidiary Body for Implementation (6-15 November 2017) SBI 52 - Fifty-second session of the Subsidiary Body for Implementation (postponed to 2021) SBSTA - Subsidiary Body for Scientific and Technological Advice SBTi - Science Based Targets initiative SCCF - Special Climate Change Fund SDA - Sectoral Decarbonization Approach SDGs - Sustainable Development Goals SECR - Streamlined Energy and Carbon Reporting framework (UK) SF6 - Sulfur hexafluoride SRCCL - Special Report on Climate Change and Land of the IPCC SRES - Special Report on Emissions Scenarios of the IPCC SR15 - IPCC's Special Report on Global Warming of 1.5 °C SSP - Shared Socioeconomic Pathway T TACC - Territorial Approach to Climate Change TAR - Third Assessment Report of the IPCC (2001) TCCC - Tarawa Climate Change Conference TCFD - Task Force on Climate-related Financial Disclosures tCO2 - Tonnes of carbon dioxide equivalent TD - Talanoa Dialogue TFI - Task Force on National Greenhouse Gas Inventories U UKCIP - Multi-disciplinary team formerly known as the UK Climate Impacts Programme, based at the Environmental Change Institute at the University of Oxford UKCP - UK Climate Projections UKCP09 - UK Climate Projections 2009 UKCP18 - UK Climate Projections 2018 UKHACC - UK Health Alliance on Climate Change UN CC:Learn - One UN Climate Change Learning Partnership UNEP - United Nations Environment Programme UNFCCC - United Nations Framework Convention on Climate Change USCAP - U.S. Climate Action Partnership W WCAS -",
    "label": 0
  },
  {
    "text": "Climate change adaptation is the process of adjusting to the effects of climate change, both current and anticipated. Adaptation aims to moderate or avoid harm for people, and is usually done alongside climate change mitigation. It also aims to exploit opportunities. Adaptation can involve interventions to help natural systems cope with changes. Adaptation can help manage impacts and risks to people and nature. The four types of adaptation actions are infrastructural, institutional, behavioural and nature-based options. Some examples are building seawalls or inland flood defenses, providing new insurance schemes, changing crop planting times or varieties, and installing green roofs or green spaces. Adaptation can be reactive (responding to climate impacts as they happen) or proactive (taking steps in anticipation of future climate change). The need for adaptation varies from place to place. Adaptation measures vary by region and community, depending on specific climate impacts and vulnerabilities. Worldwide, people living in rural areas are more exposed to food insecurity owing to limited access to food and financial resources. For instance, coastal regions might prioritize sea-level rise defenses and mangrove restoration. Arid areas could focus on water scarcity solutions, land restoration and heat management. The needs for adaptation will also depend on how much the climate changes or is expected to change. Adaptation is particularly important in developing countries because they are most vulnerable to climate change. Adaptation needs are high for food, water and other sectors important for economic output, jobs and incomes. One of the challenges is to prioritize the needs of communities, including the poorest, to help ensure they are not disproportionately affected by climate change. Adaptation plans, policies or strategies are in place in more than 70% of countries. Recent research indicates that while global awareness and planning for climate change adaptation have increased, the actual implementation of",
    "label": 0
  },
  {
    "text": "are not disproportionately affected by climate change. Adaptation plans, policies or strategies are in place in more than 70% of countries. Recent research indicates that while global awareness and planning for climate change adaptation have increased, the actual implementation of adaptation measures remains limited and uneven across regions. Agreements like the Paris Agreement encourage countries to develop adaptation plans. Other levels of government like cities and provinces also use adaptation planning. So do economic sectors. Donor countries can give money to developing countries to help develop national adaptation plans. Effective adaptation is not always autonomous; it requires substantial planning, coordination, and foresight. Studies have identified key barriers such as knowledge gaps, behavioral resistance, and market failures that slow down adaptation progress and require strategic policy intervention. Addressing these issues is crucial to prevent long-term vulnerabilities, especially in urban planning and infrastructure investments that determine resilience to climate impacts. Furthermore, adaptation is deeply connected to economic development, with decisions in industrial strategy and urban infrastructure shaping future climate vulnerability. Definition The IPCC defines climate change adaptation in this way: \"In human systems, as the process of adjustment to actual or expected climate and its effects in order to moderate harm or take advantage of beneficial opportunities.\" \"In natural systems, adaptation is the process of adjustment to actual climate and its effects; human intervention may facilitate this.\" Adaptation actions can be incremental and transformative. Incremental actions are actions that aim to maintain the essence and integrity of a system. Transformative actions are actions that change the fundamental attributes of a system in response to climate change and its impacts. Understanding the need Research on climate change adaptation has been ongoing since the 1990s. The number and variety of subtopics has greatly increased since then. Adaptation has become an established policy area in",
    "label": 0
  },
  {
    "text": "response to climate change and its impacts. Understanding the need Research on climate change adaptation has been ongoing since the 1990s. The number and variety of subtopics has greatly increased since then. Adaptation has become an established policy area in the 2010s and since the Paris Agreement, and an important topic for policy research. Climate change impacts research Scientific research into climate change adaptation generally starts with analyses of the likely effects of climate change on people, ecosystems, and the environment. These impacts cover its effects on lives, livelihoods, health and well-being, ecosystems and species, economic, social and cultural assets, and infrastructure. Impacts may include changed agricultural yields, increased floods, and droughts or coral reef bleaching. Analysis of such impacts is an important step in understanding current and future adaptation needs and options. As of 2022, the level of warming is 1.2 °C (34.2 °F) above levels before the industrial revolution. It is on track to increase to 2.5 to 2.9 °C (36.5 to 37.2 °F) by the end of the century. This is causing a variety of secondary effects. Many negative effects of climate change involve changes in extremes or the way conditions vary rather than changes in average conditions. For example, the average sea level in a port might not be as important as the height of water during a storm surge. That is because a storm surge can cause flooding. The average rainfall in an area might not be as important as how frequent and severe droughts and extreme precipitation events become. Disaster risks, response and preparedness Climate change contributes to disaster risk. Therefore, experts often see climate change adaptation as one of many processes within disaster risk reduction. In turn, disaster risk reduction is part of the broader consideration of sustainable development. Climate change adaptation and",
    "label": 0
  },
  {
    "text": "Climate change contributes to disaster risk. Therefore, experts often see climate change adaptation as one of many processes within disaster risk reduction. In turn, disaster risk reduction is part of the broader consideration of sustainable development. Climate change adaptation and disaster risk reduction have similar goals (to reduce potential impacts of hazards and increase the resilience of people at risk). They use similar concepts and are informed by similar sources and studies. Disasters are often triggered by natural hazards. A natural event such as a fire or flood is not of itself a disaster: it's only when it affects people or is caused by them that is counts as a disaster. It is argued that natural disasters are always linked to human action or inaction or rooted in anthropogenic processes. Disasters, economic loss, and the underlying vulnerabilities that drive risk are increasing. Global risks like climate change are having major impacts everywhere. Scientists forecast climate change will increase the frequency and severity of extreme weather events and disasters. Adaptation may include measures to increase preparedness and relevant disaster response capacities. Aims For humans, adaptation aims to moderate or avoid harm, and to exploit opportunities. For natural systems, humans may intervene to help adjustment. Policy aims The Paris Agreement of 2015 requires countries to keep global temperature rise this century to less than 2 °C above pre-industrial levels, and to pursue efforts to limit the temperature increase to 1.5 °C. Even if greenhouse gas emissions are stopped relatively soon, global warming and its effects will last many years. This is due to the inertia of the climate system. So both carbon neutrality (\"net zero\") and adaptation are necessary. The Global Goal on Adaptation was also established under the Paris Agreement. The specific targets and indicators for the Global Goal are in",
    "label": 0
  },
  {
    "text": "to the inertia of the climate system. So both carbon neutrality (\"net zero\") and adaptation are necessary. The Global Goal on Adaptation was also established under the Paris Agreement. The specific targets and indicators for the Global Goal are in development as of 2023. It will support the long-term adaptation goals of the governments that are parties to the agreement. It also aims to fund support for the most vulnerable countries' adaptation needs in the context of the 1.5/2 °C goal. It has three core components. These are reducing vulnerability to climate change, enhancing adaptive capacity, and strengthening resilience. Reduce risk factors: vulnerability and exposure Adaptation can help decrease climate risk by addressing three interacting risk factors. These are hazards, vulnerability, and exposure. It is not possible to directly reduce hazards. This is because hazards are affected by current and future changes in climate. Instead, adaptation addresses the risks of climate impacts that arise from the way climate-related hazards interact with the exposure and vulnerability of human and ecological systems. Exposure refers to the presence of people, livelihoods, ecosystems and other assets in places that could suffer negative effects. It is possible to reduce exposure by retreating from areas with high climate risks, such as floodplains. Improving systems for early warnings and evacuations are other ways to reduce exposure. The IPCC defines climate change vulnerability as \"the propensity or predisposition to be adversely affected\" by climate change. It can apply to humans but also to natural systems. Human and ecosystem vulnerability are interdependent. According to the IPCC, climate change vulnerability encompasses a variety of concepts and elements, including sensitivity or susceptibility to harm and lack of capacity to cope and adapt. Sensitivity to climate change could be reduced by for example increasing the storage capacity of a reservoir, or planting",
    "label": 0
  },
  {
    "text": "encompasses a variety of concepts and elements, including sensitivity or susceptibility to harm and lack of capacity to cope and adapt. Sensitivity to climate change could be reduced by for example increasing the storage capacity of a reservoir, or planting crops that are more resistant to climate variability. It is also possible to reduce vulnerability in towns and cities with green garden spaces. These can reduce heat stress and food insecurity for low-income neighbourhoods. Ecosystem-based adaptation is one way to reduce vulnerability to climate hazards. For instance, mangroves can dampen storm energy. So they can help prevent flooding. In this way, protection of the mangrove ecosystem can be a form of adaptation. Insurance and livelihood diversification increase resilience and decrease vulnerability. Other ways to decrease vulnerability include strengthening social protection and building infrastructure more resistant to hazards. Increase adaptive capacity Adaptive capacity in the context of climate change covers human, natural, or managed systems. It looks at how they respond to both climate variability and extremes. It covers the ability of a system to adjust to climate change to moderate potential damages, to take advantage of opportunities, or to cope with consequences. Adaptive capacity is the ability to reduce the likelihood of negative impacts of climate-related hazards. It does this through the ability to design and implement effective adaptation strategies, or to react to evolving hazards and stresses. Societies that can respond to change quickly and successfully have a high adaptive capacity. Conversely, high adaptive capacity does not necessarily lead to successful adaptation action. It does not necessarily succeed in goals of equity and enhancing well-being. In general, adaptation capacity differs between high and low-income countries. By some indices such as ND-GAIN, high-income countries tend to have higher adaptive capacity. However, there is strong variation within countries. The determinants of",
    "label": 0
  },
  {
    "text": "goals of equity and enhancing well-being. In general, adaptation capacity differs between high and low-income countries. By some indices such as ND-GAIN, high-income countries tend to have higher adaptive capacity. However, there is strong variation within countries. The determinants of adaptive capacity include: Economic resources: Wealthier nations are better able to bear the costs of adaptation to climate change than poorer ones. Technology: Lack of technology can impede adaptation. Information and skills: Information and trained personnel are necessary to assess and implement successful adaptation options. Social infrastructure Institutions: Nations with well-developed social institutions are likely to have greater adaptive capacity than those with less effective institutions. These are typically developing nations and economies in transition. Equity: Some believe that adaptive capacity is greater where there are government institutions and arrangements in place that allow equitable access to resources. Strengthening resilience The IPCC considers climate resilience to be \"the capacity of social, economic and ecosystems to cope with a hazardous event or trend or disturbance\". It includes the abilities to reorganise and learn. This definition is similar to that of climate change adaptation. However, resilience involves a more systematic approach to absorbing change. It involves using those changes to become more efficient. The idea is that people can intervene to reorganise the system when disturbance creates an opportunity to do so. Implemented adaptation most often builds upon resilience as a way of bouncing back to recover after a disturbance. Experts consider it to be incremental rather than transformational. On the other hand, climate resilience-focused projects can be activities to promote and support transformational adaptation. This is because transformational adaptation is connected with implementation at scale and ideally at the system-level. Strengthening resilience is therefore important for maintaining a capacity for transformation. Transformations, and the processes of transition, cover the major systems",
    "label": 0
  },
  {
    "text": "support transformational adaptation. This is because transformational adaptation is connected with implementation at scale and ideally at the system-level. Strengthening resilience is therefore important for maintaining a capacity for transformation. Transformations, and the processes of transition, cover the major systems and sectors at scale. These are energy, land and ecosystems, urban and infrastructure, and industrial and societal. Transformations may fail if they do not integrate social justice, consider power differences and political inclusion, and if they do not deliver improvements in incomes and wellbeing for everyone. Climate resilient development is a closely related area of work and research topic that has recently emerged. It describes situations in which adaptation, mitigation and development solutions are pursued together. It is able to benefit from synergies from among the actions and reduce trade-offs. Co-benefits with mitigation Strategies to limit climate change are complementary to efforts to adapt to it. Limiting warming, by reducing greenhouse gas emissions and removing them from the atmosphere, is also known as climate change mitigation. There are some synergies or co-benefits between adaptation and mitigation. Synergies include the benefits of public transport for both mitigation and adaptation. Public transport has lower greenhouse gas emissions per kilometer travelled than cars. A good public transport network also increases resilience in case of disasters. This is because evacuation and emergency access becomes easier. Reduced air pollution from public transport improves health. This in turn may lead to improved economic resilience, as healthy workers perform better. Options by type of action There are many adaptation responses. We sometimes call them adaptation measures, strategies or solutions. They help manage impacts and risks to people and nature. Current adaptation focuses on near-term climate risks. It also focuses on particular sectors, such as water and agriculture, and on regions, such as Africa and Asia. It is",
    "label": 0
  },
  {
    "text": "or solutions. They help manage impacts and risks to people and nature. Current adaptation focuses on near-term climate risks. It also focuses on particular sectors, such as water and agriculture, and on regions, such as Africa and Asia. It is important to close gaps between adaptation that is carried out and the needs relative to today's climate in order to reduce risks to a tolerable level. However, future adaptation must also anticipate future climate change risks. Some options may become less effective or entirely unfeasible as global warming increases. Adaptation responses fall into four categories that all directly aim to reduce risks and exploit opportunities: Infrastructural and technological adaptation (including engineering, built environment, and high-tech solutions); Institutional adaptation (economic organizations, laws and regulation, government policies and programmes); Behavioural and cultural (individual and household strategies as well as social and community approaches); Nature-based solutions (including ecosystem-based adaptation options). We can also group options into three categories: 1. Structural and physical adaptation (including engineering and built environment, technological, ecosystem-based, services); 2. Social adaptation (educational, informational, behavioural); 3. Institutional adaptation (economic organizations, laws and regulation, government policies and programmes). Other ways to distinguish types of adaptation are anticipatory versus reactive, autonomous versus planned and incremental versus transformational. Incremental adaptation actions aim to maintain the essence and integrity of a system. Transformative actions change the fundamental attributes of a system in response to climate change and its impacts. Autonomous adaptation is adaptation response to experienced climate and its effects. It does not involve explicit planning and does not specifically focus on addressing climate change. Planned adaptation can be reactive or anticipatory. Anticipatory adaptation is undertaken before impacts are apparent. Relying on autonomous adaptation to climate change can result in substantial costs. It is possible to avoid many of these costs with planned adaptation. Infrastructural",
    "label": 0
  },
  {
    "text": "Planned adaptation can be reactive or anticipatory. Anticipatory adaptation is undertaken before impacts are apparent. Relying on autonomous adaptation to climate change can result in substantial costs. It is possible to avoid many of these costs with planned adaptation. Infrastructural and technological options Built environment Built environment options include installing or upgrading infrastructure to protect against flooding, sea level rise, heatwaves and extreme heat. They also include infrastructure to respond to changed rainfall patterns in agriculture. This could be infrastructure for irrigation. A survey conducted in the European Union revealed that 39% of respondents cited the improvement of infrastructure as a key priority for local climate adaptation, which includes measures such as installing better drainage systems, flood barriers, storm shelters, and more resilient power grids. Early warning systems, which may include AI and sensor-based monitoring, are used to identify vulnerabilities in infrastructure and support adaptive measure. Climate change adaption is on the agenda of European municipal governments. However, apart from new agencies being established with expertise on climate change adaptation and urban planning, the adaptation of the built environment results in significant changes to the daily work of a city's administrative staff. Early warning systems Climate services Institutional options Institutional responses include zoning regulations, new building codes, new insurance schemes, and coordination mechanisms. Policies are important tools to integrate issues of climate change adaptation. At the national level, adaptation strategies appear in National Adaptation Plans (NAPS) and National Adaptation Programmes of Action (NAPA). They also occur in national climate change policies and strategies. These are at different levels of development in different countries and in cities. This is discussed further in the section below on \"implementation\". Cities, states, and provinces often have considerable responsibility in land use planning, public health, and disaster management. Institutional adaptation actions occur more frequently in",
    "label": 0
  },
  {
    "text": "in different countries and in cities. This is discussed further in the section below on \"implementation\". Cities, states, and provinces often have considerable responsibility in land use planning, public health, and disaster management. Institutional adaptation actions occur more frequently in cities than in other sectors. Some have begun to adapt to threats intensified by climate change, such as flooding, bushfires, heatwaves, and rising sea levels. Building codes Managing the codes or regulations that buildings must conform to is important for keeping people healthy and comfortable during extremes of hot and cold and protecting them from floods. There are many ways to do this. They include increasing the insulation values, adding solar shading, increasing natural ventilation or passive cooling, codes for green roofs to reduce urban heat island effects or requiring waterfront properties to have higher foundations. Land use zoning controls are central to investment in urban development. They can reduce risks to areas threatened by floods and landslides. Insurance Insurance spreads the financial impact of flooding and other extreme weather events. There is an increasing availability of such options. For example, index-based insurance is a new product which triggers payment when weather indices such as precipitation or temperature cross a threshold. It aims to help customers such as farmers deal with production risks. Access to reinsurance may make cities more resilient. Where there are failures in the private insurance market, the public sector can subsidize premiums. One study identified key equity issues for policy considerations: Transferring risk to the public purse does not reduce overall risk; Governments can spread the cost of losses across time rather than space; Governments can force home-owners in low-risk areas to cross-subsidize the insurance premiums of those in high-risk areas; Cross-subsidization is increasingly difficult for private sector insurers operating in a competitive market; Governments can",
    "label": 0
  },
  {
    "text": "cost of losses across time rather than space; Governments can force home-owners in low-risk areas to cross-subsidize the insurance premiums of those in high-risk areas; Cross-subsidization is increasingly difficult for private sector insurers operating in a competitive market; Governments can tax people to pay for tomorrow's disaster. Government-subsidized insurance, such as the U.S. National Flood Insurance Program, comes under criticism for providing a perverse incentive to develop properties in hazardous areas. This increases overall risk. Insurance can also undermine other efforts such as property level protection and resilience to increase adaptation. Appropriate land-use policies can counter this behavioural effect. These policies limit new construction where there are current or future climate risks. They also encourage the adoption of resilient building codes to mitigate potential damages. Coordination mechanisms Coordination helps achieve goals shared by a range of people or organizations. Examples are information-sharing or joint implementation of adaptation options. Coordination helps use resources effectively. It avoids duplication, promotes consistency across government, and makes it easier for all people and organizations involved to understand the work. In the food production sector, adaptation projects financed through the UNFCCC often include coordination between national governments and administrations at the state, provincial or city level. There are fewer examples of coordination between community-level and national government. Behavioural and cultural options Individuals and households play a central role in adaptation. There are many examples particularly in the global south. Behavioural adaptation is a change in the strategies, practices and actions that help to reduce risk. These can include protecting homes from flooding, protecting crops from drought, and adopting different income-earning activities. Behavioural change is the most common form of adaptation. Change in diets and food waste Food waste spoilage increases with exposure to higher temperatures and humidity. It also increases with extreme events such as flooding",
    "label": 0
  },
  {
    "text": "and adopting different income-earning activities. Behavioural change is the most common form of adaptation. Change in diets and food waste Food waste spoilage increases with exposure to higher temperatures and humidity. It also increases with extreme events such as flooding and contamination. This can happen at different points in the food supply chain. Thus it can be a risk to food security and nutrition. Adaptation measures can review the production, processing and other handling practices of suppliers. Examples include further sorting to separate damaged products, drying the product for better storage or improved packaging. Other behaviour change options for retailers and consumers include accepting fruit and vegetables that appear less than perfect, redistributing food surpluses, and lowering prices on nearly expired food. Dietary change options in regions with excess consumption of calories include replacing meat and dairy foods with a higher share of plant-based foods. This has both mitigation and adaptation benefits. Plant-based options have much lower energy and water requirements. Adaptation options can investigate the dietary patterns that are better suited to the regional, socioeconomic and cultural context. Social-cultural norms strongly affect preferences for foods. Policies such as subsidies, taxes, and marketing can also support dietary choices that help adaptation. Change in livelihood strategies Agriculture offers many opportunities for adaptation. These include changing planting times, or changing to crops and livestock that are better adapted to climate conditions and presence of pests. Other examples are breeding more resilient crops and selecting genetically modified crops. All these aim to improve food security and nutrition. Migration and managed retreat Migration counts as behavioural climate adaptation for some, although others refer to it as a climate response rather: The IPCC Sixth Assessment Report states: \"Some responses, such as migration, relocation and resettlement may or may not be considered to be adaptation\". Many",
    "label": 0
  },
  {
    "text": "as behavioural climate adaptation for some, although others refer to it as a climate response rather: The IPCC Sixth Assessment Report states: \"Some responses, such as migration, relocation and resettlement may or may not be considered to be adaptation\". Many factors influence migration decisions. It is difficult to say how much climate change influences migration. The environment is one of many factors. Economic, demographic or political factors are often important in migration decisions. Climate change is an indirect or less important cause. Seasonal migration or mobility includes traditional strategies such as pastoralism or seeking seasonal employment in urban centres. These are normally voluntary and economically motivated. Weather fluctuations and extremes can influence migration. Weather variability is an important contributor to declines in agricultural incomes and employment. Climate change has made these impacts more likely. As a result, migration has increased, particularly rural to urban movement. Measures to increase adaptive capacity, such as social protection and promoting women's empowerment, can help people with little power in migration decisions. Sometimes people are unwilling or unable to migrate. In such cases it may be necessary for the government to intervene to keep people safe. This is also referred to as managed retreat. Nature-based options Nature-based solutions (NBS) work with nature and ecosystems to provide benefits to both societies and overall biodiversity. In the context of climate change, they provide adaptation and mitigation options that benefit and support wild species and habitats. In doing this they often contribute to other sustainable development goals. Nature-based solutions is an overarching term that includes actions known as ecosystem-based adaptation. However NBS is not restricted to climate change, and often also refers to climate change mitigation. So it is a less specific term. Both approaches require benefits to people and nature to be delivered simultaneously. Supporting ecosystems and",
    "label": 0
  },
  {
    "text": "ecosystem-based adaptation. However NBS is not restricted to climate change, and often also refers to climate change mitigation. So it is a less specific term. Both approaches require benefits to people and nature to be delivered simultaneously. Supporting ecosystems and biodiversity Ecosystems adapt to global warming depending on their resilience to climate change. Humans can help adaptation in ecosystems in order to strengthen biodiversity. One example is to increase links between ecosystems so that species can migrate on their own to more favourable climate conditions. Another is to assist this migration through human transport of plants or animals. Another example is to use scientific research and development to help coral reefs survive climate change. Protection and restoration of natural and semi-natural areas also helps build resilience, making it easier for ecosystems to adapt. Supporting people and societies Many actions that promote adaptation in ecosystems also help humans adapt via ecosystem-based adaptation and nature-based solutions. For instance, restoration of natural fire regimes makes catastrophic fires less likely and reduces the human exposure to this hazard. Giving rivers more space allows natural systems to store more water. This makes floods in inhabited areas less likely. The provision of green spaces and tree planting creates shade for livestock. There is a trade-off between agricultural production and the restoration of ecosystems in some areas. Options by type of impact Some adaptation options tackle specific climate hazards like floods or drought. Other options emerge when there are risks from different hazards as well as other factors that contribute to them such as with migration. Flooding Flooding can occur in cities or towns as urban flooding. It can also take place by the sea as coastal flooding. Sea level rise can make coastal flooding worse. In some areas there are also risks of glacial lake outburst",
    "label": 0
  },
  {
    "text": "Flooding can occur in cities or towns as urban flooding. It can also take place by the sea as coastal flooding. Sea level rise can make coastal flooding worse. In some areas there are also risks of glacial lake outburst floods. There are many adaptation options for flooding: Installing better flood defences such as flood barriers, sea walls and increased pumping capacity Installing devices to prevent seawater from backflowing into storm drains Rainwater storage to deal with increased run-off from rainfall. This includes reducing paved areas or changing to water-permeable pavements, adding water-buffering vegetation, adding underground storage tanks, and subsidizing household rain barrels Raising pumps at wastewater treatment plants Buying out homeowners in flood-prone areas Raising street level to prevent flooding Using and protecting mangroves Glacial lakes in danger of outburst flooding can have their moraines replaced with concrete dams to provide protection. This may also provide hydroelectric power More frequent drenching rains may make it necessary to increase the capacity of stormwater systems. This separates stormwater from blackwater, so that overflows in peak periods do not contaminate rivers. One example is the SMART Tunnel in Kuala Lumpur. New York City produced a comprehensive report for its Rebuilding and Resiliency initiative after Hurricane Sandy. It includes making buildings less prone to flooding. It also aims to make specific problems encountered during and after the storm less likely to recur. These include weeks-long fuel shortages even in unaffected areas due to legal and transportation problems, flooded health care facilities, insurance premium increases, damage to electricity and steam generation and distribution networks, and flooding of subway and roadway tunnels. Sea level rise Related to this, a survey conducted in the European Union indicates that 35% believe they may need to relocate to an area less vulnerable to climate impacts, either locally or",
    "label": 0
  },
  {
    "text": "and flooding of subway and roadway tunnels. Sea level rise Related to this, a survey conducted in the European Union indicates that 35% believe they may need to relocate to an area less vulnerable to climate impacts, either locally or internationally, to avoid risks such as rising sea levels and floods, forest fires, or other extreme weather events. Heat waves A 2020 study projects that regions inhabited by one third of the human population could become as hot as the hottest parts of the Sahara within 50 years. This will happen without a change in patterns of population growth and without migration, unless there is a sharp reduction in greenhouse gas emissions to limit warming to 1.5 °C. The most affected regions have little adaptive capacity as of 2020. Cities are particularly affected by heat waves due to the urban heat island effect. Climate change does not cause urban heat islands. But it leads to more frequent and more intense heat waves which in turn amplify the urban heat island effect in cities. Compact, dense urban development may increase the urban heat island effect. This results in higher temperatures and increased exposure. Tree cover and green space can reduce heat in cities. They act as sources of shade and promote evaporative cooling. Other options include green roofs, passive daytime radiative cooling applications, and the use of lighter-coloured surfaces and less absorptive building materials in urban areas. These reflect more sunlight and absorb less heat. It may be necessary to change city trees to more heat-tolerant varieties. Methods for adapting to increased heat include: The use and development of air conditioning and cooling systems. Adding air conditioning can make schools and workplaces cooler. But it results in additional greenhouse gas emissions unless it uses renewable energy. Solar-energy passive cooling systems for",
    "label": 0
  },
  {
    "text": "increased heat include: The use and development of air conditioning and cooling systems. Adding air conditioning can make schools and workplaces cooler. But it results in additional greenhouse gas emissions unless it uses renewable energy. Solar-energy passive cooling systems for houses and/or refrigeration. Changed rainfall patterns in agriculture Climate change is altering global rainfall patterns. This affects agriculture. Rainfed agriculture accounts for 80% of global agriculture. Many of the 852 million poor people in the world live in parts of Asia and Africa that depend on rainfall to cultivate food crops. Climate change will modify rainfall, evaporation, runoff, and soil moisture storage. Extended drought can cause the failure of small and marginal farms. This results in increased economic, political and social disruption. Water availability strongly influences all kinds of agriculture. Changes in total seasonal precipitation or its pattern of variability are both important. Moisture stress during flowering, pollination, and grain-filling harms most crops. It is particularly harmful to corn, soybeans, and wheat. Increased evaporation from the soil and accelerated transpiration in the plants themselves will cause moisture stress. There are many adaptation options. One is to develop crop varieties with greater drought tolerance and another is to build local rainwater storage. Using small planting basins to harvest water in Zimbabwe has boosted maize yields. This happens whether rainfall is abundant or scarce. And in Niger they have led to three or fourfold increases in millet yields. Digital technologies allow farmers to adapt to changing rainfall patterns through remote sensing of soil moisture, IoT-based irrigation control, and data analytics for rainfall forecasting. Climate change can threaten food security and water security. It is possible to adapt food systems to improve food security and prevent negative impacts from climate change in the future. More spending on irrigation Demand for water for irrigation",
    "label": 0
  },
  {
    "text": "forecasting. Climate change can threaten food security and water security. It is possible to adapt food systems to improve food security and prevent negative impacts from climate change in the future. More spending on irrigation Demand for water for irrigation is likely to rise in a warmer climate. This will increase competition between agriculture and urban and industrial users. Agriculture is already the largest consumer of water in semi-arid regions. Falling water tables and the resulting increase in energy to pump water will make irrigation more expensive. This is particularly the case when drier conditions will require more water per acre. Other strategies can make the most efficient use of water resources. The International Water Management Institute has suggested five strategies that could help Asia feed its growing population in light of climate change. These are modernizing existing irrigation schemes to suit modern methods of farming; supporting farmers' efforts to find their own water supplies by tapping into groundwater in a sustainable way; looking beyond conventional Participatory Irrigation Management schemes by working with the private sector; expanding capacity and knowledge; and investing outside the irrigation sector. Drought and desertification Reforestation is one way to stop desertification fueled by climate change and non-sustainable land use. One of the most important projects is the Great Green Wall that aims to stop the southward expansion of the Sahara desert. By 2018 only 15% of it had been carried out. But there are already many positive effects. These include the restoration of over 12 million acres (5 million hectares) of degraded land in Nigeria; the planting of roughly 30 million acres of drought-resistant trees across Senegal; and the restoration of 37 million acres of land in Ethiopia. Tree maintenance led to the refilling of groundwater wells with drinking water, additional food supplies for rural",
    "label": 0
  },
  {
    "text": "the planting of roughly 30 million acres of drought-resistant trees across Senegal; and the restoration of 37 million acres of land in Ethiopia. Tree maintenance led to the refilling of groundwater wells with drinking water, additional food supplies for rural towns, and new sources of work and income for villagers. Options by sector This section looks at the main sectors and systems affected by climate change. Experts have assessed the risks and adaptation options for them. Ecosystems and their services The main risks to ecosystems from climate change are biodiversity loss, ecosystem structure change, increased tree mortality, increased wildfire, and ecosystem carbon losses. These risks are linked. Loss of species can increase the risks to ecosystem health. Wildfire is an increasing risk for people as well as to ecosystems in many parts of the world. Wildfires and increased pest infestations due to climate change caused much of the recent tree mortality in North America. Risks to seas and coastal areas include coral bleaching linked with ocean warming. This can change the composition of ecosystems. Coral bleaching and mortality also increase the risks of flooding on nearby shorelines and islands. Ocean acidification attributed to climate change drives change in coral reefs and other ecosystems such as rocky shores and kelp forests. Ecosystems can respond to climatic and other environmental pressures in different ways. Individual organisms can respond through growth, movement and other developmental processes. Species and populations can relocate or genetically adapt. Human interventions can make ecosystems more resilient and help species adapt. Examples are protecting larger areas of semi-natural habitat and creating links between parts of the landscape to help species move. Ecosystem-based adaptation actions provide benefits for both ecosystems and humans. They include restoring coastal and river systems to reduce flood risk and improve water quality, creating more green",
    "label": 0
  },
  {
    "text": "and creating links between parts of the landscape to help species move. Ecosystem-based adaptation actions provide benefits for both ecosystems and humans. They include restoring coastal and river systems to reduce flood risk and improve water quality, creating more green areas in cities to reduce temperatures, and reinstating natural fire regimes to reduce risk of severe wildfires. There are many ways to reduce the risk of disease outbreaks. They include building surveillance systems of pathogens affecting humans, wildlife and farm animals. Assisted migration of plants or animals Assisted migration is the act of moving plants or animals to a different habitat. The destination habitat may or may not have once previously held the species. The only requirement is the destination habitat must provide the bioclimatic requirements to support the species. Assisted migration aims to remove the species from a threatening environment. It aims to give them a chance to survive and reproduce in an environment that does not pose an existential threat to the species. Assisted migration is a potential solution to changes in environments due to climate change that are faster than natural selection can adapt to. It has the potential to allow species that have poor natural dispersal abilities to avoid extinction. However it has also sparked debate over the possibility of the introduction of invasive species and diseases into previously healthy ecosystems. Despite these debates, scientists and land managers have already begun the process of assisted migration for certain species. There have been several studies of the climate adaptive potential of butterflies. Health Climate change-related risks to health include direct risks from extreme weather such as cold waves, storms, or prolonged high temperatures. There are also indirect risks such as mental health impacts of undernutrition or displacement caused by extreme weather. Similarly there are mental health risks",
    "label": 0
  },
  {
    "text": "health include direct risks from extreme weather such as cold waves, storms, or prolonged high temperatures. There are also indirect risks such as mental health impacts of undernutrition or displacement caused by extreme weather. Similarly there are mental health risks from loss of access to green spaces, reduced air quality, or from anxiety about climate change. There are further risks from changes in conditions for transmission of infectious diseases. Malaria and dengue are particularly climate-sensitive. There are several approaches to adapt to new or increased infectious disease risks. These include vector control through improved housing and better sanitation conditions through WASH services. It could also include insecticide-treated bed nets and indoor spraying. For food-borne diseases it includes food processing and storage. Adaptation options for heat include expanding access to air conditioning and establishing heat action plans that include early warning systems for heatwaves. Other options are passive cooling systems to include shading and ventilation. These could be part of improved building and urban design and planning, green infrastructure or public cooling centres. Adaptation options to respond to mental health impacts include increasing funding and access to mental healthcare, incorporating mental health into climate resilience and disaster risk planning, and improving post-disaster support. Mental health also benefits from broader activities such as design of healthy natural spaces, education and cultural activities. It is also closely related to food security and nutrition. Cities Rising temperatures and heatwaves are key risks for cities. With warmer temperatures the urban heat island effect is likely to get worse. Population growth and land use change will influence human health and productivity risks in cities. Urban flooding is another key risk. This is especially the case in coastal settlements where flood risks are exacerbated by sea-level rise and storm surges. A further set of risks arises from",
    "label": 0
  },
  {
    "text": "influence human health and productivity risks in cities. Urban flooding is another key risk. This is especially the case in coastal settlements where flood risks are exacerbated by sea-level rise and storm surges. A further set of risks arises from reduced water availability. When supply cannot meet demand from expanding settlements, urban residents become exposed to water insecurity and climate impacts. This is especially so during periods of lower rainfall. These key risks differ greatly between cities, and between different groups of people in the same city. Adaptation options for cities include flood control measures within and outside properties and urban drainage projects. Other examples are nature-based solutions such as bioswales or other vegetated infrastructure and restoration and/or protection of mangroves along coastlines. Vegetation corridors, greenspace, wetlands and other green infrastructure can also reduce heat risks. Building designs such as installing air conditioning, 'cool roofs' with high-reflectance materials or solar chimneys can also help. Several institutional adaptations are particularly important for cities, for example legislation of building codes, zoning and land use measures. Many cities have integrated city-wide adaptation strategies or plans that bring together social and economic activities, civil authorities and infrastructure services. Such actions are more effective if they are implemented in partnership with local communities, national governments, research institutions, and the private and third sector. Water Climate change is affecting the overall and seasonal availability of water across regions. Climate change is projected to increase the variability of rain. There will be impacts on water quality as well as quantity. Floods can wash pollutants into water bodies and damage water infrastructure. In many places, particularly in the tropics and sub-tropics, there are longer dry spells and droughts, sometimes over consecutive years. These have contributed to drier soil conditions, lower groundwater tables and reduced or changed flows of",
    "label": 0
  },
  {
    "text": "and damage water infrastructure. In many places, particularly in the tropics and sub-tropics, there are longer dry spells and droughts, sometimes over consecutive years. These have contributed to drier soil conditions, lower groundwater tables and reduced or changed flows of rivers. There are risks to ecosystems, and across many water-using sectors of the economy. Agriculture is likely to be affected by changes in water availability, putting food security at risk. Irrigation has often contributed to groundwater depletion and changes in the water cycle. It can sometimes make a drought worse. Some of the most popular adaptations in agriculture include choosing less water-intensive crops or drought and flood-resistant varieties. They include shifting the timing of sowing and harvesting according to the start of the rainy season. There are other technological options available for saving water. Water is used for hydroelectric generation, for cooling of power plants, and in other industries such as mining. Adapting hydroelectric plant designs and control systems to operate with less water or diversifying in energy generation with other renewables are effective options. Livelihoods and communities Climate change affects livelihoods and living conditions in significant ways. These include access to natural resources and ecosystems, land and other assets. Access to basic infrastructure services such as water and sanitation, electricity, roads, telecommunications is another aspect of vulnerability of communities and livelihoods to climate change. The biggest livelihood-related risks stem from losses of agricultural yields, impacts on human health and food security, destruction of homes, and loss of income. There are also risks to fish and livestock that livelihoods depend on. Some communities and livelihoods also face risks of irreversible losses and challenges to development, as well as more complex disaster risks. The consequences of climate change are the most severe for the poorest populations. These are disproportionately more exposed",
    "label": 0
  },
  {
    "text": "on. Some communities and livelihoods also face risks of irreversible losses and challenges to development, as well as more complex disaster risks. The consequences of climate change are the most severe for the poorest populations. These are disproportionately more exposed to hazards such as temperature extremes and droughts. They usually have fewer resources and assets and less access to funding, support and political influence. There are other forms of disadvantage due to discrimination, gender inequalities and through lack of access to resources This includes people with disabilities or minority groups. Across livelihoods sectors for households and communities the most common adaptation responses are engineered and technological options. These include traditional infrastructure to protect a specific land use, ecosystem approaches such as watershed restoration or climate-smart agriculture technologies. Adaptation requires public and private investment in various natural assets. It also requires institutions that prioritize the needs of communities, including the poorest. International impacts and cascading risk International climate risks are climate risks that cross national borders. Sometimes the impacts of climate change in one country or region can have further consequences for people in other countries. Risks can spread from one country to a neighbouring country, or from one country to distant regions. Risks can also cascade and have knock-on effects elsewhere, across multiple borders and sectors. For example, an impact of the floods in Thailand in 2011 was disruption to manufacturing supply chains affecting the automotive sector and electronics industry in Japan, Europe and the USA. Options for adapting are less developed. They include developing resilient infrastructure in the originating country, increasing storage facilities to allow more buffer in the recipient country, or diversifying and re-routing trade. Costs and finance Economic costs The economic costs of adaptation to climate change will depend on how much the climate changes. Higher levels",
    "label": 0
  },
  {
    "text": "country, increasing storage facilities to allow more buffer in the recipient country, or diversifying and re-routing trade. Costs and finance Economic costs The economic costs of adaptation to climate change will depend on how much the climate changes. Higher levels of warming lead to considerably higher costs. Globally, adaptation is likely to cost tens or hundreds of billions of dollars annually for the next several decades. The IPCC's most recent summary states that adaptation will cost $15 to 411 billion per year for climate change impacts to 2030. Most estimates are well above $100 billion.\" Because these costs are much higher than the finance available, there is an adaptation gap. This is especially pressing in developing countries. This gap is wideningand forms a major barrier to adaptation. This widening has become apparent because the overwhelming majority of global tracked climate finance goes to mitigation. Only a small proportion goes on adaptation. More regional estimates are also available. For example, the Asian Development Bank has a series of studies on the Economics of Climate Change in the Asia-Pacific region. These studies provide cost analysis of both adaptation and mitigation measures. The WEAP (Water Evaluation And Planning system) assists water resources researchers and planners in assessing impacts of and adaptations to climate change. The United Nations Development Programme's Climate Change Adaptation Portal includes studies on climate change adaptation in Africa, Europe and Central Asia, and Asia and the Pacific. According to a survey conducted in the European Union, 15% of respondents believe that wealthier individuals should bear the costs of climate change adaptations through higher taxes. The same survey revealed that over a third (35%) of respondents believe that the expenses should be covered by the companies and industries that contribute most to climate change. Cost benefit analysis As of 2007 there",
    "label": 0
  },
  {
    "text": "adaptations through higher taxes. The same survey revealed that over a third (35%) of respondents believe that the expenses should be covered by the companies and industries that contribute most to climate change. Cost benefit analysis As of 2007 there was still a lack of comprehensive, global cost and benefit estimates for adaptation. Since then, an extensive research literature has emerged. Studies generally focus on adaptation in developing countries or within a sector. For many adaptation options in specific contexts, the investment will be lower than the avoided damages. But global estimates have considerable uncertainty. International finance The United Nations Framework Convention on Climate Change incorporates a financial mechanism to developing country parties to support them with adaptation. This is in Article 11 of the convention. Until 2009, three funds existed under the UNFCCC financial mechanism. The Global Environmental Facility administers the Special Climate Change Fund (SCCF) and the Least Developed Countries Fund (LDCF). The Adaptation Fund resulted from negotiations during COP15 and COP16 in 2009 and 2010. It has its own Secretariat. Initially, when the Kyoto Protocol was in operation, the Adaptation Fund was financed by a 2% levy on the Clean Development Mechanism (CDM). At the 2009 Copenhagen Summit, nations committed to the goal of sending $100 billion per year to developing countries for climate change mitigation and adaptation by 2020. The Green Climate Fund was created in 2010 as one of the channels for mobilizing this climate finance. The 2015 Paris conference, COP21, clarified that the $100 billion per year should involve a balanced split between mitigation and adaptation. As of December 2020, the promised $100 billion per year had not been fully delivered. Most developing country finance was still targeted towards mitigation. Adaptation received only 21% of the public finance provided in 2020. Global adaptation financing",
    "label": 0
  },
  {
    "text": "and adaptation. As of December 2020, the promised $100 billion per year had not been fully delivered. Most developing country finance was still targeted towards mitigation. Adaptation received only 21% of the public finance provided in 2020. Global adaptation financing from multilateral development banks exceeded €19 billion in 2021. This implies a rising trend in the financing of adaptation. Multilateral banks made a commitment to increase adaptation financing in a joint declaration on climate change at COP27. This particularly targets low-income nations, small island developing states, and underprivileged people. The European Investment Bank has said that it will raise the share it contributes to 75% for projects focusing on climate adaptation. The bank usually contributes up to 50% to a project it participates in. Also in 2022, nations agreed on a proposal to establish a loss and damage fund to support communities in averting, minimizing, and addressing damages and risks where adaptation is not enough or comes too late. The Adaptation Gap Report November 2023, published by the United Nations Environment Programme (UNEP), reveals an adaptation finance gap of $194 billion to $366 billion annually. The adaptation needs in developing countries are estimated to range from $215 billion to $387 billion per year, which is 10-18 times the current international public finance flows for adaptation. The report also notes a 15% decline in international public climate finance to developing countries, down to $21.3 billion in 2021. The urgency to increase adaptation finance is highlighted, with an average annual increase of at least 16% required from 2022 to 2025 to align with COP 26 commitments. Additionality A key feature of international adaptation finance is the concept of additionality. This reflects the linkages between adaptation finance and other levels of development aid. Many developed countries already provide international aid assistance to developing",
    "label": 0
  },
  {
    "text": "with COP 26 commitments. Additionality A key feature of international adaptation finance is the concept of additionality. This reflects the linkages between adaptation finance and other levels of development aid. Many developed countries already provide international aid assistance to developing countries. This addresses challenges such as poverty, malnutrition, food insecurity, availability of drinking water, indebtedness, illiteracy, unemployment, local resource conflicts, and lower technological development. Climate change threatens to exacerbate or stall progress on fixing some of these problems, and creates new ones. Additionality refers to the extra costs of adaptation to avoid existing aid being redirected. The four main definitions of additionality are: Climate finance classified as aid, but additional to the Millennium Development Goals; Increase on previous year's Official Development Assistance (ODA) spent on climate change mitigation; Rising ODA levels that include climate change finance but where it is limited to a specified percentage; and Increase in climate finance not connected to ODA. A criticism of additionality is that it encourages business as usual. This is because it does not account for the future risks of climate change. Some advocates have proposed integrating climate change adaptation into poverty reduction programs. From 2010 to 2020, Denmark increased its global warming adaptation aid by one third, from 0.09% of GDP to 0.12% of GDP. But this did not involve additional funds. Instead, the aid was taken from other foreign assistance funds. Politiken wrote: \"Climate assistance is taken from the poorest.\" Challenges Differing time scales Adaptation can occur in anticipation of change or be a response to those changes. For example, artificial snow-making in the European Alps responds to current climate trends. The construction of the Confederation Bridge in Canada at a higher elevation takes into account the effect of future sea-level rise on ship clearance under the bridge. Effective adaptive policy",
    "label": 0
  },
  {
    "text": "snow-making in the European Alps responds to current climate trends. The construction of the Confederation Bridge in Canada at a higher elevation takes into account the effect of future sea-level rise on ship clearance under the bridge. Effective adaptive policy can be difficult to implement because policymakers are rewarded more for enacting short-term change, rather than long-term planning. Since the impacts of climate change are generally not seen in the short term, policymakers have less incentive to act. Furthermore, climate change is occurring on a global scale. This requires a global framework for adapting to and combating climate change. The vast majority of climate change adaptation and mitigation policies are being implemented on a more local scale. This is because different regions must adapt differently. National and global policies are often more challenging to enact. Maladaptation In climate change mitigation maladaptation describes an adaptation action that results in directly increasing vulnerability to climate change and/or a decreasing overall capacity to take sustainable adaptations in the future. The term maladaptation was first introduced by the IPCC in 2001. The IPCC explains maladaptation as follows: \"actions that may lead to increased risk of adverse climate-related outcomes, including via increased greenhouse gas emissions, increased or shifted vulnerability to climate change, more inequitable outcomes, or diminished welfare, now or in the future. Most often, maladaptation is an unintended consequence.\" Much adaptation takes place in relation to short-term climate variability. But this may cause maladaptation to longer-term climate trends. The expansion of irrigation in Egypt into the Western Sinai desert after a period of higher river flows is maladaptation given the longer-term projections of drying in the region. Adaptations at one scale can have impacts at another by reducing the adaptive capacity of other people or organizations. This is often the case when broad assessments",
    "label": 0
  },
  {
    "text": "river flows is maladaptation given the longer-term projections of drying in the region. Adaptations at one scale can have impacts at another by reducing the adaptive capacity of other people or organizations. This is often the case when broad assessments of the costs and benefits of adaptation are examined at smaller scales. An adaptation may benefit some people, but have a negative effect on others. Development interventions to increase adaptive capacity have tended not to result in increased power or agency for local people. Agency is a central factor in all other aspects of adaptive capacity and so planners need to pay more attention to this factor. In general, \"community-driven bottom-up adaptation\" approaches have lower risks for maladaption than top-down technical fixes if they do not follow a holistic approach. The installation of seawalls, for example, can create problems with waterlogged fields and loss of soil fertility. The growing use of fertilizers and pesticides is an example of maladaptation. They generate additional greenhouse gases during their production which contributes to a decrease in crop yields. The use of air-conditioning is another example. If the electricity used is based on carbon intensive energy then the extra greenhouse gases produced only add to the problem. Limits to adaptation People have always adapted to climate change. Some community coping strategies already exist. Examples include changing sowing times or adopting new water-saving techniques. Traditional knowledge and coping strategies must be maintained and strengthened. If not there is a risk of weakening adaptive capacity as local knowledge of the environment is lost. Strengthening these local techniques and building upon them also makes the adoption of adaptation strategies more likely. This is because it creates more community ownership and involvement in the process. In many cases this will not be enough to adapt to new conditions.",
    "label": 0
  },
  {
    "text": "techniques and building upon them also makes the adoption of adaptation strategies more likely. This is because it creates more community ownership and involvement in the process. In many cases this will not be enough to adapt to new conditions. These may be outside the range of those previously experienced, and new techniques will be necessary. The incremental adaptations become insufficient as the vulnerabilities and risks of climate change increase. This creates a need for transformational adaptations which are much larger and costlier. Current development efforts increasingly focus on community-based climate change adaptation. They seek to enhance local knowledge, participation and ownership of adaptation strategies. The IPCC Sixth Assessment Report in 2022 put considerable emphasis on adaptation limits. It makes a distinction between soft and hard adaptation limits. The report stated that some human and natural systems already reached \"soft adaptation limits\" including human systems in Australia, Small Islands, America, Africa and Europe and some natural systems reach even the \"hard adaptation limits\" like part of corals, wetland, rainforests, ecosystems in polar and mountain regions. If the temperature rise will reach 1.5 °C (34.7 °F) additional ecosystems and human systems will reach hard adaptation limits, including regions depending on glaciers and snow water and small islands. At 2 °C (36 °F) temperature rise, soft limits will be reached by many staple crops in many areas while at 3 °C (37 °F) hard limits will be reached by parts of Europe. A 2025 study found that private-sector adaptation strategies in both developed and developing countries face deep-rooted structural barriers — including governance weaknesses, exclusion, and inequality — which limit the long-term sustainability of climate adaptation efforts. Incentivizing private investment in adaptation Climate change adaptation is a much more complex investment area than mitigation. This is mainly because of the lack of",
    "label": 0
  },
  {
    "text": "governance weaknesses, exclusion, and inequality — which limit the long-term sustainability of climate adaptation efforts. Incentivizing private investment in adaptation Climate change adaptation is a much more complex investment area than mitigation. This is mainly because of the lack of a well-defined income stream or business case with an attractive return on investment on projects. There are several specific challenges for private investment: Adaptation is often needed in non-market sectors or is focused on public goods that benefit many. So there is a shortage of projects that are attractive to the private sector; There is a mismatch between the timing of investments needed in the short term and the benefits that may occur in the medium or long term. Future returns are less attractive to investors than short-term returns; There is a lack of information about investment opportunities. This especially concerns uncertainties associated with future impacts and benefits. These are key considerations when returns may accrue over longer timeframes; There are gaps in human resources and capacities to design adaptation projects and understand financial implications of legal, economic and regulatory frameworks. However, there is considerable innovation in this area. This is increasing the potential for private sector finance to play a larger role in closing the adaptation finance gap. Economists state that climate adaptation initiatives should be an urgent priority for business investment. Trade-offs with mitigation Trade-offs between adaptation and mitigation may occur when climate-relevant actions point in different directions. For instance, compact urban development may lead to reduced greenhouse gas emissions from transport and building. On the other hand, it may increase the urban heat island effect, leading to higher temperatures and increasing exposure, making adaptation more challenging. Planning and monitoring of implementation Climate adaptation planning aims to manage the level of risks of negative impacts. Adaptation planning is",
    "label": 0
  },
  {
    "text": "it may increase the urban heat island effect, leading to higher temperatures and increasing exposure, making adaptation more challenging. Planning and monitoring of implementation Climate adaptation planning aims to manage the level of risks of negative impacts. Adaptation planning is similar to risk management. It is a continuing process of assessment, action, learning and adjustment, rather than a single set of decisions. In this way planning and implementing adaptation are both closely connected. Adaptation planning is an activity. But it is also associated with a type of adaptation. Planned adaptation is sometimes distinguished from autonomous adaptation. Another important concept in adaptation planning is mainstreaming. Mainstreaming means integrating climate change into established strategies, policies or plans. This can be more efficient than developing separate climate adaptation activities and is more likely to succeed. It can also be more sustainable. It involves changing the mindsets and practices of policymakers to bring in new issues and have them widely accepted. A key entry point for this type of integration is national development planning. It needs to take into account new and existing national policies, sectoral policies and budgets. Similarly, mainstreaming adaptation in cities should consider existing city plans, such as land use planning. There approach also has shortcomings. One criticism is that it has reduced the visibility of stand-alone adaptation programmes. Adaptation planning usually draws on assessments of risks and vulnerability to climate change. It evaluates the relative benefits and costs of different measures to reduce these risks. Following planning, the next stage is implementation. Guidance has been developed that outline these general stages of an adaptation process, such as the EU Adaptation Support Tool. Preparing the ground for adaptation Assessing climate change risks and vulnerabilities Identifying adaptation options Assessing adaptation options Implementing adaptation Monitoring and evaluating adaptation As of 2022, adaptation efforts",
    "label": 0
  },
  {
    "text": "stages of an adaptation process, such as the EU Adaptation Support Tool. Preparing the ground for adaptation Assessing climate change risks and vulnerabilities Identifying adaptation options Assessing adaptation options Implementing adaptation Monitoring and evaluating adaptation As of 2022, adaptation efforts have focused more on adaptation planning than on implementation. All regions and sectors have made progress. However, the gaps between current needs and current implementation continues to grow. Monitoring and evaluation of adaptation is crucial to ensure that adaptation action is proceeding as planned. It also provide lessons to improve them and understand which additional actions are necessary. Development and use of monitoring and evaluation systems is increasing at national and local levels. As of 2020, around a quarter of countries had a monitoring and evaluation framework in place. By country and city National governments typically have the key role in setting policies, planning, coordinating and distributing finance for climate adaptation. They are also accountable to the international community through international agreements. Many countries document their adaptation plans in their NDCs submitted under the Paris Agreement and/or national adaptation plans. Developing countries can receive support with international funding to help them develop their national adaptation plans. As of 2020, 72% of countries had a high level adaptation instrument – such as a plan, policy or strategy. Relatively few had progressed to the tangible implementation of projects: at least not to significantly reduce the climate risk their populations are exposed to. Countries have also made progress in developing plans for subnational government authorities. These include county/provincial level, sectors and city plans. In 2020, around 21% of countries had sub-national plans and 58% had sectoral plans. As of 2022, there is better integration of adaptation priorities into other national plans and planning systems. Planning is also more inclusive. This means that climate",
    "label": 0
  },
  {
    "text": "In 2020, around 21% of countries had sub-national plans and 58% had sectoral plans. As of 2022, there is better integration of adaptation priorities into other national plans and planning systems. Planning is also more inclusive. This means that climate laws and policies increasingly reference different groups such as persons with disabilities, children, young people and future generations. Many cities have integrated city-wide adaptation strategies or plans that bring together their social and economic activities, civil authorities and infrastructure services. A survey of 812 global cities found that 93% reported they are at risk from climate change, 43% did not have an adaptation plan in 2021, and 41% of cities had not carried out a climate risk and vulnerability assessment. Global goals Sustainable Development Goal 13 aims to strengthen countries' resilience and adaptive capacities to climate-related issues. This adjustment includes many areas such as infrastructure, agriculture and education. The Paris Agreement includes several provisions for adaptation. It seeks to promote the idea of global responsibility, improve communication via the adaptation component of the Nationally Determined Contributions, and includes an agreement that developed countries should provide some financial support and technology transfer to promote adaptation in more vulnerable countries. The United Nations estimates Africa would need yearly funding of $1.3 trillion to achieve the Sustainable Development Goals in the region, considering population growth. The International Monetary Fund also estimates that $50 billion may be needed only to cover the expenses of climate adaptation. History When climate change first became prominent on the international political agenda in the early 1990s, talk of adaptation was considered an unwelcome distraction from the need to reach agreement on effective measures for mitigation – which has mainly meant reducing the emissions of greenhouse gases. A few voices had spoken out in favour of adaptation even in",
    "label": 0
  },
  {
    "text": "adaptation was considered an unwelcome distraction from the need to reach agreement on effective measures for mitigation – which has mainly meant reducing the emissions of greenhouse gases. A few voices had spoken out in favour of adaptation even in the late 20th and early 21st century. In 2009 and 2010, adaptation began to receive more attention during international climate negotiations. This was after limited progress at the Copenhagen Summit had made it clear that achieving international consensus for emission reductions would be more challenging than had been hoped. In 2009, the rich nations of the world committed to providing a total of $100 billion per year to help developing nations fund their climate adaptation projects. This commitment was underscored at the 2010 Cancún Summit, and again at the 2015 Paris Conference. The promise was not fulfilled, but the amount of funding provided by the rich nations for adaptations did increase over the 2010 – 2020 period. Climate change adaptation has tended to be more of a focus for local authorities, while national and international politics has tended to focus on mitigation. There have been exceptions – in countries that feel especially exposed to the effects of climate change, sometimes the focus has been more on adaptation even at national level. See also Adaptation in Africa Green bond Climate change adaptation strategies on the German coast Climate finance Climate justice Climate Vulnerability Monitor References External links The Intergovernmental Panel on Climate Change (IPCC) Working Group II assesses the scientific literature on adaptation: Working Group II — IPCC The weADAPT platform encourages the sharing of experiences from adaptation projects to accelerate learning and climate action.",
    "label": 0
  },
  {
    "text": "Climate change and cities are deeply connected. Cities are one of the greatest contributors and likely best opportunities for addressing climate change. Cities are also one of the most vulnerable parts of the human society to the effects of climate change, and likely one of the most important solutions for reducing the environmental impact of humans. The UN projects that 68% of the world population will live in urban areas by 2050. In the year 2016, 31 mega-cities reported having at least 10 million in their population, 8 of which surpassed 20 million people. However, secondary cities - small to medium size cities (500,000 to 1 million) are rapidly increasing in number and are some of the fastest growing urbanizing areas in the world further contributing to climate change impacts. Cities have a significant influence on construction and transportation—two of the key contributors to global warming emissions. Moreover, because of processes that create climate conflict and climate refugees, city areas are expected to grow during the next several decades, stressing infrastructure and concentrating more impoverished peoples in cities.High density and urban heat island effect are examples of weather changes that impact cities due to climate change. It also causes exacerbating existing problems such as air pollution, water scarcity, and heat illness in metropolitan areas. Moreover, because most cities have been built on rivers or coastal areas, cities are frequently vulnerable to the subsequent effects of sea level rise, which cause flooding and erosion; these effects are also connected with other urban environmental problems, such as subsidence and aquifer depletion. A report by the C40 Cities Climate Leadership Group described consumption based emissions as having significantly more impact than production-based emissions within cities. The report estimates that 85% of the emissions associated with goods within a city is generated outside of",
    "label": 0
  },
  {
    "text": "report by the C40 Cities Climate Leadership Group described consumption based emissions as having significantly more impact than production-based emissions within cities. The report estimates that 85% of the emissions associated with goods within a city is generated outside of that city. Climate change adaptation and mitigation investments in cities will be important in reducing the impacts of some of the largest contributors of greenhouse gas emissions: for example, increased density allows for redistribution of land use for agriculture and reforestation, improving transportation efficiencies, and greening construction (largely due to cement's outsized role in climate change and improvements in sustainable construction practices and weatherization). Climate Change and Urbanization In the most recent past, increasing urbanization has also been proposed as a phenomenon that has a reducing effect on the global rate of carbon emission primarily because with urbanization comes technical prowess which can help drive sustainability. Lists of high impact climate change solutions tend to include city-focused solutions; for example, Project Drawdown recommends several major urban investments, including improved bicycle infrastructure, building retrofitting, district heating, public transit, and walkable cities as important solutions. In order to activate and focus attention on climate change solutions, the international community has formed coalitions of cities such as C40 Cities Climate Leadership Group and ICLEI) as well as policy goals, Sustainable Development Goal 11 (\"sustainable cities and communities”). Currently, in 2022, there is a deterioration in the progress of the goal. There is limited progress on making cities and human settlements more appropriate to live in Sub-Saharan Africa, Latin America and the Caribbean and the Pacific Island countries. There is fair progress in Central and Southern Asia and Eastern and South-Eastern Asian. However, it has been achieved in Developed countries. There is an ongoing paradigm shift in urban planning that is focused on development",
    "label": 0
  },
  {
    "text": "the Pacific Island countries. There is fair progress in Central and Southern Asia and Eastern and South-Eastern Asian. However, it has been achieved in Developed countries. There is an ongoing paradigm shift in urban planning that is focused on development of climate friendly and resilience by using climate urbanism. Climate urbanism aims to protect physical and digital infrastructures of urban economies from the hazards associated with climate change. Being that cities bring in 80% of the world's revenue, it is important for the infrastructure and planning of these urban areas to be planned out using carbon management and climate resilient infrastructure. Background More than half of the world's population is in cities, consuming a large portion of food and goods produced outside of cities. The UN projects that 68% of the world population will live in urban areas by 2050. In the year 2016, 31 mega-cities reported having at least 10 million in their population, 8 of which surpassed 20 million people. However, secondary cities - small to medium size cities (500,000 to 1 million) are rapidly increasing in number and are some of the fastest growing urbanizing areas in the world further contributing to climate change impacts. Emissions Cities globally house half of the world's people, consume 80% of the world's energy and 70% of its natural resources, and contribute more than 70% of global CO2 emissions. Cities and regions are also particularly vulnerable to climate-related hazards and pollution. Climate danger and pollution also disproportionately affect the poor, increasing inequality. With half of the world population residing in urban areas, there will be an increase in energy usage that comes with Climate Change. One of these will be AC, since climate change comes with higher temperatures many people will start needed more cooling systems, so this results in more",
    "label": 0
  },
  {
    "text": "areas, there will be an increase in energy usage that comes with Climate Change. One of these will be AC, since climate change comes with higher temperatures many people will start needed more cooling systems, so this results in more air conditioning and newer models of cooling systems. Although more people are living in cities which can result in shortages, cities actually emit less carbon than rural areas since house sizes are smaller, more gas heat over propane is used, less carbon fueled transportation is used, and more people share communal spaces such as laundry rooms and kitchens. While cities create some problems, it is important to realize that the denser population creates less carbon emissions which benefits climate change. New policies now focus on the reduction of emissions from coal-fired power plants as well as increasing motor vehicle efficiency. With regard to methods of emissions counting cities can be challenging as production of goods and services within their territory can be related either to domestic consumption or exports. Conversely the citizens also consume imported goods and services. A recent study discovered that 40% of strategies focus on reducing emissions from municipal operations that account for 1–5% of total urban emissions, while neglecting imported goods and industrial activities. To avoid double counting in any emissions calculation it should be made clear where the emissions are to be counted: at the site of production or consumption. This may be complicated given long production chains in a globalized economy. Moreover, the embodied energy and consequences of large-scale raw material extraction required for renewable energy systems and electric vehicle batteries is likely to represent its own complications – local emissions at the site of utilization are likely to be very small but life-cycle emissions can still be significant. Field of study The research",
    "label": 0
  },
  {
    "text": "renewable energy systems and electric vehicle batteries is likely to represent its own complications – local emissions at the site of utilization are likely to be very small but life-cycle emissions can still be significant. Field of study The research perspective of cities and climate change, started in the 1990s as the international community became increasingly aware of the potential impacts of climate change. Urban studies scholars Michael Hebbert and Vladmir Jankovic argue that this field of research grew out of a larger body of research on the effects of urban development and living on the environment starting as early as the 1950s. Since then, research has indicated relationships between climate change and sustainable urbanization: increase employment cities reduces poverty and increases efficiencies. Two international assessments have been published by the Urban Climate Change Research Network at The Earth Institute at Columbia University. The first of which was published in, the first of which (ARC3.1) was published in 2011, and the second of which (ARC3.2) was published in 2018. These papers act as summaries of the scholarship for the field similar to the Intergovernmental Panel on Climate Change reports. A third report is to be published between 2023 and 2025. Cities as laboratories Cities are good subjects for study because they can invest heavily in large-scale experimental policies that could be scaled elsewhere (such as San Diego's advanced urban planning practices which could be applied elsewhere in the United States). Multiple scholars approach this in different ways, but describe this \"urban laboratory\" environment good for testing a wide variety of practices. For example, the book Life After Carbon documents a number of cities which act as \"urban climate innovation laboratories\". These cities as laboratories offer an efficient way to detect climate change by looking at the effects of the greenhouse",
    "label": 0
  },
  {
    "text": "practices. For example, the book Life After Carbon documents a number of cities which act as \"urban climate innovation laboratories\". These cities as laboratories offer an efficient way to detect climate change by looking at the effects of the greenhouse effect on rooftops, street trees, and other environmental variables within a city setting. Though this method of looking at the heat waves effects in cities, it will offer a way of seeing the problem of the effect of heat that will be solved by cities within the future. Health impacts Climate change has been observed to have caused impact on human health and livelihoods in urban settings. Urbanization commonly occurs in cities with low and middle income communities that have high population density and a lack of understanding of how climate change, which degrades their environment, is affecting their health. Within urban settings, multiple climate and non-climate hazards impact cities which magnify the damages done to human health. For example, heatwaves have intensified in cities due to the combination of multiple factors adding to climate change. With heatwaves constantly increasing temperatures in cities, it has caused many heat-related illnesses such as heat stroke or heat cramps. The rise of temperatures due to climate change have also changed the distribution of diseases from mosquitoes, causing a rising rate of infectious diseases. Alongside infectious diseases and heatwaves, climate change can create natural hazards such as floods, droughts, and storms due to rising sea levels. It also harms those even more who have COVID-19, asthma, illnesses, etc. Hospitals and other healthcare places are also a big contributor to climate change with them being responsible for 4-5% global carbon dioxide emissions. They are due to medications, such as asthma inhalers, anesthetic gasses, patient and staff transportation, electricity, HVAC, and waste management. The impacts on",
    "label": 0
  },
  {
    "text": "also a big contributor to climate change with them being responsible for 4-5% global carbon dioxide emissions. They are due to medications, such as asthma inhalers, anesthetic gasses, patient and staff transportation, electricity, HVAC, and waste management. The impacts on human health in urban settings is more profound in economically and socially marginalized urban residents. Low-income and remote populations are more vulnerable to physical hazards, undernutrition, diarrheal and other infectious diseases, and the health consequences of displacement. Urban neighborhoods burdened by heat islands and other climate change problems can also experience clusters of violence, amplifying intersecting vulnerabilities in these communities. Urban resilience The Intergovernmental Panel on Climate Change (IPCC) defines resilience as \"the ability of a social or ecological system to absorb disturbances while retaining the same basic structure and ways of functioning, the capacity of self-organization, and the capacity to adapt to stress and change.\" One of the most important notions emphasized in urban resiliency theory is the need for urban systems to increase their capacity to absorb environmental disturbances. By focusing on three generalizable elements of the resiliency movement, Tyler and Moench's urban resiliency framework serves as a model that can be implemented for local planning on an international scale. The first element of urban climate resiliency focuses on 'systems' or the physical infrastructure embedded in urban systems. A critical concern of urban resiliency is linked to the idea of maintaining support systems that in turn enable the networks of provisioning and exchange for populations in urban areas. These systems concern both physical infrastructure in the city and ecosystems within or surrounding the urban center; while working to provide essential services like food production, flood control, or runoff management. For example, city electricity, a necessity of urban life, depends on the performance of generators, grids, and distant reservoirs.",
    "label": 0
  },
  {
    "text": "ecosystems within or surrounding the urban center; while working to provide essential services like food production, flood control, or runoff management. For example, city electricity, a necessity of urban life, depends on the performance of generators, grids, and distant reservoirs. The failure of these core systems jeopardizes human well-being in these urban areas, with that being said, it is crucial to maintain them in the face of impending environmental disturbances. Societies need to build resiliency into these systems in order to achieve such a feat. Resilient systems work to \"ensure that functionality is retained and can be re-instated through system linkages\" despite some failures or operational disturbances. Ensuring the functionality of these important systems is achieved through instilling and maintaining flexibility in the presence of a \"safe failure.\" Resilient systems achieve flexibility by making sure that key functions are distributed in a way that they would not all be affected by a given event at one time, what is often referred to as spatial diversity, and has multiple methods for meeting a given need, what is often referred to as functional diversity. The presence of safe failures also plays a critical role in maintaining these systems, which work by absorbing sudden shocks that may even exceed design thresholds. Environmental disturbances are certainly expected to challenge the dexterity of these systems, so the presence of safe failures almost certainly appears to be a necessity. Further, another important component of these systems is bounce-back ability. In the instance where dangerous climatic events affect these urban centers, recovering or \"bouncing-back\" is of great importance. In fact, in most disaster studies, urban resilience is often defined as \"the capacity of a city to rebound from destruction.\" This idea of bounce-back for urban systems is also engrained in governmental literature of the same topic. For",
    "label": 0
  },
  {
    "text": "importance. In fact, in most disaster studies, urban resilience is often defined as \"the capacity of a city to rebound from destruction.\" This idea of bounce-back for urban systems is also engrained in governmental literature of the same topic. For example, the former government's first Intelligence and Security Coordinator of the United Kingdom described urban resilience as \"the capacity to absorb shocks and to bounce back into functioning shape, or at the least, sufficient resilience to prevent...system collapse.\" Keeping these quotations in mind, bounce-back discourse has been and should continue to be an important part of urban climate resiliency framework. Other theorists have critiqued this idea of bounce-back, citing this as privileging the status quo, rather advocating the notion of 'bouncing forward', permitting system evolution and improvement. The next element of urban climate resiliency focuses on the social agents (also described as social actors) present in urban centers. Many of these agents depend on the urban centers for their very existence, so they share a common interest of working towards protecting and maintaining their urban surroundings. Agents in urban centers have the capacity to deliberate and rationally make decisions, which plays an important role in climate resiliency theory. One cannot overlook the role of local governments and community organizations, which will be forced to make key decisions with regards to organizing and delivering key services and plans for combating the impending effects of climate change. Perhaps most importantly, these social agents must increase their capacities with regards to the notions of \"resourcefulness and responsiveness. Responsiveness refers to the capacity of social actors and groups to organize and re-organize, as well as the ability to anticipate and plan for disruptive events. Resourcefulness refers to the capacity of social actors in urban centers to mobilize varying assets and resources in order to",
    "label": 0
  },
  {
    "text": "social actors and groups to organize and re-organize, as well as the ability to anticipate and plan for disruptive events. Resourcefulness refers to the capacity of social actors in urban centers to mobilize varying assets and resources in order to take action. Urban centers will be able to better fend for themselves in the heat of climatic disturbances when responsiveness and resourcefulness is collectively achieved in an effective manner. Regional and national differences Cities in different parts of the world face different, unique challenges and opportunities in the face of climate change. However, one linking factor is their inevitable adherence to \"Dominant global patterns of urbanization and industrialization\" which often catalyzes \"large-scale modification of the drivers for hydrologic and biogeochemical processes\". Urbanization and industrialization patterns are particularly evident for regions such as Asia, Africa, and South America, regions that are currently understood as experiencing related rapid shifts in population and economic prowess. Beginning in the 2020s, a number of cities worldwide began creating Chief Heat Officer positions to organize and manage work counteracting the urban heat island effect. Africa Africa is urbanizing faster than any other continent, and it is estimated that by 2030, more than one billion Africans will live in cities. This rapid urbanization, coupled with the many interlinked and complex challenges as a result of climate change, pose a significant barrier to Africa's sustainable development. Much of this Urban Development is informal, with urban residents settling in informal settlements and slums often on the outskirts of cities. This phenomenon suggests that lower-income countries should be targeted in initiatives to increase infrastructural sustainability. A recent study found that in \"countries with per capita incomes of below USD 15,000 per year (at PPP-adjusted 2011 USD) carbon pricing has, on average, progressive distributional effects\" and that \"carbon pricing tends to",
    "label": 0
  },
  {
    "text": "initiatives to increase infrastructural sustainability. A recent study found that in \"countries with per capita incomes of below USD 15,000 per year (at PPP-adjusted 2011 USD) carbon pricing has, on average, progressive distributional effects\" and that \"carbon pricing tends to be regressive in countries with relatively higher income,\" indicating that carbon taxing and shifting carbon prices might incentivize governments to shift to green energy as the baseline energy consumption method for developing peri-urban areas. Although urbanization is seen in a positive light, the effects of it can be negative on those being urbanized. African cities are exposed to multiple climate threats including floods, drought, water stress, sea level rise, heat waves, storms and cyclones, and the related effects of food insecurity and disease outbreaks like cholera and malaria from floods and droughts. Climate impacts in rural areas, such as desertification, biodiversity loss, soil erosion and declines in agricultural productivity, are also driving rural-urban migration of poor rural communities to cities. To achieve sustainable development and climate resilience in cities in Africa, and elsewhere, it is important to consider these urban-rural interlinkages. Increasing attention is being paid to the important role of peri-urban areas in urban climate resilience, particularly regarding the ecosystem services that these areas provide and which are rapidly deteriorating in Sub-Saharan Africa. Peri-urban ecosystems can provide functions such as controlling floods, reducing the urban heat island effect, purifying air and water, supporting food and water security, and managing waste. Asia China China currently has one of the fastest-growing industrial economies in the world, and the effects of this rapid urbanization have not been without climate change implications. The country is one of the largest by land area, and so the most prominent region regarding urbanization is the Yangtze River Delta, or YRD, as it is considered \"China's most",
    "label": 0
  },
  {
    "text": "rapid urbanization have not been without climate change implications. The country is one of the largest by land area, and so the most prominent region regarding urbanization is the Yangtze River Delta, or YRD, as it is considered \"China's most developed, dynamic, densely populated and concentrated industrial area\" and is allegedly \"growing into an influential world-class metropolitan area and playing an important role in China's economic and social development\". In this way urbanization in China could be understood as intimately related to not only the functionality of their economic system, but the society therein; something that makes climate change mitigation an intersectional issue concerning more than simply infrastructure. The data show that \"[h]igh-administrative-level cities had stronger adaptation, lower vulnerability, and higher readiness than ordinary prefecture-level cities.\" China's large-scale population migration to the Yangtze River Delta and agglomeration due to rapid urbanization, and blind expansion in the construction of eastern coastal cities due to population pressure is even more unfavorable for urban climate governance. Historically, data has shown that \"climate change has been shaping the Delta and its socio-economic development\" and that such socio-economic development in the region \"has shaped its geography and built environment, which, however, are not adaptable to future climate change\". Thus, it has been stated that \"It is imperative to adopt policies and programs to mitigate and adapt to climate change\" in the YRD, specifically, policies that are aimed at reducing the impact of particular climate threats based on the YRD's geography. This includes the region's current infrastructure in the mitigation of flood disasters and promotion of efficient energy usage at the local level. A national-level policy analysis done on the drylands of northern China presents the notion of \"sustainable urban landscape planning (SULP)\" that specifically aims to \"avoid occupying important natural habitats and corridors, prime croplands,",
    "label": 0
  },
  {
    "text": "efficient energy usage at the local level. A national-level policy analysis done on the drylands of northern China presents the notion of \"sustainable urban landscape planning (SULP)\" that specifically aims to \"avoid occupying important natural habitats and corridors, prime croplands, and floodplains\". The research indicates that adopting SULPs moving into the future can \"effectively manage the impacts of climate change on water resource capacity and reduce water stress\" not only within the northern China experimental model but for \"drylands around the world\". South Asia South Asia's urban population grew by 130 million between 2001 and 2011—more than the entire population of Japan—and is poised to rise by almost 250 million by 2030. But, urbanization in South Asia is characterized by higher poverty, slums, pollution and crowding and congestion. At least 130 million South Asians—more than the entire population of Mexico—live in informal urban settlements characterized by poor construction, insecure tenure and underserviced plots. Despite being a water-rich zone, climate projection models suggest that by 2050, between 52 and 146 million people living in South Asia could face increased water scarcity due to climate change, accounting for 18% of the global population exposed to water scarcity. Urban water access is particularly critical in South Asia as it remains home to more than 40% of the world's poor (living on less than US$1.25 per day) and 35% of the world's undernourished. A study done of selected Himalayan cities in India and Nepal found that none of them have a robust system of water planning and governance to tackle the water challenges emerging from rapid urbanization and climate change. Indias urbanization is characterized by informal settlements or slums, which lack basic amenities and are oftentimes hazardous areas such as floodplains or steep slopes. Khulna, Bangladesh is also facing issues as well. As sea",
    "label": 0
  },
  {
    "text": "from rapid urbanization and climate change. Indias urbanization is characterized by informal settlements or slums, which lack basic amenities and are oftentimes hazardous areas such as floodplains or steep slopes. Khulna, Bangladesh is also facing issues as well. As sea levels begin to rise, due to climate change, salinity will move inwards, reducing the amount of safe drinking water available to the people of Khulna. There are plans being put in place to make the quality of water in cities better, but this decreases the availability to those in the informal urban areas. As of now they rely on using on as little water as possible, specifically for their crops. Europe North and South America Brazil Areas of South America were also cited in recent studies that highlight the dangers of urbanization on both local and transnational climates, and for a country like Brazil, one of the highest populated nations in the world as well as the majority holder of the Amazon rainforest. The United Nations Development Programme highlights the Amazon rainforest as serving a \"key function in the global climate systems,\" granted its profound usefulness in capturing CO2 emissions. UN research has indicated that because of Brazil's climate being so intimately reliant on the health of the rainforest, deforestation measures are currently seen as having adverse effects on the rainforest's \"natural adaptive capacities\" towards extreme climate shifts, thus predisposing Brazil to what are expected to be increased volatility in temperature and rainfall patterns. More specifically, it is expected that if global warming continues on its current path without vast mitigation strategies being put in place, what is currently predicted to be an average 2 °C increase in temperature at the global scale could look like a 4 °C within Brazil and the surrounding Amazon region. Rapid urbanization in other",
    "label": 0
  },
  {
    "text": "mitigation strategies being put in place, what is currently predicted to be an average 2 °C increase in temperature at the global scale could look like a 4 °C within Brazil and the surrounding Amazon region. Rapid urbanization in other countries will also result in higher need for resources. This includes resources that will cause further deforestation of the Amazon Rainforest to obtain. This will inevitably create a lot more climate issues, as we continue to lose more trees in the Amazon rainforest. Issues of climate change in Brazil do not start and end at what has already been done with regards to urbanization; it is very much an issue rooted in socioeconomic contexts. Factor analysis and multilevel regression models sponsored by the U.S. Forest Service revealed that for all of Brazil, \"income inequality significantly predicts higher levels of a key component of vulnerability in urban Brazilian municipalities\" to flood hazards. The future of Brazil's effect of climate is likely to change since though its NDC Brazil has made the commitment to lower their greenhouse gas emissions by 37% below their 2005 levels by 2025. This will likely serve as a challenge within the cities of Brazil since 86% of the whole countries population lives in the urban areas, and this is likely to increase to 92% by 2050. As for deforestation, since Brazil is home to the Amazon rainforest, Brazil has always had a high deforestation rate. Brazil's deforestation was at a high in 2004 with having 27.77 thousand kilometers of forest being destroyed, having a low in 2012 with only 4.57 thousand kilometers of forest being destroyed, and since then it has been back on the incline with 10.85 thousand kilometers of forest being destroyed. United States The United States, as one of the largest industrialized nations in",
    "label": 0
  },
  {
    "text": "with only 4.57 thousand kilometers of forest being destroyed, and since then it has been back on the incline with 10.85 thousand kilometers of forest being destroyed. United States The United States, as one of the largest industrialized nations in the world, also has issues regarding infrastructural insufficiencies linked to climate change. Take a study of Las Vegas topology as an indicator. Research that created three land use/land cover maps, or LULC maps, of Las Vegas in 1900 (albeit hypothetical), 1992, and 2006 found that \"urbanization in Las Vegas produces a classic urban heat island (UHI) at night but a minor cooling trend during the day\". In addition to temperature changes in the city, \"increased surface roughness\" caused by the addition of skyscrapers/closely packed buildings in its own way were found \"to have a mechanical effect of slowing down the climatological wind Windfield over the urban area\". Cities in the United States that are heavily industrialized, such as Los Angeles, are responsible for a large number of greenhouse emissions due to the amount of transportation needed for millions of people living in one city. Such unnatural environmental phenomena furthers the notion that urbanization has a role in determining local climate, although researchers acknowledge that more studies need to be conducted in the field. Cities play an important role in investing in climate innovation in the United States. Often local climate policies in cities, preempt larger policies pursued by the states or federal government. For example, following the United States withdrawal from the Paris Agreement a coalition of cities, under the banner of Mayors National Climate Action Agenda. A 2020 study of US cities found that 45 of the 100 largest cities in the U.S. had made commitments by 2017, which led to a reduction of 6% of U.S. emissions by",
    "label": 0
  },
  {
    "text": "banner of Mayors National Climate Action Agenda. A 2020 study of US cities found that 45 of the 100 largest cities in the U.S. had made commitments by 2017, which led to a reduction of 6% of U.S. emissions by 2020. Clean Air Act Since the Clean Air Act's passing in 1963 as a landmark piece of legislation aimed at controlling air quality at the national level, research has indicated that \"the mean wet deposition flux... has decreased in the U.S. over time\" since its enactment. Even then, however, the same research indicated that measurements in the amounts of chemical pollutants contaminating rain, snow, and fog \"follows an exponential probability density function at all sites\". Such a finding suggests that alleged variability in rainfall patterns is the likely driving factor for the study's seemingly promising results, as opposed to there being a clear significance stemming from the policy change. It is within this context that while beneficial, the Clean Air Act alone cannot stand as the only firm rationale for climate policies in the United States moving forward. Mayors National Climate Action Agenda International policy Several major international communities of cities and policies have been formed to include more cities in climate action. C40 SDG 11: Sustainable cities and communities Global Covenant of Mayors for Climate and Energy See also List of most-polluted cities by particulate matter concentration Climate change in New York City Climate change in Washington, D.C. Climate change in Australia by city Climate change in London Incorporation of nature within a city Urban heat island Zero-carbon city References External links UN Climate Change Course on Cities and Climate Change",
    "label": 0
  },
  {
    "text": "Climate Change Isn’t Everything: Liberating Climate Politics from Alarmism is a book by Mike Hulme published in 2023 by Polity Press. Synopsis Climate Change Isn’t Everything: Liberating Climate Politics from Alarmism is a book by Mike Hulme, Professor of Human Geography at the University of Cambridge, where the author introduces the readers to his meaning of the term / neologism ‘climatism’. According to a blurb for the book offered by Ted Nordhaus of the Breakthrough Institute climatism reduces the condition of the world to the fate of global temperature or to the atmospheric concentration of Carbon dioxide, to the detriment of tackling serious issues as varied as poverty, liberty, biodiversity loss, inequality and international diplomacy. Hulme notes that the term has been used before, and noticeably in Steve Goreham’s Climatism, Science, Common Sense and the 21st Century Hottest Topic, New Lenox Books 2010. While Goreham contest the human origin of climate change, Hulme holds the opposite view, p. 168. Main The author opens the book with a discussion of how the Syrian Civil War, started in March 2011 after civil unrest following the torture by President Assad's security agents of young Syrians schoolboys, was blamed on climate change: according to the narrative, a multi-year draught had displaced agricultural labourers to towns and cities, with a consequent unrest for lack of available jobs. The narrative was upheld by authoritative figures such as the US secretary of state John Kerry, President Barack Obama, Prince Charles and important institutions such as the World Bank, Friends of the Earth. As a result, in the following years the flux of migrants escaping the civil war was likewise blamed on climate change by the President of the European Commission Jean-Claude Juncker, p. 2. Hulme disagrees with this narrative and argues that blaming the civil war in",
    "label": 0
  },
  {
    "text": "following years the flux of migrants escaping the civil war was likewise blamed on climate change by the President of the European Commission Jean-Claude Juncker, p. 2. Hulme disagrees with this narrative and argues that blaming the civil war in Syria, the increase of hate speech and racist tweets on Twitter, or floods devastation on climate is ultimately a distraction from tackling their major underlying causes. p. 2. According to a review Hulme is clear that he believes human behaviour is changing the climate and that society should move toward net zero emissions. Hulme compares ‘climatic determinism’ to racial determinism in presenting value-based judgments as science and offers an alternative based on what he calls climatic pragmatism, as a recipe that is contextually sensitive, diverse and pragmatic (p. 11 of Hulme's book). Still for review the author – living a tension between critique and sympathy regarding climate change – is aware of the risk of his work being appropriated by climate deniers, and has accepted the risk to fight what he perceives as the dangers of climatism. Each chapter ends with a section entitled ‘Retort’ where likely objections are discussed, and a ‘Further Reading’ section at the end of the book offers suggestions relating to climate science, the relation between expertise, politics and democracy, the different meanings of climate change, the power of narratives, climate reductionism and climate anxieties. For Hulme climate change has become the lens through which we perceive and address societal challenges. This has made climate as a \"self-sufficient\" narrative capable of explaining political, socio-ecological, and ethical dilemmas. Hulme dedicates a chapter to reconstruct the genesis of climatism in ten \"moves\", Among these, the adoption of global temperature as a \"flawed index for capturing the full range of complex relationships between climate and human welfare and ecological",
    "label": 0
  },
  {
    "text": "dilemmas. Hulme dedicates a chapter to reconstruct the genesis of climatism in ten \"moves\", Among these, the adoption of global temperature as a \"flawed index for capturing the full range of complex relationships between climate and human welfare and ecological integrity\" (move 3) appear as especially relevant. For Hulme the moves have led to the ideology of climatism, making climate change into the \"leitmotif of contemporary politics.\" A final chapter entitled \"If Not Climatism, Then What?\" offers Hulme's recipes, under the banner that \"Wicked problems need clumsy solutions\": these include foregrounding scientific uncertainty, defusing deadline-ism, and acknowledging the plurality of values and goals as. The chapters A succinct summary of the book chapter by chapter is offered in: ‘From Climate to climatism’, How an Ideology is Made. Introduces climatism as an ideology. ‘How did Climatism Arise’, Fetishizing Global Temperature. Describes the ascent of climatism in ten \"moves\". ‘Are the Sciences Climatist’, The Noble Lie and Other Misdemeanors.Presents use and misuse of climate scenarios, with particular attention to IPCC's Representative Concentration Pathway 8.5. ‘Why is Climatism So Alluring’, Master-Narratives and Polarizing Moralism. The main ingredient of climatism: climatism as a master narrative, its Manichean worldview and its apocalyptic rhetoric. ‘Why is Climatism Dangerous’, The Narrowing of Political Vision. Exemplifies the perverse effects of compressing all ailment of the world under a single heading, unlike the broad spectrum of the sustainable development goals. The case of deforestation in Indonesia driven by European Union biofuel policies. ‘If Not Climatism, Then What’, Wicked Problems Need Clumsy Solutions. Gives a recipe for progress: replace a culture of hubris with one of humility, discontinue presenting the future in terms of cliff-edges or no-return points, embrace a plurality of values and perspectives, including site-, culture- and context-specific values and perspectives. ‘Some Objections’, You sound Just Like…. Anticipates",
    "label": 0
  },
  {
    "text": "culture of hubris with one of humility, discontinue presenting the future in terms of cliff-edges or no-return points, embrace a plurality of values and perspectives, including site-, culture- and context-specific values and perspectives. ‘Some Objections’, You sound Just Like…. Anticipates objections to the positions expressed in the book. Reception The reception of the book is varied. For Teresa de León:Hulme has the merit of having shown how the social encounter with nature has been narrowed by a generic distinction of villains and saviors of the Earth, which has triggered ethical reflections about what is more socially sensitive to nature and what is not. For Volker Hahn, writing in the blog of Roger Pielke Jr.:Hulme’s book may not deliver radically new or surprising insights. But it is a concise digest of the current climate discourse and depicts where things are going wrong. For Nicholas Clairmont writing in The New Atlantis: The book is a welcome remedy for a climate discourse beset by scientization, the antidemocratic process by which “scientific statements substitute — or at least become a short-hand — for ethical or political reasoning and argument.” A more critical view is offered by Simon Maxwell:It’s hard to decide whether Mike Hulme’s new book is: (a) a salutary warning to over-enthusiastic advocates of climate action, (b) a case of over-egging the pudding in regard to same, or (c) and despite protestations to the contrary, a gift to climate deniers. The book has elements of all three, but I confess Hulme’s central assertion, that ‘climatism’ has emerged as a technocratic, pervasive and apocalyptic ‘ideology’ which explains all the world’s ills and claims pre-eminence in policy, strikes me mainly as a case of over-egging the pudding. References External links https://www.geog.cam.ac.uk/people/hulme/ Prof. Hulme page at the University of Cambridge Book page at Polity Press",
    "label": 0
  },
  {
    "text": "Climate change litigation, also known as climate litigation, is an emerging body of environmental law using legal practice to set case law precedent to further climate change mitigation efforts from public institutions, such as governments and companies. In the face of slow climate change politics delaying climate change mitigation, activists and lawyers have increased efforts to use national and international judiciary systems to advance the effort. Climate litigation typically engages in one of five types of legal claims: Constitutional law (focused on breaches of constitutional rights by the state), administrative law (challenging the merits of administrative decision making), private law (challenging corporations or other organizations for negligence, nuisance, etc., fraud or consumer protection (challenging companies for misrepresenting information about climate impacts), or human rights (claiming that failure to act on climate change is a failure to protect human rights). Litigants pursuing such cases have had mixed results. Since the early 2000s, the legal frameworks for combating climate change have increasingly been available through legislation, and an increasing body of court cases have developed an international body of law connecting climate action to legal challenges, related to constitutional law, administrative law, private law, consumer protection law or human rights. Many of the successful cases and approaches have focused on advancing the needs of climate justice and the youth climate movement. Since 2015, there has been a trend in the use of human rights arguments in climate lawsuits, in part due to the recognition of the right to a healthy environment in more jurisdictions and at the United Nations. High-profile climate litigation cases brought against states include Leghari v. Pakistan, Juliana v. United States (both 2015), Urgenda v. The Netherlands (2019), and Neubauer v. Germany (2021), while Milieudefensie v Royal Dutch Shell (2021) is the highest-profile case against a corporation to date.",
    "label": 0
  },
  {
    "text": "cases brought against states include Leghari v. Pakistan, Juliana v. United States (both 2015), Urgenda v. The Netherlands (2019), and Neubauer v. Germany (2021), while Milieudefensie v Royal Dutch Shell (2021) is the highest-profile case against a corporation to date. Environmental activists have asserted that investor-owned coal, oil, and gas corporations could be legally and morally liable for climate-related human rights violations, even though political decisions could prevent them from engaging in such violations. Litigations are often carried out via collective pooling of effort and resources such as via organizations like Greenpeace, such as Greenpeace Poland which sued a coal utility and Greenpeace Germany which sued a car manufacturer. Such cases may take many years to unfold, and have occasionally been unsuccessful despite lengthy efforts, as was the case with Juliana v. United States. The 2010s saw a growing trend of activist cases successfully being won in global courts. The 2017 UN Litigation Report identified 884 cases in 24 countries, including 654 cases in the United States and 230 cases in all other countries combined. As of July 1, 2020, the number of cases has almost doubled to at least 1,550 climate change cases filed in 38 countries (39 including the courts of the European Union), with approximately 1,200 cases filed in the US and over 350 filed in all other countries combined. By December 2022, the number had grown to 2,180, including 1,522 in the U.S. The number of litigation cases is expected to continue rising in the 2020s. In March 2025, the U.S. Supreme Court declined a request by Republican-led states to block efforts by Democratic-led states to file lawsuits in state courts holding energy companies accountable for the damages caused by climate change. There is a growing number of litigation cases, and international decisions can influence domestic",
    "label": 0
  },
  {
    "text": "by Republican-led states to block efforts by Democratic-led states to file lawsuits in state courts holding energy companies accountable for the damages caused by climate change. There is a growing number of litigation cases, and international decisions can influence domestic courts. However, some cases work in the opposite direction: they challenge climate action and are not aligned with climate goals. Methods and types of laws Climate litigation typically falls into one of five broad areas of law: Constitutional law — focused on breaches of constitutional rights by the state. Administrative law — challenging the merits of administrative decision making within existing on-the-books laws, such as not granting permissions for high-emissions projects. Private law — challenging corporations or other organizations for negligence, nuisance, trespass, public trust, and unjust enrichment. Fraud or consumer protection — typically challenging companies for misrepresenting information about climate impacts. Human rights — claiming that failure to act on climate change or to protect related natural resources, such as the atmosphere or the rainforest, fails to protect human rights. These areas are not static. For instance, Smith v Fonterra Co-operative Group Ltd argues for the new tort of climate change damage and the New Zealand Supreme Court duly ruled in 2024 that this novel civil wrong can be asserted in future proceedings. Scale The 2017 UN Litigation Report identified 884 cases in 24 countries, including 654 cases in the United States and 230 cases in all other countries combined. As of July 1, 2020, the number of cases has almost doubled to at least 1,550 climate change cases filed in 38 countries (39 including the courts of the European Union), with approximately 1,200 cases filed in the US and over 350 filed in all other countries combined. By December 2022, the number had grown to 2,180, including 1,522",
    "label": 0
  },
  {
    "text": "filed in 38 countries (39 including the courts of the European Union), with approximately 1,200 cases filed in the US and over 350 filed in all other countries combined. By December 2022, the number had grown to 2,180, including 1,522 in the U.S. Scholars have observed a \"rapidly growing landscape of climate litigation\" (as of 2024) and say that courts are shaping law and governance trajectories, beyond the narrow confines of domestic law. By type of action Cases to increase accountability Courts play a critical role in creating, increasing and imposing accountability on and for public authorities and private actors for climate and related harms caused by their actions or their failure to act. Courts articulate what it means for public and private actors to be accountable in relation to climate (in)action. National courts have ordered governments to: legislate on climate change (e.g. Shrestha v Office of the Prime Minister, 2018); define sufficiently ambitious mitigation targets (e.g. Klimaatzaak v Belgium, 2021); develop a clear long-term emission-reduction strategy (e.g. Friends of the Irish Environment v Ireland, 2020) or a realistic long-term emission reduction pathway (e.g. Neubauer v Germany, 2021); present complete information on how it plans to achieve its statutory carbon budget (e.g. Friends of the Earth v Secretary of State for Business, Energy and Industrial Strategy, 2022); comply with a statutory carbon budget (e.g. Grande-Synthe v France, 2021) or to take appropriate measures to achieve a statutory carbon budget (e.g. Oxfam v France, 2021). Between governments and companies In the United States, Friends of the Earth, Greenpeace together with the cities of Boulder, Arcata and Oakland filed against the Export-Import Bank of the United States and the Overseas Private Investment Corporation (state-owned enterprises of the United States government), which were accused of financing fossil-fuel projects detrimental to a stable climate,",
    "label": 0
  },
  {
    "text": "cities of Boulder, Arcata and Oakland filed against the Export-Import Bank of the United States and the Overseas Private Investment Corporation (state-owned enterprises of the United States government), which were accused of financing fossil-fuel projects detrimental to a stable climate, in violation of the National Environmental Policy Act (case filed in 2002 and settled in 2009). In 2017, San Francisco, Oakland and other California coastal communities sued multiple fossil-fuel companies for rising sea levels; they lost. In 2018, the city of New York announced that it is taking five fossil fuel firms (BP, ExxonMobil, Chevron, ConocoPhillips and Shell) to federal court due to their contribution to climate change (from which the city is already suffering). In 2020, Charleston, South Carolina, followed a similar strategy. In June 2023, Multnomah County, Oregon sued several fossil fuel companies and industry trade groups, seeking at least $50 billion to help the county study and implement harm reduction strategies. The suit also asks for $50 million to cover past damages, and $1.5 billion in future damages. The lawsuit alleges that parties, including ExxonMobil, Chevron and the American Petroleum Institute, deceptively used \"pseudo-science, fabricated doubt, and a well-funded, sustained public relations campaign\" to subvert scientific consensus over the course of decades. Cases involving multilateral institutions European Union In 2018, ten families from European countries, Kenya and Fiji filed a suit against the European Union for the threats against their homes caused by the EU greenhouse emissions. The European Union adopted an anti-slapp directive aiming to protect human right defenders and journalists from lawsuits intended to silence them. European Court of Human Rights Verein KlimaSeniorinnen Schweiz v. Switzerland (2024) was a landmark case of the European Court of Human Rights in which the court ruled that Switzerland violated the European Convention on Human Rights by failing to",
    "label": 0
  },
  {
    "text": "them. European Court of Human Rights Verein KlimaSeniorinnen Schweiz v. Switzerland (2024) was a landmark case of the European Court of Human Rights in which the court ruled that Switzerland violated the European Convention on Human Rights by failing to adequately address climate change. It is the first case in which an international court has ruled that state inaction related to climate change violates human rights. United Nations On 29 March 2023, the United Nations adopted a resolution calling for the International Court of Justice (ICJ) to \"strengthen countries' obligations to curb warming and protect communities from climate disaster\". In 2025 ICJ was expected to issue a decision clarifying legal requirements on states to respond to the climate crisis and articulating consequences that countries should face for failure to meet those requirements. In July 2025, the ICJ has said in an advisory opinion, a \"clean, healthy and sustainable environment\" is a human right, and that failing to protect the planet from the impacts of climate change may be a violation of international law. By country Australia As of February 2020, Australia had the second most number of cases pending in the world, with almost 200 cases. Cases in Australia include Torres Strait Islanders v. Australia (2019), in which the United Nations Human Rights Committee found that the Australian government had violated the Islanders' human rights by failure to act on climate change, Youth Verdict v. Waratah Coal (2020), and Sharma v. Minister for the Environment (2020), in which eight young people unsuccessfully argued for an injunction against the expansion of a Whitehaven coal mine. Belgium In June 2021, after a six year long legal battle, the Court of First Instance ruled that the climate targets of the government of Belgium are too low and therefore \"breached the right to life",
    "label": 0
  },
  {
    "text": "a Whitehaven coal mine. Belgium In June 2021, after a six year long legal battle, the Court of First Instance ruled that the climate targets of the government of Belgium are too low and therefore \"breached the right to life (article 2) and the right to respect for private and family life (article 8)\" of the European Convention on Human Rights. Colombia A group of children in Colombia sued the government to protect the Amazon rainforest from deforestation due to the deforestation's contribution to climate change. In 2018, the Supreme Court ruled that the Colombian rainforest was an \"entity subject of rights\" requiring protection and restoration. France In 2020, an administrative court case in France, required the Macron administration to review their policies to address climate change to make sure they were significant enough to meet Paris Agreement commitments. Germany In 2021, Germany's supreme constitutional court ruled in Neubauer v. Germany that the government's climate protection measures are insufficient to protect future generations and that the government had until the end of 2022 to improve its Climate Protection Act. A court case brought by German citizens against their government in 2022 based on a newly minted human right to breathe clean and healthy air could pave the way for future legislation with regards to climate action. In 2023, the Berlin-Brandenburg Higher Administrative Court said the government's action on transport and housing fell short under a law setting upper limits for carbon emissions for individual sectors. Under the ruling, Berlin must present emergency programmes to bring its policy on transport and housing back in line with the current Climate Protection Act from 2024 to 2030. Republic of Ireland In July 2020, Friends of the Irish Environment won a landmark case against the Irish government for failing to take sufficient action to",
    "label": 0
  },
  {
    "text": "housing back in line with the current Climate Protection Act from 2024 to 2030. Republic of Ireland In July 2020, Friends of the Irish Environment won a landmark case against the Irish government for failing to take sufficient action to address the climate and ecological crisis. The Supreme Court of Ireland ruled that the Irish government's 2017 National Mitigation Plan was inadequate, specifying that it did not provide enough detail on how it would reduce greenhouse gas emissions. Italy Giudizio Universale lawsuit On 5 June 2021, a group of 24 associations and 179 citizens (17 of whom were minor), led by non-profit association A Sud ('To South'), officially filed a lawsuit against the Italian government in the civil court in Rome, with the main goals of holding national institutions \"accountable for the state of danger caused by [their] inertia in tackling the climate change emergency\", as well as ruling that Italy must cut its greenhouse gas emissions from 1990 levels by 92% within 2030. This last target, which set more ambitious targets than the European Green Deal, was based on independent researches on international climate politics made by Climate Analytics and the New Climate Institute. The co-plaintiffs, which included Fridays For Future members and meteorologist Luca Mercalli, were assisted by three attorneys specialized in environmental law. Other notable environmentalist organizations, including Legambiente and Greenpeace, opted not to support the lawsuit: president of Greenpeace Italy, Giuseppe Onufrio, justified the decision by stating that court cases should focus on influential companies, rather than institutions, to become more effective. Eni lawsuit On 9 May 2023, Greenpeace Italy and advocacy group ReCommon, together with 12 Italian plaintiffs from several areas directly affected by climate change, officially announced that they would file a lawsuit against national energy company Eni, as well as the Ministry of",
    "label": 0
  },
  {
    "text": "May 2023, Greenpeace Italy and advocacy group ReCommon, together with 12 Italian plaintiffs from several areas directly affected by climate change, officially announced that they would file a lawsuit against national energy company Eni, as well as the Ministry of Economy and Finance and Cassa Depositi e Prestiti (both involved as co-owners), requesting to set the beginning of the hearings in November of the same year. Also known as La Giusta Causa ('The Right Cause'), and based on the Milieudefensie et al v Royal Dutch Shell court case, it became the first climate lawsuit ever filed against a private-owned company in Italy. The allegations focused on Eni's central role in increasing fossil fuel usage throughout the latest decades, despite being aware of the emissions' worst risks. A DeSmog inquiry revealed further evidence supporting the lawsuit's claims: firstly, a study commissioned by Eni itself from an affiliate research centre between 1969 and 1970, which had underlined the risk of a \"catastrophic\" climate crisis by 2000 posed by an unchecked rise in fossil fuel usage; secondly, a 1978 report produced by Tecneco, another company owned by Eni, which had accurately estimated that the CO2 concentration would have reached 375-400 ppm by 2000, while noting that such changes to the thermal balance of the atmosphere could have had \"serious consequences for the biosphere\". DeSmog's investigation also found that Eni's official magazine, Ecos, had repeatedly included references to climate change in articles written throughout the late 1980s and 1990s, while hosting advertising campaigns wrongly claiming that natural gas was a \"clean fuel\". The plaintiffs asked the court to \"acknowledge the damage and the violation of [their] human rights to life, health and an undisturbed personal life\" and rule that Eni must cut their emissions from 2020 levels by 45% within 2030, in order to",
    "label": 0
  },
  {
    "text": "asked the court to \"acknowledge the damage and the violation of [their] human rights to life, health and an undisturbed personal life\" and rule that Eni must cut their emissions from 2020 levels by 45% within 2030, in order to reach the goals set by the Paris Agreement. In an official response, Eni's board said they would prove the lawsuit was \"groundless\". The first hearing of the court case took place on 16 February 2024. Korea On 29 August 2024, the Constitutional Court of Korea ruled that the absence of legally binding targets for greenhouse gas reductions for 2031-2049 violated the constitutional rights of future generations, saying that this lack of long-term targets shifted an excessive burden to the future. Netherlands The Urgenda case is an important global precedent for climate litigation. In 2012, the Dutch lawyer Roger Cox gave the idea of judicial intervention to force action against climate change based on government targets for 2030 emissions reductions. In 2013, the Urgenda Foundation, with 900 co-plaintiffs, filed a lawsuit against the Government of the Netherlands \"for not taking sufficient measures to reduce greenhouse gas emissions that cause dangerous climate change\". In 2015, the District Court of The Hague ruled that the government of the Netherlands must do more to reduce greenhouse gas emissions to protect its citizens from climate change. It was described as a \"precedent-setting judgment\" and as the \"world's first climate liability suit\". In 2018, a court of appeal in The Hague has upheld the precedent-setting judgment that forces the Dutch government to step up its efforts to curb greenhouse-gas emissions in the Netherlands. In December 2019, the Supreme Court of the Netherlands upheld the ruling on appeal. Thus, affirming that the government must cut carbon dioxide emissions by 25% from 1990 levels by the end of",
    "label": 0
  },
  {
    "text": "to curb greenhouse-gas emissions in the Netherlands. In December 2019, the Supreme Court of the Netherlands upheld the ruling on appeal. Thus, affirming that the government must cut carbon dioxide emissions by 25% from 1990 levels by the end of 2020, on the basis that climate change poses a risk to human health. Additional cases in the Netherlands include Milieudefensie et al v Royal Dutch Shell. The case was decided in May 2021, the district court of The Hague ordered Royal Dutch Shell to cut its global carbon emissions by 45% by the end of 2030 compared to 2019 levels, and affirmed the responsibility of the company for scope 3 emissions, e.g., emissions from suppliers and customers of its products. New Zealand In 2024, the New Zealand Supreme Court gave leave for Māori climate activist Mike Smith to sue seven corporations for their roles in causing climate change and the common law harms that resulted. Several aspects of Smith v Fonterra Co-operative Group Limited are notable. Smith argued that the principles of tikanga Māori — a traditional system of obligations and recognitions of wrong — can be used to inform New Zealand common law. Smith argued that the activities of the seven defendants — by directly emitting greenhouse gasses or supplying fossil fuels — fall under the established torts of public nuisance and negligence and a new tort of climate change damage. Smith further argued that these seven corporations are harming his tribe's land, coastal waters, and traditional culture. Smith belongs to the Northland tribes of Ngāpuhi and Ngāti Kahu. This judgment simply allows Smith to now pursue these matters in the High Court. The defendants have indicated that they will seek to convince the court that climate change responses are better left to government policy and not subject to",
    "label": 0
  },
  {
    "text": "This judgment simply allows Smith to now pursue these matters in the High Court. The defendants have indicated that they will seek to convince the court that climate change responses are better left to government policy and not subject to civil litigation. Pakistan In Pakistan in 2015 Lahore High Court ruled in Asghar Leghari vs. Federation of Pakistan that the government was violating the National Climate Change Policy of 2012 and the Framework for Implementation of Climate Change Policy (2014–2030) by failing to meet goals set by the policies. In response, a Climate Change Commission was required to be formed in order to help Pakistan meet its climate goals. The case is considered significant in the history of human rights-based climate litigation. Peru In 2017, Saul Luciano Lliuya sued RWE to protect his hometown of Huaraz from a swollen glacier lake at risk of overflowing. Philippines In May 2024, Nicol Melgar Marba, a Typhoon Odette survivor from Dinagat Islands, Philippines, and other victims of climate disasters from around the world filed before the Paris Criminal Court a criminal complaint against French oil company TotalEnergies, demanding compensation for \"losses and damages for climate impacts\". South Africa In December 2024 the Supreme Court in South Africa stopped the plans of the government to add 1,500 megawatts of coal-fired power. The court said it is “unlawful and invalid”, and required from the minister and the regulator to pay costs to the complainants. Before it environmentalists already had some victories in South Africa's courts about pollution and drillings. Turkey Article 56 of the Constitution of Turkey says that, \"Everyone has the right to live in a healthy and balanced environment. It is the duty of the State and citizens to improve the natural environment, to protect the environmental health and to prevent environmental pollution.\"",
    "label": 0
  },
  {
    "text": "of Turkey says that, \"Everyone has the right to live in a healthy and balanced environment. It is the duty of the State and citizens to improve the natural environment, to protect the environmental health and to prevent environmental pollution.\" Turkey has ratified the Paris Agreement and says that its greenhouse gas emissions will be net zero by 2053, but the government has no plan to phase out coal. As of 2025 eight cases have been filed several of which are ongoing. In 2020 and 2021 sixteen nongovernment organizations filed lawsuits requesting the president shutdown 37 large coal-fired power stations and over 600 mines. In addition to climate change arguments the plaintiffs alleged that cancer cases are increased and the COVID-19 pandemic was worsened by their air pollution. The case was rejected by the 11th administrative court of Ankara for various reasons. In 2023 young climate activists opened a case alleging that the nationally determined contribution (NDC) was inadequate. The three youth climate activists filed a lawsuit against President Erdoğan and the Ministry of Environment, Urbanisation and Climate Change because Turkey's Nationally Determined Contribution is not to reduce its greenhouse gas emissions. They alleged that there is no effective climate action plan for energy. They alleged that these violate their human rights stated in the constitution, such as the environmental clause in the constitution. The Council of State rejected the case on the grounds that it was an ‘annulment of an administrative action’ case but the NDC is not an administrative action. As of 2025 the case is at the Constitutional Court. In 2022 and 2023 cases were brought about Lake Marmara drying up. In 2024 a court decided to pause the process of reclassifying the land as not wetland, on the grounds that it could be rewetted: Doğa are",
    "label": 0
  },
  {
    "text": "Constitutional Court. In 2022 and 2023 cases were brought about Lake Marmara drying up. In 2024 a court decided to pause the process of reclassifying the land as not wetland, on the grounds that it could be rewetted: Doğa are calling for the court to annul the reclassification. United Kingdom In December 2020, three British citizens, Marina Tricks, Adetola Onamade, Jerry Amokwandoh, and the climate litigation charity, Plan B, announced that they were taking legal action against the UK government for failing to take sufficient action to address the climate and ecological crisis. The plaintiffs announced that they will allege that the government's ongoing funding of fossil fuels both in the UK and other countries constitute a violation of their rights to life and to family life, as well as violating the Paris Agreement and the UK Climate Change Act of 2008. In 2022, it was claimed in McGaughey and Davies v Universities Superannuation Scheme Ltd that the directors of the UK's largest pension fund, USS Ltd had breached their duty to act for proper purposes under the Companies Act 2006 section 171, by failing to have a plan to divest fossil fuels from the fund's portfolio. The claim did not succeed in the High Court, and the claimants appealed to the Court of Appeal, being granted permission for a June 2023 hearing. The case alleges that the right to life must be used to interpret duties in company law, and that because fossil fuels must cease to exist, any investments using them pose a \"risk of significant financial detriment\". In February 2023, ClientEarth filed a derivative action claim against Shell's board of directors for putting the company at risk by not transitioning away from fossil fuels quickly enough. ClientEarth said the lawsuit marked \"the first time ever that a",
    "label": 0
  },
  {
    "text": "In February 2023, ClientEarth filed a derivative action claim against Shell's board of directors for putting the company at risk by not transitioning away from fossil fuels quickly enough. ClientEarth said the lawsuit marked \"the first time ever that a company's board has been challenged on its failure to properly prepare for the energy transition\". United States As of February 2020, the U.S. had the most pending cases with over 1,000 in the court system. Examples include Connecticut v. ExxonMobil Corp. and Massachusetts v. Environmental Protection Agency. In the United States climate change litigation addresses existing principal laws to make their claim, most of them focusing on private and administrative law. The most popular principal laws to use are NEPA (the National Environmental Policy Act), with 322 cases filed under its jurisdiction, the Clean Air Act, with 215 cases filed under its jurisdiction, the Endangered Species Act, with 163 cases filed under its jurisdiction. As more efforts continue on the front of climate change, as of August 2022, the federal government continues to approve agreements and class actions in terms of additional climate change initiatives. In addition, since 2015, there are about two dozen liability and fraud cases brought against some of the world's largest oil companies by various states for their role in denying climate policy leading to increased risks and costs borne to state governments. These states include New Jersey, District of Columbia, Delaware, Connecticut, Minnesota, Rhode Island, Massachusetts, and Vermont. Like Minnesota and the District of Columbia before it, New Jersey has also included the industry's top US trade group, the American Petroleum Institute in addition to ExxonMobil, Shell Oil, Chevron, BP and ConocoPhillips. Actions using the Endangered Species Act In the Endangered Species Act (ESA) case, Tennessee Valley Authority v. Hill, the Supreme Court stated that",
    "label": 0
  },
  {
    "text": "US trade group, the American Petroleum Institute in addition to ExxonMobil, Shell Oil, Chevron, BP and ConocoPhillips. Actions using the Endangered Species Act In the Endangered Species Act (ESA) case, Tennessee Valley Authority v. Hill, the Supreme Court stated that the ESA mandates federal agencies to insure their actions do not jeopardize any species that are listed as endangered in the ESA. Climate change litigation cases that use the ESA primarily focus on articles 7 and 9 of the statue. Article 7 states that all actions carried out by federal agencies must be unlikely to jeopardize the continued existence or result in the destruction of endangered species. Article 9 focuses not just on federal agencies but everybody, banning the taking of any endangered species by any party, be it federal, state, or private. The first step for climate change activists is to make sure that species threatened by climate change are listed on the ESA by the Fish and Wildlife Service (FWS). Oftentimes this alone can be a lengthy process. In December 2005 the Center for Biological Diversity joined with two other US NGOs (Greenpeace and the Natural Resources Defense Council) to petition that the Arctic Polar Bear be listed on the ESA. The FWS under the Bush administration stretched the process out for years, missing many key deadlines and listing the species as \"threatened\" instead of endangered while the science was clearly in favor of an endangered listing. Facing mass public pressure and scientific consensus the FWS officially listed the species as endangered in May 2008. Actions using the National Environmental Policy Act The National Environmental Policy Act (NEPA) recognizes that actions taken by the US government can have significant environmental impact and requires that all federal agencies consider these environmental implications when doing \"major federal actions\". This can",
    "label": 0
  },
  {
    "text": "National Environmental Policy Act The National Environmental Policy Act (NEPA) recognizes that actions taken by the US government can have significant environmental impact and requires that all federal agencies consider these environmental implications when doing \"major federal actions\". This can be done either through an environmental assessment (EA) or a more thorough environmental impact statement (EIS), how thorough the analyzation has to be depends on the nature of the proposed action. Actions using the Clear Air Act The Clean Air Act (CAA) regulates air pollutants both from stationary and mobile sources. The Act was passed in the 1970s before there was widespread knowledge about greenhouse gases (GHGs) but in 2007 the Supreme Court decided the EPA did have to regulate GHGs under the CAA due to the famous Massachusetts vs. The EPA case. In 2009 the state of California was able to use the CAA to create stronger vehicle emission standards than the national standard, which quickly led to the Obama administration adopting these stricter emission standards on a national level. These standards were called the Corporate Average Fuel Efficiency (CAFE) standards and included regulations of GHGs. Massachusetts v. EPA Massachusetts v. Environmental Protection Agency before the Supreme Court of the United States allowed the EPA to regulate greenhouse gases under the Clean Air Act. A similar approach was taken by California Attorney General Bill Lockyer who filed a lawsuit California v. General Motors Corp. to force car manufacturers to reduce vehicles' emissions of carbon dioxide. This lawsuit was found to lack legal merit and was tossed out. A third case, Comer v. Murphy Oil USA, Inc., a class action lawsuit filed by Gerald Maples, a trial attorney in Mississippi, in an effort to force fossil fuel and chemical companies to pay for damages caused by global warming. Described as",
    "label": 0
  },
  {
    "text": "case, Comer v. Murphy Oil USA, Inc., a class action lawsuit filed by Gerald Maples, a trial attorney in Mississippi, in an effort to force fossil fuel and chemical companies to pay for damages caused by global warming. Described as a nuisance lawsuit, it was dismissed by District Court. However, the District Court's decision was overturned by the United States Court of Appeals for the Fifth Circuit, which instructed the District Court to reinstate several of the plaintiffs' climate change-related claims on 22 October 2009. The Sierra Club sued the U.S. government over failure to raise automobile fuel efficiency standards, and thereby decrease carbon dioxide emissions. Held v. Montana Held v. Montana was the first constitutional law climate lawsuit to go to trial in the United States, on June 12, 2023. The case was filed in March 2020 by sixteen youth residents of Montana, then aged 2 through 18, who argued that the state's support of the fossil fuel industry had worsened the effects of climate change on their lives, thus denying their right to a \"clean and healthful environment in Montana for present and future generations\":Art. IX, § 1 as required by the Constitution of Montana. On August 14, 2023, the trial court judge ruled in the youth plaintiffs' favor, although the state indicated it would appeal the decision. Montana's Supreme Court heard oral arguments on July 10, 2024, its seven justices taking the case under advisement. On December 18, 2024, the Montana Supreme Court upheld the county court ruling. Mayanna Berrin v. Delta Air Lines Inc. Mayanna Berrin v. Delta Air Lines Inc. is a civil action lawsuit about Delta Air Lines' claim of carbon neutrality. Others After the landmark ruling of the Netherlands in 2015, groups in other countries tried the same judicial approach. For instance, groups",
    "label": 0
  },
  {
    "text": "v. Delta Air Lines Inc. is a civil action lawsuit about Delta Air Lines' claim of carbon neutrality. Others After the landmark ruling of the Netherlands in 2015, groups in other countries tried the same judicial approach. For instance, groups went to court in order to protect people from climate change in Brazil, Belgium, India, New Zealand, Norway, South Africa, Switzerland and the United States. Dismissed cases There are also cases that have been dismissed, whether due to lack of standing (e.g. Carvalho v Parliament and Council, 2021) or due to the limits of judicial functions under the doctrine of the separation of powers (e.g. Juliana v United States, 2020). Juliana v. United States In 2015, a number of American youth, represented by Our Children's Trust, filed a lawsuit against the United States government, contending that their future lives would be harmed due to the government's inactivity towards mitigating climate change. While similar suits had been filed and dismissed by the courts for numerous reasons, Juliana v. United States gained traction when a District Judge Ann Aiken ruled that the case had merit to continue, and that \"a climate system capable of sustaining human life\" was a fundamental right under the United States Constitution. The lawsuit was eventually dismissed. La Rose et al. vs. Her Majesty the Queen (Canada) In October 2019, a group of 15 youths filed a lawsuit against the government of Canada, claiming that the government's lack of climate change action was a violation of their rights to life, liberty and equality. The lawsuit was dismissed in November 2020. Problematic aspects Scholars have pointed out that there are also potential negative impacts of successful cases – sometimes referred to as backlash litigation. As the number of successful cases increases the energy transition risk to some companies operating",
    "label": 0
  },
  {
    "text": "November 2020. Problematic aspects Scholars have pointed out that there are also potential negative impacts of successful cases – sometimes referred to as backlash litigation. As the number of successful cases increases the energy transition risk to some companies operating in high-emitting sectors, it is possible that they will challenge government action on climate change. For example, they might argue that there was an alleged breach of international investment agreements even if governments' actions were taken to comply with a judicial decision. One example is the case of RWE v The Netherlands, in which RWE, a German energy company, filed suit against the Dutch government under the Energy Charter Treaty, alleging that the government failed to allow adequate time and resources to enable the company to transition away from coal. A study from 2024 found that: \"Nearly 50 of the more than 230 recorded cases filed in 2023 include non‑aligned arguments. The vast majority of these were filed in the US. At times, actors involved in such cases appear to be intentionally seeking to use legal tactics to obstruct climate action\". See also Climate justice Environmental law Human rights and climate change Oslo Principles on Global Obligations to Reduce Climate Change References External links Climate Change Litigation Databases, a database maintained by the Sabin Centre for Climate Change Law at Columbia Law School Litigation Cases, a database maintained by the LSE Grantham Research Institute on Climate Change and the Environment The UCS Science Hub for Climate Litigation, one aggregation of litigation-relevant resources",
    "label": 0
  },
  {
    "text": "Climate envoys are individuals that oversee and direct climate change diplomacy efforts. They are often appointed to their positions by regional organizations, national governments, blocs, or international entities. For instance, the Caribbean Community (CARICOM) appointed James Fletcher to the role of climate envoy in 2025. During the Biden Administration, the United States (US) established a US Special Presidential Envoy for Climate position, to which former US Secretary of State John Kerry was appointed. The European Union (EU) appointed Anthony Agotha as their Special Envoy for Climate and Environment. Both the United Nations (UN) and the World Health Organization (WHO) have elected climate envoys as well. Climate envoys are of critical importance to the global climate change movement. They promote action and awareness through the implementation of legislation, programs, and policies as well as through collaboration on the local, state, national, and international levels. By facilitating diplomatic communication, coordinating joint efforts among different parties, and representing specific interests in international negotiations, climate envoys serve as crucial liaisons in multilevel climate governance, helping to guide and enact effective global climate initiatives. As a global crisis, climate change and the mobilization against it is deeply ingrained in politics and international diplomacy. Therefore, recognizing the profound impact, capabilities, and responsibilities of climate envoys is fundamental. History of Climate Envoys and International Climate Action The climate change movement began to gain momentum during the 1970s and 1980s, coinciding with the rise in global environmentalism during this time. The publication of prominent works, including Rachel Carson’s Silent Spring (1962), the Club of Rome’s Limits to Growth (1972), and the World Commission on Environment and Development’s Our Common Future (1987), helped raise public awareness of humanity’s global and destructive environmental impact. Furthermore, the discovery of the Antarctic ozone hole in 1985 by British scientists sparked widespread concern",
    "label": 0
  },
  {
    "text": "(1972), and the World Commission on Environment and Development’s Our Common Future (1987), helped raise public awareness of humanity’s global and destructive environmental impact. Furthermore, the discovery of the Antarctic ozone hole in 1985 by British scientists sparked widespread concern and highlighted the critical need for international policies that address the growing threat of climate change. From this revelation emerged the 1985 Vienna Convention for the Protection of the Ozone Layer — a framework for coordinated international efforts to prevent ozone depletion — followed by the landmark Montreal Protocol of 1987, which limited the global production and consumption of nearly 100 ozone-depleting substances (ODSs) and has therefore played a crucial role in ozone restoration. ODS levels have since declined by 97%, with a full recovery of the stratospheric layer expected by mid-century. Because ODSs are potent greenhouse gases (GHGs), these imposed restrictions have also led to a significant reduction in global GHG emissions, in turn combating a major driving force behind climate change. The Montreal Protocol is still widely considered as one of the most monumental international climate change mitigation achievements in history. Its profound impact and success was an early lesson of the importance of international scientific cooperation and diplomacy as well as environmental leadership. The numerous diplomats, delegates, and ambassadors that participated in the creation and implementation of the treaty — of which included U.S. Department of State negotiator Richard Elliot Benedick — laid the foundation for the future official role of climate envoys. With the Montreal Protocol serving as a quintessential example of successful climate action, a greater emphasis has been placed on global collaboration in the climate change movement. Over the past 35 years, various international conferences and negotiations have been held to orchestrate effective climate change mitigation and adaptation protocols. These forums set the stage",
    "label": 0
  },
  {
    "text": "greater emphasis has been placed on global collaboration in the climate change movement. Over the past 35 years, various international conferences and negotiations have been held to orchestrate effective climate change mitigation and adaptation protocols. These forums set the stage for pivotal milestones and led to the establishment of key programs and organizations. The 1979 First World Climate Conference (WCC-1) in Geneva, organized by the World Meteorological Organization (WMO) and attended by scientists from 53 countries and 24 international organizations, was one of the first major international climate change-focused summits. It resulted in the establishment of the World Climate Programme (WCP), which encompasses the World Climate Research Programme (WCRP) that oversees predictive and analytical research on global climate patterns, trends, and variability. The Intergovernmental Panel on Climate Change (IPCC), jointly founded in 1988 by WMO and the United Nations Environment Programme (UNEP), consists of members from 195 countries that meet annually or biannually to compile and provide governments with updated scientific data that guides climate action and policymaking. That same year, over 300 scientists and policymakers participated in the Toronto Conference to address the role of atmospheric pollution in perpetuating climate change. In 1990, the Second World Climate Conference (WCC-2) prompted the establishment of the Global Climate Observing System (GCOS), which routinely monitors and assesses the state of global climate. Additionally, it paved the way for the United Nations Framework Convention on Climate Change (UNFCCC) — a foundational global agreement that laid the groundwork for coordinated international responses to and negotiations surrounding the climate crisis, with its primary aim being the reduction of GHG emissions and prevention of anthropogenic disturbances within natural climatic processes. The UNFCCC was introduced and opened for signature at the 1992 Rio Earth Summit, formally known as the United Nations Conference on Environment and Development (UNCED).",
    "label": 0
  },
  {
    "text": "the reduction of GHG emissions and prevention of anthropogenic disturbances within natural climatic processes. The UNFCCC was introduced and opened for signature at the 1992 Rio Earth Summit, formally known as the United Nations Conference on Environment and Development (UNCED). 155 countries signed the UNFCCC at the time, though as of 2022, it has been ratified by all 198 countries. Other significant outcomes of the UNCED included the Rio Declaration on Environment and Development, Agenda 21, and the Statement of Forest Principles — all three of which were signed and adopted by the 178 countries in attendance. The Intergovernmental Meeting on the World Climate Programme took place about a year later, in which 360 delegates from 134 nations and 83 specialists from 37 international organizations convened to discuss the objectives of The Climate Agenda and establishment of national climate programs, subsequently leading to the appointment of the Interagency Committee for the Climate Agenda (IACCA), which served as a liaison for various international climate entities during the rest of the 1990s. Following the UNFCCC’s official entry into force in 1994, the first Conference of the Parties (COP1) was held in Berlin in 1995. These annual forums eventually gave rise to central components or extensions of the UNFCCC, including the Kyoto Protocol of 1997 (via COP3), which mandated all 37 industrialized nations and the European Union to cut down their GHG emissions, and Paris Agreement of 2015 (via COP21), which was a consensus among the 195 parties of the UN to take action against climate change. In addition to the emergence of notable climate reforms as a result of international alliances, the global mobilization of forces against climate change was led by several influential climate change activists and political figures. Former United Kingdom (UK) Prime Minister Margaret Thatcher, former US Vice President",
    "label": 0
  },
  {
    "text": "notable climate reforms as a result of international alliances, the global mobilization of forces against climate change was led by several influential climate change activists and political figures. Former United Kingdom (UK) Prime Minister Margaret Thatcher, former US Vice President Al Gore, former US Under Secretary of State for Global Affairs Frank E. Loy, former US President George H.W. Bush, former Iraqi Ambassador to the US Fareed Mustafa Kamil Yasseen, and former Head of China's State Environmental Protection Administration (SEPA) Xie Zhenhua were among those known for their work and leadership on the frontlines of the global climate change movement during the late twentieth century. With GHG emissions and temperatures reaching unprecedented extremes, the need for an official position revolving around climate diplomacy became evident. The official title of climate envoy was first coined in 2007 by the UN. On May 1, 2007, Ban Ki-moon, who was the Secretary-General at the time, appointed three individuals to serve as UN Special Envoys on Climate Change: former Norwegian Prime Minister and World Commission on Environment and Development chair Gro Harlem Brundtland, former UN General Assembly president and Minister of Foreign Affairs and Trade of the Republic of Korea Han Seung Soo, and former Chilean president Ricardo Lagos. Since then, the title and role of climate envoy has become well-established. Examples of past and current climate envoys include: Adao Soares Barbosa, who is Timor-Leste's current Special Envoy and Ambassador at large for Climate Affairs. Ali Daud Mohamed, who is the current Climate Change Envoy of the Republic of Kenya. Anthony Agotha, who is the current Ambassador at large and the Special Envoy for Climate and Environment at the European External Action Service (EEAS). Avinash Persaud, who is the current Special Envoy on Climate Finance to Barbadian Prime Minister Mia Mottley. Bader Omar Al-Dafa,",
    "label": 0
  },
  {
    "text": "the current Ambassador at large and the Special Envoy for Climate and Environment at the European External Action Service (EEAS). Avinash Persaud, who is the current Special Envoy on Climate Finance to Barbadian Prime Minister Mia Mottley. Bader Omar Al-Dafa, who is Qatar's current Special Envoy for Climate Change and Sustainability. Benedikt Höskuldsson, who formerly served as Iceland's Special Envoy for Climate from 2022 to 2024. Caroline Dumas, who is the current International Organization for Migration (IOM) Director General's Special Envoy for Migration and Climate Change. Fareed Mustafa Kamil Yasseen, who formerly served as the Climate Envoy of the Republic of Iraq. James Fletcher, who is the current Caribbean Community (CARICOM) Climate Envoy. Jennifer Lee Morgan, who has served as the Special Envoy for International Climate Action at the German Federal Foreign Office since 2022. John Kerry, who was appointed by the Biden Administration as the first-ever official US Special Presidential Envoy for Climate from 2021 to 2024. Kamal Amakrane, who is the current Climate Envoy of the President of the UN General Assembly. Kathy Jetn̄il-Kijiner, who is a prominent Marshallese climate change activist and currently serves as the nation's Climate Envoy. Liu Zhenmin, who is China's current Special Envoy for Climate Change, a position he was appointed to following Xie Zhenhua's retirement in 2024. Mark Carney, who was appointed to the role of UN Special Envoy on Climate Change in 2019 and subsequently the UN Secretary-General's Special Envoy on Climate Action and Finance in 2020. Michael Bloomberg, who was appointed to the position of UN Special Envoy for Cities and Climate Change in 2014 by former Secretary-General Ban Ki-moon and then to the position of UN Special Envoy for Climate Action in 2018 by Secretary-General António Guterres. Since 2021, he has served as the UN Special Envoy for Climate",
    "label": 0
  },
  {
    "text": "and Climate Change in 2014 by former Secretary-General Ban Ki-moon and then to the position of UN Special Envoy for Climate Action in 2018 by Secretary-General António Guterres. Since 2021, he has served as the UN Special Envoy for Climate Ambition and Solutions. Rachel Kyte, who has served as the UK Climate Envoy or Special Representative on Climate since 2024 and was previously the World Bank Group's Special Envoy for Climate Change until 2015. Rachmat Witoelar, who has served as the Indonesian President’s Special Envoy for Climate Change since 2010. Ravi Menon, who became Singapore's first-ever Ambassador for Climate Action and Senior Adviser to the National Climate Change Secretariat at the Prime Minister's Office in 2024. Ruel Yamuna, who is Papua New Guinea's current Special Envoy for Climate and Environment. Sabra Ibrahim Noordeen, who was appointed by former Maldivian President Ibrahim Mohamed Solih to become the nation's first-ever Special Envoy for Climate Change. Sadiq Khan, who has served as the Mayor of London since 2016 and was appointed to the role of Special Envoy of the Fossil Free Cities for the Fossil Fuel Non-Proliferation Treaty Initiative on June 23, 2025. Spencer Linus Thomas, who is the Ambassador and Special Envoy for Multilateral Environmental Agreements in Grenada, serving as the nation's chief negotiator for climate change and biodiversity. Sultan Al Jaber, who is the United Arab Emirates (UAE)'s current Special Envoy for Climate Change. Susan Biniaz, who was the US State Department's Principal Deputy Special Envoy for Climate under John Kerry from 2021 to 2024. Todd Stern, who served as the US Special Envoy for Climate Change from 2009 to 2016 and was the country’s chief negotiator at the 2015 Paris Climate Agreement. Vanessa Kerry, who was appointed as the WHO’s first-ever Director-General Special Envoy for Climate Change and Health in",
    "label": 0
  },
  {
    "text": "US Special Envoy for Climate Change from 2009 to 2016 and was the country’s chief negotiator at the 2015 Paris Climate Agreement. Vanessa Kerry, who was appointed as the WHO’s first-ever Director-General Special Envoy for Climate Change and Health in 2023. Walter Kälin, who has served as the Envoy of the Chair of the Platform on Disaster Displacement, a state-led initiative dedicated to protecting those displaced from disasters and climate change, since 2017. Xie Zhenhua, who was China’s Special Envoy on Climate Change from 2021 to 2024. US Special Presidential Envoy for Climate Change In November 2020, Joe Biden, who was the President-elect at the time, announced his appointment of former US Secretary of State John Kerry to the newly established Cabinet-level position of Special Presidential Envoy for Climate Change. Kerry, who had previously served under the 2013-2017 Obama Administration, was notable for his persistent, outspoken activism against climate change as well as his pivotal role in revolutionary climate action milestones. His prioritization of climate change was evident throughout his tenure as the Senate Foreign Relations Committee Chairman from 2009 to 2013, having even selected climate change to be the focal topic of his very first hearing. As Secretary of State, Kerry was the chief US negotiator for both the 2015 Paris Agreement and the 2016 Kigali Amendment to the Montreal Protocol, which concentrated on reducing the production and consumption of potent GHGs called hydrofluorocarbons (HFCs). In addition, during his chairmanship of the Arctic Council from 2015 to 2017, Kerry proposed and implemented initiatives that revolved around addressing the climate crisis, one example being the 2015 Iqaluit Declaration that recognized the importance of GHG emission reductions to mitigating climate change and improving climatic and health conditions in the Arctic. Ultimately, Kerry’s extensive background and experience in climate change leadership were",
    "label": 0
  },
  {
    "text": "crisis, one example being the 2015 Iqaluit Declaration that recognized the importance of GHG emission reductions to mitigating climate change and improving climatic and health conditions in the Arctic. Ultimately, Kerry’s extensive background and experience in climate change leadership were key factors that contributed to his appointment as the US’s first Special Presidential Envoy for Climate Change. Through this position, Kerry not only held Cabinet-level status but was also reserved a seat on the National Security Council (NSC), making him the first-ever climate-oriented NSC member. Given that it was unprecedented for a climate envoy to participate in the NSC, Kerry’s inauguration on January 20, 2021 ushered in a new era of climate action in the US. By considering and placing climate change in the context of national security-related discussions and decision-making, the Biden Administration demonstrated their recognition of the climate crisis as a dire threat to national security alongside their unwavering commitment to effective mobilization against climate change. In his role as the US Special Presidential Envoy for Climate Change, Kerry was responsible for guiding US diplomacy, mobilization, and international negotiations in regard to the climate crisis as well as ensuring that the US is actively participating and contributing to the global climate change movement. This was accomplished through measures to reduce GHG emissions, spearhead clean energy initiatives, align finance flows to climate objectives, and build the resilience and adaptability of communities to climate change impacts. With a seat in the NSC, Kerry’s influence as a climate envoy exceeded that of his predecessors, providing him with the opportunity to offer a climate-oriented perspective on presidential decisions concerning foreign affairs and national security as well as ensure that the climate crisis is taken into account in these decisions. His input helped shape the Administration’s plan of action in terms of national",
    "label": 0
  },
  {
    "text": "a climate-oriented perspective on presidential decisions concerning foreign affairs and national security as well as ensure that the climate crisis is taken into account in these decisions. His input helped shape the Administration’s plan of action in terms of national and global climate mitigation and adaptation strategies, reinforcing the country’s capacity to deal with the detrimental, long-term consequences of climate change. However, due to the Trump Administration’s opposing stance on climate change, Kerry also faced the unique challenge of having to navigate political tensions and division surrounding this topic. During his tenure, Kerry was most known for his successful endeavors in securing a partnership with China to address and tackle this global issue. Given that China and the US rank first and second, respectively, in national carbon emissions, one of the primary objectives of Kerry’s agenda was to establish an alliance with China to achieve shared climate goals, including the transition towards cleaner energy practices and reduction of GHG emissions. Despite geopolitical tensions between the two countries complicating these efforts, US-China cooperation on the climate change front have generated fruitful outcomes, including the launching and operationalization of the Working Group on Enhancing Climate Action. This Working Group, which was co-led by Kerry and China's former Special Envoy for Climate Change Xie Zhenhua, was formed to strengthen relations between the two nations and facilitate dialogue and collaboration in regards to global climate change mitigation. It identified key domains and areas of focus to include energy transition, methane, circular economy and resource efficiency, low-carbon and sustainable provinces/states and cities, and deforestation. The group additionally agreed to stay in close communication; share respective experiences; exchange information regarding GHG emission-reducing policies, measures, and technologies; and organize collaborative projects in order to learn from one another and advance cooperation. These objectives were outlined in the",
    "label": 0
  },
  {
    "text": "Climate finance in Democratic Republic of the Congo comprises a mixture of the Forest Investment Program (FIP), domestic and internationally sourced funding for climate change sustainability, control and resilience. DRC has high temperatures and decreases in precipitation, making it prone to floods and droughts. This is why climate change affects the population, agriculture, health and biological diversity, with a health risk of about 40%. DRC invested $10 million to enhance activities of forest preservation and the development of a green economy. It is therefore imperative to source climate finance as 40 million people who depend on the Congo Basin for food, health, livelihoods and ecosystem services may likely experience hunger and poverty with more climate change. The World Bank on the climate issue in DRC remarked that \"economic losses could reach up to 17% of GDP by 2050 if reforms to diversify the economy and attract more climate investments are not taken. Climate impacts could also increase total health costs from $92 million in 2010 to $260 million by 2050.\" It offered a four-point agenda to help DRC. These are stronger and greener infrastructure and services, more climate-ready education, health systems and social services, more investments in natural capital and better climate governance to leverage carbon markets. The indication is that every sector needs to be part of the climate ecosystem to help DRC in financing and mitigating climate. UNDP stated that countries need more finance for their climate targets than what they can source domestically. In its report in 2009, high-income countries with a significant historical contribution to climate change had committed to raising US$100 billion every year by 2020 to fund climate action in low-income countries. The report from UNDP stated that 50 percent of mitigation financing tracked in Africa between 2011- 2021 came from MDBs ($37 billion)",
    "label": 0
  },
  {
    "text": "change had committed to raising US$100 billion every year by 2020 to fund climate action in low-income countries. The report from UNDP stated that 50 percent of mitigation financing tracked in Africa between 2011- 2021 came from MDBs ($37 billion) and 45 percent from bilateral sources ($33.47 billion). However, the UNDP report also stated that Central Africa has the smallest population of Africa's sub-regions and receives the lowest amount of climate finance. DRC is in Central Africa and faces the challenges of low climate finance. Overview It should be recalled that in 2013, the Democratic of Congo was ranked as the poorest country in the world. Perhaps, this affected its climate restoration and deepened the environmental challenge. Thus, it needs support from local and international bodies to mitigate its climate issues. Again, DRC is known to have half of Africa's forests with large freshwater resources, and mineral reserves. All these are critical for a green transition. This puts it in the class of countries that contribute to global climate action. The World Bank stated, \"DRC forests can generate an estimated value of $223 billion – 398 billion per year from stored carbon and associated ecosystem services needed to mitigate the impacts of disasters and enhance the resilience of DRC communities. However, if not protected, the loss of 40% of its current extent, could mean that DRC's Land Use, Land-use Change and Forestry (LULUCF) sector becomes a net source of carbon and no longer a sink. The total cost to the world of such a loss in carbon stock—and therefore the capacity of the forests to provide carbon sequestration services—would be about $95.3 billion.\" DRC has reasons to protect its country from climate change. According to the Center on International Cooperation, \"DRC is home to wetlands, including the largest tropical peatland",
    "label": 0
  },
  {
    "text": "capacity of the forests to provide carbon sequestration services—would be about $95.3 billion.\" DRC has reasons to protect its country from climate change. According to the Center on International Cooperation, \"DRC is home to wetlands, including the largest tropical peatland complex, covering approximately 123,750 kilometers, or 75 percent of the Congo Basin.\" These need protection from climate change to conserve the natural and cultural spaces of the country to enable economic growth and sustainable development. Government finance in the Democratic Republic of Congo In 2015 the government of The Democratic Republic of Congo signed a grant agreement with the Green Climate Fund, which provided the sum of USD 300,000 for capacity building in the African State. The Minister of Environment, Nature Conservation, and Sustainable Development of DR Congo was present during the signing of the grant agreement. This fund was meant to build institutional capacity including the technical and operational skills for engagement. In 2020 the Republic of Congo and The Green Climate Fund (GCF) signed a Privileges and Immunities agreement. It became the 24th country to sign such an agreement with GCF. Its importance lies in effective operation and activities with the investment fund. It should be remembered that the Democratic Republic of Congo was one of the 158 countries including the European Union that signed the Global Methane Pledge with the commitment to reduce the global methane emissions before 2030 in keeping the 1.5°C warming limit within reach. DRC was part of the 130 countries to sign the Glasgow Leaders' Declaration on Forests and Land Use, to end the forest reverse loss and land degradation by 2030. It also signed a new Forest, Agriculture and Commodity Trade (FACT) Statement, which the UK and Indonesia supported. According to The Leaders' Declaration: $12bn in public funding to \"support work to",
    "label": 0
  },
  {
    "text": "the forest reverse loss and land degradation by 2030. It also signed a new Forest, Agriculture and Commodity Trade (FACT) Statement, which the UK and Indonesia supported. According to The Leaders' Declaration: $12bn in public funding to \"support work to protect, restore and sustainably manage forests\" from 12 countries, to be delivered over 2021-2025. $7.2bn in private investment from corporate and philanthropic funds. Of that $19.2bn, at least $1.5bn was earmarked specifically for protecting the forests of Africa's Congo Basin. Of that $19.2bn, at least $1.7bn was pledged towards supporting Indigenous peoples and local communities and advancing their land tenure rights. A commitment from the chief executives of more than 30 financial institutions to divest from activities linked to commodity-driven deforestation. In 2022, DRC was one of the countries in the developing sector that contributed to global warming mitigation. Although the developed countries raised 116 billion US dollars to help and support climate change in the developing countries, the report had it that DRC had a deficit in climate funding with the combined donors' opposition to logging and oil exploitation. This caused the National perception to view it as an international conspiracy against the Congolese state, which was seen as a 'victim' of an unequal exchange. However, USD 500 million was signed with the Central African Forest Initiative (CAFI) as stated in the second letter. International financiers In 2015, GCF approved requests of about twenty developing countries that chose UNDP, UNEP or GIZ as partners to carry out its program activities. This was hoped to allow USD 5.4 million in grants to the developing countries under GCF. With this, UNDP received its first disbursement for eight countries in which the Republic of Congo was a beneficiary with Bangladesh, Central African Republic, Guinea, India, Liberia, Timor-Leste, and Swaziland under the USD1.35",
    "label": 0
  },
  {
    "text": "grants to the developing countries under GCF. With this, UNDP received its first disbursement for eight countries in which the Republic of Congo was a beneficiary with Bangladesh, Central African Republic, Guinea, India, Liberia, Timor-Leste, and Swaziland under the USD1.35 million. As of December 31, 2023, data from the Climate Investment Fund (CIF) revealed that it approved three projects for DRC at $28.5m and was expecting co-financing of $96.15m. The co-financing was expected to come from the: Government at $8.56% (8.24M), MBD at 85.65% (82.35M) and the Private sector at 5.79% (5.56M). USAID supported the Democratic Republic of Congo for improved renewable energy. This was part of the support for climate mitigation. The areas of financing included: Establishing two new energy regulatory agencies, which provide essential oversight to the sector. Supporting the government to implement a 2014 electricity law by drafting over two dozen decrees, orders, and regulations, of which 14 have already been adopted. Providing 1,434 households and 44 businesses access to clean and renewable electricity in Fiscal Year 2020 from a hydropower plant. Leveraging over $77 million of funding to provide reliable, affordable energy to the city of Goma. Enabling the establishment of 15,612 new grid and off-grid direct connections to date (10,386 in FY 2020). Mobilizing a $2.25 million investment for clean energy projects, which will continue to demonstrate the opportunities for off-grid companies in the DRC. Paris Agreement The endorsement of the Paris Agreement in 2015 created opportunities to safeguard climate change for Sustainable Development Goals (SDGs), economic growth and improvement of livelihoods. Thus in 2015, The Democratic Republic of the Congo submitted its new climate action plan to the UN Framework Convention on Climate Change (UNFCCC). Part of the agreement was to ensure an average global temperature of 2 degrees Celsius for sustainable development. References",
    "label": 0
  },
  {
    "text": "Climate finance in Nigeria includes a mixture of domestic and internationally sourced funding for climate change adaptation, mitigation and resilience. As Africa's largest economy and one of the continent's leading oil producers, Nigeria faces significant challenges related to climate change, including flooding, desertification, and dependence on fossil fuels, which directly impact its economy and food security. The country seeks to align its climate policies with the goals of the Paris Agreement, utilizing a combination of public and private funds and international cooperation to achieve the targets set out in its Nationally Determined Contribution (NDC). Context and Vulnerabilities Agricultural and Forestry Context Nigeria has a total land area of 910,770 km2, of which 77% is devoted to agricultural crops (including arable land, permanent crops and pastures) and 22% is covered by forests as of 2020. According to the World Bank, the agricultural sector is responsible for 34% of job creation in the country. However, soil degradation affects about 33% of agricultural land, exacerbated by climate change. The agriculture sector still accounts for about 23% of the country's GDP according to 2021 World Bank data. Changing rainfall patterns, increased flooding, and desertification in the north of the country threaten food security and livelihoods in Nigeria; the country is already facing food shortages compared to domestic demand. The increase in imports of staple foods, such as rice, exemplifies the difficulty in meeting the country's food demand. In 2019, it was found that Nigeria consumed 7 million tons of the grain, but only produced 3.7 million tons themselves. According to the 2012 Post-Disaster Needs Assessment (PDNA) Report, the floods, which began in July of that year, resulted in an estimated US$16.9 billion in damage, highlighting the need for investments in adaptation. Energy Context Nigeria's energy sector is dominated by biofuels and waste (43.4% of",
    "label": 0
  },
  {
    "text": "Assessment (PDNA) Report, the floods, which began in July of that year, resulted in an estimated US$16.9 billion in damage, highlighting the need for investments in adaptation. Energy Context Nigeria's energy sector is dominated by biofuels and waste (43.4% of total energy supply), mainly firewood and charcoal, used by 65% of households for daily needs such as cooking. Electricity is largely generated by natural gas (75%), while renewable sources such as hydropower (1.1%) and solar (0.3% in 2022) represent a small fraction of the energy mix. Despite its vast renewable potential, Nigeria has only 2,062 MW of installed hydropower capacity out of an estimated 14,120 MW potential. Solar potential is estimated at up to 210 GW, yet only 1% of the country's territory is currently utilized. The installed electricity capacity is 12.5 GW, but only 3,500 to 5,000 MW is available due to significant transmission and distribution losses (28-40%). Even so, Nigeria's electricity demand far exceeds the grid's maximum capacity, at around 17,556 MWh/h in 2020, according to official data. In 2023, the electricity access rate was 61.2%, with a significant disparity between urban (89%) and rural areas (less than 33%) in a context where frequent electricity grid failures cost the economy around US$29 billion per year. Economic Context Nigeria is considered the largest economy on the African continent, with a GDP of US$188.27 billion in 2023 and a population of 233.3 million, but its GDP per capita is the lowest among the ten largest African economies due to its large population. The country's financial sector is the third-largest tier-1 banking market in Africa, with banks such as FBN Holdings, Access Bank, and Zenith Bank leading regional rankings. The Nigerian Stock Exchange (NGX) reached a market capitalization of approximately US$47 billion as of June 2025, with 148 listed companies, although",
    "label": 0
  },
  {
    "text": "banking market in Africa, with banks such as FBN Holdings, Access Bank, and Zenith Bank leading regional rankings. The Nigerian Stock Exchange (NGX) reached a market capitalization of approximately US$47 billion as of June 2025, with 148 listed companies, although none are exclusively dedicated to renewable energy. Despite the potential, green finance is limited as investment incentives prioritize the fossil fuel sector as Nigeria is Africa's largest crude oil producer. In 2020, the country spent $71.3 billion on imports, exceeding exports by $43.6 billion, mostly due to reliance on imported refined petroleum despite domestic refining capacity. Political and Legal frameworks Nigeria has integrated climate change into its policies through documents such as the \"Nigeria Energy Transition Plan\" (2021) and the \"National Energy Policy\" (2022), which promote renewable energy targets and expansion of the sustainable investment market. The 2021 Climate Change Act established the National Climate Change Council, responsible for coordinating climate policy and managing the National Climate Change Fund. To attract investment, the government offers incentives such as feed-in tariffs, tax exemptions, reduced import duties, and special economic zones (SEZs) that allow full tax exemption and profit repatriation. Foreigners can hold 100% equity in projects, subject to registration with the Corporate Affairs Commission (CAC) and approvals from the Nigerian Electricity Regulatory Commission (NERC) and the Federal Competition and Consumer Protection Commission (FCCPC). However, corruption, bureaucracy, and delays in approving power purchase agreements (PPAs) hamper renewable energy investments. Since 2019, the Nigerian Stock Exchange has required ESG (environmental, social, and governance) reporting for listed companies, and from 2027, sustainability reporting aligned with IFRS S2 standards will be mandatory. The Nigerian Sustainable Banking Principles (NSBP) require banks to integrate environmental and social factors into their decisions and report impacts transparently. Nationally determined contributions Nigeria has been a party to the United Nations",
    "label": 0
  },
  {
    "text": "with IFRS S2 standards will be mandatory. The Nigerian Sustainable Banking Principles (NSBP) require banks to integrate environmental and social factors into their decisions and report impacts transparently. Nationally determined contributions Nigeria has been a party to the United Nations Framework Convention on Climate Change (UNFCCC) since 1994 and ratified the Kyoto Protocol in 2004. The country's first Nationally Determined Contribution (NDC) was submitted in 2015, with a commitment to reduce emissions of short-lived climate pollutants and hydrofluorocarbon 47% by 2030, at a projected cost of US$542 billion. In 2021, Nigeria published its updated NDC. The NDCs include targets to increase the share of renewable energy to 30% by 2030 and to achieve universal energy access by 2060. Meanwhile, achieving carbon neutrality by 2060 requires an estimated US$410 billion in investment. Climate Finance Flows Public Between 2012 and 2016, the majority of climate finance in Nigeria came from the national budget, with US$4.8 million allocated from the Climate Investment Fund, a £17,000 DFID grant, and US$352,000 from the Global Environment Facility. Additionally, in 2024, the Development Bank of Nigeria was also accredited to access between US$50 million and US$250 million from the Green Climate Fund to support climate change measures. Climate finance also flowed from the Heinrich Böll Foundation (HBS) between 2012 and 2016 worth €423,320. According to the 2012 Post-Disaster Needs Assessment (PDNA) Report for the UNFCCC in 2017, important financing needs include flood management and the need to repair damages of US$16.9 billion. In addition, investments of about US$177 billion are needed to mitigate the challenges of waste, water resources, and greenhouse gases. In 2024, the government allocated resources to the Climate Change Fund, with support from the World Bank, the International Finance Corporation (IFC), and the IMF, to expand green finance. In April 2025, the Minister of",
    "label": 0
  },
  {
    "text": "water resources, and greenhouse gases. In 2024, the government allocated resources to the Climate Change Fund, with support from the World Bank, the International Finance Corporation (IFC), and the IMF, to expand green finance. In April 2025, the Minister of Finance and Coordinating Minister for the Economy, Wale Edun, met with Dr. Nkiruka Madueke, Director-General of the Nigeria Climate Change Council (NCCC), to discuss the launch of the Climate Change Fund, aimed at supporting climate solutions and boosting green finance. However, the prioritization of fossil fuel investments and the flaring of 7 billion cubic meters of gas annually, resulting in losses of US$2.5 billion, limit progress in clean energy. Private In addition to government funding, there is funding from the C40 Group of Major Cities for Climate Leadership, worth US$3 million, US$1,515,512 from Fidelity Bank, and US$2 million from the World Bank. Banks such as Access Bank, First Bank of Nigeria, and United Bank for Africa have adhered to the Partnership for Carbon Accounting Finance (PCAF) standards to promote emissions transparency. Despite this, according to a 2022 analysis titled Nigeria Green Tagging Banking Review, the banking sector continues to lend primarily to carbon-intensive industries. In 2017, the Nigerian government issued Africa's first sovereign bond worth US$29 million, which financed solar energy projects for universities and off-grid communities. Between 2019 and 2021, four corporate green bonds and one sovereign green bond were issued worth approximately US$38.02 million. International Cooperation and External Financing Nigeria relies heavily on external financing to meet its NDC climate targets. Organizations such as the Green Climate Fund, the World Bank, and the German Society for International Cooperation (GIZ) are among the key sources. In 2023, the Nigerian Bank of Industry partnered with FSD Africa to expand climate finance. However, about 75% of international public climate finance is",
    "label": 0
  },
  {
    "text": "the World Bank, and the German Society for International Cooperation (GIZ) are among the key sources. In 2023, the Nigerian Bank of Industry partnered with FSD Africa to expand climate finance. However, about 75% of international public climate finance is provided through debt, increasing the country's financial burden. Main sectors benefited Energy The transition to renewable energy is a national priority, with emphasis on hydroelectric projects, such as the Kainji plant (760 MW), commissioned in 1968; and solar projects, including off-grid systems and mini-grids. In 2016, the government signed PPAs with 14 solar developers, but projects stagnated due to the reduction of feed-in tariffs. In 2022, installed renewable energy capacity was 24.5%, with hydroelectric representing 96% of this total. In 2023, investment in clean energy increased by more than 900% compared to 2022, totaling US$69.3 million, of which the majority is from public-private partnerships. Transport The transportation sector, dominated by tricycles (keke napap), minibuses (danfos), and motorcycles (okadas), is a major source of urban pollution. The electric vehicle (EV) market is on the rise in the country, driven by concerns about pollution and fuel costs. Some initiatives promote EV adoption through subsidies and free charging stations, although charging infrastructure remains limited. Agriculture and Forests The country has committed to restoring 4 million hectares of degraded land by 2030 under the African Forest Landscape Restoration Initiative (AFR100) and achieving Land Degradation Neutrality (LDN) in the same period, with an estimated investment of US$194 billion. Future Prospects Nigeria has vast renewable potential, with 14,120 MW in hydropower and up to 210 GW in solar, but underutilization persists. The removal of fuel subsidies in 2023 and the liberalization of the foreign exchange market have increased its attractiveness to investors, but regulatory instability remains an obstacle. See also Climate change in Kenya Climate finance",
    "label": 0
  },
  {
    "text": "Climate inertia or climate change inertia is the phenomenon by which a planet's climate system shows a resistance or slowness to deviate away from a given dynamic state. It can accompany stability and other effects of feedback within complex systems, and includes the inertia exhibited by physical movements of matter and exchanges of energy. The term is a colloquialism used to encompass and loosely describe a set of interactions that extend the timescales around climate sensitivity. Inertia has been associated with the drivers of, and the responses to, climate change. Increasing fossil-fuel carbon emissions are a primary inertial driver of change to Earth's climate during recent decades, and have risen along with the collective socioeconomic inertia of its 8 billion human inhabitants. Many system components have exhibited inertial responses to this driver, also known as a forcing. The rate of rise in global surface temperature (GST) has especially been resisted by 1) the thermal inertia of the planet's surface, primarily its ocean, and 2) inertial behavior within its carbon cycle feedback. Various other biogeochemical feedbacks have contributed further resiliency. Energy stored in the ocean following the inertial responses principally determines near-term irreversible change known as climate commitment. Earth's inertial responses are important because they provide the planet's diversity of life and its human civilization further time to adapt to an acceptable degree of planetary change. However, unadaptable change like that accompanying some tipping points may only be avoidable with early understanding and mitigation of the risk of such dangerous outcomes. This is because inertia also delays much surface warming unless and until action is taken to rapidly reduce emissions. An aim of Integrated assessment modelling, summarized for example as Shared Socioeconomic Pathways (SSP), is to explore Earth system risks that accompany large inertia and uncertainty in the trajectory of human",
    "label": 0
  },
  {
    "text": "and until action is taken to rapidly reduce emissions. An aim of Integrated assessment modelling, summarized for example as Shared Socioeconomic Pathways (SSP), is to explore Earth system risks that accompany large inertia and uncertainty in the trajectory of human drivers of change. Inertial timescales The paleoclimate record shows that Earth's climate system has evolved along various pathways and with multiple timescales. Its relatively stable states which can persist for many millennia have been interrupted by short to long transitional periods of relative instability. Studies of climate sensitivity and inertia are concerned with quantifying the most basic manner in which a sustained forcing perturbation will cause the system to deviate within or initially away from its relatively stable state of the present Holocene epoch. \"Time constants\" are useful metrics for summarizing the first-order (linear) impacts of the various inertial phenomena within both simple and complex systems. They quantify the time after which 63% of a full output response occurs following the step change of an input. They are observed from data or can be estimated from numerical simulation or a lumped system analysis. In climate science these methods can be applied to Earth's energy cycle, water cycle, carbon cycle and elsewhere. For example, heat transport and storage in the ocean, cryosphere, land and atmosphere are elements within a lumped thermal analysis. Response times to radiative forcing via the atmosphere typically increase with depth below the surface. Inertial time constants indicate a base rate for forced changes, but lengthy values provide no guarantee of long-term system evolution along a smooth pathway. Numerous higher-order tipping elements having various trigger thresholds and transition timescales have been identified within Earth's present state. Such events might precipitate a nonlinear rearrangement of internal energy flows along with more rapid shifts in climate and/or other systems at",
    "label": 0
  },
  {
    "text": "Numerous higher-order tipping elements having various trigger thresholds and transition timescales have been identified within Earth's present state. Such events might precipitate a nonlinear rearrangement of internal energy flows along with more rapid shifts in climate and/or other systems at regional to global scale. Climate response time The response of global surface temperature (GST) to a step-like doubling of the atmospheric CO2 concentration, and its resultant forcing, is defined as the Equilibrium Climate Sensitivity (ECS). The ECS response extends over short and long timescales, however the main time constant associated with ECS has been identified by Jule Charney, James Hansen and others as a useful metric to help guide policymaking. RCPs, SSPs, and other similar scenarios have also been used by researchers to simulate the rate of forced climate changes. By definition, ECS presumes that ongoing emissions will offset the ocean and land carbon sinks following the step-wise perturbation in atmospheric CO2. ECS response time is proportional to ECS and is principally regulated by the thermal inertia of the uppermost mixed layer and adjacent lower ocean layers. Main time constants fitted to the results from climate models have ranged from a few decades when ECS is low, to as long as a century when ECS is high. A portion of the variation between estimates arises from different treatments of heat transport into the deep ocean. Components Thermal inertia Thermal inertia is a term which refers to the observed delays in a body's temperature response during heat transfers. A body with large thermal inertia can store a big amount of energy because of its heat capacity, and can effectively transmit energy according to its heat transfer coefficient. The consequences of thermal inertia are inherently expressed via many climate change feedbacks because of their temperature dependencies; including through the strong stabilizing feedback",
    "label": 0
  },
  {
    "text": "because of its heat capacity, and can effectively transmit energy according to its heat transfer coefficient. The consequences of thermal inertia are inherently expressed via many climate change feedbacks because of their temperature dependencies; including through the strong stabilizing feedback of the Planck response. Ocean inertia The global ocean is Earth's largest thermal reservoir that functions to regulate the planet's climate; acting as both a sink and a source of energy. The ocean's thermal inertia delays some global warming for decades or centuries. It is accounted for in global climate models, and has been confirmed via measurements of ocean heat content. The observed transient climate sensitivity is proportional to the thermal inertia time scale of the shallower ocean. Ice sheet inertia Even after CO2 emissions are lowered, the melting of ice sheets will persist and further increase sea-level rise for centuries. The slower transportation of heat into the extreme deep ocean, subsurface land sediments, and thick ice sheets will continue until the new Earth system equilibrium has been reached. Permafrost also takes longer to respond to a warming planet because of thermal inertia, due to ice rich materials and permafrost thickness. Inertia from carbon cycle feedbacks Earth's carbon cycle feedback includes a destabilizing positive feedback (identified as the climate-carbon feedback) which prolongs warming for centuries, and a stabilizing negative feedback (identified as the concentration-carbon feedback) which limits the ultimate warming response to fossil carbon emissions. The near-term effect following emissions is asymmetric with latter mechanism being about four times larger, and results in a significant net slowing contribution to the inertia of the climate system during the first few decades following emissions. Ecological inertia Depending on the ecosystem, effects of climate change could show quickly, while others take more time to respond. For instance, coral bleaching can occur in a",
    "label": 0
  },
  {
    "text": "inertia of the climate system during the first few decades following emissions. Ecological inertia Depending on the ecosystem, effects of climate change could show quickly, while others take more time to respond. For instance, coral bleaching can occur in a single warm season, while trees may be able to persist for decades under a changing climate, but be unable to regenerate. Changes in the frequency of extreme weather events could disrupt ecosystems as a consequence, depending on individual response times of species. Policy implications of inertia The IPCC concluded that the inertia and uncertainty of the climate system, ecosystems, and socioeconomic systems implies that margins for safety should be considered. Thus, setting strategies, targets, and time tables for avoiding dangerous interference through climate change. Further the IPCC concluded in their 2001 report that the stabilization of atmospheric CO2 concentration, temperature, or sea level is affected by: The inertia of the climate system, which will cause climate change to continue for a period after mitigation actions are implemented. Uncertainty regarding the location of possible thresholds of irreversible change and the behavior of the system in their vicinity. The time lags between adoption of mitigation goals and their achievement. See also Earth's energy budget Planetary boundaries Systems theory and Systems analysis References",
    "label": 0
  },
  {
    "text": "Climate information services (CIS) entail the dissemination of climate data in a way that aids people and organizations in making decisions. CIS helps its users foresee and control the hazards associated with a changing and unpredictable climate. It encompasses a knowledge loop that includes targeted user communities' access to, interpretation of, communication of, and use of pertinent, accurate, and trustworthy climate information, as well as their feedback on that use. Climate information services involve the timely production, translation and delivery of useful climate data, information and knowledge. Climate services are systems to deliver the best available climate information to end-users in the most usable and accessible formats. They aim to support climate change adaptation, mitigation and risk management decisions. There is a vast range of practices and products for interpreting, analyzing, and communicating climate data. They often combine different sources and different types of knowledge. They aim to fulfil a well-specified need. These climate services mark a shift from supply-driven information products that result from scientific research. Instead they are demand-driven and take greater account of users’ needs and decision-making. To do so they require different types of user–producer engagement, depending on what the service aims to deliver. This type of collaboration is called co-design. Climate services vary in their structure and objectives. They are set up to help users cope with current climate variability and limit the damage caused by climate-related disasters. They can also serve as an important measure to reduce risks in a particular sector. One example is Copernicus Climate Change Service (C3S), which provides free and open access to climate data, tools and information used for a variety of purposes. Another example is Participatory Integrated Climate Services for Agriculture (PICSA). This is a participatory approach which combines historical climate data and forecasts with farmers’ local contextual",
    "label": 0
  },
  {
    "text": "open access to climate data, tools and information used for a variety of purposes. Another example is Participatory Integrated Climate Services for Agriculture (PICSA). This is a participatory approach which combines historical climate data and forecasts with farmers’ local contextual knowledge. Definition Climate information (CI) refers to the gathering and analysis of actual weather and climate observations as well as simulations of the climate for the past, the present and the future. Climate information services entail the dissemination of climate data in a way that aids people and organizations in making decisions. CIS helps its users foresee and control the hazards associated with a shifting and unpredictable environment. Types of climate information There are three main types of climate information commonly provided through CS. These are forecasts, agrometeorological services, and early warnings. Forecasts use in farming operations: Several CS projects in the African agriculture sector have applied forecasts of varying timescale in providing risk warning and risk response advisory services to farmers. The five timescale forecasts most widely used in the production of climate risk warnings and risk response advisory services are: (1) Weather forecasts (daily to weekly), (2) Seasonal forecasts (on a timescale of 1–6 months), (3) Short-term forecasts (1–5 years), (4) Intra-decadal/Medium-term forecasts (5–10 years), and (5) Decadal forecasts. The most commonly used among these forecasts are short-term, seasonal, and weather forecasts. Intra-decadal and decadal forecasts are sparingly used in agricultural risk management, even though they may be more useful for making a strategic decision and anticipatory adaptation plans. Agrometeorological services: Agrometeorological services are the second most common type of climate information provided by the CS. Included in this category is information provided to manage the impact of both climate change and climate variability. This includes advisory information on the scheduling of planting operations, weeding, fertilizer applications, etc.",
    "label": 0
  },
  {
    "text": "type of climate information provided by the CS. Included in this category is information provided to manage the impact of both climate change and climate variability. This includes advisory information on the scheduling of planting operations, weeding, fertilizer applications, etc. CS is also, in some cases, used to provide information on climate-smart agriculture practices (CSA). The type of CSA information communicated includes conservation farming practices like ridging, minimum tillage, soil conservation practices, etc. The use of CS to communicate agrometeorological services and CSA to farmers is acknowledged as a valuable innovation to assist decision-making and develop farmers’ specific adaptive capacities. Early warning interventions: The third type of climate information provided through CS are early warnings. Early warning intervention provisioning is commonly used for drought, flood, and wildfire risk warnings. Early warnings are rarely solely disseminated to users; rather, they are provided in combination with agrometeorological services. The early and timely delivery of early warnings is increasingly being facilitated through the integration of ICT CS dissemination strategy. International initiatives In 2009, the Global Framework for Climate Services (GFCS) of the World Meteorological Organization was established, with the goal to enable better management of the risks of climate risks through the integration of climate information and prediction into planning, policy and practice. To achieve its mission, the GFCS works across five priority areas: Agriculture and Food Security (working with partners in the UN 'system' such as the Food and Agriculture Organization and the World Food Programme) Disaster Risk Reduction (working with the UN Office for Disaster Risk Reduction to include climate information and prediction into managing disaster risk) Energy (developing tailored weather–water–climate services working with the energy industry) Health (working with the World Health Organisation and other organisations to develop reliable health and climate-related tools and services for various time scales) Water",
    "label": 0
  },
  {
    "text": "and prediction into managing disaster risk) Energy (developing tailored weather–water–climate services working with the energy industry) Health (working with the World Health Organisation and other organisations to develop reliable health and climate-related tools and services for various time scales) Water (working with UN-Water and the Global Water Partnership to focus on the climate services required to support Integrated Water Resources Management) Examples by region Africa CIS has become a popular initiative in national and regional initiatives aimed at enhancing stakeholders’ access to tailored and contextual climate information for adapting farming practices to climate and socioeconomic risks in Africa. The primary economic sectors in Africa are extremely susceptible to the effects of climate change and fluctuation. Using agriculture as an example, the sector employs up to 80% of the population and provides about 30% of the GDP. Since more than 90% of our agriculture is rainfed, climate observation systems suggest that rainfall in Africa is becoming more unpredictable. This sector looks to be one of the most hit by climate variability and change. Farmers who have access to high-quality data customized to their needs can plan what and when to plant; policymakers who have access to precise data and analysis can make well-informed judgments. Governments are also considerably more likely to incorporate climate policies by using demand-led, evidence-based data. However, due to the global disparity in the supply of climate information services, farmers in Africa are vulnerable to climate and extreme weather risk. With robust climate information, Africa can safeguard the economic gains and advances in social development seen across the continent over the last decade. There are data gaps at various levels because the majority of the acquired data were written down on paper rather than being electronically catalogued. Not withstanding the hurdles facing the private sector, a few private",
    "label": 0
  },
  {
    "text": "continent over the last decade. There are data gaps at various levels because the majority of the acquired data were written down on paper rather than being electronically catalogued. Not withstanding the hurdles facing the private sector, a few private sectors have been successful in offering climate information services throughout Africa. There is a developing market for climate information services for the business sector. Additionally, there is a growing understanding that through public-private partnerships, private service providers might make investments in necessary machinery and provide accurate climatic information. Climate services are provided by the private sector at many different stages of the value chain. Its services include everything from delivering customized, value-added products and services to end users to assisting with weather monitoring. Seasonal forecasts, during the beginning of the rainy season (and the planting season for farmers), and other services are typically offered by the private sector. Additionally, they offer information on extreme weather conditions to disaster relief organizations and vulnerable communities. Viamo offers a variety of services, including the provision of information in any phone owner's preferred language, including weather and climatic data, for a fee. Farmers and members of the informal sector can access essential information and services thanks to Esoko's Digital Farmer Services. Several initiatives for scaling up the implementation of CIS in the African agriculture sector have been adopted. Some of these initiatives include: the African Center of Meteorological Applications for Development (in 1995), the Climate Services Partnership (in 2011), the Global Framework of Climate Services (in 2012), and the Climate Services for Resilient Development Partnership (in 2017). These initiatives have been used in several approaches to facilitate the production and dissemination of climate information to stakeholders in the agricultural sector. The Global Framework of Climate Services, for example, implemented several projects in many African",
    "label": 0
  },
  {
    "text": "Partnership (in 2017). These initiatives have been used in several approaches to facilitate the production and dissemination of climate information to stakeholders in the agricultural sector. The Global Framework of Climate Services, for example, implemented several projects in many African countries that aimed to facilitate timely delivery of contextual climate information to stakeholders through a collaborative participatory process. Similarly, the African Centre of Meteorological Application for Development initiative implemented several projects aimed at producing forecasts of an appropriate timeline that are most suitable to decision-making in the agricultural sector Europe In 2015, the European Commission launched the Climate Services Roadmap. Research and innovation has been supported through the European Research Area for Climate Services (ERA4CS) Programme. The new EU Adaptation Strategy from 2021 also highlighted the key role for climate services in supporting climate change adaptation. The climate services sector in Europe is quite well established. One example in Europe is Copernicus Climate Change Service (C3S), which provides free and open access to climate data, tools and information used for a variety of purposes. Another example is Participatory Integrated Climate Services for Agriculture (PICSA). This is a participatory approach which combines historical climate data and forecasts with farmers’ local contextual knowledge. See also Climate change adaptation References",
    "label": 0
  },
  {
    "text": "Climate psychology is a field that aims to further our understanding of our psychological processes' relationship to the climate and our environment. It aims to study both how the climate can impact our own thoughts and behaviors, as well as how our thoughts and behaviors impact the climate. This field often focuses on climate change, both in our reaction to it and how our behaviors can be changed in order to minimize the impact humanity has on the climate. These behavior changes include: engaging with the public about climate change, contributing at a personal, communal, cultural and political level by supporting effective change through activists, scientists, and policy makers, and finally nurturing psychological resilience to the destructive impacts climate change creates now and in the future. Climate psychology includes many subfields and focuses including: the effects of climate change on mental health, the psychological impact of climate change, the psychological explanation of climate inaction, and climate change denial. Climate psychology is a sub-discipline of environmental psychology. History The origins of climate psychology can be traced back to the work of psychoanalyst Harold Searles and his work on the unconscious factors that influence the estrangement of people from the rest of nature. It has also been strongly related to the field of Ecopsychology, as Sigmund Freud tied the interests of the ego to the natural world. Due to the increase in society-wide acceptance of the dangers of climate change, there has been greater interest in understanding the psychological processes underlying the resistance to taking appropriate action, and in particular, the phenomenon of climate change denial. More recently, a literature base by climate psychologists has started to focus on the powerful emotions associated with climate change and planetary-wide biodiversity loss. Academic discipline Climate psychology is a trans-disciplinary approach to research and practice.",
    "label": 0
  },
  {
    "text": "of climate change denial. More recently, a literature base by climate psychologists has started to focus on the powerful emotions associated with climate change and planetary-wide biodiversity loss. Academic discipline Climate psychology is a trans-disciplinary approach to research and practice. It focuses on the society-wide reluctance to take appropriate action in relation to the escalating threat of climate change. It seems the problem as requiring a deeper approach, that examines our resistance to knowing and acting, rather than seeing it as an \"information deficit\" to be treated by cognitive or behavioral approaches. It stresses the significance of human emotions, identities and cultural assumptions. Furthermore, it acknowledges the human subject as nested within their social and ecological context. In order to meet its aims and develop its approach, climate psychology draws on a broad range of perspectives, including: literature, philosophy, world religions, the arts, humanities and systems thinking. The core of the approach is based on various psychotherapeutic traditions and psycho-social studies, allowing climate psychologists to understand the unconscious or unacknowledged emotions and processes influencing people's thoughts, motivations and behaviors. This applies especially to these processes that manifest in the broader context of the wider society and culture. As of 2020, the discipline of climate psychology had grown to include many subfields. This is in response to the spread of, what has recently been called, climate anxiety, which is a manifestation of the decades-old understanding of eco-anxiety. Climate psychologists are working with the United Nations, national and local governments, with corporations, NGOs, and individuals. Climate psychology in practice In recent years, climate psychologists are facilitating support groups for activists, particularly those active in the support of pro-environmental behaviors across society. Climate psychologists have also engaged directly with climate activists, and even engaged in climate activism themselves. For example, in August 2022,",
    "label": 0
  },
  {
    "text": "climate psychologists are facilitating support groups for activists, particularly those active in the support of pro-environmental behaviors across society. Climate psychologists have also engaged directly with climate activists, and even engaged in climate activism themselves. For example, in August 2022, scientists and their colleagues came together to protest rebellion outside of the Department of Business, Energy, and Industrial Strategy in London. During this time, as shown on the news, many climate scientists were having mental breakdowns and showing extreme signs of emotional turmoil and anguish. Climate psychologists over the years have watched not only scientists go through this environmental change, seeing how it has negatively impacted millions. They support groups through behavioral practices and studies to help obtain precise data and comprehension from person to person within these activist groups. The United States Agency for International Development (USAID) reports that roughly 971 million individuals are residing in regions with moderate to high exposure to climate hazards due to industrial development, environmental exploitation, and excessive consumerism, particularly in the Asia-Pacific and South Asia regions. In response to the issues and difficulties resulting from climate change, the Psychological Association of the Philippines (PAP) is actively providing psychological aid during natural disasters and catastrophic events. In addition, psychologists around the globe encourage networking and connections to maintain knowledge exchange and create a community of climate action proponents to assure that all individuals have access to the aid and amenities needed in areas currently under pressure from the ongoing climate crisis. Climate and Mental Health The climate can have various impacts on mental health. For example, increased temperature can be linked to a worsening of a variety of mental health issues such as aggression, anxiety, dementia, mood, and suicide. Especially when combined with increased humidity, heat waves not only worsen existing mental health issues",
    "label": 0
  },
  {
    "text": "For example, increased temperature can be linked to a worsening of a variety of mental health issues such as aggression, anxiety, dementia, mood, and suicide. Especially when combined with increased humidity, heat waves not only worsen existing mental health issues but also reduce the effectiveness of some psychotropic medications. The worsening of these symptoms can lead to increases in crime rates and hospital admissions rates during heat waves. The increased prevalence of natural disasters can also cause mental distress which can cause PTSD in many patients, which is a pressing concern for some climate psychologists. Natural disasters have also been linked to acute stress disorder, drug abuse disorder, and depression in some people. Climate change may also result in socioeconomic impacts; the associated economic hardship can negatively impact mental health, leading to stress and depression. Workers often face worse conditions due to climate change leading to increased risk of injury. The negative impacts on physical health can then lead to decreases in mental health as well. Workers may then become demoralized and lose interest in their work as a result of worsened mental health. These socioeconomic impacts can also lead to disproportionate impacts on minorities and repressed groups within a society. For example women are often disproportionately impacted compared to men in the aftermath of a natural disaster. Due to the impacts of climate change on mental health, psychologists and social workers have begun to take climate into account when assessing patients. This includes reaching out to the community and applying psychological principals to decrease climate change and to address climate anxiety in clinical sessions.Psychologists may council patients with climate related anxiety and attempt to shift those anxieties into positive changes. Psycho-analytical approaches Psycho-analytical approaches are approaches based on the ideas of Sigmund Freud. They focus on how people respond",
    "label": 0
  },
  {
    "text": "climate anxiety in clinical sessions.Psychologists may council patients with climate related anxiety and attempt to shift those anxieties into positive changes. Psycho-analytical approaches Psycho-analytical approaches are approaches based on the ideas of Sigmund Freud. They focus on how people respond to anxiety, a response which in turn may trigger psychological defense mechanisms. The psychological defenses triggered will often define how that person then reacts to climate change. Climate psychologists use this to explain reactions such as climate change denial, apathy, and inaction in towards climate change. Psychologists consider how coping responses can be adaptive or maladaptive, and climate psychologists use environmental impact of a behavior to determine the adaptivity of the response. For example, climate psychologists might ask if responses promote positive psychological adjustment and stimulate appropriate and proportional pro-environmental action, or do they serve to justify the individual in their inaction and allow them to refrain from the necessary, radical changes? Recent research highlights that people’s underlying values play a key role in motivating climate-friendly behavior. Individuals who hold strong biospheric values—those emphasizing concern for nature and the environment—are more likely to support climate policies and adopt sustainable behaviors, whereas egoistic or hedonic values can reduce such engagement. Psycho-social approaches A psycho-social approach to climate psychology examines the interplay between internal, psychological factors and external, sociocultural factors- such as values, beliefs, and norms- in people's responses to climate change. For example, researchers have looked at how cultures with a history of water scarcity in the environment tend to prioritize long-term thinking and reject indulgence. Researchers have also looked at how people's values shift in response to hearing news about climate change, such as worsening water scarcity. Some psycho-social approaches use qualitative methodology for understanding the lived experience of research participants, which has been adopted by researchers seeking to investigate",
    "label": 0
  },
  {
    "text": "how people's values shift in response to hearing news about climate change, such as worsening water scarcity. Some psycho-social approaches use qualitative methodology for understanding the lived experience of research participants, which has been adopted by researchers seeking to investigate how climate change and environmental destruction are experienced by different groups across society. In this case, 'lived experience' refers to the feelings, thoughts and imaginations and the meaning frames which both affect and are effected by those experiences. Coping responses to impending climate destabilization are psycho-social phenomena, culturally sanctioned and maintained by social norms and structures, not simply isolated psychological processes. For example, modern mass consumerism is dictated by the needs of a globalized, deregulated economy, yet it is one of the driving forces of climate change. It has been suggested that this \"culture of un-care\" performs an ideological function, insulating consumers from experiencing too much anxiety and moral disquiet. Cultural mechanisms also support ways of down-regulating the powerful feelings that would otherwise be elicited by the awareness of potential threats. These include strong, embedded cultural assumptions such as entitlement, exceptionalism, and faith in progress. Entitlement is the belief that certain groups or species deserve more than others and is embedded in the unequal relations governing developed and developing human societies. Exceptionalism is the idea that one's species, nation, ethnic group or individual self is special and therefore absolved from the rules that apply to others, giving license to breach natural limits of resource consumption. Faith in progress, a key element of post-industrial ideology, results in a conviction that science and technology can solve every problem, therefore encouraging wishful thinking and false optimism. Many people perceive unfairness in how they are affected by Climate change. This is often caused by the wealth inequality correlated with climate change. In other words,",
    "label": 0
  },
  {
    "text": "Climate security is a political and policy framework that looks at the impacts of climate on security. Climate security often refers to the national and international security risks induced, directly or indirectly, by changes in climate patterns. It is a concept that summons the idea that climate-related change amplifies existing risks in society that endangers the security of humans, ecosystems, economy, infrastructure and societies. Climate-related security risks have far-reaching implications for the way the world manages peace and security. Climate actions to adapt and mitigate impacts can also have a negative effect on human security if mishandled. The term climate security was initially promoted by national security analysts in the US and later Europe, but has since been adopted by a wide variety of actors including the United Nations, low and middle income states, civil society organizations and academia. The term is used in fields such as politics, diplomacy, environment and security with increasing frequency. There are also critics of the term who argue that the term encourages a militarized response to the climate crisis, and ignores issues of maldistribution and inequity that underpin both the climate crisis and vulnerability to its impacts. Those who look at the national and international security risks argue that climate change has the potential to exacerbate existing tensions or create new ones – serving as a threat multiplier. For example, climate change is seen as a threat to military operations and national security, as the rise in sea level can affect military bases or extreme heat events can undermine the operability of armies. Climate change is also seen as a catalyst for violent conflict and a threat to international security, although the causality of climate and conflict is also debated. Due to the growing importance of climate security on the agendas of many governments,",
    "label": 0
  },
  {
    "text": "change is also seen as a catalyst for violent conflict and a threat to international security, although the causality of climate and conflict is also debated. Due to the growing importance of climate security on the agendas of many governments, international organizations, and other bodies some now run programs which are designed to mitigate the effects of climate change on conflict. These practices are known as climate security practices. These practices stem from a variety of actors with different motivations in the sphere of development, diplomacy and defense; both NATO and the UN Security Council are involved in these practices. Definition Climate security looks at the impacts of climate on security. Climate security often refers to the national and international security risks induced, directly or indirectly, by changes in climate patterns. It is a concept that summons the idea that climate-related change amplifies existing risks in society that endangers the security of humans, ecosystems, economy, infrastructure and societies. Background Climate security refers to the security risks induced, directly or indirectly, by changes in climate patterns. Climate change has been identified as a severe-to-catastrophic threat to international security in the 21st century by multiple risk and security reports. The 2020 Global Catastrophic Risks report, issued by the Global Challenges Foundation, concluded that climate change has a high likelihood to end civilization. 70% of international governments consider climate change to be a national security issue. Policy interest in climate security risks has grown rapidly and affects the policy agenda in relation to food and energy security, migration policy, and diplomatic efforts. Development The term climate security was initially promoted by national security analysts in the US and later Europe, but has since been adopted by a wide variety of actors including the United Nations, low and middle income states, civil society organizations",
    "label": 0
  },
  {
    "text": "The term climate security was initially promoted by national security analysts in the US and later Europe, but has since been adopted by a wide variety of actors including the United Nations, low and middle income states, civil society organizations and academia. The term is used in fields such as politics, diplomacy, environment and security with increasing frequency. Within academia, climate security emerged from a discourse of environmental security and was first mentioned in the Brundtland Report in 1987. During the '70s and '80s the Jason advisory group, concerned with security, conducted research on climate change. Global climate change became an international issue with the broadening of the concept of security which emerged in the 1980s in the post-Cold War era. The broadening of the concept of security sought to look beyond the military domain, and include political, economic, societal, and environmental areas in the security agenda. The term security can refer to a broad range of securities including national, international, ecological and human security. To map the different ways in which climate change is conceptualized, scholar Matt McDonald identifies four discourses of climate security advanced by policymakers, lobbyists, environmental advocates, civil society groups and academic analysts. He divides them into national, human, international and ecological types of security which respectively concern nation-states, 'people', the international community, and the 'ecosystem'. National climate security is the most dominant of the four discourses as it focuses on the threat climate change poses to nation-states and the maintenance of its sovereignty and 'territorial integrity' from an external threat. This discourse is advanced by national security institutions where the nation-state is viewed as the most capable provider of security through the military apparatus. This discourse has also been advanced by policy think tanks embracing the concept of 'threat multiplier'. The international security discourse focuses",
    "label": 0
  },
  {
    "text": "national security institutions where the nation-state is viewed as the most capable provider of security through the military apparatus. This discourse has also been advanced by policy think tanks embracing the concept of 'threat multiplier'. The international security discourse focuses on internationalism and global cooperation where international organizations are viewed as providers of security. Mitigation and adaptation strategies are central to this discourse, such as the transition to low carbon economies and the transfer of technology, sources, and expertise to developing countries. International organizations such as the UN Environment Program are involved in these processes and the more recent Sustainable Development Goals are an embodiment of such discourse. However, the UN Security Council plays a central role as the securitizing agent, which has been often criticized by developing countries, such as Group 77 and the non-Aligned Movement, as they are concerned climate change will be used to justify military intervention and increased military budgets by powerful countries. The human security discourse emerged as a counteracting alternative to national security, and was embraced first by the United Nations Development Program in 1994. It seeks to center the wellbeing of people rather than states. For the UN agencies, mitigation strategies and the redistribution of resources are seen as central to providing security to populations. The ecological security discourse is seldom included in dominant policy or academic debates. While many International Relations (IR) scholars link climate change with security and conflict through a traditional military approach, there is an ongoing debate on whether climate change and environmental issues should be securitized and who and what is really protected. The scholars who theorized the concept of securitization allowed to deepen and broaden concepts of security beyond traditional military security through discourse methodology and 'speech acts.' For example, Copenhagen School scholars, such as Barry Buzan",
    "label": 0
  },
  {
    "text": "and what is really protected. The scholars who theorized the concept of securitization allowed to deepen and broaden concepts of security beyond traditional military security through discourse methodology and 'speech acts.' For example, Copenhagen School scholars, such as Barry Buzan and Ole Wæver, argue that security justifies urgency and exceptionalism, focusing on defense, the military and the state and that climate change should instead be placed into 'normal politics' and removed from the security agenda. Furthermore, some scholars note how securitization theory, stemming as a response to traditional realism theory in the post-Cold War era, is mostly a Eurocentric field and does not include the legacies of colonialism and racial hierarchies inform global politics and governance. The impacts of climate change, highlighted in 1990 by the First Assessment Report (FAR) of the Intergovernmental Panel on Climate Change (IPCC) and in 1992 by the United Nations Framework Convention on Climate Change (UNFCCC), highlighted the need for climate change to be viewed as a security threat and influenced international entities to do so. A report in 2003 by Peter Schwartz and Doug Randall looked at potential implications from climate-related scenarios for the national security of the United States, and concluded, \"We have created a climate change scenario that although not the most likely, is plausible, and would challenge United States national security in ways that should be considered immediately.\" In 2008, the EU published a report on climate change and international security, defining climate change as a 'threat multiplier' affecting EU own security and interests. Critiques and alternatives The climate security approach has become prominent among political and policy spheres and has been called inevitable by some countries, inviting the UN Security Council to adopt more militarized approaches. However, some scholars and activists criticize climate security, arguing that framing climate change as",
    "label": 0
  },
  {
    "text": "has become prominent among political and policy spheres and has been called inevitable by some countries, inviting the UN Security Council to adopt more militarized approaches. However, some scholars and activists criticize climate security, arguing that framing climate change as a security issue can be problematic as it could increase solutions that rely on militaries which can worsen the injustices of those most affected by the climate crisis. This can also mean that security solutions end up benefiting the status quo, ignoring the well-being of the rest, such as refugees and other marginalized communities. The climate security approach has also a significant impact on borders and migration, as its narrative emphasizes the 'threat' of climate-induced mass migration. Indeed, the border industrial complex is expected to grow globally by 7% annually. As the Transnational Institute report \"Global Climate Wall\" shows, the seven biggest GHG emitters the United States, Germany, Japan, the United Kingdom, Canada, France, and Australia spent collectively at least twice on border and immigration control than on climate finance between 2013 and 2018. The EU's budget for Frontex has increased by 2763% since its establishment in 2016 through 2021. Social movements and organizations, such as Climate Justice alliance, We are Dissenters, Grassroots Global Justice Alliance, Indigenous Environmental Network, call for a bigger emphasis on climate justice and environmental justice rather than climate security. Climate justice puts the emphasis on the root causes of climate change, like colonialism and neocolonialism, global inequality, globalization and exploitative economic systems such as the exploitation of natural resources. Many call this addressing the era of climate colonialism. Indeed, many proponents of climate justice call for bigger support for Indigenous people and other frontline communities that are fighting for climate change and also already protecting 80% of Earth's biodiversity. Many civil society actors also call",
    "label": 0
  },
  {
    "text": "of climate colonialism. Indeed, many proponents of climate justice call for bigger support for Indigenous people and other frontline communities that are fighting for climate change and also already protecting 80% of Earth's biodiversity. Many civil society actors also call for climate reparations on top of more climate finance, and also the establishment of Loss and Damage Finance Facility (LDFF), which has been proposed by low-income countries, as well as sovereign debt cancellation. This way, low-income countries could tackle the impacts of climate change for which they are bear the least responsibility. From an academic standpoint, the concept of ecological security, allows for a more systemic approach to climate change that examines the structural roots of the climate crisis as the overlapping economic, political, and social issues of the global system. Effects of climate change Climate change is a global challenge which will affect all countries in the long-term as the impact of climate change is spread unevenly across different regions. However, there may be a disproportionately harsher effect in fragile contexts and/or socially vulnerable and marginalized groups due to climate change vulnerability. For example, the Bay of Bengal, which includes Bangladesh, Myanmar, India, Indonesia, and Sri Lanka, is one of the world's most climate-vulnerable regions. Marginalized groups and minority communities, both in the Global North and Global South, are most affected by the effects of climate change, for which they are least responsible, which many call environmental injustice (see also climate justice). Indeed, the richest 10% (circa 630 million people), of the world's population, mostly from EU and North America, are in fact responsible for 52% of carbon emissions, whereas the poorest 50% (circa 3.1 billion people) were responsible for only 7% of cumulative emissions. The Global North is responsible for 92 percent of GHG emissions and climate change",
    "label": 0
  },
  {
    "text": "America, are in fact responsible for 52% of carbon emissions, whereas the poorest 50% (circa 3.1 billion people) were responsible for only 7% of cumulative emissions. The Global North is responsible for 92 percent of GHG emissions and climate change is devastating the Global South. To account for this, the UNFCCC embodies the notion of \"common but differentiated responsibilities (CBDR)\" which addresses developed countries' responsibility to transfer aid and technology to developing countries. Conflicts Studies have shown that extreme weather events can damage economies, lower food production and raise inequality, which can increase risks of violence when combined with other factors. An article by leading experts found that climate change has influenced between 3% and 20% of armed conflict in the last century, that an increase of 2 °C above pre-industrial levels more than doubles the current risk of conflict, increasing it to 13%, and that an increase of 4 °C multiplies the risk by five, up to a 26% risk. Other researchers have specified the conditions under which climate change increases the risk of conflict and violence. These include a history of political instability, agricultural dependence, low levels of development, the political of ethnic groups from the political system, and insufficient conflict resolution institutions. At the micro level, temperature volatility associated with climate change has likewise been found to act as a risk multiplier for short-term spikes in interpersonal violent crime. A report by the Global Peace Index found that 971 million people lived in areas with either a high or very high climate change exposure and that 400 million of those people lived in countries with low levels of peacefulness. It warned that climate change can increase the likelihood of violent conflict by impacting upon resource availability, job security, and by causing forced migration. Predicting future risks of",
    "label": 0
  },
  {
    "text": "million of those people lived in countries with low levels of peacefulness. It warned that climate change can increase the likelihood of violent conflict by impacting upon resource availability, job security, and by causing forced migration. Predicting future risks of climate change and conflict remains difficult, despite the existence of several predictive models and tools. Future climate change is likely to be very different from what humanity has experienced previously and the ability of societies to adapt is unclear. A 2016 article suggested that conflict over climate-related water issues could lead to nuclear conflict between India and Pakistan. However, other scholars believe that climate change are unlikely to have major impacts on the nature of interstate wars, but have expressed concerns about its impacts on civil wars and communal conflicts. Based on a meta-analysis of 60 studies, Hsiang, Burke and Miguel concluded in 2013 that warmer temperatures and more extreme rainfall could increase interpersonal violence by 4%, and intergroup conflict by 14% (median estimates). However, their results have been disputed by other researchers as being not sufficiently robust to alternative model specifications. Recent studies by authors like Buhaug, Detges, Ide and von Uexkull have been more careful. They agree that climate-related disasters (including heatwaves, droughts, storms and floods) modestly increase armed conflict risks, but only in the presence of contextual factors like agricultural dependence, insufficient infrastructure, the political exclusion of ethnic groups, insufficient conflict management, and high disaster vulnerability. Climate change is therefore rather a \"risk multiplier\" that amplifies existing risks of conflict. In line with this and other reviews of the topic, an expert assessment published 2019 in Nature concludes that between 3% and 20% of intrastate, armed conflict risks in the previous century were affected by climate-related factors, but that other drivers of conflict are far more important.",
    "label": 0
  },
  {
    "text": "of the topic, an expert assessment published 2019 in Nature concludes that between 3% and 20% of intrastate, armed conflict risks in the previous century were affected by climate-related factors, but that other drivers of conflict are far more important. The expert assessment itself notes that major knowledge gaps and uncertainties continue to exist in the research field, especially regarding the pathways connecting climate change to conflict risk. Recently, researchers have paid increased attention to the impacts of climate change on low-intensity and even non-violent conflicts, such as riots or demonstrations. Even if people do not have the means or motivation to use violence, they can engage in such forms of conflict, for instance in the face of high food prices or water scarcity. Studies indeed show that in vulnerable societies, the anticipated consequences of climate change such as reduced food and water security increase the risk of protests. These conflicts often add to and trigger the escalation of deeper social and political struggles. On a country by country basis, several case studies have linked climate change to increased violent conflicts between farmers and herders in Kenya and Sudan, but have found mixed results for Ghana, Mali, Nigeria and Tanzania. Evidence is also ambiguous and highly contested for high-intensity conflicts such as civil wars. Some experts suggest a contribution of climate change to civil wars in Mali, Nigeria, Somalia and Sudan. Other studies suggest that there is very little evidence for these causal claims, including for the cases of Darfur, Egypt, and Lake Chad. The most prominent example of these debates is the Syrian civil war. Several studies claim that a climate-induced drought between 2006 and 2009 led to mass migration into urban areas, contributing to grievances and unrest that erupted in the 2011 protests. The repression of the latter",
    "label": 0
  },
  {
    "text": "debates is the Syrian civil war. Several studies claim that a climate-induced drought between 2006 and 2009 led to mass migration into urban areas, contributing to grievances and unrest that erupted in the 2011 protests. The repression of the latter marked the start of the civil war. A team around Jan Selby argues that these claims are overstated and that political decisions and mismanagement, rather than climate change and migration, have caused the onset of the war. Several recent studies find that the debate is not yet settled because there is evidence for both positions, yet a lack of comprehensive empirical data. There are a number of studies that criticize how climate-conflict research is based on a deterministic and conflict-oriented worldview, that findings of statistical studies on the topic are based on problematic models and biased datasets, and that constructivist approaches are largely ignored. Existing research also predominantly focuses on a few, well-known and already conflict-ridden regions, such as Sub-Saharan Africa and the Middle East. This raises questions about sampling biases as well as implications for less-considered regions like Latin America and the Pacific, with topics such as peaceful adaptation and environmental peacebuilding also understudied. The IPCC's Sixth Assessment Report concluded in 2022: \"Climate hazards have affected armed conflict within countries (medium confidence), but the influence of climate is small compared to socio-economic, political, and cultural factors (high confidence). Climate increases conflict risk by undermining food and water security, income and livelihoods, in situations where there are large populations, weather-sensitive economic activities, weak institutions and high levels of poverty and inequality (high confidence).\" Many politicians, decision makers, and journalists have drawn a connection between climate change and conflict. Already in a 2007 study on the topic, the German Advisory Council on Global Change identified four pathways potentially connecting climate change",
    "label": 0
  },
  {
    "text": "inequality (high confidence).\" Many politicians, decision makers, and journalists have drawn a connection between climate change and conflict. Already in a 2007 study on the topic, the German Advisory Council on Global Change identified four pathways potentially connecting climate change to conflict: degradation of freshwater resources, food insecurity, an increasing frequency and intensity of natural disasters, and increasing or changing migration patterns. A more recent 2021 report from the US Office of the Director of National Intelligence predicts intensifying physical effects of climate change \"will exacerbate geopolitical flashpoints, particularly after 2030, and key countries and regions will face increasing risks of instability and need for humanitarian assistance.\" The United Nations Security Council has discussed the links between climate change and security various times, even though the position of its member states vary. Other key decision makers in the USA, the European Union, and NATO are also concerned about climate conflict risks. In some cases, climate change could also decrease conflict risks. This happens either if climate-related disasters impose financial and logistical constraints on conflict parties or if various social groups come together to cooperate about the shared challenge of climate change (environmental peacebuilding). Adaptation Energy At least since 2010, the U.S. military begun to push aggressively to develop, evaluate and deploy renewable energy to decrease its need to transport fossil fuels. Based on the 2015 annual report from NATO, the alliance plans investments in renewables and energy efficiency to reduce risks to soldiers, and cites the impacts from climate change on security as a reason. Military Operations and Climate Security The main concern for military strategists - and central to many military and national security strategies - is the operability of armed forces during climate change. There are concerns over the impact of climate change on infrastructure, such as military",
    "label": 0
  },
  {
    "text": "The main concern for military strategists - and central to many military and national security strategies - is the operability of armed forces during climate change. There are concerns over the impact of climate change on infrastructure, such as military bases, as well as on the capacity to fight, for example in extreme heat. A 2018 Pentagon report revealed that half of 3,500 military sites were suffering the effects of six key categories of extreme weather events, such as storm surge, wildfires and droughts. There have consequently been efforts to 'green' the military and prepare it for a climate changed world, through the installation of solar panels at military bases, alternative fuels in shipping and renewable energy equipment. There are also concerns about the reliance on fossil fuels, which can pose vulnerabilities for armed forces. The Pentagon alone is the world's largest consumer of fossil fuel. For example, during the US invasion of Iraq, one in nearly 40 fuel convoys in Iraq in 2007 resulted in a death or serious injury. At least since 2010, the U.S. military begun to push to develop, evaluate and deploy renewable energy to decrease its need to transport fossil fuels. The NATO's 2021 Climate Change and Security Action Plan proposes strategies to protect its assets along with a promise of GHG emissions reduction by 2050. However, because military emissions reporting is only voluntary, there is a lack of transparent data on militaries' GHG emissions. A 2019 study by Brown University estimated 1.2 billion metric tons of GHG have been consumed by the U.S. military alone since the beginning of the war on terror in 2001. Additionally, Scientists for Global Responsibility have calculated UK military emissions to be 11 million tonnes, and EU emissions to be 24.8 million tonnes with France contributing to a third",
    "label": 0
  },
  {
    "text": "alone since the beginning of the war on terror in 2001. Additionally, Scientists for Global Responsibility have calculated UK military emissions to be 11 million tonnes, and EU emissions to be 24.8 million tonnes with France contributing to a third of the total. The military's sustainability plans have been criticized as 'greenwashing.' Additionally, militarism and war have caused devastating environmental damages. The chemical contamination left in Afghanistan and the nuclear contamination in the Marshalls Islands are some examples of American imperialism and its environmental legacy. Climate security practices Due to the growing importance of climate security on the agendas of many governments, international organizations, and other bodies some now run programs which are designed to mitigate the effects of climate change on conflict. These practices are known as climate security practices which are defined by von Lossow et al. as \"tangible actions implemented by a (local or central) government, organization, community, private actor or individual to help prevent, reduce, mitigate or adapt (to) security risks and threats related to impacts of climate change and related environmental degradation\". The Planetary Security Initiative at the Clingendael Institute maintain an updated list of climate security practices. These practices stem from a variety of actors with different motivations in the sphere of development, diplomacy and defence. An example is the Arms to Farms project in Kauswagan municipality, the Philippines. An insurgency in the area was aggravated by food insecurity because irregular rainfall that caused poor harvests led to an uptick in insurgent recruitment sparking further violence. The project successfully integrated former insurgents into the community by training them in agricultural methods and fostering trust between communities, increasing food security, peace and human security overall. Another example is a division of the UN peacekeeping mission in Mali (MINUSMA) that seeks to solve community conflicts, which",
    "label": 0
  },
  {
    "text": "community by training them in agricultural methods and fostering trust between communities, increasing food security, peace and human security overall. Another example is a division of the UN peacekeeping mission in Mali (MINUSMA) that seeks to solve community conflicts, which can stem from climate change caused resource shortages. One project in Kidal built a new and more effective water pump in order to solve the issue of conflict between different stakeholders in the area over water which risked a violent confrontation. A growing number of non-military and civil society organizations are advocating for a national security approach including Brookings Institution and the Council on Foreign Relations (US), the International Institute for Strategic Studies and Chatham House (UK), Stockholm International Peace Research Institute, Clingendael (Netherlands), French Institute for International and Strategic Affairs, Adelphi (Germany) and the Australian Strategic Policy Institute. Environmental groups have also embraced a national security approach such as the World Wildlife Fund, the Environmental Defense Fund and Nature Conservancy (US) and E3G in Europe. The grassroots group Extinction Rebellion Netherlands even invited a Dutch military general to contribute to their 'rebel' handbook. Even though these groups are often more concerned with human security they seek to involve the military as allies, driven by the belief that it can help achieve broader political and economic support. The field of climate security practices is still young and even though the issue is growing in importance, some actors are still reluctant to get involved due to the uncertainty inherent in the new field. Because climatic change will only increase in the near future von Lossow et al. conclude that expanding the number of climate security practices in vulnerable areas of the world has \"huge potential to catalyse more sustainable and long-term peace and stability\". Political approaches The transnational character of climate-related",
    "label": 0
  },
  {
    "text": "near future von Lossow et al. conclude that expanding the number of climate security practices in vulnerable areas of the world has \"huge potential to catalyse more sustainable and long-term peace and stability\". Political approaches The transnational character of climate-related security risks often goes beyond the capacity of national governments to respond adequately. Many parts of governments or state leaders acknowledge climate change as an issue for human security, national or regional security. Despite ongoing concerns about the securitization of climate change, it has had little effect on the policies and activities of national governments, which have tended to take 'business as usual' approaches to managing and containing international migration. NATO NATO stated in 2015 that climate change is a significant security threat and that \"Its bite is already being felt\". In 2021, NATO agreed a Climate Change and Security Action Plan that committed the alliance to 1) analyze the impact of climate change on NATO's strategic environment and NATO's assets, installations, missions and operations 2) incorporate climate change considerations into its work 3) contribute to the mitigation of climate change and 4) exchange with partner countries, as well as with international and regional organizations that are active on climate change and security. It is important to note that the deployment of security forces can sometimes lead to insecurity, rather than security, for certain populations.For example, the 20-year US-led and NATO-supported military invasion and occupation of Afghanistan was launched in order to obtain security from terrorism and fight the war on terror, but it ended up fueling more war, conflict, and the return to power of the Taliban as a result of the withdrawal of United States troops from Afghanistan (2020–2021). United Nations Although climate change is first and foremost dealt within the United Nations Framework Convention on Climate Change",
    "label": 0
  },
  {
    "text": "and the return to power of the Taliban as a result of the withdrawal of United States troops from Afghanistan (2020–2021). United Nations Although climate change is first and foremost dealt within the United Nations Framework Convention on Climate Change (UNFCCC) and now also under the Paris agreement, the security implications of climate change do not have an institutional home within the United Nations system, and hence remain largely unaddressed, in spite of the urgency of the threat it poses to peace and security in several regions. The UN, through its COP - The Conference of the Parties - is the supreme body to negotiate climate frameworks under the UNFCCC Convention. It consists of the representatives of the Parties to the Convention and holds its sessions every year, and takes decisions which are necessary to ensure the effective implementation of the provisions of the Convention and regularly reviews the implementation of these provisions. Preventing \"dangerous\" human interference with the climate system is the ultimate aim of the UNFCCC. The UNFCCC is a \"Rio Convention\", one of three adopted at the \"Rio Earth Summit\" in 1992. The UNFCCC entered into force on March 21, 1994. Today, it has near-universal membership. The COP has discussed Climate Security during panels, workshops as session, but not as a programmatic track. The greater focus on this topic by the UN has led to the launch in October 2018 of the inter-agency DPPA-UNDP-UN Environment cooperation called the Climate Security Mechanism. United Nations Security Council The UN Security Council first debated climate security and energy in 2007 and in 2011 issued a presidential statement expressing concern at the possible adverse security effects of climate change. There has been a series of informal Arria-Formula meetings on issues related to climate change. In July 2018, Sweden initiated a debate",
    "label": 0
  },
  {
    "text": "and in 2011 issued a presidential statement expressing concern at the possible adverse security effects of climate change. There has been a series of informal Arria-Formula meetings on issues related to climate change. In July 2018, Sweden initiated a debate on Climate and Security in the United Nations Security Council. In 2021 the UN Security Council convened for a high-level open debate on climate security. Climate change grew beyond its categorization as a hypothetical, existential risk and became an operational concern of relevance to other peace and security practitioners beyond the diplomats in the Security Council. However, some countries, especially low and middle income countries (LMICs), do not think climate change should be seen as a security issue. When the topic of climate and security first emerged in the UN, LMICs opposed the securitization of climate change. In 2006 the Group of 77 (G77) argued that 'the United Nations Framework Convention on Climate Change (UNFCCC) is the primary international, intergovernmental forum for negotiating the global response to climate change' and that the richest countries should not only address the 'consequences [of climate change] but mainly the roots of the problem.' The G77 also stated that it is inappropriate to consider the issue of energy in the UNSC, 'reaffirming the key role of energy in achieving the goals of sustainable development, poverty eradication and achieving the MDGs [Millennium Development Goals]'. In 2013, the G77 and China argued that the UNSC was 'not the appropriate forum for this discussion' and that such issues should be assigned to the Economic and Social Council (ECOSOC) and the UN General Assembly. The G77 has not issued public positions since then. Country and continent examples Africa Climate change has had devastating effects on the African continent, affecting the poorest communities. It has escalated food insecurity, led",
    "label": 0
  },
  {
    "text": "(ECOSOC) and the UN General Assembly. The G77 has not issued public positions since then. Country and continent examples Africa Climate change has had devastating effects on the African continent, affecting the poorest communities. It has escalated food insecurity, led to the displacement of populations and exerted extreme pressure on the available water resources. Africa's exposure to climate change is high due to the legacy of colonialism, inequitable global trade arrangements, its low adaptive capacity and limited government capabilities, making it the most vulnerable continent. A 2007 report by the UN Secretary-General, Ban Ki-Moon points out that, climate change and environmental degradation were partly responsible for the Darfur, Sudan conflict. Between 1967 and 2007, the total rainfall in the area had reduced by 30 percent and the expansion of the Sahara was beyond a mile every year. The ensuing friction between farmers and pastoralists over the reducing grazing land and the few water sources available was at the heart of the Darfur civil war. More recently US and European security analysts refer to the Sahel and the Lake Chad basin as a 'hotspot' because of its severe climate-related vulnerabilities, communal violence, jihadist insurgencies, political instability, and internal and regional displacement. Climate change has caused rainfall variations and desertification threatening the well-being of people whose lives depend on Lake Chad. Reports suggest that Lake Chad is shrinking at a fast speed, which is creating sharp competition for water. In 2017, the UNSC adopted Resolution 2349 connecting conflict and water scarcity in the region. In 2020, Niger co-organized a UN Security Council meeting on climate security, following a 2018 UNSC statement on the Lake Chad Basin which identified climate change and the shrinking of Lake Chad as one of the root causes of the Boko Haram uprisings. However scholars disagree on whether",
    "label": 0
  },
  {
    "text": "meeting on climate security, following a 2018 UNSC statement on the Lake Chad Basin which identified climate change and the shrinking of Lake Chad as one of the root causes of the Boko Haram uprisings. However scholars disagree on whether the desertification of Lake Chad has indeed led to conflicts, because there are other factors such as pre-existing socioeconomic and political conditions, the influx of arms into the region, unfair terms of trade, religious issues, and the marginalization of pastoralist communities. Australia A 2018 published report by the Australian Senate noted how \"climate change as a current and existential national security risk... defined as one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development.\" Several reports by the Australian Security Leaders Climate Group have warned that climate change will pose a serious security threat to Australia, urging policy makers to strive for more ambitious mitigation and adaptation measures. Academic assessments have nuanced such claims, but generally agree that Australia is vulnerable to climate change in the human security and national security realm. European Union The European Council's conclusions on climate diplomacy state that \"Climate change is a decisive global challenge which, if not urgently managed, will put at risk ... peace, stability and security.\" The Intelligence on European Pensions and Institutional Investment think-tank published a 2018 report with the key point, \"Climate change is an existential risk whose elimination must become a corporate objective\". In June 2018 European External Action Service (EEAS) High-Level Event hosted an event themed \"Climate, Peace and Security: The Time for Action\". The EU's comprehensive approach to security would suggest that the EU is well placed to respond to climate-related security risks. However, recent scientific research shows that the European Union has not",
    "label": 0
  },
  {
    "text": "themed \"Climate, Peace and Security: The Time for Action\". The EU's comprehensive approach to security would suggest that the EU is well placed to respond to climate-related security risks. However, recent scientific research shows that the European Union has not yet developed a fully coherent policy. United Kingdom The UK's former Foreign and Commonwealth Office (FCO) (now the Foreign, Commonwealth and Development Office, FCDO) was the first to push the UN to hold a meeting in the UN Security Council on climate change and urged the UNFCCC to take action in 2007. In 2011, the then UK Department for International Development (DFID) committed to spending 30% of its aid in fragile and conflict-affected states by 2014–15. Between 2013 and 2015, the FCO had discussions on the climate, security and conflict in 'fragile' states at G8 and G7 meetings.In 2014, David Cameron noted that \"Climate change is one of the most serious threats facing our world\". A 2018 article in UK's The Independent also argued that the U.S.' Trump administration is \"putting British national security at risk\", according to over 100 climate scientists. In the same year, the former UK Special Representative for Climate Change, Rear Admiral Neil Morisetti, claimed that 'Climate change will require more deployment of the British military in conflict prevention, conflict resolution or responding to increased humanitarian requirements due to extreme weather impacts.' The UK-established International Climate Fund (ICF) identifies climate change as a 'threat multiplier' within 'fragile' states. In 2015 a report by the Joint Committee on the National Security Strategy (NSS) confirmed that climate change presents a risk to national security and that spending on security would need to adapt. On September 23, 2021, Lord Ahmad of Wimbledon, UK Minister for the United Nations stated that climate change threatened the safety of the country and",
    "label": 0
  },
  {
    "text": "presents a risk to national security and that spending on security would need to adapt. On September 23, 2021, Lord Ahmad of Wimbledon, UK Minister for the United Nations stated that climate change threatened the safety of the country and all people. Recently the United Kingdom hosted the 26th UN Climate Change Conference of the Parties (COP26) in Glasgow on October 31 – 12 November 12, 2021. The UK's 'Integrated Review of Defence, Security, Development, and Foreign Policy', published in March 2021, argues that dealing with climate change and biodiversity loss was its 'number one international priority' and identified African countries as vulnerable countries to climate change, which can amplify insecurity, migration and instability on the continent. United States In the United States, analysis of climate security and the development of policy ideas for addressing it has been led by the Center for Climate and Security, founded by Francesco Femia and Caitlin Werrell in 2011, which is now an institute of the Council on Strategic Risks. US intelligence analysts have expressed concern about the \"serious security risks\" of climate change since the 1980s. In 2007, the Council on Foreign Relations released a report titled, Climate Change and National Security: An Agenda for Action, stating that \"Climate change presents a serious threat to the security and prosperity of the United States and other countries.\" A 2012 report published by the Joint Global Change Research Institute indicated that second and third order impacts of climate change, such as migration and state stability, are of concern for the US defense and intelligence communities. A 2015 report published by the White House found that climate change puts coastal areas at risk, that a changing Arctic poses risks to other parts of the country, risk for infrastructure, and increases demands on military resources. In 2016,",
    "label": 0
  },
  {
    "text": "A 2015 report published by the White House found that climate change puts coastal areas at risk, that a changing Arctic poses risks to other parts of the country, risk for infrastructure, and increases demands on military resources. In 2016, Director of National Intelligence James Clapper noted: \"Unpredictable instability has become the 'new normal,' and this trend will continue for the foreseeable future...Extreme weather, climate change, environmental degradation, rising demand for food and water, poor policy decisions and inadequate infrastructure will magnify this instability.\" A 2015 Pentagon report pointed out how climate denial threatens national security. In 2017, the Trump administration removed climate change from its national security strategy. But in January 2019 the Pentagon released a report stating that climate change is a national security threat to USA. In June 2019, in the course of House Select Committee on Intelligence hearings on the national security implications of climate change, the White House blocked the submission of a statement by the State Department's Bureau of Intelligence and Research Office, and the analyst who wrote the statement resigned. The idea of creating a presidential committee on climate security has been proposed. As part of the United States National Defense Authorization Act the U.S. Congress asked the Department of Defense for a report on climate matters. The report was published in 2019, and notes, \"The effects of a changing climate are a national security issue with potential impacts to Department of Defense (DoD or the Department) missions, operational plans, and installations.\" In 2021, United States President Biden declared climate change a national security priority. See also Climate resilience Energy security Environmental monitoring Food security Water conflict Water security References External links Global Military Advisory Council On Climate Change How Climate Change Became a National Security Problem (Wired, 2015) National Security in the",
    "label": 0
  },
  {
    "text": "A climate spiral (sometimes referred to as a temperature spiral) is an animated data visualization graphic designed as a \"simple and effective demonstration of the progression of global warming\", especially for general audiences. The original climate spiral was published on 9 May 2016 by British climate scientist Ed Hawkins to portray global average temperature anomaly (change) since 1850. The visualization graphic has since been expanded to represent other time-varying quantities such as atmospheric CO2 concentration, carbon budget, and arctic sea ice volume. Background Hawkins credited a \"Friday afternoon\" email from Norwegian climatologist Jan Fuglestvedt for the idea of converting a conventional coloured line chart into a spiral, and thanked Fuglestvedt's wife, Taran Fæhn, for having suggested it to Fuglestvedt. Fæhn, a researcher for Statistics Norway and the Oslo Centre for Research on Environmentally Friendly Energy, had suggested that connecting December to the following January would show temperature evolution in a more dynamic way. Ensuing email discussions refined the design of the climate spiral, which Hawkins published on Monday 9 May 2016. Dissemination Expecting \"only some vague interest\", Hawkins later wrote that his tweet of the new graphic had been viewed 3.4 million times in its first year. The tweeted graphic is widely described as having gone viral. Within a day, Climate Central writer Andrea Thompson remarked in The Guardian that the \"metaphoric spiral\" of the planet spiraling toward catastrophic consequences \"has become a literal one\". Initially, Hawkins posited that the graphic resonated because it \"doesn’t require any complex interpretation\". In a 2019 paper, Hawkins et al. further surmised that the design and communication aspects of the graphic resonated with viewers for a variety of reasons, including: its selection to graph temperature (a quantity that the public feels is relevant and understandable), its production by scientists (who tend to be viewed",
    "label": 0
  },
  {
    "text": "design and communication aspects of the graphic resonated with viewers for a variety of reasons, including: its selection to graph temperature (a quantity that the public feels is relevant and understandable), its production by scientists (who tend to be viewed as \"trusted messengers\"), its being intuitive and eye-catching (not a \"boring\" scientific graph), its similarity to a clock (which is normally regular and predictable but which provides a \"visual surprise\" at the end, portraying the \"fortuitous\" large temperature increases encountered very recently), its animated nature (not a static graph), and its short duration (holding viewers' attention). The paper further noted that sharing the graphic on social media allowed it to be \"consumed within the social media bubble rather than requiring a journey to another website\", allowing it to be \"subsequently amplified by journalists, the media, and highly popular accounts\". Content Climate spirals use changing distance from a center point to represent change of a dependent variable (archetypically, global average temperature). The independent variable, time, is broken down into months (represented by constantly changing rotational angle about the center point) and years (line colour that evolves as years pass). Hawkins explained that in his implementation, colours represent time: purple for early years, through blue, green to yellow for most recent years. He made the graphics in MATLAB and used the \"viridis\" colour scale, conscious of choosing a colour scale that makes the graphics legible to colour blind viewers. As the graphic includes red concentric circles denoting temperature changes of 1.5°C and 2.0°C, Hawkins explained that \"the relationship between current global temperatures and the internationally discussed target limits is also clear without much complex interpretation needed\". Ria Misra wrote in Gizmodo that the graphic \"lets the noise of tiny variations fade into the background while still showcasing, very simply, the undeniable trend\".",
    "label": 0
  },
  {
    "text": "and the internationally discussed target limits is also clear without much complex interpretation needed\". Ria Misra wrote in Gizmodo that the graphic \"lets the noise of tiny variations fade into the background while still showcasing, very simply, the undeniable trend\". Describing how global warming \"appears to burst outward toward the end of the animation\", Hawkins described how sulfate aerosols no longer counteracted the warming effect greenhouse gases after the 1970s, and noted how strong El Niño events in 1998 and 2016 lead to higher temperature plateaus more recently. The first climate spiral portrayed data from HadCRUT4.4 from January 1850 – March 2016, graphing relative to the 1850-1900 mean temperature, the same pre-industrial global average used in the IPCC Fifth Assessment Report. Applications and influence Three days after the first climate spiral was published, it was the subject of an article on the U.S Department of State's ShareAmerica website, noting the \"compelling\" graphic \"shows just how fast\" \"world temperatures are spiraling upward\". A climate spiral was featured in the opening ceremony of the August 2016 Summer Olympics. The design was on the shortlist for the Kantar Information is Beautiful Awards 2016. In January 2017, \"Spiraling Global Temperatures\" was nominated for the Shorty Awards GIF of the Year. In January 2017, the spiral was tweeted by Bernie Sanders and the U.S. National Park Service, both conveying how almost all recorded warmest years have been recent years. A 2022 study emphasized the importance of user-centered design in climate data visualizations, highlighting tools like the climate spiral as effective means to enhance public understanding of climate change. Extensions of the climate spiral concept In May 2016 United States Geological Survey scientist Jay Alder extended Hawkins' historical spiral to the year 2100, creating a predictive spiral graphic showing a possible future trajectory of global warming",
    "label": 0
  },
  {
    "text": "of climate change. Extensions of the climate spiral concept In May 2016 United States Geological Survey scientist Jay Alder extended Hawkins' historical spiral to the year 2100, creating a predictive spiral graphic showing a possible future trajectory of global warming given the then-current carbon emission trend. Hawkins extended his two-dimensional spiral design to a three-dimensional version in which the graphic appears as an expanding cone-shaped structure. Hawkins' original climate spiral application (global average temperature change) has been expanded to represent other time-varying quantities such as atmospheric CO2 concentration, carbon budget, and arctic sea ice volume. Critical response The day of the climate spiral's first publication (9 May 2016), Brad Plumer wrote in Vox that the \"mesmerizing\" GIF was \"one of the clearest visualizations of global warming\" he had ever seen. The following day (10 May), Jason Samenow wrote in The Washington Post that the spiral graph was \"the most compelling global warming visualization ever made\", and, likewise, former Climate Central senior science writer Andrew Freedman wrote in Mashable that it was \"the most compelling climate change visualization we’ve ever seen\". Michael E. Mann, creator of the hockey stick graph, said that the spiral graphic was \"an interesting and worthwhile approach to representing the data graphically\", and PRI's Timothy McGrath wrote that the spiral was \"a simple, elegant illustration of a dark history and a potentially terrifying future\". In BusinessGreen, environmental journalist James Murray praised the graphic's \"elegant simplicity\", asserting that \"the mistakes, misinterpretations, and misinformation contained in so many climate sceptic arguments are steamrollered by the straightforward force of this spiral\". On 11 May, Chris Mooney wrote in The Washington Post that, with his \"startling animation\" Hawkins had \"hit a grand slam—and not through some clever turn of phrase or some new metaphor or framing, but rather, through viral data",
    "label": 0
  },
  {
    "text": "spiral\". On 11 May, Chris Mooney wrote in The Washington Post that, with his \"startling animation\" Hawkins had \"hit a grand slam—and not through some clever turn of phrase or some new metaphor or framing, but rather, through viral data visualization\". Sarah Rense, first writing in Esquire that \"climate science is unpalatable\" and \"depressing data\", characterized the new graphic as being \"as mesmerizing as it is depressing\", \"a cool GIF with pretty colours (to which) people will pay attention\". In late May 2016, Brian Kahn, communications coordinator at the International Research Institute for Climate and Society, wrote that the spiral was a \"revolutionary new way to look at global temperatures\" and posited that the graphic's popularity \"can be attributed in part to its hypnotic nature and the visceral way it shows the present predicament of climate change\". In July 2016, freelance journalist Chelsea Harvey wrote in The Washington Post that, \"at a time when climate science communication efforts are often viewed as dense or difficult for general audiences to understand, these types of striking graphics may help climate scientists connect with the public in a way that is both clear and attention-grabbing\". Two years later, in May 2018 Jason Samenow commented that, though many Hawkins visualizations had \"resonated among science communicators\", climate spirals were the scientist's best-known visualizations. After his 2016 development of the climate spiral, Hawkins received the Royal Meteorological Society’s 2017 Climate Science Communication Prize. After developing the warming stripes graphic in May 2018, Hawkins, a lead author for the IPCC 6th Assessment Report, received the Royal Society's 2018 Kavli Medal \"for significant contributions to understanding and quantifying natural climate variability and long-term climate change, and for actively communicating climate science and its various implications with broad audiences\". In a 2019 paper, Hawkins et al. acknowledged that a",
    "label": 0
  },
  {
    "text": "2018 Kavli Medal \"for significant contributions to understanding and quantifying natural climate variability and long-term climate change, and for actively communicating climate science and its various implications with broad audiences\". In a 2019 paper, Hawkins et al. acknowledged that a \"possible scientific criticism\" of the design was that uncertainty in the temperature data is not visualized. Also, viewers might interpret the area of the 1.5°C and 2.0°C circles as representing the temperature change rather than the radius of the graphed line itself, though noting the temperature limit circles are clearly labeled. In May 2017 Hawkins responded to a criticism that the human eye might incorrectly interpret the change in area within the spiral rather than the change in radius by noting that the radii are clearly labeled. Informally, climate spirals have been referred to as spirographs. See also Climate change – Human-caused changes to climate on Earth Climate change art – Art inspired by climate change Craftivism – Form of activism centered on practices of craft Data and information visualization – Visual representation of data Environmental communication – Type of communication Scientific consensus on climate change – Evaluation of climate change by the scientific community The Tempestry Project – Arts project promoting climate change awareness Warming stripes – Data visualization graphics depicting trends of annual temperature changes Notes References Further reading Windhager, Florian; Schreder, Günther; Mayr, Eva (2019). \"On Inconvenient Images: Exploring the Design Space of Engaging Climate Change Visualizations for Public Audiences\". Workshop on Visualisation in Environmental Sciences (EnvirVis). The Eurographics Association: 1–8. doi:10.2312/envirvis.20191098. ISBN 9783038680864. — Survey of climate change visualizations \"Carbon Budget / CO2 Concentration / Global Mean Temperature\". openclimatedata.net. Potsdam Institute for Climate Impact Research. (List of available spirals)— Synchronized side-by-side graphics of the progression of these quantities External links Climate spiral at the Museum of",
    "label": 0
  },
  {
    "text": "Coastal flooding occurs when dry and low-lying land is submerged (flooded) by seawater. The range of a coastal flooding is a result of the elevation of floodwater that penetrates the inland which is controlled by the topography of the coastal land exposed to flooding. The seawater can flood the land via several different paths: direct flooding, overtopping or breaching of a barrier. Coastal flooding is largely a natural event. Due to the effects of climate change (e.g. sea level rise and an increase in extreme weather events) and an increase in the population living in coastal areas, the damage caused by coastal flood events has intensified and more people are being affected. Coastal areas are sometimes flooded by unusually high tides, such as spring tides, especially when compounded by high winds and storm surges. This was the cause of the North Sea flood of 1953 which flooded large swathes of the Netherlands and the East coast of England. When humans modify the coastal environment this can make coastal flooding worse. Extraction of water from groundwater reservoirs in the coastal zone can instigate subsidence of the land, thus increasing the risk of flooding. Engineered protection structures along the coast, such as sea walls, alter the natural processes of the beach. This can lead to erosion on adjacent stretches of the coast which also increases the risk of flooding. Reduction and control of coastal flooding is carried out using structural methods to hold back or redirect flood waters. Non-structural methods include coastal management, behavioral and institutional response to adapt to the processes. Natural defenses include physical features like gravel bars and sand dune systems, but also ecosystems such as salt marshes, seagrass and mangrove forests which have a buffering function. Mangroves, wetlands and seagrass meadows are often considered to provide significant protection",
    "label": 0
  },
  {
    "text": "Natural defenses include physical features like gravel bars and sand dune systems, but also ecosystems such as salt marshes, seagrass and mangrove forests which have a buffering function. Mangroves, wetlands and seagrass meadows are often considered to provide significant protection against storm waves, tsunamis, and shoreline erosion through their ability to attenuate wave energy. To protect the coastal zone from flooding, the natural defenses should, therefore, be protected and maintained in for example Marine Protected Areas (MPAs). Types The seawater can flood the land via several different paths: Direct flooding — where the sea height exceeds the elevation of the land, often where waves have not built up a natural barrier such as a dune Overtopping of a barrier — the barrier may be natural or human-engineered and overtopping occurs due to swelling conditions during storms or high tides often on open stretches of the coast. The height of the waves exceeds the height of the barrier and water flows over the top of the barrier to flood the land behind it. Overtopping can result in high velocity flows that can erode significant amounts of the land surface which can undermine defense structures. Breaching of a barrier — again the barrier may be natural (sand dune) or human-engineered (sea wall), and breaching occurs on open coasts exposed to large waves. Breaching occurs when the barrier is broken down or destroyed by waves allowing the seawater to extend inland and flood the areas Causes Coastal flooding can result from a variety of different causes including storm surges created by storms like hurricanes and tropical cyclones, rising sea levels due to climate change and tsunamis. Storms and storm surges Storms, including hurricanes and tropical cyclones, can cause flooding through storm surges which are waves significantly larger than normal. If a storm event",
    "label": 0
  },
  {
    "text": "hurricanes and tropical cyclones, rising sea levels due to climate change and tsunamis. Storms and storm surges Storms, including hurricanes and tropical cyclones, can cause flooding through storm surges which are waves significantly larger than normal. If a storm event coincides with the high astronomical tide, extensive flooding can occur. Storm surges involve three processes: wind setup barometric setup wave setup Wind blowing in an onshore direction (from the sea towards the land) can cause the water to 'pile-up' against the coast; this is known as wind setup. Low atmospheric pressure is associated with storm systems and this tends to increase the surface sea level; this is a barometric setup. Finally increased wave breaking height results in a higher water level in the surf zone, which is wave setup. These three processes interact to create waves that can overtop natural and engineered coastal protection structures thus penetrating seawater further inland than normal. Sea level rise Tidal flooding Tsunami waves Coastal areas can be significantly flooded as the result of tsunami waves which propagate through the ocean as the result of the displacement of a significant body of water through earthquakes, landslides, volcanic eruptions, and glacier calvings. There is also evidence to suggest that significant tsunami have been caused in the past by meteor impact into the ocean. Tsunami waves are so destructive due to the velocity of the approaching waves, the height of the waves when they reach land, and the debris the water entrains as it flows over land can cause further damage. Depending on the magnitude of the tsunami waves and floods, it could cause severe injuries which call for precautionary interventions that prevent overwhelming aftermaths. It was reported that more than 200,000 people were killed in the earthquake and subsequent tsunami that hit the Indian Ocean, on",
    "label": 0
  },
  {
    "text": "tsunami waves and floods, it could cause severe injuries which call for precautionary interventions that prevent overwhelming aftermaths. It was reported that more than 200,000 people were killed in the earthquake and subsequent tsunami that hit the Indian Ocean, on December 26, 2004. Not to mention, several diseases are a result of floods ranging from hypertension to chronic obstructive pulmonary diseases. Impacts Social and economic impacts The coastal zone (the area both within 100 kilometres distance of the coast and 100 metres elevation of sea level) is home to a large and growing proportion of the global population. Over 50 percent of the global population and 65 percent of cities with populations over five million people are in the coastal zone. In addition to the significant number of people at risk of coastal flooding, these coastal urban centres are producing a considerable amount of the global Gross Domestic Product (GDP). People's lives, homes, businesses, and city infrastructure like roads, railways, and industrial plants are all at risk of coastal flooding with massive potential social and economic costs. The recent earthquakes and tsunami in Indonesia in 2004 and in Japan in March 2011 clearly illustrate the devastation coastal flooding can produce. Indirect economic costs can be incurred if economically important sandy beaches are eroded resulting in a loss of tourism in areas dependent on the attractiveness of those beaches. Environmental impacts Coastal flooding can result in a wide variety of environmental impacts on different spatial and temporal scales. Flooding can destroy coastal habitats such as coastal wetlands and estuaries and can erode dune systems. These places are characterized by their high biological diversity therefore coastal flooding can cause significant biodiversity loss and potentially species extinctions. In addition to this, these coastal features are the coasts natural buffering system against storm waves;",
    "label": 0
  },
  {
    "text": "erode dune systems. These places are characterized by their high biological diversity therefore coastal flooding can cause significant biodiversity loss and potentially species extinctions. In addition to this, these coastal features are the coasts natural buffering system against storm waves; consistent coastal flooding and sea-level rise can cause this natural protection to be reduced allowing waves to penetrate greater distances inland exacerbating erosion and furthering coastal flooding. \"By 2050, \"moderate\" (typically damaging) flooding is expected to occur, on average, more than 10 times as often as it does today, and can be intensified by local factors.\" Prolonged inundation of seawater after flooding can also cause salination of agriculturally productive soils thus resulting in a loss of productivity for long periods of time. The effects of the soil salinization, which is brought on by sea levels rising and differed precipitation patterns, impacts agricultural production, ultimately leading towards food and water shortages. Food crops and forests can be completely killed off by salination of soils or wiped out by the movement of floodwaters. Coastal freshwater bodies including lakes, lagoons, and coastal freshwater aquifers can also be affected by saltwater intrusion. This can destroy these water bodies as habitats for freshwater organisms and sources of drinking water for towns and cities. Reduction and control Flood control Non-structural mechanisms If human systems are affected by flooding, an adaption to how that system operates on the coast through behavioral and institutional changes is required, these changes are the so-called non-structural mechanisms of coastal flooding response. Building regulations, coastal hazard zoning, urban development planning, spreading the risk through insurance, and enhancing public awareness are some ways of achieving this. Adapting to the risk of flood occurrence can be the best option if the cost of building defense structures outweighs any benefits or if the natural processes",
    "label": 0
  },
  {
    "text": "risk through insurance, and enhancing public awareness are some ways of achieving this. Adapting to the risk of flood occurrence can be the best option if the cost of building defense structures outweighs any benefits or if the natural processes in that stretch of coastline add to its natural character and attractiveness. A more extreme and often difficult to accept the response to coastal flooding is abandoning the area (also known as managed retreat) prone to flooding. This however raises issues for where the people and infrastructure affected would go and what sort of compensation should/could be paid. Engineered defenses There are a variety of ways in which humans are trying to prevent the flooding of coastal environments, typically through so-called hard engineering structures such as flood barriers, seawalls and levees. That armouring of the coast is typical to protect towns and cities which have developed right up to the beachfront. Enhancing depositional processes along the coast can also help prevent coastal flooding. Structures such as groynes, breakwaters, and artificial headlands promote the deposition of sediment on the beach thus helping to buffer against storm waves and surges as the wave energy is spent on moving the sediments in the beach than on moving water inland. Natural defenses Coastal areas do provide natural protective structures to guard against coastal flooding. These include physical features like gravel bars and sand dune systems, but also ecosystems such as salt marshes, seagrass and mangrove forests have a buffering function. Mangroves, wetlands and seagrass meadows are often considered to provide significant protection against storm waves, tsunamis, and shoreline erosion through their ability to attenuate wave energy. To protect the coastal zone from flooding, the natural defenses should, therefore, be protected and maintained in for example Marine Protected Areas (MPAs). Longer term aspects and research",
    "label": 0
  },
  {
    "text": "waves, tsunamis, and shoreline erosion through their ability to attenuate wave energy. To protect the coastal zone from flooding, the natural defenses should, therefore, be protected and maintained in for example Marine Protected Areas (MPAs). Longer term aspects and research Reducing global sea level rise is one way to prevent significant flooding of coastal areas. This could be minimised by further reducing greenhouse gas emissions. However, even if significant emission decreases are achieved, there is already a substantial amount of sea level rise into the future. International climate change policies like the Paris Agreement are seeking to mitigate the future effects of climate change, including sea level rise. In addition, more immediate measures of engineered and natural defenses are put in place to prevent coastal flooding. Examples Examples of countries with existing coastal flooding problems include: The Netherlands: Flood control in the Netherlands Bangladesh: Floods in Bangladesh Great Britain: The Thames Barrier is one of the world's largest flood barriers and serves to protect London from flooding during exceptionally high tides and storm surges. The Barrier can be lifted at high tide to prevent sea waters flooding London and can be lowered to release stormwater runoff from the Thames catchment. New Zealand: Flooding of the low-lying coastal zone South Canterbury Plains in New Zealand can result in prolonged inundation, which can affect the productivity of the affected pastoral agriculture for several years. The Caribbean: Rising sea levels, population growth, and extreme climate events such as El Niño are projected to expose over 9 million people and extensive land and infrastructure to coastal flooding by the end of the 21st century, highlighting the urgent need for adaptation and risk reduction strategies. Hurricane Katrina in New Orleans Hurricane Katrina made landfall as a category 3 cyclone on the Saffir–Simpson hurricane wind scale,",
    "label": 0
  },
  {
    "text": "to coastal flooding by the end of the 21st century, highlighting the urgent need for adaptation and risk reduction strategies. Hurricane Katrina in New Orleans Hurricane Katrina made landfall as a category 3 cyclone on the Saffir–Simpson hurricane wind scale, indicating that it had become an only moderate level storm. However, the catastrophic damage caused by the extensive flooding was the result of the highest recorded storm surges in North America. For several days prior to the landfall of Katrina, wave setup was generated by the persistent winds of the cyclonic rotation of the system. This prolonged wave set up coupled with the very low central pressure level meant massive storm surges were generated. Storm surges overtopped and breached the levees and floodwalls intended to protect the city from inundation. Unfortunately, New Orleans is inherently prone to coastal flooding for a number of factors. Firstly, much of New Orleans is below sea level and is bordered by the Mississippi River therefore protection against flooding from both the sea and the river has become dependent on engineered structures. Land-use change and modification to natural systems in the Mississippi River have rendered the natural defenses for the city less effective. Wetland loss has been calculated to be around 1,900 square miles (4,920 square kilometres) since 1930. This is a significant amount as four miles of wetland are estimated to reduce the height of a storm surge by one foot (30 centimeters). Indonesia and Japan earthquake-related tsunamis 2004 Indian Ocean earthquake and tsunami: An earthquake of approximately magnitude 9.0 struck off the coast of Sumatra, Indonesia causing the propagation of a massive tsunami throughout the Indian Ocean. This tsunami caused significant loss of human life, an estimate of 280,000 – 300,000 people has been reported and caused extensive damage to villages, towns, and",
    "label": 0
  },
  {
    "text": "Deforestation is a primary contributor to climate change, and climate change affects the health of forests. Land use change, especially in the form of deforestation, is the second largest source of carbon dioxide emissions from human activities, after the burning of fossil fuels. Greenhouse gases are emitted from deforestation during the burning of forest biomass and decomposition of remaining plant material and soil carbon. Global models and national greenhouse gas inventories give similar results for deforestation emissions. As of 2019, deforestation is responsible for about 11% of global greenhouse gas emissions. Carbon emissions from tropical deforestation are accelerating. When forests grow they are a carbon sink and therefore have potential to mitigate the effects of climate change. Some of the effects of climate change, such as more wildfires, invasive species, and more extreme weather events can lead to more forest loss. The relationship between deforestation and climate change is one of a positive (amplifying) climate feedback. The more trees that are removed equals larger effects of climate change which, in turn, results in the loss of more trees. Forests cover 31% of the land area on Earth. Every year, 75,700 square kilometers (18.7 million acres) of the forest is lost. There was a 12% increase in the loss of primary tropical forests from 2019 to 2020. Deforestation has many causes and drivers. Examples include agricultural clearcutting, livestock grazing, logging for timber, and wildfires. Causes of deforestation Causes not linked to climate change Causes due to climate change Effects of deforestation on climate change aspects Irreversible deforestation would result in a permanent rise in the global surface temperature. Moreover, it suggests that standing tropical forests help cool the average global temperature by more than 1 °C or 1.8 °F. Deforestation of tropical forests may risk triggering tipping points in the climate",
    "label": 0
  },
  {
    "text": "permanent rise in the global surface temperature. Moreover, it suggests that standing tropical forests help cool the average global temperature by more than 1 °C or 1.8 °F. Deforestation of tropical forests may risk triggering tipping points in the climate system and of forest ecosystem collapse which would also have effects on climate change. Several studies since the early 1990s have shown that large-scale deforestation north of 50°N leads to overall net global cooling while tropical deforestation produces substantial warming. Carbon-centric metrics are inadequate because biophysical mechanisms other than CO2 impacts are important, especially the much higher albedo of bare high-latitude ground vis-à-vis intact forest. Deforestation, particularly in large swaths of the Amazon, where nearly 20% of the rainforest has been clear cut, has climactic effects and effects on water sources as well as on the soil. Moreover, the type of land usage after deforestation also produces varied results. When deforested land is converted to pasture land for livestock grazing it has a greater effect on the ecosystem than forest to cropland conversions. Other effect of deforestation in the Amazon rainforest is seen through the greater amount of carbon dioxide emission. The Amazon rainforest absorbs one-fourth of the carbon dioxide emissions on Earth, however, the amount of CO2 absorbed today decreases by 30% than it was in the 1990s due to deforestation. Modeling studies have concluded that there are two crucial moments that can lead to devastating effects in the Amazon rainforest which are increase in temperature by 4 °C or 7.2 °F and deforestation reaching a level of 40%. Forest fires Statistics have shown that there is a direct correlation between forest fires and deforestation. Statistics regarding the Brazilian Amazon area during the early 2000s have shown that fires and the air pollution that accompanies these fires mirror the",
    "label": 0
  },
  {
    "text": "Forest fires Statistics have shown that there is a direct correlation between forest fires and deforestation. Statistics regarding the Brazilian Amazon area during the early 2000s have shown that fires and the air pollution that accompanies these fires mirror the patterns of deforestation and \"high deforestation rates led to frequent fires\". The Amazon rainforest has recently experienced fires that occurred inside the forest when wildfires tend to occur on the outer edges of the forest. Wetlands have faced an increase in forest fires as well. Due to the change in temperature, the climate around forests have become warm and dry, conditions that allow forest fires to occur. Under unmitigated climate change, by the end of the century, 21% of the Amazon would be vulnerable to post‐fire grass invasion. In 3% of the Amazon, fire return intervals are already shorter than the time required for grass exclusion by canopy recovery, implying a high risk of irreversible shifts to a fire‐maintained degraded forest grassy state. The south‐eastern region of the Amazon is currently at highest risk of irreversible degradation. According to a study in tropical peatland forest of Borneo, deforestation also contributes to the increase in fire risk. Carbon sequestration through forestry Concerns with forestry projects Changes in rainfall As a consequence of reduced evapotranspiration, precipitation is also reduced. This implies having a hotter and drier climate, and a longer dry season. This change in climate has drastic ecological and global impacts including increases in severity and frequency of fires, and disruption in the pollination process that will likely spread beyond the area of deforestation. According to a study published in 2023, tropical deforestation has led to a significant decrease in the amount of observed precipitation. By the year 2100, researchers anticipate that deforestation in the Congo will diminish regional precipitation levels",
    "label": 0
  },
  {
    "text": "area of deforestation. According to a study published in 2023, tropical deforestation has led to a significant decrease in the amount of observed precipitation. By the year 2100, researchers anticipate that deforestation in the Congo will diminish regional precipitation levels by up to 8-10%. Decreasing albedo Deforestation changes the landscape and reflectivity of earth's surface, i.e. decreasing Albedo. This results in an increase in the absorption of light energy from the sun in the form of heat, enhancing global warming. Policies and programs to reduce deforestation Reducing emissions from deforestation and forest degradation in developing countries The Bali Action Plan The Bali Action Plan was developed in December 2007 in Bali, Indonesia. It is a direct result of the Kyoto Protocol of December 1997. One of the key elements of The Bali Action Plan involves a concerted effort by the member countries of the Kyoto Protocol to enact and create policy approaches that incentivize emissions reduction caused by deforestation and forest degradation in the developing world. It emphasized the importance of sustainable forest management and conservation practices in mitigating climate change. This coupled with the increased attention to carbon emission stocks as a way to provide additional resource flows to the developing countries. Trillion Tree Campaign The Billion Tree Campaign was launched in 2006 by the United Nations Environment Programme (UNEP) as a response to the challenges of climate change, as well as to a wider array of sustainability challenges, from water supply to biodiversity loss. Its initial target was the planting of one billion trees in 2007. Only one year later in 2008, the campaign's objective was raised to 7 billion trees—a target to be met by the climate change conference that was held in Copenhagen, Denmark in December 2009. Three months before the conference, the 7 billion planted",
    "label": 0
  },
  {
    "text": "The Dry Corridor or Central American Dry Corridor (CADC) is a tropical dry forest region on the Pacific Coast of Central America. This area, which extends from southern Mexico to Panama, is extremely vulnerable to climate change due to much of the population living in rural areas and in poverty, and thus dependent on grain crops for their livelihood. Climate change's effects on the region have increased the severity of droughts because of increased evaporation, reduced soil moisture, and salinization of freshwater resources. Particularly vulnerable to climate change are the areas of Guatemala, El Salvador, and Honduras. Since 2001, these areas have suffered from irregular drought patterns due to changes in the El Niño-Southern Oscillation (ENSO). During an El Niño event in 2009 (the year the term \"Dry Corridor\" was penned), it is estimated that 50-100% of crops in these regions were affected by the water deficit, and between 2014 and 2016, millions of people in the dry corridor needed food aid due to drought during this period, which resulted in losses of the corn crop. By 2018, experts estimated that at least 25% of households in the region experienced food insecurity. As a result, relief agencies have been advised to take a \"Food first\" response when addressing this crisis, focusing initially on areas at highest risk of food insecurity. History Drought impact has been especially severe in Honduras and Guatemala. Spain colonized Central America in the 16th Century, with ambitions of wealth, religious conversion, and territorial accumulation. The Audiencia of Guatemala was established in 1543 and encompassed present-day Guatemala, Honduras, Nicaragua, and Costa Rice. This region was under Spanish control until independence movements in the early 19th century. Ultimately, this colonization conquest decreased Indigenous populations and introduced forced labor. Currently, 90% of Central America's population lives in the Dry",
    "label": 0
  },
  {
    "text": "Honduras, Nicaragua, and Costa Rice. This region was under Spanish control until independence movements in the early 19th century. Ultimately, this colonization conquest decreased Indigenous populations and introduced forced labor. Currently, 90% of Central America's population lives in the Dry Corridor. 80% of people living in the Dry Corridor experience poverty. Drought and Climate Change There is strong evidence linking climate change to severe drought events. Land and ocean evaporation increases as global warming occurs, reducing soil moisture. Similarly, global warming is expected to aggravate precipitation patterns. Moreover, projections indicate that agricultural droughts are projected to increase because, with more evaporation, there will be less runoff. Global warming has led to rising sea levels, thus salinizing aquifers and estuaries, making this water unavailable. As a result, water supply droughts will occur, as people will need to depend more on other freshwater sources, like groundwater. Ultimately, climate change leads to more variability and severity in droughts. Specifically, people living in the Dry Corridor are heavily dependent on agriculture. Droughts affect water resources and thus agricultural yields. 2019 was the fifth straight year of drought, and the \"second consecutive year of failed yields for subsistence farmers.\" The region's climate is increasingly characterized by higher temperatures and prolonged dry periods. Concurrently, agricultural pests have proliferated, seasonal rains have become less predictable or entirely absent, and the frequency and intensity of flooding events have escalated. An effort to aid farmers in this climate crisis is led by the UN Environment Programme. They are working with farmers throughout the region to boost the resilience of rural communities and improve food security. Collaboration with the Central American Integration System and the Food and Agriculture Organization of the United Nations is helping to reverse land degradation by planting trees and sharing resources. Climate Migration Approximately 8% of",
    "label": 0
  },
  {
    "text": "rural communities and improve food security. Collaboration with the Central American Integration System and the Food and Agriculture Organization of the United Nations is helping to reverse land degradation by planting trees and sharing resources. Climate Migration Approximately 8% of families in the region report that they plan to migrate in an attempt to improve their situations, with the increase in emigration of \"500% between 2010 and 2015.\" A World Bank report projects that up to 4 million people from Central America and Mexico will become climate change migrants, people who are displaced due to climate-related weather events, by 2050 if measures are not taken to prevent climate change and adapt agricultural practices. Migrants typically first travel to nearby urban areas, with fewer continuing north to Mexico, and fewer still traveling all the way to the USA border. Climate migration such as that seen in the Dry Corridor is one of the sources of conflict at the US-Mexico border. While some U.S. political figures have described recent migrations in inflammatory terms, officials from U.S. Customs and Border Protection have acknowledged that environmental factors—particularly crop failure—are among the primary drivers of migration from the region. The United States has played a significant role in the global climate patterns that underlie these migration pressures. Since 1850, the US has been the top contributor to climate change, creating 25% of global greenhouse gas emissions, including carbon dioxide, methane, and nitrous oxide. Transportation is the largest contributing factor, followed by electric power. In comparison, El Salvador and Nicaragua, Honduras, and Guatemala are responsible for .01, .02, and .03 percent if historical greenhouse gas emissions, respectively. Political theorists and ethicists argue that climate migrants should be allowed entry into the United States as compensation for their displacement and because the US, through its significant contribution",
    "label": 0
  },
  {
    "text": ".01, .02, and .03 percent if historical greenhouse gas emissions, respectively. Political theorists and ethicists argue that climate migrants should be allowed entry into the United States as compensation for their displacement and because the US, through its significant contribution to climate change, has harmed those seeking shelter. Climate Justice Climate Injustice is the unequal distribution of climate change-related consequences, like health or weather event impacts. It is a global human rights movement aiming to create a more equitable, sustainable future. The effects of climate change disproportionately affect historically marginalized people, such as Indigenous or poverty-stricken communities. According to the World Food Programme, the climate induced drought and resulting climate migration in the Dry Corridor is a case of climate injustice. Climate and weather variability exacerbate social issues, including food insecurity, poverty, and inequality. Socioeconomic components, like income from agriculture or household wealth status, impact a person's conflict due to climate-related events. Stakeholders will need to be address improvement from multiple angles, including providing short-term food security and assistance, addressing climate change on a global scale, and sustainable development initiatives to promote robust crop production in these areas facing new climates. Ultimately, according to the University of Manchester, as the largest contributor to climate change (92%), it is the Global North's responsibility to take steps to mitigate climate change. The Industrial Revolution led up to this point, as burning fossil fuels led to economic growth for countries that had the resources to industrialize. Now, however, it is difficult for many of these countries to provide necessary energy and resources to their citizens in a manner where they will continue to grow their GDP and reduce emissions. Developing countries, like those located in the Dry Corridor, are the ones who have to deal with the consequences of climate change. They are",
    "label": 0
  },
  {
    "text": "This is a glossary for the terminology often encountered in undergraduate quantum mechanics courses. Cautions: Different authors may have different definitions for the same term. The discussions are restricted to Schrödinger picture and non-relativistic quantum mechanics. Notation: | x ⟩ {\\displaystyle |x\\rangle } - position eigenstate | α ⟩ , | β ⟩ , | γ ⟩ . . . {\\displaystyle |\\alpha \\rangle ,|\\beta \\rangle ,|\\gamma \\rangle ...} - wave function of the state of the system Ψ {\\displaystyle \\Psi } - total wave function of a system ψ {\\displaystyle \\psi } - wave function of a system (maybe a particle) ψ α ( x , t ) {\\displaystyle \\psi _{\\alpha }(x,t)} - wave function of a particle in position representation, equal to ⟨ x | α ⟩ {\\displaystyle \\langle x|\\alpha \\rangle } Formalism Kinematical postulates a complete set of wave functions A basis of the Hilbert space of wave functions with respect to a system. bra The Hermitian conjugate of a ket is called a bra. ⟨ α | = ( | α ⟩ ) † {\\displaystyle \\langle \\alpha |=(|\\alpha \\rangle )^{\\dagger }} . See \"bra–ket notation\". Bra–ket notation The bra–ket notation is a way to represent the states and operators of a system by angle brackets and vertical bars, for example, | α ⟩ {\\displaystyle |\\alpha \\rangle } and | α ⟩ ⟨ β | {\\displaystyle |\\alpha \\rangle \\langle \\beta |} . Density matrix Physically, the density matrix is a way to represent pure states and mixed states. The density matrix of pure state whose ket is | α ⟩ {\\displaystyle |\\alpha \\rangle } is | α ⟩ ⟨ α | {\\displaystyle |\\alpha \\rangle \\langle \\alpha |} . Mathematically, a density matrix has to satisfy the following conditions: Tr ⁡ ( ρ ) = 1 {\\displaystyle \\operatorname {Tr}",
    "label": 0
  },
  {
    "text": "α ⟩ {\\displaystyle |\\alpha \\rangle } is | α ⟩ ⟨ α | {\\displaystyle |\\alpha \\rangle \\langle \\alpha |} . Mathematically, a density matrix has to satisfy the following conditions: Tr ⁡ ( ρ ) = 1 {\\displaystyle \\operatorname {Tr} (\\rho )=1} ρ † = ρ {\\displaystyle \\rho ^{\\dagger }=\\rho } Density operator Synonymous to \"density matrix\". Dirac notation Synonymous to \"bra–ket notation\". Hilbert space Given a system, the possible pure state can be represented as a vector in a Hilbert space. Each ray (vectors differ by phase and magnitude only) in the corresponding Hilbert space represent a state. Ket A wave function expressed in the form | a ⟩ {\\displaystyle |a\\rangle } is called a ket. See \"bra–ket notation\". Mixed state A mixed state is a statistical ensemble of pure state. criterion: Normalizable wave function A wave function | α ′ ⟩ {\\displaystyle |\\alpha '\\rangle } is said to be normalizable if ⟨ α ′ | α ′ ⟩ < ∞ {\\displaystyle \\langle \\alpha '|\\alpha '\\rangle <\\infty } . A normalizable wave function can be made to be normalized by | a ′ ⟩ → α = | α ′ ⟩ ⟨ α ′ | α ′ ⟩ {\\displaystyle |a'\\rangle \\to \\alpha ={\\frac {|\\alpha '\\rangle }{\\sqrt {\\langle \\alpha '|\\alpha '\\rangle }}}} . Normalized wave function A wave function | a ⟩ {\\displaystyle |a\\rangle } is said to be normalized if ⟨ a | a ⟩ = 1 {\\displaystyle \\langle a|a\\rangle =1} . Pure state A state which can be represented as a wave function / ket in Hilbert space / solution of Schrödinger equation is called pure state. See \"mixed state\". Quantum numbers a way of representing a state by several numbers, which corresponds to a complete set of commuting observables. A common example of quantum numbers is the",
    "label": 0
  },
  {
    "text": "/ solution of Schrödinger equation is called pure state. See \"mixed state\". Quantum numbers a way of representing a state by several numbers, which corresponds to a complete set of commuting observables. A common example of quantum numbers is the possible state of an electron in a central potential: ( n , ℓ , m , s ) {\\displaystyle (n,\\ell ,m,s)} , which corresponds to the eigenstate of observables H {\\displaystyle H} (in terms of r {\\displaystyle r} ), L {\\displaystyle L} (magnitude of angular momentum), L z {\\displaystyle L_{z}} (angular momentum in z {\\displaystyle z} -direction), and S z {\\displaystyle S_{z}} . Spin wave function Part of a wave function of particle(s). See \"total wave function of a particle\". Spinor Synonymous to \"spin wave function\". Spatial wave function Part of a wave function of particle(s). See \"total wave function of a particle\". State A state is a complete description of the observable properties of a physical system. Sometimes the word is used as a synonym of \"wave function\" or \"pure state\". State vector synonymous to \"wave function\". Statistical ensemble A large number of copies of a system. System A sufficiently isolated part in the universe for investigation. Tensor product of Hilbert space When we are considering the total system as a composite system of two subsystems A and B, the wave functions of the composite system are in a Hilbert space H A ⊗ H B {\\displaystyle H_{A}\\otimes H_{B}} , if the Hilbert space of the wave functions for A and B are H A {\\displaystyle H_{A}} and H B {\\displaystyle H_{B}} respectively. Total wave function of a particle For single-particle system, the total wave function Ψ {\\displaystyle \\Psi } of a particle can be expressed as a product of spatial wave function and the spinor. The total wave",
    "label": 0
  },
  {
    "text": "B {\\displaystyle H_{B}} respectively. Total wave function of a particle For single-particle system, the total wave function Ψ {\\displaystyle \\Psi } of a particle can be expressed as a product of spatial wave function and the spinor. The total wave functions are in the tensor product space of the Hilbert space of the spatial part (which is spanned by the position eigenstates) and the Hilbert space for the spin. Wave function The word \"wave function\" could mean one of following: A vector in Hilbert space which can represent a state; synonymous to \"ket\" or \"state vector\". The state vector in a specific basis. It can be seen as a covariant vector in this case. The state vector in position representation, e.g. ψ α ( x 0 ) = ⟨ x 0 | α ⟩ {\\displaystyle \\psi _{\\alpha }(x_{0})=\\langle x_{0}|\\alpha \\rangle } , where | x 0 ⟩ {\\displaystyle |x_{0}\\rangle } is the position eigenstate. Dynamics Degeneracy See \"degenerate energy level\". Degenerate energy level If the energy of different state (wave functions which are not scalar multiple of each other) is the same, the energy level is called degenerate. There is no degeneracy in a 1D system. Energy spectrum The energy spectrum refers to the possible energy of a system. For bound system (bound states), the energy spectrum is discrete; for unbound system (scattering states), the energy spectrum is continuous. related mathematical topics: Sturm–Liouville equation Hamiltonian H ^ {\\displaystyle {\\hat {H}}} The operator represents the total energy of the system. Schrödinger equation The Schrödinger equation relates the Hamiltonian operator acting on a wave function to its time evolution (Equation 1): i ℏ ∂ ∂ t | α ⟩ = H ^ | α ⟩ {\\displaystyle i\\hbar {\\frac {\\partial }{\\partial t}}|\\alpha \\rangle ={\\hat {H}}|\\alpha \\rangle } Equation (1) is sometimes called \"Time-Dependent",
    "label": 0
  },
  {
    "text": "a wave function to its time evolution (Equation 1): i ℏ ∂ ∂ t | α ⟩ = H ^ | α ⟩ {\\displaystyle i\\hbar {\\frac {\\partial }{\\partial t}}|\\alpha \\rangle ={\\hat {H}}|\\alpha \\rangle } Equation (1) is sometimes called \"Time-Dependent Schrödinger equation\" (TDSE). Time-Independent Schrödinger Equation (TISE) A modification of the Time-Dependent Schrödinger equation as an eigenvalue problem. The solutions are energy eigenstates of the system (Equation 2): E | α ⟩ = H ^ | α ⟩ {\\displaystyle E|\\alpha \\rangle ={\\hat {H}}|\\alpha \\rangle } Dynamics related to single particle in a potential / other spatial properties In this situation, the SE is given by the form i ℏ ∂ ∂ t Ψ α ( r , t ) = H ^ Ψ α ( r , t ) = ( − ℏ 2 2 m ∇ 2 + V ( r ) ) Ψ α ( r , t ) = − ℏ 2 2 m ∇ 2 Ψ α ( r , t ) + V ( r ) Ψ α ( r , t ) {\\displaystyle i\\hbar {\\frac {\\partial }{\\partial t}}\\Psi _{\\alpha }(\\mathbf {r} ,\\,t)={\\hat {H}}\\Psi _{\\alpha }(\\mathbf {r} ,\\,t)=\\left(-{\\frac {\\hbar ^{2}}{2m}}\\nabla ^{2}+V(\\mathbf {r} )\\right)\\Psi _{\\alpha }(\\mathbf {r} ,\\,t)=-{\\frac {\\hbar ^{2}}{2m}}\\nabla ^{2}\\Psi _{\\alpha }(\\mathbf {r} ,\\,t)+V(\\mathbf {r} )\\Psi _{\\alpha }(\\mathbf {r} ,\\,t)} It can be derived from (1) by considering Ψ α ( x , t ) := ⟨ x | α ⟩ {\\displaystyle \\Psi _{\\alpha }(x,t):=\\langle x|\\alpha \\rangle } and H ^ := − ℏ 2 2 m ∇ 2 + V ^ {\\displaystyle {\\hat {H}}:=-{\\frac {\\hbar ^{2}}{2m}}\\nabla ^{2}+{\\hat {V}}} Bound state A state is called bound state if its position probability density at infinite tends to zero for all the time. Roughly speaking, we can expect to find the particle(s) in a finite size region",
    "label": 0
  },
  {
    "text": "{\\hbar ^{2}}{2m}}\\nabla ^{2}+{\\hat {V}}} Bound state A state is called bound state if its position probability density at infinite tends to zero for all the time. Roughly speaking, we can expect to find the particle(s) in a finite size region with certain probability. More precisely, | ψ ( r , t ) | 2 → 0 {\\displaystyle |\\psi (\\mathbf {r} ,t)|^{2}\\to 0} when | r | → + ∞ {\\displaystyle |\\mathbf {r} |\\to +\\infty } , for all t > 0 {\\displaystyle t>0} . There is a criterion in terms of energy: Let E {\\displaystyle E} be the expectation energy of the state. It is a bound state if and only if E < min ⁡ { V ( r → − ∞ ) , V ( r → + ∞ ) } {\\displaystyle E<\\operatorname {min} \\{V(r\\to -\\infty ),V(r\\to +\\infty )\\}} . Position representation and momentum representation Position representation of a wave function Ψ α ( x , t ) := ⟨ x | α ⟩ {\\displaystyle \\Psi _{\\alpha }(x,t):=\\langle x|\\alpha \\rangle } , momentum representation of a wave function Ψ ~ α ( p , t ) := ⟨ p | α ⟩ {\\displaystyle {\\tilde {\\Psi }}_{\\alpha }(p,t):=\\langle p|\\alpha \\rangle } ; where | x ⟩ {\\displaystyle |x\\rangle } is the position eigenstate and | p ⟩ {\\displaystyle |p\\rangle } the momentum eigenstate respectively. The two representations are linked by Fourier transform. Probability amplitude A probability amplitude is of the form ⟨ α | ψ ⟩ {\\displaystyle \\langle \\alpha |\\psi \\rangle } . Probability current Having the metaphor of probability density as mass density, then probability current J {\\displaystyle J} is the current: J ( x , t ) = i ℏ 2 m ( ψ ∂ ψ ∗ ∂ x − ∂ ψ ∂ x ψ ) {\\displaystyle",
    "label": 0
  },
  {
    "text": "probability density as mass density, then probability current J {\\displaystyle J} is the current: J ( x , t ) = i ℏ 2 m ( ψ ∂ ψ ∗ ∂ x − ∂ ψ ∂ x ψ ) {\\displaystyle J(x,t)={\\frac {i\\hbar }{2m}}\\left(\\psi {\\frac {\\partial \\psi ^{*}}{\\partial x}}-{\\frac {\\partial \\psi }{\\partial x}}\\psi \\right)} The probability current and probability density together satisfy the continuity equation: ∂ ∂ t | ψ ( x , t ) | 2 + ∇ ⋅ J ( x , t ) = 0 {\\displaystyle {\\frac {\\partial }{\\partial t}}|\\psi (x,t)|^{2}+\\nabla \\cdot \\mathbf {J} (x,t)=0} Probability density Given the wave function of a particle, | ψ ( x , t ) | 2 {\\displaystyle |\\psi (x,t)|^{2}} is the probability density at position x {\\displaystyle x} and time t {\\displaystyle t} . | ψ ( x 0 , t ) | 2 d x {\\displaystyle |\\psi (x_{0},t)|^{2}\\,dx} means the probability of finding the particle near x 0 {\\displaystyle x_{0}} . Scattering state The wave function of scattering state can be understood as a propagating wave. See also \"bound state\". There is a criterion in terms of energy: Let E {\\displaystyle E} be the expectation energy of the state. It is a scattering state if and only if E > min ⁡ { V ( r → − ∞ ) , V ( r → + ∞ ) } {\\displaystyle E>\\operatorname {min} \\{V(r\\to -\\infty ),V(r\\to +\\infty )\\}} . Square-integrable Square-integrable is a necessary condition for a function being the position/momentum representation of a wave function of a bound state of the system. Given the position representation Ψ ( x , t ) {\\displaystyle \\Psi (x,t)} of a state vector of a wave function, square-integrable means: 1D case: ∫ − ∞ + ∞ | Ψ ( x , t ) |",
    "label": 0
  },
  {
    "text": "the system. Given the position representation Ψ ( x , t ) {\\displaystyle \\Psi (x,t)} of a state vector of a wave function, square-integrable means: 1D case: ∫ − ∞ + ∞ | Ψ ( x , t ) | 2 d x < + ∞ {\\displaystyle \\int _{-\\infty }^{+\\infty }|\\Psi (x,t)|^{2}\\,dx<+\\infty } . 3D case: ∫ V | Ψ ( r , t ) | 2 d V < + ∞ {\\displaystyle \\int _{V}|\\Psi (\\mathbf {r} ,t)|^{2}\\,dV<+\\infty } . Stationary state A stationary state of a bound system is an eigenstate of Hamiltonian operator. Classically, it corresponds to standing wave. It is equivalent to the following things: an eigenstate of the Hamiltonian operator an eigenfunction of Time-Independent Schrödinger Equation a state of definite energy a state which \"every expectation value is constant in time\" a state whose probability density ( | ψ ( x , t ) | 2 {\\displaystyle |\\psi (x,t)|^{2}} ) does not change with respect to time, i.e. d d t | Ψ ( x , t ) | 2 = 0 {\\displaystyle {\\frac {d}{dt}}|\\Psi (x,t)|^{2}=0} Measurement postulates Born's rule The probability of the state | α ⟩ {\\displaystyle |\\alpha \\rangle } collapse to an eigenstate | k ⟩ {\\displaystyle |k\\rangle } of an observable is given by | ⟨ k | α ⟩ | 2 {\\displaystyle |\\langle k|\\alpha \\rangle |^{2}} . Collapse \"Collapse\" means the sudden process which the state of the system will \"suddenly\" change to an eigenstate of the observable during measurement. Eigenstates An eigenstate of an operator A {\\displaystyle A} is a vector satisfied the eigenvalue equation: A | α ⟩ = c | α ⟩ {\\displaystyle A|\\alpha \\rangle =c|\\alpha \\rangle } , where c {\\displaystyle c} is a scalar. Usually, in bra–ket notation, the eigenstate will be represented by its corresponding",
    "label": 0
  },
  {
    "text": "vector satisfied the eigenvalue equation: A | α ⟩ = c | α ⟩ {\\displaystyle A|\\alpha \\rangle =c|\\alpha \\rangle } , where c {\\displaystyle c} is a scalar. Usually, in bra–ket notation, the eigenstate will be represented by its corresponding eigenvalue if the corresponding observable is understood. Expectation value The expectation value ⟨ M ⟩ {\\displaystyle \\langle M\\rangle } of the observable M with respect to a state | α {\\displaystyle |\\alpha } is the average outcome of measuring M {\\displaystyle M} with respect to an ensemble of state | α {\\displaystyle |\\alpha } . ⟨ M ⟩ {\\displaystyle \\langle M\\rangle } can be calculated by: ⟨ M ⟩ = ⟨ α | M | α ⟩ . {\\displaystyle \\langle M\\rangle =\\langle \\alpha |M|\\alpha \\rangle .} If the state is given by a density matrix ρ {\\displaystyle \\rho } , ⟨ M ⟩ = Tr ⁡ ( M ρ ) {\\displaystyle \\langle M\\rangle =\\operatorname {Tr} (M\\rho )} . Hermitian operator An operator satisfying A = A † {\\displaystyle A=A^{\\dagger }} . Equivalently, ⟨ α | A | α ⟩ = ⟨ α | A † | α ⟩ {\\displaystyle \\langle \\alpha |A|\\alpha \\rangle =\\langle \\alpha |A^{\\dagger }|\\alpha \\rangle } for all allowable wave function | α ⟩ {\\displaystyle |\\alpha \\rangle } . Observable Mathematically, it is represented by a Hermitian operator. Indistinguishable particles Exchange Intrinsically identical particles If the intrinsic properties (properties that can be measured but are independent of the quantum state, e.g. charge, total spin, mass) of two particles are the same, they are said to be (intrinsically) identical. Indistinguishable particles If a system shows measurable differences when one of its particles is replaced by another particle, these two particles are called distinguishable. Bosons Bosons are particles with integer spin (s = 0, 1, 2, ... ). They",
    "label": 0
  },
  {
    "text": "identical. Indistinguishable particles If a system shows measurable differences when one of its particles is replaced by another particle, these two particles are called distinguishable. Bosons Bosons are particles with integer spin (s = 0, 1, 2, ... ). They can either be elementary (like photons) or composite (such as mesons, nuclei or even atoms). There are five known elementary bosons: the four force carrying gauge bosons γ (photon), g (gluon), Z (Z boson) and W (W boson), as well as the Higgs boson. Fermions Fermions are particles with half-integer spin (s = 1/2, 3/2, 5/2, ... ). Like bosons, they can be elementary or composite particles. There are two types of elementary fermions: quarks and leptons, which are the main constituents of ordinary matter. Anti-symmetrization of wave functions Symmetrization of wave functions Pauli exclusion principle Quantum statistical mechanics Bose–Einstein distribution Bose–Einstein condensation Bose–Einstein condensation state (BEC state) Fermi energy Fermi–Dirac distribution Slater determinant Nonlocality Entanglement Bell's inequality Entangled state separable state no-cloning theorem Rotation: spin/angular momentum Spin angular momentum Clebsch–Gordan coefficients singlet state and triplet state Approximation methods adiabatic approximation Born–Oppenheimer approximation WKB approximation time-dependent perturbation theory time-independent perturbation theory Historical Terms / semi-classical treatment Ehrenfest theorem A theorem connecting the classical mechanics and result derived from Schrödinger equation. first quantization x → x ^ , p → i ℏ ∂ ∂ x {\\displaystyle x\\to {\\hat {x}},\\,p\\to i\\hbar {\\frac {\\partial }{\\partial x}}} wave–particle duality Uncategorized terms uncertainty principle Canonical commutation relations The canonical commutation relations are the commutators between canonically conjugate variables. For example, position x ^ {\\displaystyle {\\hat {x}}} and momentum p ^ {\\displaystyle {\\hat {p}}} : [ x ^ , p ^ ] = x ^ p ^ − p ^ x ^ = i ℏ {\\displaystyle [{\\hat {x}},{\\hat {p}}]={\\hat {x}}{\\hat {p}}-{\\hat {p}}{\\hat {x}}=i\\hbar } Path integral",
    "label": 0
  },
  {
    "text": "Quantum mechanics is the study of matter and matter's interactions with energy on the scale of atomic and subatomic particles. By contrast, classical physics explains matter and energy only on a scale familiar to human experience, including the behavior of astronomical bodies such as the Moon. Classical physics is still used in much of modern science and technology. However, towards the end of the 19th century, scientists discovered phenomena in both the large (macro) and the small (micro) worlds that classical physics could not explain. The desire to resolve inconsistencies between observed phenomena and classical theory led to a revolution in physics, a shift in the original scientific paradigm: the development of quantum mechanics. Many aspects of quantum mechanics yield unexpected results, defying expectations and deemed counterintuitive. These aspects can seem paradoxical as they map behaviors quite differently from those seen at larger scales. In the words of quantum physicist Richard Feynman, quantum mechanics deals with \"nature as She is—absurd\". Features of quantum mechanics often defy simple explanations in everyday language. One example of this is the uncertainty principle: precise measurements of position cannot be combined with precise measurements of velocity. Another example is entanglement: a measurement made on one particle (such as an electron that is measured to have spin 'up') will correlate with a measurement on a second particle (an electron will be found to have spin 'down') if the two particles have a shared history. This will apply even if it is impossible for the result of the first measurement to have been transmitted to the second particle before the second measurement takes place. Quantum mechanics helps people understand chemistry, because it explains how atoms interact with each other and form molecules. Many remarkable phenomena can be explained using quantum mechanics, like superfluidity. For example, if liquid",
    "label": 0
  },
  {
    "text": "particle before the second measurement takes place. Quantum mechanics helps people understand chemistry, because it explains how atoms interact with each other and form molecules. Many remarkable phenomena can be explained using quantum mechanics, like superfluidity. For example, if liquid helium cooled to a temperature near absolute zero is placed in a container, it spontaneously flows up and over the rim of its container; this is an effect which cannot be explained by classical physics. History James C. Maxwell's unification of the equations governing electricity, magnetism, and light in the late 19th century led to experiments on the interaction of light and matter. Some of these experiments had aspects which could not be explained until quantum mechanics emerged in the early part of the 20th century. Evidence of quanta from the photoelectric effect The seeds of the quantum revolution appear in the discovery by J.J. Thomson in 1897 that cathode rays were not continuous but \"corpuscles\" (electrons). Electrons had been named just six years earlier as part of the emerging theory of atoms. In 1900, Max Planck, unconvinced by the atomic theory, discovered that he needed discrete entities like atoms or electrons to explain black-body radiation. Very hot – red hot or white hot – objects look similar when heated to the same temperature. This look results from a common curve of light intensity at different frequencies (colors), which is called black-body radiation. White hot objects have intensity across many colors in the visible range. The lowest frequencies above visible colors are infrared light, which also give off heat. Continuous wave theories of light and matter cannot explain the black-body radiation curve. Planck spread the heat energy among individual \"oscillators\" of an undefined character but with discrete energy capacity; this model explained black-body radiation. At the time, electrons, atoms, and",
    "label": 0
  },
  {
    "text": "wave theories of light and matter cannot explain the black-body radiation curve. Planck spread the heat energy among individual \"oscillators\" of an undefined character but with discrete energy capacity; this model explained black-body radiation. At the time, electrons, atoms, and discrete oscillators were all exotic ideas to explain exotic phenomena. But in 1905 Albert Einstein proposed that light was also corpuscular, consisting of \"energy quanta\", in contradiction to the established science of light as a continuous wave, stretching back a hundred years to Thomas Young's work on diffraction. Einstein's revolutionary proposal started by reanalyzing Planck's black-body theory, arriving at the same conclusions by using the new \"energy quanta\". Einstein then showed how energy quanta connected to Thomson's electron. In 1902, Philipp Lenard directed light from an arc lamp onto freshly cleaned metal plates housed in an evacuated glass tube. He measured the electric current coming off the metal plate, at higher and lower intensities of light and for different metals. Lenard showed that amount of current – the number of electrons – depended on the intensity of the light, but that the velocity of these electrons did not depend on intensity. This is the photoelectric effect. The continuous wave theories of the time predicted that more light intensity would accelerate the same amount of current to higher velocity, contrary to this experiment. Einstein's energy quanta explained the volume increase: one electron is ejected for each quantum: more quanta mean more electrons. Einstein then predicted that the electron velocity would increase in direct proportion to the light frequency above a fixed value that depended upon the metal. Here the idea is that energy in energy-quanta depends upon the light frequency; the energy transferred to the electron comes in proportion to the light frequency. The type of metal gives a barrier, the",
    "label": 0
  },
  {
    "text": "value that depended upon the metal. Here the idea is that energy in energy-quanta depends upon the light frequency; the energy transferred to the electron comes in proportion to the light frequency. The type of metal gives a barrier, the fixed value, that the electrons must climb over to exit their atoms, to be emitted from the metal surface and be measured. Ten years elapsed before Millikan's definitive experiment verified Einstein's prediction. During that time many scientists rejected the revolutionary idea of quanta. But Planck's and Einstein's concept was in the air and soon began to affect other physics and quantum theories. Quantization of bound electrons in atoms Experiments with light and matter in the late 1800s uncovered a reproducible but puzzling regularity. When light was shown through purified gases, certain frequencies (colors) did not pass. These dark absorption 'lines' followed a distinctive pattern: the gaps between the lines decreased steadily. By 1889, the Rydberg formula predicted the lines for hydrogen gas using only a constant number and the integers to index the lines. The origin of this regularity was unknown. Solving this mystery would eventually become the first major step toward quantum mechanics. Throughout the 19th century evidence grew for the atomic nature of matter. With Thomson's discovery of the electron in 1897, scientists began the search for a model of the interior of the atom. Thomson proposed negative electrons swimming in a pool of positive charge. Between 1908 and 1911, Rutherford showed that the positive part was only 1/3000th of the diameter of the atom. Models of \"planetary\" electrons orbiting a nuclear \"Sun\" were proposed, but cannot explain why the electron does not simply fall into the positive charge. In 1913 Niels Bohr and Ernest Rutherford connected the new atom models to the mystery of the Rydberg formula:",
    "label": 0
  },
  {
    "text": "electrons orbiting a nuclear \"Sun\" were proposed, but cannot explain why the electron does not simply fall into the positive charge. In 1913 Niels Bohr and Ernest Rutherford connected the new atom models to the mystery of the Rydberg formula: the orbital radius of the electrons were constrained and the resulting energy differences matched the energy differences in the absorption lines. This meant that absorption and emission of light from atoms was energy quantized: only specific energies that matched the difference in orbital energy would be emitted or absorbed. Trading one mystery – the regular pattern of the Rydberg formula – for another mystery – constraints on electron orbits – might not seem like a big advance, but the new atom model summarized many other experimental findings. The quantization of the photoelectric effect and now the quantization of the electron orbits set the stage for the final revolution. Throughout the first and the modern era of quantum mechanics the concept that classical mechanics must be valid macroscopically constrained possible quantum models. This concept was formalized by Bohr in 1923 as the correspondence principle. It requires quantum theory to converge to classical limits. A related concept is Ehrenfest's theorem, which shows that the average values obtained from quantum mechanics (e.g. position and momentum) obey classical laws. Quantization of spin In 1922 Otto Stern and Walther Gerlach demonstrated that the magnetic properties of silver atoms defy classical explanation, the work contributing to Stern’s 1943 Nobel Prize in Physics. They fired a beam of silver atoms through a magnetic field. According to classical physics, the atoms should have emerged in a spray, with a continuous range of directions. Instead, the beam separated into two, and only two, diverging streams of atoms. Unlike the other quantum effects known at the time, this striking result",
    "label": 0
  },
  {
    "text": "physics, the atoms should have emerged in a spray, with a continuous range of directions. Instead, the beam separated into two, and only two, diverging streams of atoms. Unlike the other quantum effects known at the time, this striking result involves the state of a single atom. In 1927, Thomas Erwin Phipps and John Bellamy Taylor obtained a similar, but less pronounced effect using hydrogen atoms in their ground state, thereby eliminating any doubts that may have been caused by the use of silver atoms. In 1924, Wolfgang Pauli called it \"two-valuedness not describable classically\" and associated it with electrons in the outermost shell. The experiments lead to formulation of its theory described to arise from spin of the electron in 1925, by Samuel Goudsmit and George Uhlenbeck, under the advice of Paul Ehrenfest. Quantization of matter In 1924 Louis de Broglie proposed that electrons in an atom are constrained not in \"orbits\" but as standing waves. In detail his solution did not work, but his hypothesis – that the electron \"corpuscle\" moves in the atom as a wave – spurred Erwin Schrödinger to develop a wave equation for electrons; when applied to hydrogen the Rydberg formula was accurately reproduced. Max Born's 1924 paper \"Zur Quantenmechanik\" was the first use of the words \"quantum mechanics\" in print. His later work included developing quantum collision models; in a footnote to a 1926 paper he proposed the Born rule connecting theoretical models to experiment. In 1927 at Bell Labs, Clinton Davisson and Lester Germer fired slow-moving electrons at a crystalline nickel target which showed a diffraction pattern indicating wave nature of electron whose theory was fully explained by Hans Bethe. A similar experiment by George Paget Thomson and Alexander Reid, firing electrons at thin celluloid foils and later metal films, observing rings,",
    "label": 0
  },
  {
    "text": "which showed a diffraction pattern indicating wave nature of electron whose theory was fully explained by Hans Bethe. A similar experiment by George Paget Thomson and Alexander Reid, firing electrons at thin celluloid foils and later metal films, observing rings, independently discovered matter wave nature of electrons. Further developments In 1928 Paul Dirac published his relativistic wave equation simultaneously incorporating relativity, predicting anti-matter, and providing a complete theory for the Stern–Gerlach result. These successes launched a new fundamental understanding of our world at small scale: quantum mechanics. Planck and Einstein started the revolution with quanta that broke down the continuous models of matter and light. Twenty years later \"corpuscles\" like electrons came to be modeled as continuous waves. This result came to be called wave-particle duality, one iconic idea along with the uncertainty principle that sets quantum mechanics apart from older models of physics. Quantum radiation, quantum fields In 1923 Compton demonstrated that the Planck-Einstein energy quanta from light also had momentum; three years later the \"energy quanta\" got a new name \"photon\". Despite its role in almost all stages of the quantum revolution, no explicit model for light quanta existed until 1927 when Paul Dirac began work on a quantum theory of radiation that became quantum electrodynamics. Over the following decades this work evolved into quantum field theory, the basis for modern quantum optics and particle physics. Wave–particle duality The concept of wave–particle duality says that neither the classical concept of \"particle\" nor of \"wave\" can fully describe the behavior of quantum-scale objects, either photons or matter. Wave–particle duality is an example of the principle of complementarity in quantum physics. An elegant example of wave-particle duality is the double-slit experiment. In the double-slit experiment, as originally performed by Thomas Young in 1803, and then Augustin Fresnel a decade later,",
    "label": 0
  },
  {
    "text": "is an example of the principle of complementarity in quantum physics. An elegant example of wave-particle duality is the double-slit experiment. In the double-slit experiment, as originally performed by Thomas Young in 1803, and then Augustin Fresnel a decade later, a beam of light is directed through two narrow, closely spaced slits, producing an interference pattern of light and dark bands on a screen. The same behavior can be demonstrated in water waves: the double-slit experiment was seen as a demonstration of the wave nature of light. Variations of the double-slit experiment have been performed using electrons, atoms, and even large molecules, and the same type of interference pattern is seen. Thus it has been demonstrated that all matter possesses wave characteristics. If the source intensity is turned down, the same interference pattern will slowly build up, one \"count\" or particle (e.g. photon or electron) at a time. The quantum system acts as a wave when passing through the double slits, but as a particle when it is detected. This is a typical feature of quantum complementarity: a quantum system acts as a wave in an experiment to measure its wave-like properties, and like a particle in an experiment to measure its particle-like properties. The point on the detector screen where any individual particle shows up is the result of a random process. However, the distribution pattern of many individual particles mimics the diffraction pattern produced by waves. Uncertainty principle Suppose it is desired to measure the position and speed of an object—for example, a car going through a radar speed trap. It can be assumed that the car has a definite position and speed at a particular moment in time. How accurately these values can be measured depends on the quality of the measuring equipment. If the precision of",
    "label": 0
  },
  {
    "text": "speed trap. It can be assumed that the car has a definite position and speed at a particular moment in time. How accurately these values can be measured depends on the quality of the measuring equipment. If the precision of the measuring equipment is improved, it provides a result closer to the true value. It might be assumed that the speed of the car and its position could be operationally defined and measured simultaneously, as precisely as might be desired. In 1927, Heisenberg proved that this last assumption is not correct. Quantum mechanics shows that certain pairs of physical properties, for example, position and speed, cannot be simultaneously measured, nor defined in operational terms, to arbitrary precision: the more precisely one property is measured, or defined in operational terms, the less precisely can the other be thus treated. This statement is known as the uncertainty principle. The uncertainty principle is not only a statement about the accuracy of our measuring equipment but, more deeply, is about the conceptual nature of the measured quantities—the assumption that the car had simultaneously defined position and speed does not work in quantum mechanics. On a scale of cars and people, these uncertainties are negligible, but when dealing with atoms and electrons they become critical. Heisenberg gave, as an illustration, the measurement of the position and momentum of an electron using a photon of light. In measuring the electron's position, the higher the frequency of the photon, the more accurate is the measurement of the position of the impact of the photon with the electron, but the greater is the disturbance of the electron. This is because from the impact with the photon, the electron absorbs a random amount of energy, rendering the measurement obtained of its momentum increasingly uncertain, for one is necessarily measuring",
    "label": 0
  },
  {
    "text": "but the greater is the disturbance of the electron. This is because from the impact with the photon, the electron absorbs a random amount of energy, rendering the measurement obtained of its momentum increasingly uncertain, for one is necessarily measuring its post-impact disturbed momentum from the collision products and not its original momentum (momentum which should be simultaneously measured with position). With a photon of lower frequency, the disturbance (and hence uncertainty) in the momentum is less, but so is the accuracy of the measurement of the position of the impact. At the heart of the uncertainty principle is a fact that for any mathematical analysis in the position and velocity domains, achieving a sharper (more precise) curve in the position domain can only be done at the expense of a more gradual (less precise) curve in the speed domain, and vice versa. More sharpness in the position domain requires contributions from more frequencies in the speed domain to create the narrower curve, and vice versa. It is a fundamental tradeoff inherent in any such related or complementary measurements, but is only really noticeable at the smallest (Planck) scale, near the size of elementary particles. The uncertainty principle shows mathematically that the product of the uncertainty in the position and momentum of a particle (momentum is velocity multiplied by mass) could never be less than a certain value, and that this value is related to the Planck constant. Wave function collapse Wave function collapse means that a measurement has forced or converted a quantum (probabilistic or potential) state into a definite measured value. This phenomenon is only seen in quantum mechanics rather than classical mechanics. For example, before a photon actually \"shows up\" on a detection screen it can be described only with a set of probabilities for where it",
    "label": 0
  },
  {
    "text": "definite measured value. This phenomenon is only seen in quantum mechanics rather than classical mechanics. For example, before a photon actually \"shows up\" on a detection screen it can be described only with a set of probabilities for where it might show up. When it does appear, for instance in the CCD of an electronic camera, the time and space where it interacted with the device are known within very tight limits. However, the photon has disappeared in the process of being captured (measured), and its quantum wave function has disappeared with it. In its place, some macroscopic physical change in the detection screen has appeared, e.g., an exposed spot in a sheet of photographic film, or a change in electric potential in some cell of a CCD. Eigenstates and eigenvalues Because of the uncertainty principle, statements about both the position and momentum of particles can assign only a probability that the position or momentum has some numerical value. Therefore, it is necessary to formulate clearly the difference between the state of something indeterminate, such as an electron in a probability cloud, and the state of something having a definite value. When an object can definitely be \"pinned-down\" in some respect, it is said to possess an eigenstate. In the Stern–Gerlach experiment discussed above, the quantum model predicts two possible values of spin for the atom compared to the magnetic axis. These two eigenstates are named arbitrarily 'up' and 'down'. The quantum model predicts these states will be measured with equal probability, but no intermediate values will be seen. This is what the Stern–Gerlach experiment shows. The eigenstates of spin about the vertical axis are not simultaneously eigenstates of spin about the horizontal axis, so this atom has an equal probability of being found to have either value of spin",
    "label": 0
  },
  {
    "text": "is what the Stern–Gerlach experiment shows. The eigenstates of spin about the vertical axis are not simultaneously eigenstates of spin about the horizontal axis, so this atom has an equal probability of being found to have either value of spin about the horizontal axis. As described in the section above, measuring the spin about the horizontal axis can allow an atom that was spun up to spin down: measuring its spin about the horizontal axis collapses its wave function into one of the eigenstates of this measurement, which means it is no longer in an eigenstate of spin about the vertical axis, so can take either value. The Pauli exclusion principle In 1924, Wolfgang Pauli proposed a new quantum degree of freedom (or quantum number), with two possible values, to resolve inconsistencies between observed molecular spectra and the predictions of quantum mechanics. In particular, the spectrum of atomic hydrogen had a doublet, or pair of lines differing by a small amount, where only one line was expected. Pauli formulated his exclusion principle, stating, \"There cannot exist an atom in such a quantum state that two electrons within [it] have the same set of quantum numbers.\" A year later, Uhlenbeck and Goudsmit identified Pauli's new degree of freedom with the property called spin whose effects were observed in the Stern–Gerlach experiment. Dirac wave equation In 1928, Paul Dirac extended the Pauli equation, which described spinning electrons, to account for special relativity. The result was a theory that dealt properly with events, such as the speed at which an electron orbits the nucleus, occurring at a substantial fraction of the speed of light. By using the simplest electromagnetic interaction, Dirac was able to predict the value of the magnetic moment associated with the electron's spin and found the experimentally observed value, which",
    "label": 0
  },
  {
    "text": "nucleus, occurring at a substantial fraction of the speed of light. By using the simplest electromagnetic interaction, Dirac was able to predict the value of the magnetic moment associated with the electron's spin and found the experimentally observed value, which was too large to be that of a spinning charged sphere governed by classical physics. He was able to solve for the spectral lines of the hydrogen atom and to reproduce from physical first principles Sommerfeld's successful formula for the fine structure of the hydrogen spectrum. Dirac's equations sometimes yielded a negative value for energy, for which he proposed a novel solution: he posited the existence of an antielectron and a dynamical vacuum. This led to the many-particle quantum field theory. Quantum entanglement In quantum physics, a group of particles can interact or be created together in such a way that the quantum state of each particle of the group cannot be described independently of the state of the others, including when the particles are separated by a large distance. This is known as quantum entanglement. An early landmark in the study of entanglement was the Einstein–Podolsky–Rosen (EPR) paradox, a thought experiment proposed by Albert Einstein, Boris Podolsky and Nathan Rosen which argues that the description of physical reality provided by quantum mechanics is incomplete. In a 1935 paper titled \"Can Quantum-Mechanical Description of Physical Reality be Considered Complete?\", they argued for the existence of \"elements of reality\" that were not part of quantum theory, and speculated that it should be possible to construct a theory containing these hidden variables. The thought experiment involves a pair of particles prepared in what would later become known as an entangled state. Einstein, Podolsky, and Rosen pointed out that, in this state, if the position of the first particle were measured, the result",
    "label": 0
  },
  {
    "text": "The thought experiment involves a pair of particles prepared in what would later become known as an entangled state. Einstein, Podolsky, and Rosen pointed out that, in this state, if the position of the first particle were measured, the result of measuring the position of the second particle could be predicted. If instead the momentum of the first particle were measured, then the result of measuring the momentum of the second particle could be predicted. They argued that no action taken on the first particle could instantaneously affect the other, since this would involve information being transmitted faster than light, which is forbidden by the theory of relativity. They invoked a principle, later known as the \"EPR criterion of reality\", positing that: \"If, without in any way disturbing a system, we can predict with certainty (i.e., with probability equal to unity) the value of a physical quantity, then there exists an element of reality corresponding to that quantity.\" From this, they inferred that the second particle must have a definite value of both position and of momentum prior to either quantity being measured. But quantum mechanics considers these two observables incompatible and thus does not associate simultaneous values for both to any system. Einstein, Podolsky, and Rosen therefore concluded that quantum theory does not provide a complete description of reality. In the same year, Erwin Schrödinger used the word \"entanglement\" and declared: \"I would not call that one but rather the characteristic trait of quantum mechanics.\" The Irish physicist John Stewart Bell carried the analysis of quantum entanglement much further. He deduced that if measurements are performed independently on the two separated particles of an entangled pair, then the assumption that the outcomes depend upon hidden variables within each half implies a mathematical constraint on how the outcomes on the",
    "label": 0
  },
  {
    "text": "He deduced that if measurements are performed independently on the two separated particles of an entangled pair, then the assumption that the outcomes depend upon hidden variables within each half implies a mathematical constraint on how the outcomes on the two measurements are correlated. This constraint would later be named the Bell inequality. Bell then showed that quantum physics predicts correlations that violate this inequality. Consequently, the only way that hidden variables could explain the predictions of quantum physics is if they are \"nonlocal\", which is to say that somehow the two particles are able to interact instantaneously no matter how widely they ever become separated. Performing experiments like those that Bell suggested, physicists have found that nature obeys quantum mechanics and violates Bell inequalities. In other words, the results of these experiments are incompatible with any local hidden variable theory. Quantum field theory The idea of quantum field theory began in the late 1920s with British physicist Paul Dirac, when he attempted to quantize the energy of the electromagnetic field; just as in quantum mechanics the energy of an electron in the hydrogen atom was quantized. Quantization is a procedure for constructing a quantum theory starting from a classical theory. Merriam-Webster defines a field in physics as \"a region or space in which a given effect (such as magnetism) exists\". Other effects that manifest themselves as fields are gravitation and static electricity. In 2008, physicist Richard Hammond wrote: Sometimes we distinguish between quantum mechanics (QM) and quantum field theory (QFT). QM refers to a system in which the number of particles is fixed, and the fields (such as the electromechanical field) are continuous classical entities. QFT ... goes a step further and allows for the creation and annihilation of particles ... He added, however, that quantum mechanics is often",
    "label": 0
  },
  {
    "text": "of particles is fixed, and the fields (such as the electromechanical field) are continuous classical entities. QFT ... goes a step further and allows for the creation and annihilation of particles ... He added, however, that quantum mechanics is often used to refer to \"the entire notion of quantum view\". In 1931, Dirac proposed the existence of particles that later became known as antimatter. Dirac shared the Nobel Prize in Physics for 1933 with Schrödinger \"for the discovery of new productive forms of atomic theory\". Quantum electrodynamics Quantum electrodynamics (QED) is the name of the quantum theory of the electromagnetic force. Understanding QED begins with understanding electromagnetism. Electromagnetism can be called \"electrodynamics\" because it is a dynamic interaction between electrical and magnetic forces. Electromagnetism begins with the electric charge. Electric charges are the sources of and create, electric fields. An electric field is a field that exerts a force on any particles that carry electric charges, at any point in space. This includes the electron, proton, and even quarks, among others. As a force is exerted, electric charges move, a current flows, and a magnetic field is produced. The changing magnetic field, in turn, causes electric current (often moving electrons). The physical description of interacting charged particles, electrical currents, electrical fields, and magnetic fields is called electromagnetism. In 1928 Paul Dirac produced a relativistic quantum theory of electromagnetism. This was the progenitor to modern quantum electrodynamics, in that it had essential ingredients of the modern theory. However, the problem of unsolvable infinities developed in this relativistic quantum theory. Years later, renormalization largely solved this problem. Initially viewed as a provisional, suspect procedure by some of its originators, renormalization eventually was embraced as an important and self-consistent tool in QED and other fields of physics. Also, in the late 1940s Feynman",
    "label": 0
  },
  {
    "text": "renormalization largely solved this problem. Initially viewed as a provisional, suspect procedure by some of its originators, renormalization eventually was embraced as an important and self-consistent tool in QED and other fields of physics. Also, in the late 1940s Feynman diagrams provided a way to make predictions with QED by finding a probability amplitude for each possible way that an interaction could occur. The diagrams showed in particular that the electromagnetic force is the exchange of photons between interacting particles. The Lamb shift is an example of a quantum electrodynamics prediction that has been experimentally verified. It is an effect whereby the quantum nature of the electromagnetic field makes the energy levels in an atom or ion deviate slightly from what they would otherwise be. As a result, spectral lines may shift or split. Similarly, within a freely propagating electromagnetic wave, the current can also be just an abstract displacement current, instead of involving charge carriers. In QED, its full description makes essential use of short-lived virtual particles. There, QED again validates an earlier, rather mysterious concept. Standard Model The Standard Model of particle physics is the quantum field theory that describes three of the four known fundamental forces (electromagnetic, weak and strong interactions – excluding gravity) in the universe and classifies all known elementary particles. It was developed in stages throughout the latter half of the 20th century, through the work of many scientists worldwide, with the current formulation being finalized in the mid-1970s upon experimental confirmation of the existence of quarks. Since then, proof of the top quark (1995), the tau neutrino (2000), and the Higgs boson (2012) have added further credence to the Standard Model. In addition, the Standard Model has predicted various properties of weak neutral currents and the W and Z bosons with great accuracy.",
    "label": 0
  },
  {
    "text": "(1995), the tau neutrino (2000), and the Higgs boson (2012) have added further credence to the Standard Model. In addition, the Standard Model has predicted various properties of weak neutral currents and the W and Z bosons with great accuracy. Although the Standard Model is believed to be theoretically self-consistent and has demonstrated success in providing experimental predictions, it leaves some physical phenomena unexplained and so falls short of being a complete theory of fundamental interactions. For example, it does not fully explain baryon asymmetry, incorporate the full theory of gravitation as described by general relativity, or account for the universe's accelerating expansion as possibly described by dark energy. The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations and their non-zero masses. Accordingly, it is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) to explain experimental results at variance with the Standard Model, such as the existence of dark matter and neutrino oscillations. Interpretations The physical measurements, equations, and predictions pertinent to quantum mechanics are all consistent and hold a very high level of confirmation. However, the question of what these abstract models say about the underlying nature of the real world has received competing answers. These interpretations are widely varying and sometimes somewhat abstract. For instance, the Copenhagen interpretation states that before a measurement, statements about a particle's properties are completely meaningless, while the many-worlds interpretation describes the existence of a multiverse made up of every possible universe. Light behaves in some aspects like particles and in other aspects like waves. Matter—the \"stuff\" of the universe consisting of particles such as electrons and atoms—exhibits wavelike behavior too.",
    "label": 0
  },
  {
    "text": "the existence of a multiverse made up of every possible universe. Light behaves in some aspects like particles and in other aspects like waves. Matter—the \"stuff\" of the universe consisting of particles such as electrons and atoms—exhibits wavelike behavior too. Some light sources, such as neon lights, give off only certain specific frequencies of light, a small set of distinct pure colors determined by neon's atomic structure. Quantum mechanics shows that light, along with all other forms of electromagnetic radiation, comes in discrete units, called photons, and predicts its spectral energies (corresponding to pure colors), and the intensities of its light beams. A single photon is a quantum, or smallest observable particle, of the electromagnetic field. A partial photon is never experimentally observed. More broadly, quantum mechanics shows that many properties of objects, such as position, speed, and angular momentum, that appeared continuous in the zoomed-out view of classical mechanics, turn out to be (in the very tiny, zoomed-in scale of quantum mechanics) quantized. Such properties of elementary particles are required to take on one of a set of small, discrete allowable values, and since the gap between these values is also small, the discontinuities are only apparent at very tiny (atomic) scales. Applications Everyday applications The relationship between the frequency of electromagnetic radiation and the energy of each photon is why ultraviolet light can cause sunburn, but visible or infrared light cannot. A photon of ultraviolet light delivers a high amount of energy—enough to contribute to cellular damage such as occurs in a sunburn. A photon of infrared light delivers less energy—only enough to warm one's skin. So, an infrared lamp can warm a large surface, perhaps large enough to keep people comfortable in a cold room, but it cannot give anyone a sunburn. Technological applications Applications of quantum",
    "label": 0
  },
  {
    "text": "delivers less energy—only enough to warm one's skin. So, an infrared lamp can warm a large surface, perhaps large enough to keep people comfortable in a cold room, but it cannot give anyone a sunburn. Technological applications Applications of quantum mechanics include the laser, the transistor, the electron microscope, and magnetic resonance imaging. A special class of quantum mechanical applications is related to macroscopic quantum phenomena such as superfluid helium and superconductors. The study of semiconductors led to the invention of the diode and the transistor, which are indispensable for modern electronics. In even a simple light switch, quantum tunneling is absolutely vital, as otherwise the electrons in the electric current could not penetrate the potential barrier made up of a layer of oxide. Flash memory chips found in USB drives also use quantum tunneling, to erase their memory cells. See also Einstein's thought experiments Macroscopic quantum phenomena Philosophy of physics Quantum computing Virtual particle Teaching quantum mechanics List of textbooks on classical and quantum mechanics References Bibliography Bernstein, Jeremy (2005). \"Max Born and the quantum theory\". American Journal of Physics. 73 (11): 999–1008. Bibcode:2005AmJPh..73..999B. doi:10.1119/1.2060717. Beller, Mara (2001). Quantum Dialogue: The Making of a Revolution. University of Chicago Press. Bohr, Niels (1958). Atomic Physics and Human Knowledge. John Wiley & Sons]. ISBN 0486479285. OCLC 530611. {{cite book}}: ISBN / Date incompatibility (help) de Broglie, Louis (1953). The Revolution in Physics. Noonday Press. LCCN 53010401. Bronner, Patrick; Strunz, Andreas; Silberhorn, Christine; Meyn, Jan-Peter (2009). \"Demonstrating quantum random with single photons\". European Journal of Physics. 30 (5): 1189–1200. Bibcode:2009EJPh...30.1189B. doi:10.1088/0143-0807/30/5/026. S2CID 7903179. Einstein, Albert (1934). Essays in Science. Philosophical Library. ISBN 0486470113. LCCN 55003947. {{cite book}}: ISBN / Date incompatibility (help) Feigl, Herbert; Brodbeck, May (1953). Readings in the Philosophy of Science. Appleton-Century-Crofts. ISBN 0390304883. LCCN 53006438. {{cite book}}: ISBN /",
    "label": 0
  },
  {
    "text": "7903179. Einstein, Albert (1934). Essays in Science. Philosophical Library. ISBN 0486470113. LCCN 55003947. {{cite book}}: ISBN / Date incompatibility (help) Feigl, Herbert; Brodbeck, May (1953). Readings in the Philosophy of Science. Appleton-Century-Crofts. ISBN 0390304883. LCCN 53006438. {{cite book}}: ISBN / Date incompatibility (help) Feynman, Richard P. (1949). \"Space-Time Approach to Quantum Electrodynamics\". Physical Review. 76 (6): 769–89. Bibcode:1949PhRv...76..769F. doi:10.1103/PhysRev.76.769. Feynman, Richard P. (1990). QED, The Strange Theory of Light and Matter. Penguin Books. ISBN 978-0140125054. Fowler, Michael (1999). The Bohr Atom. University of Virginia. Heisenberg, Werner (1958). Physics and Philosophy. Harper and Brothers. ISBN 0061305499. LCCN 99010404. {{cite book}}: ISBN / Date incompatibility (help) Lakshmibala, S. (2004). \"Heisenberg, Matrix Mechanics and the Uncertainty Principle\". Resonance: Journal of Science Education. 9 (8): 46–56. doi:10.1007/bf02837577. S2CID 29893512. Liboff, Richard L. (1992). Introductory Quantum Mechanics (2nd ed.). Addison-Wesley Pub. Co. ISBN 9780201547153. Lindsay, Robert Bruce; Margenau, Henry (1957). Foundations of Physics. Dover. ISBN 0918024188. LCCN 57014416. {{cite book}}: ISBN / Date incompatibility (help) McEvoy, J. P.; Zarate, Oscar (2004). Introducing Quantum Theory. Icon Books. ISBN 1874166374. Nave, Carl Rod (2005). \"Quantum Physics\". HyperPhysics. Georgia State University. Peat, F. David (2002). From Certainty to Uncertainty: The Story of Science and Ideas in the Twenty-First Century. Joseph Henry Press. Reichenbach, Hans (1944). Philosophic Foundations of Quantum Mechanics. University of California Press. ISBN 0486404595. LCCN a44004471. {{cite book}}: ISBN / Date incompatibility (help) Schilpp, Paul Arthur (1949). Albert Einstein: Philosopher-Scientist. Tudor Publishing Company. LCCN 50005340. Scientific American Reader, 1953. Sears, Francis Weston (1949). Optics (3rd ed.). Addison-Wesley. ISBN 0195046013. LCCN 51001018. {{cite book}}: ISBN / Date incompatibility (help) Shimony, A. (1983). \"(title not given in citation)\". Foundations of Quantum Mechanics in the Light of New Technology (S. Kamefuchi et al., eds.). Tokyo: Japan Physical Society. p. 225.; cited in: Popescu, Sandu; Daniel Rohrlich (1996). \"Action and",
    "label": 0
  },
  {
    "text": "incompatibility (help) Shimony, A. (1983). \"(title not given in citation)\". Foundations of Quantum Mechanics in the Light of New Technology (S. Kamefuchi et al., eds.). Tokyo: Japan Physical Society. p. 225.; cited in: Popescu, Sandu; Daniel Rohrlich (1996). \"Action and Passion at a Distance: An Essay in Honor of Professor Abner Shimony\". arXiv:quant-ph/9605004. Tavel, Morton; Tavel, Judith (illustrations) (2002). Contemporary physics and the limits of knowledge. Rutgers University Press. ISBN 978-0813530772. Van Vleck, J. H.,1928, \"The Correspondence Principle in the Statistical Interpretation of Quantum Mechanics\", Proc. Natl. Acad. Sci. 14: 179. Westmoreland; Benjamin Schumacher (1998). \"Quantum Entanglement and the Nonexistence of Superluminal Signals\". arXiv:quant-ph/9801014. Wheeler, John Archibald; Feynman, Richard P. (1949). \"Classical Electrodynamics in Terms of Direct Interparticle Action\" (PDF). Reviews of Modern Physics. 21 (3): 425–33. Bibcode:1949RvMP...21..425W. doi:10.1103/RevModPhys.21.425. Wieman, Carl; Perkins, Katherine (2005). \"Transforming Physics Education\". Physics Today. 58 (11): 36. Bibcode:2005PhT....58k..36W. doi:10.1063/1.2155756. Further reading The following titles, all by working physicists, attempt to communicate quantum theory to laypeople, using a minimum of technical apparatus. Jim Al-Khalili (2003). Quantum: A Guide for the Perplexed. Weidenfeld & Nicolson. ISBN 978-1780225340. Chester, Marvin (1987). Primer of Quantum Mechanics. John Wiley. ISBN 0486428788. Brian Cox and Jeff Forshaw (2011) The Quantum Universe. Allen Lane. ISBN 978-1846144325. Richard Feynman (1985). QED: The Strange Theory of Light and Matter. Princeton University Press. ISBN 0691083886. Ford, Kenneth (2005). The Quantum World. Harvard Univ. Press. Includes elementary particle physics. Ghirardi, GianCarlo (2004). Sneaking a Look at God's Cards, Gerald Malsbary, trans. Princeton Univ. Press. The most technical of the works cited here. Passages using algebra, trigonometry, and bra–ket notation can be passed over on a first reading. Tony Hey and Walters, Patrick (2003). The New Quantum Universe. Cambridge Univ. Press. Includes much about the technologies quantum theory has made possible. ISBN 978-0521564571. Vladimir G. Ivancevic, Tijana",
    "label": 0
  },
  {
    "text": "and bra–ket notation can be passed over on a first reading. Tony Hey and Walters, Patrick (2003). The New Quantum Universe. Cambridge Univ. Press. Includes much about the technologies quantum theory has made possible. ISBN 978-0521564571. Vladimir G. Ivancevic, Tijana T. Ivancevic (2008). Quantum leap: from Dirac and Feynman, Across the universe, to human body and mind. World Scientific Publishing Company. Provides an intuitive introduction in non-mathematical terms and an introduction in comparatively basic mathematical terms. ISBN 978-9812819277. J. P. McEvoy and Oscar Zarate (2004). Introducing Quantum Theory. Totem Books. ISBN 1840465778' N. David Mermin (1990). \"Spooky actions at a distance: mysteries of the QT\" in his Boojums all the way through. Cambridge Univ. Press: 110–76. The author is a rare physicist who tries to communicate to philosophers and humanists. ISBN 978-0521388801. Roland Omnès (1999). Understanding Quantum Mechanics. Princeton Univ. Press. ISBN 978-0691004358. Victor Stenger (2000). Timeless Reality: Symmetry, Simplicity, and Multiple Universes. Buffalo NY: Prometheus Books. Chpts. 5–8. ISBN 978-1573928595. Martinus Veltman (2003). Facts and Mysteries in Elementary Particle Physics. World Scientific Publishing Company. ISBN 978-9812381491. External links \"Microscopic World – Introduction to Quantum Mechanics\". by Takada, Kenjiro, emeritus professor at Kyushu University The Quantum Exchange (tutorials and open-source learning software). Atoms and the Periodic Table Single and double slit interference Time-Evolution of a Wavepacket in a Square Well An animated demonstration of a wave packet dispersion over time. Carroll, Sean M. \"Quantum Mechanics (an embarrassment)\". Sixty Symbols. Brady Haran for the University of Nottingham.",
    "label": 0
  },
  {
    "text": "The mathematical formulations of quantum mechanics are those mathematical formalisms that permit a rigorous description of quantum mechanics. This mathematical formalism uses mainly a part of functional analysis, especially Hilbert spaces, which are a kind of linear space. Such are distinguished from mathematical formalisms for physics theories developed prior to the early 1900s by the use of abstract mathematical structures, such as infinite-dimensional Hilbert spaces (L2 space mainly), and operators on these spaces. In brief, values of physical observables such as energy and momentum were no longer considered as values of functions on phase space, but as eigenvalues; more precisely as spectral values of linear operators in Hilbert space. These formulations of quantum mechanics continue to be used today. At the heart of the description are ideas of quantum state and quantum observables, which are radically different from those used in previous models of physical reality. While the mathematics permits calculation of many quantities that can be measured experimentally, there is a definite theoretical limit to values that can be simultaneously measured. This limitation was first elucidated by Heisenberg through a thought experiment, and is represented mathematically in the new formalism by the non-commutativity of operators representing quantum observables. Prior to the development of quantum mechanics as a separate theory, the mathematics used in physics consisted mainly of formal mathematical analysis, beginning with calculus, and increasing in complexity up to differential geometry and partial differential equations. Probability theory was used in statistical mechanics. Geometric intuition played a strong role in the first two and, accordingly, theories of relativity were formulated entirely in terms of differential geometric concepts. The phenomenology of quantum physics arose roughly between 1895 and 1915, and for the 10 to 15 years before the development of quantum mechanics (around 1925) physicists continued to think of quantum theory",
    "label": 0
  },
  {
    "text": "entirely in terms of differential geometric concepts. The phenomenology of quantum physics arose roughly between 1895 and 1915, and for the 10 to 15 years before the development of quantum mechanics (around 1925) physicists continued to think of quantum theory within the confines of what is now called classical physics, and in particular within the same mathematical structures. The most sophisticated example of this is the Sommerfeld–Wilson–Ishiwara quantization rule, which was formulated entirely on the classical phase space. History of the formalism The \"old quantum theory\" and the need for new mathematics In the 1890s, Planck was able to derive the blackbody spectrum, which was later used to avoid the classical ultraviolet catastrophe by making the unorthodox assumption that, in the interaction of electromagnetic radiation with matter, energy could only be exchanged in discrete units which he called quanta. Planck postulated a direct proportionality between the frequency of radiation and the quantum of energy at that frequency. The proportionality constant, h, is now called the Planck constant in his honor. In 1905, Einstein explained certain features of the photoelectric effect by assuming that Planck's energy quanta were actual particles, which were later dubbed photons. All of these developments were phenomenological and challenged the theoretical physics of the time. Bohr and Sommerfeld went on to modify classical mechanics in an attempt to deduce the Bohr model from first principles. They proposed that, of all closed classical orbits traced by a mechanical system in its phase space, only the ones that enclosed an area which was a multiple of the Planck constant were actually allowed. The most sophisticated version of this formalism was the so-called Sommerfeld–Wilson–Ishiwara quantization. Although the Bohr model of the hydrogen atom could be explained in this way, the spectrum of the helium atom (classically an unsolvable 3-body problem)",
    "label": 0
  },
  {
    "text": "were actually allowed. The most sophisticated version of this formalism was the so-called Sommerfeld–Wilson–Ishiwara quantization. Although the Bohr model of the hydrogen atom could be explained in this way, the spectrum of the helium atom (classically an unsolvable 3-body problem) could not be predicted. The mathematical status of quantum theory remained uncertain for some time. In 1923, de Broglie proposed that wave–particle duality applied not only to photons but to electrons and every other physical system. The situation changed rapidly in the years 1925–1930, when working mathematical foundations were found through the groundbreaking work of Erwin Schrödinger, Werner Heisenberg, Max Born, Pascual Jordan, and the foundational work of John von Neumann, Hermann Weyl and Paul Dirac, and it became possible to unify several different approaches in terms of a fresh set of ideas. The physical interpretation of the theory was also clarified in these years after Werner Heisenberg discovered the uncertainty relations and Niels Bohr introduced the idea of complementarity. The \"new quantum theory\" Werner Heisenberg's matrix mechanics was the first successful attempt at replicating the observed quantization of atomic spectra. Later in the same year, Schrödinger created his wave mechanics. Schrödinger's formalism was considered easier to understand, visualize and calculate as it led to differential equations, which physicists were already familiar with solving. Within a year, it was shown that the two theories were equivalent. Schrödinger himself initially did not understand the fundamental probabilistic nature of quantum mechanics, as he thought that the absolute square of the wave function of an electron should be interpreted as the charge density of an object smeared out over an extended, possibly infinite, volume of space. It was Max Born who introduced the interpretation of the absolute square of the wave function as the probability distribution of the position of a pointlike object.",
    "label": 0
  },
  {
    "text": "of an object smeared out over an extended, possibly infinite, volume of space. It was Max Born who introduced the interpretation of the absolute square of the wave function as the probability distribution of the position of a pointlike object. Born's idea was soon taken over by Niels Bohr in Copenhagen who then became the \"father\" of the Copenhagen interpretation of quantum mechanics. Schrödinger's wave function can be seen to be closely related to the classical Hamilton–Jacobi equation. The correspondence to classical mechanics was even more explicit, although somewhat more formal, in Heisenberg's matrix mechanics. In his PhD thesis project, Paul Dirac discovered that the equation for the operators in the Heisenberg representation, as it is now called, closely translates to classical equations for the dynamics of certain quantities in the Hamiltonian formalism of classical mechanics, when one expresses them through Poisson brackets, a procedure now known as canonical quantization. Already before Schrödinger, the young postdoctoral fellow Werner Heisenberg invented his matrix mechanics, which was the first correct quantum mechanics – the essential breakthrough. Heisenberg's matrix mechanics formulation was based on algebras of infinite matrices, a very radical formulation in light of the mathematics of classical physics, although he started from the index-terminology of the experimentalists of that time, not even aware that his \"index-schemes\" were matrices, as Born soon pointed out to him. In fact, in these early years, linear algebra was not generally popular with physicists in its present form. Although Schrödinger himself after a year proved the equivalence of his wave-mechanics and Heisenberg's matrix mechanics, the reconciliation of the two approaches and their modern abstraction as motions in Hilbert space is generally attributed to Paul Dirac, who wrote a lucid account in his 1930 classic The Principles of Quantum Mechanics. He is the third, and possibly most",
    "label": 0
  },
  {
    "text": "of the two approaches and their modern abstraction as motions in Hilbert space is generally attributed to Paul Dirac, who wrote a lucid account in his 1930 classic The Principles of Quantum Mechanics. He is the third, and possibly most important, pillar of that field (he soon was the only one to have discovered a relativistic generalization of the theory). In his above-mentioned account, he introduced the bra–ket notation, together with an abstract formulation in terms of the Hilbert space used in functional analysis; he showed that Schrödinger's and Heisenberg's approaches were two different representations of the same theory, and found a third, most general one, which represented the dynamics of the system. His work was particularly fruitful in many types of generalizations of the field. The first complete mathematical formulation of this approach, known as the Dirac–von Neumann axioms, is generally credited to John von Neumann's 1932 book Mathematical Foundations of Quantum Mechanics, although Hermann Weyl had already referred to Hilbert spaces (which he called unitary spaces) in his 1927 classic paper and 1928 book. It was developed in parallel with a new approach to the mathematical spectral theory based on linear operators rather than the quadratic forms that were David Hilbert's approach a generation earlier. Though theories of quantum mechanics continue to evolve to this day, there is a basic framework for the mathematical formulation of quantum mechanics which underlies most approaches and can be traced back to the mathematical work of John von Neumann. In other words, discussions about interpretation of the theory, and extensions to it, are now mostly conducted on the basis of shared assumptions about the mathematical foundations. Later developments The application of the new quantum theory to electromagnetism resulted in quantum field theory, which was developed starting around 1930. Quantum field theory has",
    "label": 0
  },
  {
    "text": "are now mostly conducted on the basis of shared assumptions about the mathematical foundations. Later developments The application of the new quantum theory to electromagnetism resulted in quantum field theory, which was developed starting around 1930. Quantum field theory has driven the development of more sophisticated formulations of quantum mechanics, of which the ones presented here are simple special cases. Path integral formulation Phase-space formulation of quantum mechanics & geometric quantization quantum field theory in curved spacetime axiomatic, algebraic and constructive quantum field theory C*-algebra formalism Generalized statistical model of quantum mechanics A related topic is the relationship to classical mechanics. Any new physical theory is supposed to reduce to successful old theories in some approximation. For quantum mechanics, this translates into the need to study the so-called classical limit of quantum mechanics. Also, as Bohr emphasized, human cognitive abilities and language are inextricably linked to the classical realm, and so classical descriptions are intuitively more accessible than quantum ones. In particular, quantization, namely the construction of a quantum theory whose classical limit is a given and known classical theory, becomes an important area of quantum physics in itself. Finally, some of the originators of quantum theory (notably Einstein and Schrödinger) were unhappy with what they thought were the philosophical implications of quantum mechanics. In particular, Einstein took the position that quantum mechanics must be incomplete, which motivated research into so-called hidden-variable theories. The issue of hidden variables has become in part an experimental issue with the help of quantum optics. Postulates of quantum mechanics A physical system is generally described by three basic ingredients: states; observables; and dynamics (or law of time evolution) or, more generally, a group of physical symmetries. A classical description can be given in a fairly direct way by a phase space model of mechanics:",
    "label": 0
  },
  {
    "text": "described by three basic ingredients: states; observables; and dynamics (or law of time evolution) or, more generally, a group of physical symmetries. A classical description can be given in a fairly direct way by a phase space model of mechanics: states are points in a phase space formulated by symplectic manifold, observables are real-valued functions on it, time evolution is given by a one-parameter group of symplectic transformations of the phase space, and physical symmetries are realized by symplectic transformations. A quantum description normally consists of a Hilbert space of states, observables are self-adjoint operators on the space of states, time evolution is given by a one-parameter group of unitary transformations on the Hilbert space of states, and physical symmetries are realized by unitary transformations. (It is possible, to map this Hilbert-space picture to a phase space formulation, invertibly. See below.) The following summary of the mathematical framework of quantum mechanics can be partly traced back to the Dirac–von Neumann axioms. Description of the state of a system Each isolated physical system is associated with a (topologically) separable complex Hilbert space H with inner product ⟨φ|ψ⟩. Separability is a mathematically convenient hypothesis, with the physical interpretation that the state is uniquely determined by countably many observations. Quantum states can be identified with equivalence classes in H, where two vectors (of length 1) represent the same state if they differ only by a phase factor: | ψ k ⟩ ∼ | ψ l ⟩ ⇔ | ψ k ⟩ = e i α | ψ l ⟩ , α ∈ R . {\\displaystyle |\\psi _{k}\\rangle \\sim |\\psi _{l}\\rangle \\;\\;\\Leftrightarrow \\;\\;|\\psi _{k}\\rangle =e^{i\\alpha }|\\psi _{l}\\rangle ,\\quad \\ \\alpha \\in \\mathbb {R} .} As such, a quantum state is an element of a projective Hilbert space, conventionally termed a \"ray\". Accompanying Postulate I",
    "label": 0
  },
  {
    "text": ". {\\displaystyle |\\psi _{k}\\rangle \\sim |\\psi _{l}\\rangle \\;\\;\\Leftrightarrow \\;\\;|\\psi _{k}\\rangle =e^{i\\alpha }|\\psi _{l}\\rangle ,\\quad \\ \\alpha \\in \\mathbb {R} .} As such, a quantum state is an element of a projective Hilbert space, conventionally termed a \"ray\". Accompanying Postulate I is the composite system postulate: In the presence of quantum entanglement, the quantum state of the composite system cannot be factored as a tensor product of states of its local constituents; Instead, it is expressed as a sum, or superposition, of tensor products of states of component subsystems. A subsystem in an entangled composite system generally cannot be described by a state vector (or a ray), but instead is described by a density operator; Such quantum state is known as a mixed state. The density operator of a mixed state is a trace class, nonnegative (positive semi-definite) self-adjoint operator ρ normalized to be of trace 1. In turn, any density operator of a mixed state can be represented as a subsystem of a larger composite system in a pure state (see purification theorem). In the absence of quantum entanglement, the quantum state of the composite system is called a separable state. The density matrix of a bipartite system in a separable state can be expressed as ρ = ∑ k p k ρ 1 k ⊗ ρ 2 k {\\displaystyle \\rho =\\sum _{k}p_{k}\\rho _{1}^{k}\\otimes \\rho _{2}^{k}} , where ∑ k p k = 1 {\\displaystyle \\;\\sum _{k}p_{k}=1} . If there is only a single non-zero p k {\\displaystyle p_{k}} , then the state can be expressed just as ρ = ρ 1 ⊗ ρ 2 , {\\textstyle \\rho =\\rho _{1}\\otimes \\rho _{2},} and is called simply separable or product state. Measurement on a system Description of physical quantities Physical observables are represented by Hermitian matrices on H. Since these operators",
    "label": 0
  },
  {
    "text": "ρ 1 ⊗ ρ 2 , {\\textstyle \\rho =\\rho _{1}\\otimes \\rho _{2},} and is called simply separable or product state. Measurement on a system Description of physical quantities Physical observables are represented by Hermitian matrices on H. Since these operators are Hermitian, their eigenvalues are always real, and represent the possible outcomes/results from measuring the corresponding observable. If the spectrum of the observable is discrete, then the possible results are quantized. Results of measurement By spectral theory, we can associate a probability measure to the values of A in any state ψ. We can also show that the possible values of the observable A in any state must belong to the spectrum of A. The expectation value (in the sense of probability theory) of the observable A for the system in state represented by the unit vector ψ ∈ H is ⟨ ψ | A | ψ ⟩ {\\displaystyle \\langle \\psi |A|\\psi \\rangle } . If we represent the state ψ in the basis formed by the eigenvectors of A, then the square of the modulus of the component attached to a given eigenvector is the probability of observing its corresponding eigenvalue. For a mixed state ρ, the expected value of A in the state ρ is tr ⁡ ( A ρ ) {\\displaystyle \\operatorname {tr} (A\\rho )} , and the probability of obtaining an eigenvalue a n {\\displaystyle a_{n}} in a discrete, nondegenerate spectrum of the corresponding observable A {\\displaystyle A} is given by P ( a n ) = tr ⁡ ( | a n ⟩ ⟨ a n | ρ ) = ⟨ a n | ρ | a n ⟩ {\\displaystyle \\mathbb {P} (a_{n})=\\operatorname {tr} (|a_{n}\\rangle \\langle a_{n}|\\rho )=\\langle a_{n}|\\rho |a_{n}\\rangle } . If the eigenvalue a n {\\displaystyle a_{n}} has degenerate, orthonormal eigenvectors { |",
    "label": 0
  },
  {
    "text": "n | ρ ) = ⟨ a n | ρ | a n ⟩ {\\displaystyle \\mathbb {P} (a_{n})=\\operatorname {tr} (|a_{n}\\rangle \\langle a_{n}|\\rho )=\\langle a_{n}|\\rho |a_{n}\\rangle } . If the eigenvalue a n {\\displaystyle a_{n}} has degenerate, orthonormal eigenvectors { | a n 1 ⟩ , | a n 2 ⟩ , … , | a n m ⟩ } {\\displaystyle \\{|a_{n1}\\rangle ,|a_{n2}\\rangle ,\\dots ,|a_{nm}\\rangle \\}} , then the projection operator onto the eigensubspace can be defined as the identity operator in the eigensubspace: P n = | a n 1 ⟩ ⟨ a n 1 | + | a n 2 ⟩ ⟨ a n 2 | + ⋯ + | a n m ⟩ ⟨ a n m | , {\\displaystyle P_{n}=|a_{n1}\\rangle \\langle a_{n1}|+|a_{n2}\\rangle \\langle a_{n2}|+\\dots +|a_{nm}\\rangle \\langle a_{nm}|,} and then P ( a n ) = tr ⁡ ( P n ρ ) {\\displaystyle \\mathbb {P} (a_{n})=\\operatorname {tr} (P_{n}\\rho )} . Postulates II.a and II.b are collectively known as the Born rule of quantum mechanics. Effect of measurement on the state When a measurement is performed, only one result is obtained (according to some interpretations of quantum mechanics). This is modeled mathematically as the processing of additional information from the measurement, confining the probabilities of an immediate second measurement of the same observable. In the case of a discrete, non-degenerate spectrum, two sequential measurements of the same observable will always give the same value assuming the second immediately follows the first. Therefore, the state vector must change as a result of measurement, and collapse onto the eigensubspace associated with the eigenvalue measured. For a mixed state ρ, after obtaining an eigenvalue a n {\\displaystyle a_{n}} in a discrete, nondegenerate spectrum of the corresponding observable A {\\displaystyle A} , the updated state is given by ρ ′ = P",
    "label": 0
  },
  {
    "text": "with the eigenvalue measured. For a mixed state ρ, after obtaining an eigenvalue a n {\\displaystyle a_{n}} in a discrete, nondegenerate spectrum of the corresponding observable A {\\displaystyle A} , the updated state is given by ρ ′ = P n ρ P n † tr ⁡ ( P n ρ P n † ) {\\textstyle \\rho '={\\frac {P_{n}\\rho P_{n}^{\\dagger }}{\\operatorname {tr} (P_{n}\\rho P_{n}^{\\dagger })}}} . If the eigenvalue a n {\\displaystyle a_{n}} has degenerate, orthonormal eigenvectors { | a n 1 ⟩ , | a n 2 ⟩ , … , | a n m ⟩ } {\\displaystyle \\{|a_{n1}\\rangle ,|a_{n2}\\rangle ,\\dots ,|a_{nm}\\rangle \\}} , then the projection operator onto the eigensubspace is P n = | a n 1 ⟩ ⟨ a n 1 | + | a n 2 ⟩ ⟨ a n 2 | + ⋯ + | a n m ⟩ ⟨ a n m | {\\displaystyle P_{n}=|a_{n1}\\rangle \\langle a_{n1}|+|a_{n2}\\rangle \\langle a_{n2}|+\\dots +|a_{nm}\\rangle \\langle a_{nm}|} . Postulates II.c is sometimes called the \"state update rule\" or \"collapse rule\"; Together with the Born rule (Postulates II.a and II.b), they form a complete representation of measurements, and are sometimes collectively called the measurement postulate(s). Note that the projection-valued measures (PVM) described in the measurement postulate(s) can be generalized to positive operator-valued measures (POVM), which is the most general kind of measurement in quantum mechanics. A POVM can be understood as the effect on a component subsystem when a PVM is performed on a larger, composite system (see Naimark's dilation theorem). Time evolution of a system The Schrödinger equation describes how a state vector evolves in time. Depending on the text, it may be derived from some other assumptions, motivated on heuristic grounds, or asserted as a postulate. Derivations include using the de Broglie relation between wavelength and momentum",
    "label": 0
  },
  {
    "text": "describes how a state vector evolves in time. Depending on the text, it may be derived from some other assumptions, motivated on heuristic grounds, or asserted as a postulate. Derivations include using the de Broglie relation between wavelength and momentum or path integrals. Equivalently, the time evolution postulate can be stated as: For a closed system in a mixed state ρ, the time evolution is ρ ( t ) = U ( t ; t 0 ) ρ ( t 0 ) U † ( t ; t 0 ) {\\displaystyle \\rho (t)=U(t;t_{0})\\rho (t_{0})U^{\\dagger }(t;t_{0})} . The evolution of an open quantum system can be described by quantum operations (in an operator sum formalism) and quantum instruments, and generally does not have to be unitary. Other implications of the postulates Physical symmetries act on the Hilbert space of quantum states unitarily or antiunitarily due to Wigner's theorem (supersymmetry is another matter entirely). Density operators are those that are in the closure of the convex hull of the one-dimensional orthogonal projectors. Conversely, one-dimensional orthogonal projectors are extreme points of the set of density operators. Physicists also call one-dimensional orthogonal projectors pure states and other density operators mixed states. One can in this formalism state Heisenberg's uncertainty principle and prove it as a theorem, although the exact historical sequence of events, concerning who derived what and under which framework, is the subject of historical investigations outside the scope of this article. Furthermore, to the postulates of quantum mechanics one should also add basic statements on the properties of spin and Pauli's exclusion principle, see below. Spin In addition to their other properties, all particles possess a quantity called spin, an intrinsic angular momentum. Despite the name, particles do not literally spin around an axis, and quantum mechanical spin has no correspondence in",
    "label": 0
  },
  {
    "text": "principle, see below. Spin In addition to their other properties, all particles possess a quantity called spin, an intrinsic angular momentum. Despite the name, particles do not literally spin around an axis, and quantum mechanical spin has no correspondence in classical physics. In the position representation, a spinless wavefunction has position r and time t as continuous variables, ψ = ψ(r, t). For spin wavefunctions the spin is an additional discrete variable: ψ = ψ(r, t, σ), where σ takes the values; σ = − S ℏ , − ( S − 1 ) ℏ , … , 0 , … , + ( S − 1 ) ℏ , + S ℏ . {\\displaystyle \\sigma =-S\\hbar ,-(S-1)\\hbar ,\\dots ,0,\\dots ,+(S-1)\\hbar ,+S\\hbar \\,.} That is, the state of a single particle with spin S is represented by a (2S + 1)-component spinor of complex-valued wave functions. Two classes of particles with very different behaviour are bosons which have integer spin (S = 0, 1, 2, ...), and fermions possessing half-integer spin (S = 1⁄2, 3⁄2, 5⁄2, ...). Symmetrization postulate In quantum mechanics, two particles can be distinguished from one another using two methods. By performing a measurement of intrinsic properties of each particle, particles of different types can be distinguished. Otherwise, if the particles are identical, their trajectories can be tracked which distinguishes the particles based on the locality of each particle. While the second method is permitted in classical mechanics, (i.e. all classical particles are treated with distinguishability), the same cannot be said for quantum mechanical particles since the process is infeasible due to the fundamental uncertainty principles that govern small scales. Hence the requirement of indistinguishability of quantum particles is presented by the symmetrization postulate. The postulate is applicable to a system of bosons or fermions, for example,",
    "label": 0
  },
  {
    "text": "process is infeasible due to the fundamental uncertainty principles that govern small scales. Hence the requirement of indistinguishability of quantum particles is presented by the symmetrization postulate. The postulate is applicable to a system of bosons or fermions, for example, in predicting the spectra of helium atom. The postulate, explained in the following sections, can be stated as follows: Exceptions can occur when the particles are constrained to two spatial dimensions where existence of particles known as anyons are possible which are said to have a continuum of statistical properties spanning the range between fermions and bosons. The connection between behaviour of identical particles and their spin is given by spin statistics theorem. It can be shown that two particles localized in different regions of space can still be represented using a symmetrized/antisymmetrized wavefunction and that independent treatment of these wavefunctions gives the same result. Hence the symmetrization postulate is applicable in the general case of a system of identical particles. Exchange Degeneracy In a system of identical particles, let P be known as exchange operator that acts on the wavefunction as: P ( ⋯ | ψ ⟩ | ϕ ⟩ ⋯ ) ≡ ⋯ | ϕ ⟩ | ψ ⟩ ⋯ {\\displaystyle P{\\bigg (}\\cdots |\\psi \\rangle |\\phi \\rangle \\cdots {\\bigg )}\\equiv \\cdots |\\phi \\rangle |\\psi \\rangle \\cdots } If a physical system of identical particles is given, wavefunction of all particles can be well known from observation but these cannot be labelled to each particle. Thus, the above exchanged wavefunction represents the same physical state as the original state which implies that the wavefunction is not unique. This is known as exchange degeneracy. More generally, consider a linear combination of such states, | Ψ ⟩ {\\displaystyle |\\Psi \\rangle } . For the best representation of the physical system, we",
    "label": 0
  },
  {
    "text": "which implies that the wavefunction is not unique. This is known as exchange degeneracy. More generally, consider a linear combination of such states, | Ψ ⟩ {\\displaystyle |\\Psi \\rangle } . For the best representation of the physical system, we expect this to be an eigenvector of P since exchange operator is not excepted to give completely different vectors in projective Hilbert space. Since P 2 = 1 {\\displaystyle P^{2}=1} , the possible eigenvalues of P are +1 and −1. The | Ψ ⟩ {\\displaystyle |\\Psi \\rangle } states for identical particle system are represented as symmetric for +1 eigenvalue or antisymmetric for -1 eigenvalue as follows: P | ⋯ n i , n j ⋯ ; S ⟩ = + | ⋯ n i , n j ⋯ ; S ⟩ {\\displaystyle P|\\cdots n_{i},n_{j}\\cdots ;S\\rangle =+|\\cdots n_{i},n_{j}\\cdots ;S\\rangle } P | ⋯ n i , n j ⋯ ; A ⟩ = − | ⋯ n i , n j ⋯ ; A ⟩ {\\displaystyle P|\\cdots n_{i},n_{j}\\cdots ;A\\rangle =-|\\cdots n_{i},n_{j}\\cdots ;A\\rangle } The explicit symmetric/antisymmetric form of | Ψ ⟩ {\\displaystyle |\\Psi \\rangle } is constructed using a symmetrizer or antisymmetrizer operator. Particles that form symmetric states are called bosons and those that form antisymmetric states are called as fermions. The relation of spin with this classification is given from spin statistics theorem which shows that integer spin particles are bosons and half integer spin particles are fermions. Pauli exclusion principle The property of spin relates to another basic property concerning systems of N identical particles: the Pauli exclusion principle, which is a consequence of the following permutation behaviour of an N-particle wave function; again in the position representation one must postulate that for the transposition of any two of the N particles one always should have i.e., on",
    "label": 0
  },
  {
    "text": "principle, which is a consequence of the following permutation behaviour of an N-particle wave function; again in the position representation one must postulate that for the transposition of any two of the N particles one always should have i.e., on transposition of the arguments of any two particles the wavefunction should reproduce, apart from a prefactor (−1)2S which is +1 for bosons, but (−1) for fermions. Electrons are fermions with S = 1/2; quanta of light are bosons with S = 1. Due to the form of anti-symmetrized wavefunction: Ψ n 1 ⋯ n N ( A ) ( x 1 , … , x N ) = 1 N ! | ψ n 1 ( x 1 ) ψ n 1 ( x 2 ) ⋯ ψ n 1 ( x N ) ψ n 2 ( x 1 ) ψ n 2 ( x 2 ) ⋯ ψ n 2 ( x N ) ⋮ ⋮ ⋱ ⋮ ψ n N ( x 1 ) ψ n N ( x 2 ) ⋯ ψ n N ( x N ) | {\\displaystyle \\Psi _{n_{1}\\cdots n_{N}}^{(A)}(x_{1},\\ldots ,x_{N})={\\frac {1}{\\sqrt {N!}}}\\left|{\\begin{matrix}\\psi _{n_{1}}(x_{1})&\\psi _{n_{1}}(x_{2})&\\cdots &\\psi _{n_{1}}(x_{N})\\\\\\psi _{n_{2}}(x_{1})&\\psi _{n_{2}}(x_{2})&\\cdots &\\psi _{n_{2}}(x_{N})\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\\\psi _{n_{N}}(x_{1})&\\psi _{n_{N}}(x_{2})&\\cdots &\\psi _{n_{N}}(x_{N})\\\\\\end{matrix}}\\right|} if the wavefunction of each particle is completely determined by a set of quantum number, then two fermions cannot share the same set of quantum numbers since the resulting function cannot be anti-symmetrized (i.e. above formula gives zero). The same cannot be said of Bosons since their wavefunction is: | x 1 x 2 ⋯ x N ; S ⟩ = ∏ j n j ! N ! ∑ p | x p ( 1 ) ⟩ | x p ( 2 ) ⟩ ⋯ | x p ( N ) ⟩ {\\displaystyle |x_{1}x_{2}\\cdots",
    "label": 0
  },
  {
    "text": "⋯ x N ; S ⟩ = ∏ j n j ! N ! ∑ p | x p ( 1 ) ⟩ | x p ( 2 ) ⟩ ⋯ | x p ( N ) ⟩ {\\displaystyle |x_{1}x_{2}\\cdots x_{N};S\\rangle ={\\frac {\\prod _{j}n_{j}!}{N!}}\\sum _{p}\\left|x_{p(1)}\\right\\rangle \\left|x_{p(2)}\\right\\rangle \\cdots \\left|x_{p(N)}\\right\\rangle } where n j {\\displaystyle n_{j}} is the number of particles with same wavefunction. Exceptions for symmetrization postulate In nonrelativistic quantum mechanics all particles are either bosons or fermions; in relativistic quantum theories also \"supersymmetric\" theories exist, where a particle is a linear combination of a bosonic and a fermionic part. Only in dimension d = 2 can one construct entities where (−1)2S is replaced by an arbitrary complex number with magnitude 1, called anyons. In relativistic quantum mechanics, spin statistic theorem can prove that under certain set of assumptions that the integer spins particles are classified as bosons and half spin particles are classified as fermions. Anyons which form neither symmetric nor antisymmetric states are said to have fractional spin. Although spin and the Pauli principle can only be derived from relativistic generalizations of quantum mechanics, the properties mentioned in the last two paragraphs belong to the basic postulates already in the non-relativistic limit. Especially, many important properties in natural science, e.g. the periodic system of chemistry, are consequences of the two properties. Mathematical structure of quantum mechanics Pictures of dynamics Summary: Representations The original form of the Schrödinger equation depends on choosing a particular representation of Heisenberg's canonical commutation relations. The Stone–von Neumann theorem dictates that all irreducible representations of the finite-dimensional Heisenberg commutation relations are unitarily equivalent. A systematic understanding of its consequences has led to the phase space formulation of quantum mechanics, which works in full phase space instead of Hilbert space, so then with a more intuitive",
    "label": 0
  },
  {
    "text": "the finite-dimensional Heisenberg commutation relations are unitarily equivalent. A systematic understanding of its consequences has led to the phase space formulation of quantum mechanics, which works in full phase space instead of Hilbert space, so then with a more intuitive link to the classical limit thereof. This picture also simplifies considerations of quantization, the deformation extension from classical to quantum mechanics. The quantum harmonic oscillator is an exactly solvable system where the different representations are easily compared. There, apart from the Heisenberg, or Schrödinger (position or momentum), or phase-space representations, one also encounters the Fock (number) representation and the Segal–Bargmann (Fock-space or coherent state) representation (named after Irving Segal and Valentine Bargmann). All four are unitarily equivalent. Time as an operator The framework presented so far singles out time as the parameter that everything depends on. It is possible to formulate mechanics in such a way that time becomes itself an observable associated with a self-adjoint operator. At the classical level, it is possible to arbitrarily parameterize the trajectories of particles in terms of an unphysical parameter s, and in that case the time t becomes an additional generalized coordinate of the physical system. At the quantum level, translations in s would be generated by a \"Hamiltonian\" H − E, where E is the energy operator and H is the \"ordinary\" Hamiltonian. However, since s is an unphysical parameter, physical states must be left invariant by \"s-evolution\", and so the physical state space is the kernel of H − E (this requires the use of a rigged Hilbert space and a renormalization of the norm). This is related to the quantization of constrained systems and quantization of gauge theories. It is also possible to formulate a quantum theory of \"events\" where time becomes an observable. Problem of measurement The picture",
    "label": 0
  },
  {
    "text": "a renormalization of the norm). This is related to the quantization of constrained systems and quantization of gauge theories. It is also possible to formulate a quantum theory of \"events\" where time becomes an observable. Problem of measurement The picture given in the preceding paragraphs is sufficient for description of a completely isolated system. However, it fails to account for one of the main differences between quantum mechanics and classical mechanics, that is, the effects of measurement. The von Neumann description of quantum measurement of an observable A, when the system is prepared in a pure state ψ is the following (note, however, that von Neumann's description dates back to the 1930s and is based on experiments as performed during that time – more specifically the Compton–Simon experiment; it is not applicable to most present-day measurements within the quantum domain): Let A have spectral resolution A = ∫ λ d E A ⁡ ( λ ) , {\\displaystyle A=\\int \\lambda \\,d\\operatorname {E} _{A}(\\lambda ),} where EA is the resolution of the identity (also called projection-valued measure) associated with A. Then the probability of the measurement outcome lying in an interval B of R is |EA(B) ψ|2. In other words, the probability is obtained by integrating the characteristic function of B against the countably additive measure ⟨ ψ ∣ E A ⁡ ψ ⟩ . {\\displaystyle \\langle \\psi \\mid \\operatorname {E} _{A}\\psi \\rangle .} If the measured value is contained in B, then immediately after the measurement, the system will be in the (generally non-normalized) state EA(B)ψ. If the measured value does not lie in B, replace B by its complement for the above state. For example, suppose the state space is the n-dimensional complex Hilbert space Cn and A is a Hermitian matrix with eigenvalues λi, with corresponding eigenvectors ψi.",
    "label": 0
  },
  {
    "text": "does not lie in B, replace B by its complement for the above state. For example, suppose the state space is the n-dimensional complex Hilbert space Cn and A is a Hermitian matrix with eigenvalues λi, with corresponding eigenvectors ψi. The projection-valued measure associated with A, EA, is then E A ⁡ ( B ) = | ψ i ⟩ ⟨ ψ i | , {\\displaystyle \\operatorname {E} _{A}(B)=|\\psi _{i}\\rangle \\langle \\psi _{i}|,} where B is a Borel set containing only the single eigenvalue λi. If the system is prepared in state | ψ ⟩ {\\displaystyle |\\psi \\rangle } Then the probability of a measurement returning the value λi can be calculated by integrating the spectral measure ⟨ ψ ∣ E A ⁡ ψ ⟩ {\\displaystyle \\langle \\psi \\mid \\operatorname {E} _{A}\\psi \\rangle } over Bi. This gives trivially ⟨ ψ | ψ i ⟩ ⟨ ψ i ∣ ψ ⟩ = | ⟨ ψ ∣ ψ i ⟩ | 2 . {\\displaystyle \\langle \\psi |\\psi _{i}\\rangle \\langle \\psi _{i}\\mid \\psi \\rangle =|\\langle \\psi \\mid \\psi _{i}\\rangle |^{2}.} The characteristic property of the von Neumann measurement scheme is that repeating the same measurement will give the same results. This is also called the projection postulate. A more general formulation replaces the projection-valued measure with a positive-operator valued measure (POVM). To illustrate, take again the finite-dimensional case. Here we would replace the rank-1 projections | ψ i ⟩ ⟨ ψ i | {\\displaystyle |\\psi _{i}\\rangle \\langle \\psi _{i}|} by a finite set of positive operators F i F i ∗ {\\displaystyle F_{i}F_{i}^{*}} whose sum is still the identity operator as before (the resolution of identity). Just as a set of possible outcomes {λ1 ... λn} is associated to a projection-valued measure, the same can be said for a POVM. Suppose the",
    "label": 0
  },
  {
    "text": "F_{i}F_{i}^{*}} whose sum is still the identity operator as before (the resolution of identity). Just as a set of possible outcomes {λ1 ... λn} is associated to a projection-valued measure, the same can be said for a POVM. Suppose the measurement outcome is λi. Instead of collapsing to the (unnormalized) state | ψ i ⟩ ⟨ ψ i | ψ ⟩ {\\displaystyle |\\psi _{i}\\rangle \\langle \\psi _{i}|\\psi \\rangle } after the measurement, the system now will be in the state F i | ψ ⟩ . {\\displaystyle F_{i}|\\psi \\rangle .} Since the Fi Fi* operators need not be mutually orthogonal projections, the projection postulate of von Neumann no longer holds. The same formulation applies to general mixed states. In von Neumann's approach, the state transformation due to measurement is distinct from that due to time evolution in several ways. For example, time evolution is deterministic and unitary whereas measurement is non-deterministic and non-unitary. However, since both types of state transformation take one quantum state to another, this difference was viewed by many as unsatisfactory. The POVM formalism views measurement as one among many other quantum operations, which are described by completely positive maps which do not increase the trace. List of mathematical tools Part of the folklore of the subject concerns the mathematical physics textbook Methods of Mathematical Physics put together by Richard Courant from David Hilbert's Göttingen University courses. The story is told (by mathematicians) that physicists had dismissed the material as not interesting in the current research areas, until the advent of Schrödinger's equation. At that point it was realised that the mathematics of the new quantum mechanics was already laid out in it. It is also said that Heisenberg had consulted Hilbert about his matrix mechanics, and Hilbert observed that his own experience with infinite-dimensional matrices had",
    "label": 0
  },
  {
    "text": "was realised that the mathematics of the new quantum mechanics was already laid out in it. It is also said that Heisenberg had consulted Hilbert about his matrix mechanics, and Hilbert observed that his own experience with infinite-dimensional matrices had derived from differential equations, advice which Heisenberg ignored, missing the opportunity to unify the theory as Weyl and Dirac did a few years later. Whatever the basis of the anecdotes, the mathematics of the theory was conventional at the time, whereas the physics was radically new. The main tools include: linear algebra: complex numbers, eigenvectors, eigenvalues functional analysis: Hilbert spaces, linear operators, spectral theory differential equations: partial differential equations, separation of variables, ordinary differential equations, Sturm–Liouville theory, eigenfunctions harmonic analysis: Fourier transforms See also List of mathematical topics in quantum theory Quantum foundations Symmetry in quantum mechanics Notes References Bäuerle, Gerard G. A.; de Kerf, Eddy A. (1990). Lie Algebras, Part 1: Finite and Infinite Dimensional Lie Algebras and Applications in Physics. Studies in Mathematical Physics. Amsterdam: North Holland. ISBN 0-444-88776-8. Byron, Frederick W.; Fuller, Robert W. (1992). Mathematics of Classical and Quantum Physics. New York: Courier Corporation. ISBN 978-0-486-67164-2. Cohen-Tannoudji, Claude; Diu, Bernard; Laloë, Franck (2020). Quantum mechanics. Volume 2: Angular momentum, spin, and approximation methods. Weinheim: Wiley-VCH Verlag GmbH & Co. KGaA. ISBN 978-3-527-82272-0. Dirac, P. A. M. (1925). \"The Fundamental Equations of Quantum Mechanics\". Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 109 (752): 642–653. Bibcode:1925RSPSA.109..642D. doi:10.1098/rspa.1925.0150. Edwards, David A. (1979). \"The mathematical foundations of quantum mechanics\". Synthese. 42 (1). Springer Science and Business Media LLC: 1–70. doi:10.1007/bf00413704. ISSN 0039-7857. S2CID 46969028. Greenstein, George; Zajonc, Arthur (2006). The Quantum Challenge. Sudbury, Mass.: Jones & Bartlett Learning. ISBN 978-0-7637-2470-2. Jauch, J. M.; Wigner, E. P.; Yanase, M. M. (1997). \"Some Comments Concerning Measurements in Quantum",
    "label": 0
  },
  {
    "text": "Media LLC: 1–70. doi:10.1007/bf00413704. ISSN 0039-7857. S2CID 46969028. Greenstein, George; Zajonc, Arthur (2006). The Quantum Challenge. Sudbury, Mass.: Jones & Bartlett Learning. ISBN 978-0-7637-2470-2. Jauch, J. M.; Wigner, E. P.; Yanase, M. M. (1997). \"Some Comments Concerning Measurements in Quantum Mechanics\". Part I: Particles and Fields. Part II: Foundations of Quantum Mechanics. Berlin, Heidelberg: Springer Berlin Heidelberg. pp. 475–482. doi:10.1007/978-3-662-09203-3_52. ISBN 978-3-642-08179-8. Solem, J. C.; Biedenharn, L. C. (1993). \"Understanding geometrical phases in quantum mechanics: An elementary example\". Foundations of Physics. 23 (2): 185–195. Bibcode:1993FoPh...23..185S. doi:10.1007/BF01883623. S2CID 121930907. Streater, Raymond Frederick; Wightman, Arthur Strong (2000). PCT, Spin and Statistics, and All that. Princeton, NJ: Princeton University Press. ISBN 978-0-691-07062-9. Sakurai, Jun John; Napolitano, Jim (2021). Modern quantum mechanics (3rd ed.). Cambridge: Cambridge University Press. ISBN 978-1-108-47322-4. Weyl, Hermann (1950) . The Theory of Groups and Quantum Mechanics. Translated by Robertson, H. P. Dover. Further reading Auyang, Sunny Y. (1995). How is Quantum Field Theory Possible?. New York, NY: Oxford University Press on Demand. ISBN 978-0-19-509344-5. Emch, Gérard G. (1972). Algebraic Methods in Statistical Mechanics and Quantum Field Theory. New York: John Wiley & Sons. ISBN 0-471-23900-3. Giachetta, Giovanni; Mangiarotti, Luigi; Sardanashvily, Gennadi (2005). Geometric and Algebraic Topological Methods in Quantum Mechanics. WORLD SCIENTIFIC. arXiv:math-ph/0410040. doi:10.1142/5731. ISBN 978-981-256-129-9. Gleason, Andrew M. (1957). \"Measures on the Closed Subspaces of a Hilbert Space\". Journal of Mathematics and Mechanics. 6 (6). Indiana University Mathematics Department: 885–893. JSTOR 24900629. Hall, Brian C. (2013). Quantum Theory for Mathematicians. Graduate Texts in Mathematics. Vol. 267. New York, NY: Springer New York. Bibcode:2013qtm..book.....H. doi:10.1007/978-1-4614-7116-5. ISBN 978-1-4614-7115-8. ISSN 0072-5285. S2CID 117837329. Jauch, Josef Maria (1968). Foundations of Quantum Mechanics. Reading, Mass.: Addison-Wesley. ISBN 0-201-03298-8. Jost, R. (1965). The General Theory of Quantized Fields. Lectures in applied mathematics. American Mathematical Society. Kuhn, Thomas S. (1987). Black-Body Theory and the Quantum",
    "label": 0
  },
  {
    "text": "S2CID 117837329. Jauch, Josef Maria (1968). Foundations of Quantum Mechanics. Reading, Mass.: Addison-Wesley. ISBN 0-201-03298-8. Jost, R. (1965). The General Theory of Quantized Fields. Lectures in applied mathematics. American Mathematical Society. Kuhn, Thomas S. (1987). Black-Body Theory and the Quantum Discontinuity, 1894-1912. Chicago: University of Chicago Press. ISBN 978-0-226-45800-7. Landsman, Klaas (2017). Foundations of Quantum Theory. Fundamental Theories of Physics. Vol. 188. Cham: Springer International Publishing. doi:10.1007/978-3-319-51777-3. ISBN 978-3-319-51776-6. ISSN 0168-1222. Mackey, George W. (2004). Mathematical Foundations of Quantum Mechanics. Mineola, N.Y: Courier Corporation. ISBN 978-0-486-43517-6. McMahon, David (2013). Quantum Mechanics Demystified, 2nd Edition (PDF). New York, NY: McGraw-Hill Prof Med/Tech. ISBN 978-0-07-176563-3. Moretti, Valter (2017). Spectral Theory and Quantum Mechanics. Unitext. Vol. 110. Cham: Springer International Publishing. doi:10.1007/978-3-319-70706-8. ISBN 978-3-319-70705-1. ISSN 2038-5714. S2CID 125121522. Moretti, Valter (2019). Fundamental Mathematical Structures of Quantum Theory. Cham: Springer International Publishing. doi:10.1007/978-3-030-18346-2. ISBN 978-3-030-18345-5. S2CID 197485828. Prugovecki, Eduard (2006). Quantum Mechanics in Hilbert Space. Mineola, NY: Courier Dover Publications. ISBN 978-0-486-45327-9. Reed, Michael; Simon, Barry (1972). Methods of Modern Mathematical Physics. New York: Academic Press. ISBN 978-0-12-585001-8. Shankar, R. (2013). Principles of Quantum Mechanics (PDF). Springer. ISBN 978-1-4615-7675-4. Teschl, Gerald (2009). Mathematical Methods in Quantum Mechanics (PDF). Providence, R.I: American Mathematical Soc. ISBN 978-0-8218-4660-5. von Neumann, John (2018). Mathematical Foundations of Quantum Mechanics. Princeton Oxford: Princeton University Press. ISBN 978-0-691-17856-1. Weaver, Nik (2001). Mathematical Quantization. Chapman and Hall/CRC. doi:10.1201/9781420036237. ISBN 978-0-429-07514-8.",
    "label": 0
  },
  {
    "text": "Quantum calculus, sometimes called calculus without limits, is equivalent to traditional infinitesimal calculus without the notion of limits. The two types of calculus in quantum calculus are q-calculus and h-calculus. The goal of both types is to find \"analogs\" of mathematical objects, where, after taking a certain limit, the original object is returned. In q-calculus, the limit as q tends to 1 is taken of the q-analog. Likewise, in h-calculus, the limit as h tends to 0 is taken of the h-analog. The parameters q {\\displaystyle q} and h {\\displaystyle h} can be related by the formula q = e h {\\displaystyle q=e^{h}} . Differentiation The q-differential and h-differential are defined as: d q ( f ( x ) ) = f ( q x ) − f ( x ) {\\displaystyle d_{q}(f(x))=f(qx)-f(x)} and d h ( f ( x ) ) = f ( x + h ) − f ( x ) {\\displaystyle d_{h}(f(x))=f(x+h)-f(x)} , respectively. The q-derivative and h-derivative are then defined as D q ( f ( x ) ) = d q ( f ( x ) ) d q ( x ) = f ( q x ) − f ( x ) q x − x {\\displaystyle D_{q}(f(x))={\\frac {d_{q}(f(x))}{d_{q}(x)}}={\\frac {f(qx)-f(x)}{qx-x}}} and D h ( f ( x ) ) = d h ( f ( x ) ) d h ( x ) = f ( x + h ) − f ( x ) h {\\displaystyle D_{h}(f(x))={\\frac {d_{h}(f(x))}{d_{h}(x)}}={\\frac {f(x+h)-f(x)}{h}}} respectively. By taking the limit as q → 1 {\\displaystyle q\\rightarrow 1} of the q-derivative or as h → 0 {\\displaystyle h\\rightarrow 0} of the h-derivative, one can obtain the derivative: lim q → 1 D q f ( x ) = lim h → 0 D h f ( x ) =",
    "label": 0
  },
  {
    "text": "the q-derivative or as h → 0 {\\displaystyle h\\rightarrow 0} of the h-derivative, one can obtain the derivative: lim q → 1 D q f ( x ) = lim h → 0 D h f ( x ) = d d x ( f ( x ) ) {\\displaystyle \\lim _{q\\rightarrow 1}D_{q}f(x)=\\lim _{h\\rightarrow 0}D_{h}f(x)={\\frac {d}{dx}}{\\Bigl (}f(x){\\Bigr )}} Integration q-integral A function F(x) is a q-antiderivative of f(x) if DqF(x) = f(x). The q-antiderivative (or q-integral) is denoted by ∫ f ( x ) d q x {\\textstyle \\int f(x)\\,d_{q}x} and an expression for F(x) can be found from: ∫ f ( x ) d q x = ( 1 − q ) ∑ j = 0 ∞ x q j f ( x q j ) {\\textstyle \\int f(x)\\,d_{q}x=(1-q)\\sum _{j=0}^{\\infty }xq^{j}f(xq^{j})} , which is called the Jackson integral of f(x). For 0 < q < 1, the series converges to a function F(x) on an interval (0,A] if |f(x)xα| is bounded on the interval (0, A] for some 0 ≤ α < 1. The q-integral is a Riemann–Stieltjes integral with respect to a step function having infinitely many points of increase at the points qj..The jump at the point qj is qj. Calling this step function gq(t) gives dgq(t) = dqt. h-integral A function F(x) is an h-antiderivative of f(x) if DhF(x) = f(x). The h-integral is denoted by ∫ f ( x ) d h x {\\textstyle \\int f(x)\\,d_{h}x} . If a and b differ by an integer multiple of h then the definite integral ∫ a b f ( x ) d h x {\\textstyle \\int _{a}^{b}f(x)\\,d_{h}x} is given by a Riemann sum of f(x) on the interval [a, b], partitioned into sub-intervals of equal width h. The motivation of h-integral comes from the Riemann sum of",
    "label": 0
  },
  {
    "text": "f ( x ) d h x {\\textstyle \\int _{a}^{b}f(x)\\,d_{h}x} is given by a Riemann sum of f(x) on the interval [a, b], partitioned into sub-intervals of equal width h. The motivation of h-integral comes from the Riemann sum of f(x). Following the idea of the motivation of classical integrals, some of the properties of classical integrals hold in h-integral. This notion has broad applications in numerical analysis, and especially finite difference calculus. Example In infinitesimal calculus, the derivative of the function x n {\\displaystyle x^{n}} is n x n − 1 {\\displaystyle nx^{n-1}} (for some positive integer n {\\displaystyle n} ). The corresponding expressions in q-calculus and h-calculus are: D q ( x n ) = 1 − q n 1 − q x n − 1 = [ n ] q x n − 1 {\\displaystyle D_{q}(x^{n})={\\frac {1-q^{n}}{1-q}}x^{n-1}=[n]_{q}\\ x^{n-1}} where [ n ] q {\\displaystyle [n]_{q}} is the q-bracket [ n ] q = 1 − q n 1 − q {\\displaystyle [n]_{q}={\\frac {1-q^{n}}{1-q}}} and D h ( x n ) = ( x + h ) n − x n h = 1 h ( ∑ k = 0 n ( n k ) x n − k h k − x n ) = 1 h ∑ k = 1 n ( n k ) x n − k h k = ∑ k = 1 n ( n k ) x n − k h k − 1 = n x n − 1 + n ( n − 1 ) 2 h x n − 2 + ⋯ + n h n − 2 x + h n − 1 , {\\displaystyle {\\begin{aligned}D_{h}(x^{n})&={\\frac {(x+h)^{n}-x^{n}}{h}}\\\\&={\\frac {1}{h}}\\left(\\sum _{k=0}^{n}{{\\binom {n}{k}}x^{n-k}h^{k}-x^{n}}\\right)\\\\&={\\frac {1}{h}}\\sum _{k=1}^{n}{{\\binom {n}{k}}x^{n-k}h^{k}}\\\\&=\\sum _{k=1}^{n}{{\\binom {n}{k}}x^{n-k}h^{k-1}}\\\\&=nx^{n-1}+{\\frac {n(n-1)}{2}}hx^{n-2}+\\cdots +nh^{n-2}x+h^{n-1},\\end{aligned}}} respectively. The expression [ n ] q x n −",
    "label": 0
  },
  {
    "text": "− 2 + ⋯ + n h n − 2 x + h n − 1 , {\\displaystyle {\\begin{aligned}D_{h}(x^{n})&={\\frac {(x+h)^{n}-x^{n}}{h}}\\\\&={\\frac {1}{h}}\\left(\\sum _{k=0}^{n}{{\\binom {n}{k}}x^{n-k}h^{k}-x^{n}}\\right)\\\\&={\\frac {1}{h}}\\sum _{k=1}^{n}{{\\binom {n}{k}}x^{n-k}h^{k}}\\\\&=\\sum _{k=1}^{n}{{\\binom {n}{k}}x^{n-k}h^{k-1}}\\\\&=nx^{n-1}+{\\frac {n(n-1)}{2}}hx^{n-2}+\\cdots +nh^{n-2}x+h^{n-1},\\end{aligned}}} respectively. The expression [ n ] q x n − 1 {\\displaystyle [n]_{q}x^{n-1}} is then the q-analog and ∑ k = 1 n ( n k ) x n − k h k − 1 {\\textstyle \\sum _{k=1}^{n}{{\\binom {n}{k}}x^{n-k}h^{k-1}}} is the h-analog of the power rule for positive integral powers. The q-Taylor expansion allows for the definition of q-analogs of all of the usual functions, such as the sine function, whose q-derivative is the q-analog of cosine. History The h-calculus is the calculus of finite differences, which was studied by George Boole and others, and has proven useful in combinatorics and fluid mechanics. In a sense, q-calculus dates back to Leonhard Euler and Carl Gustav Jacobi, but has only recently begun to find usefulness in quantum mechanics, given its intimate connection with commutativity relations and Lie algebras, specifically quantum groups. See also Noncommutative geometry Quantum differential calculus Time scale calculus q-analog Basic hypergeometric series Quantum dilogarithm References Further reading George Gasper, Mizan Rahman, Basic Hypergeometric Series, 2nd ed, Cambridge University Press (2004), ISBN 978-0-511-52625-1, doi:10.1017/CBO9780511526251 Jackson, F. H. (1908). \"On q-functions and a certain difference operator\". Transactions of the Royal Society of Edinburgh. 46 (2): 253–281. doi:10.1017/S0080456800002751. S2CID 123927312. Exton, H. (1983). q-Hypergeometric Functions and Applications. New York: Halstead Press. ISBN 0-85312-491-4. Kac, Victor; Cheung, Pokman (2002). Quantum calculus. Universitext. Springer-Verlag. ISBN 0-387-95341-8.",
    "label": 0
  },
  {
    "text": "Quantum mechanics is the fundamental physical theory that describes the behavior of matter and of light; its unusual characteristics typically occur at and below the scale of atoms. It is the foundation of all quantum physics, which includes quantum chemistry, quantum biology, quantum field theory, quantum technology, and quantum information science. Quantum mechanics can describe many systems that classical physics cannot. Classical physics can describe many aspects of nature at an ordinary (macroscopic and (optical) microscopic) scale, but is not sufficient for describing them at very small submicroscopic (atomic and subatomic) scales. Classical mechanics can be derived from quantum mechanics as an approximation that is valid at ordinary scales. Quantum systems have bound states that are quantized to discrete values of energy, momentum, angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of both particles and waves (wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle). Quantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the \"old quantum theory\", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements",
    "label": 0
  },
  {
    "text": "Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield. Overview and fundamental concepts Quantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and subatomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms, but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative. Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known as quantum electrodynamics (QED), has been shown to agree with experiment to within 1 part in 1012 when predicting the magnetic properties of an electron. A fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only gives probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be",
    "label": 0
  }
]
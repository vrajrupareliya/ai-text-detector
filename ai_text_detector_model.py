# -*- coding: utf-8 -*-
"""ai-text-detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lBpmcLyiwopgbyn_nTtpB-LTObr5Oj7A
"""

import numpy as np
import torch
from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from google.colab import files
files.upload()

DATA_FILES="/content/human_ai_data.json"
MODEL_NAME="microsoft/deberta-v3-base"
OUTPUT_DIR = "./ai_detector_model"
BATCH_SIZE = 8
EPOCHS = 3
LEARNING_RATE = 2e-5

from datasets import DatasetDict

print(f"Loading data from {DATA_FILES}...")
# Load JSON directly into a Hugging Face Dataset
dataset = load_dataset("json", data_files=DATA_FILES, split="train")

print("Total samples:", len(dataset))

# 2. Convert to Pandas to easily drop duplicates
df = dataset.to_pandas()
initial_count = len(df)

# Drop duplicates based on the 'text' column
df = df.drop_duplicates(subset=["text"])
final_count = len(df)

print(f"Removed {initial_count - final_count} duplicate rows.")

# 3. Convert back to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

"""Train Test split"""

train_testvalid = dataset.train_test_split(test_size=0.2, seed=42)

test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)

final_datasets = DatasetDict({
    'train': train_testvalid['train'],
    'validation': test_valid['train'],
    'test': test_valid['test']
})

print(f"Data Split Complete:")
print(f"Train size: {len(final_datasets['train'])} (80%)")
print(f"Valid size: {len(final_datasets['validation'])} (10%)")
print(f"Test size:  {len(final_datasets['test'])} (10%)")

train_texts = set(final_datasets['train']["text"])
val_texts   = set(final_datasets['validation']["text"])

print(len(train_texts & val_texts))

from collections import Counter
Counter(final_datasets['validation']["label"])

"""Tokenization"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512,
)

print("Tokenizing data...")
tokenized_datasets = final_datasets.map(preprocess_function, batched=True)

"""Model"""

print("Initializing model...")
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2
)


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average="binary"
      )
    acc = accuracy_score(labels, predictions)

    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall,
    }

"""Training Arguments"""

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    gradient_accumulation_steps=2,
    warmup_ratio=0.1,
    logging_steps=50,
    fp16=True,
    report_to="none",
)



"""Model Training"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics
)

trainer.train()



"""Testing Model"""

import random
import json
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# 1. Load your trained model (Epoch 2 checkpoint)
checkpoint_path = "/content/ai_detector_model/checkpoint-364"  # Update if needed
model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)
tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

# 2. Load one random AI sample from YOUR generated file
with open("human_ai_data.json", "r") as f:
    data = json.load(f)
    # If data is a dict {'data': [...]}, adjust accordingly. Assuming list based on previous steps.
    if isinstance(data, dict): data = data.get('data', [])

    # Filter for AI only (Label 1)
    ai_samples = [d['text'] for d in data if d.get('label') == 1]

# Pick a random one
test_text = random.choice(ai_samples)

print(f"--- TESTING ON TRAINED DISTRIBUTION (GPT-Neo) ---")
print(f"Text Snippet: {test_text[:100]}...")
result = classifier(test_text)
print(f"Result: {result}")



from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

best_checkpoint = "/content/ai_detector_model/checkpoint-364"

print(f"Loading best model from: {best_checkpoint}")
model = AutoModelForSequenceClassification.from_pretrained(best_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(best_checkpoint)
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

tests = [
    # 1. Easy AI (ChatGPT style)
    "In conclusion, machine learning is a pivotal technology that enables computers to learn from data.",

    # 2. Easy Human (Casual/Messy)
    "i honestly dont know why this is broken, maybe i missed a semicolon somewhere lol.",

    # 3. Tricky Human (Formal/Academic - The true test)
    # If this is flagged as AI, your model is biased against smart humans.
    "The industrial revolution marked a major turning point in history; almost every aspect of daily life was influenced in some way.",

    # 4. Tricky AI (Creative/Story)
    # AI trying to sound human.
    "The old man sat on the porch, watching the sun dip below the horizon. 'It's been a long time,' he whispered."
]

print("\n--- FINAL EVALUATION ---")
for i, text in enumerate(tests):
    result = classifier(text)
    label = result[0]['label']
    score = result[0]['score']
    # Map LABEL_1 to AI, LABEL_0 to Human
    verdict = "AI   " if label == "LABEL_1" else "Human"
    print(f"Test {i+1}: {verdict} (Confidence: {score:.4f})")



"""Uploading Model to Huggingface"""

!huggingface-cli login

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# 1. SETUP PATHS
# Use the exact folder name of your best model (e.g., checkpoint-364)
local_model_path = "/content/ai_detector_model/checkpoint-364"

# Replace 'your-username' with your actual Hugging Face username
# Replace 'project-name' with what you want to call it
repo_name = "vraj33/ai-text-detector-deberta"

print(f"Loading model from {local_model_path}...")

# 2. LOAD
# We load it first to ensure we aren't uploading broken files
model = AutoModelForSequenceClassification.from_pretrained(local_model_path)
tokenizer = AutoTokenizer.from_pretrained(local_model_path)

# 3. PUSH TO HUB
print(f"Uploading to Hugging Face: {repo_name}...")
# This creates the repo if it doesn't exist
model.push_to_hub(repo_name)
tokenizer.push_to_hub(repo_name)

print(f"âœ… Success! Your model is live at: https://huggingface.co/{repo_name}")